{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Cloud Pipeline Introduction Cloud Pipeline Introduction Why Cloud Pipeline Components Why Cloud Pipeline Cloud Pipeline solution from EPAM provides an easy and scalable approach to perform a wide range of analysis tasks in the cloud environment. This solution takes the best of two approaches: classic HPC solutions (based on GridEngine schedulers family) and SaaS cloud solutions. Feature Classic \"HPC\" \"Cloud Pipeline\" SaaS Pipelines customization High . Direct scripting - any level of customization High . Direct scripting - any level of customization Low . Provide specific pipeline definition languages or even only a graphical editor Backward compatibility N/A High . \"Classic\" HPC scripts and NGS tools can be run without any changes Low . Scripts have to be rewritten according to the supported languages and storage structures User Interface Command Line Graphical Interface for User interaction and a Command Line Interface for automation scripts Graphical Interface Calculation power scalability Low . New nodes shall be deployed and supported on-premises. Idle nodes are still consuming resources High . New nodes are started according to the job request and terminated as soon as they are not needed anymore. Each job can precisely define required CPU/RAM/Disk resources or even select optimal node up to speed up execution (e.g. memory optimized nodes for cellranger pipelines) High . Scalable as \"Cloud Pipeline\" but sometimes limits user to predefined nodes setup Deployment and vendor-lock Deployed on-premises and introduces no vendor-lock Can be deployed in AWS/GCP/Azure or on-premises, thus introduces no vendor-lock Consumed as an Internet service (no on-premises deployment available), all processes are tied to this specific vendor Security High . All data and analysis processes are located in a controlled network. High . All data and analysis processes are located in a controlled cloud VPC. All security configurations are performed by user's security officers Low . No direct control over security configuration. SaaS vendor has full access to the data storages. Components The main components of the Cloud Pipeline are shown below:","title":"Introduction"},{"location":"#cloud-pipeline-introduction","text":"Cloud Pipeline Introduction Why Cloud Pipeline Components","title":"Cloud Pipeline Introduction"},{"location":"#why-cloud-pipeline","text":"Cloud Pipeline solution from EPAM provides an easy and scalable approach to perform a wide range of analysis tasks in the cloud environment. This solution takes the best of two approaches: classic HPC solutions (based on GridEngine schedulers family) and SaaS cloud solutions. Feature Classic \"HPC\" \"Cloud Pipeline\" SaaS Pipelines customization High . Direct scripting - any level of customization High . Direct scripting - any level of customization Low . Provide specific pipeline definition languages or even only a graphical editor Backward compatibility N/A High . \"Classic\" HPC scripts and NGS tools can be run without any changes Low . Scripts have to be rewritten according to the supported languages and storage structures User Interface Command Line Graphical Interface for User interaction and a Command Line Interface for automation scripts Graphical Interface Calculation power scalability Low . New nodes shall be deployed and supported on-premises. Idle nodes are still consuming resources High . New nodes are started according to the job request and terminated as soon as they are not needed anymore. Each job can precisely define required CPU/RAM/Disk resources or even select optimal node up to speed up execution (e.g. memory optimized nodes for cellranger pipelines) High . Scalable as \"Cloud Pipeline\" but sometimes limits user to predefined nodes setup Deployment and vendor-lock Deployed on-premises and introduces no vendor-lock Can be deployed in AWS/GCP/Azure or on-premises, thus introduces no vendor-lock Consumed as an Internet service (no on-premises deployment available), all processes are tied to this specific vendor Security High . All data and analysis processes are located in a controlled network. High . All data and analysis processes are located in a controlled cloud VPC. All security configurations are performed by user's security officers Low . No direct control over security configuration. SaaS vendor has full access to the data storages.","title":"Why Cloud Pipeline"},{"location":"#components","text":"The main components of the Cloud Pipeline are shown below:","title":"Components"},{"location":"api/API_tutorials/API_tutorials/","text":"API tutorials - Usage scenario This sections provides a number of implementations of the data transfer/processing automation via the Cloud Pipeline API capabilities. We'll use a quite common usage scenario to implement the automation via different approaches: process a local 10xGenomics dataset (e.g. produced by the on-prem machinery) in the Cloud Pipeline compute environment. The scenario for the automation consists of the following steps: Upload a dataset (directory with the FASTQ files to the S3 bucket) Run the dataset processing using the cellranger count command Download the data processing results back to a local filesystem The subsequent sections provide implementation examples in a number of languages: pipe CLI implementation Direct HTTP calls via curl JavaScript implementation","title":"API tutorials"},{"location":"api/API_tutorials/API_tutorials/#api-tutorials-usage-scenario","text":"This sections provides a number of implementations of the data transfer/processing automation via the Cloud Pipeline API capabilities. We'll use a quite common usage scenario to implement the automation via different approaches: process a local 10xGenomics dataset (e.g. produced by the on-prem machinery) in the Cloud Pipeline compute environment. The scenario for the automation consists of the following steps: Upload a dataset (directory with the FASTQ files to the S3 bucket) Run the dataset processing using the cellranger count command Download the data processing results back to a local filesystem The subsequent sections provide implementation examples in a number of languages: pipe CLI implementation Direct HTTP calls via curl JavaScript implementation","title":"API tutorials - Usage scenario"},{"location":"api/API_tutorials/Automation_via_CLI/","text":"\"pipe\" CLI implementation This approach does not use the HTTP/REST API directly. Instead, a command-line wrapper is used. pipe CLI is a command line utility, distributed together with the Cloud Pipeline. It offers a variety of the commands for an easy automation of the common tasks. More details on the pipe command line interface are available in the corresponding docs section This utility can be used for any backend Cloud Provider, which is enabled for the CLoud Pipeline deployment (e.g. the command are all the same if using AWS/GCP/Azure). The listed below cellranger_pipe.sh script, implements the \"Usage scenario\" using the only \"pipe\" command to communicate with the cloud platform. #!/bin/bash ############################################################# # Example dataset: # https://cloud-pipeline-oss-builds.s3.amazonaws.com/tools/cellranger/data/tiny-fastq/tiny-fastq.tgz # Example run command: # cellranger.sh --fastqs ~/tiny-fastq \\ # --transcriptome tiny \\ # --workdir s3://my_bucket/tiny-example \\ # --copy-back ############################################################# ############################################################# # Parse options ############################################################# POSITIONAL=() while [[ $# -gt 0 ]]; do key=\"$1\" case $key in -f|--fastqs) FASTQS=\"$2\" shift shift ;; -t|--transcriptome) TRANSCRIPTOME=\"$2\" shift shift ;; -w|--workdir) WORKDIR=\"$2\" shift shift ;; -i|--instance) INSTANCE_TYPE=\"$2\" shift shift ;; -d|--disk) INSTANCE_DISK=\"$2\" shift shift ;; -c|--copy-back) COPY_BACK=1 shift esac done ############################################################# # Check prerequisites ############################################################# if ! command -v pipe > /dev/null 2>&1; then cat << EOF [ERROR] `pipe` Command Line Interface is not available. Please follow the installation and configuration instructions, available in the Cloud Pipeline GUI: * Login to the GUI * Open \"Settings\" (from the left panel) * Click \"Get access key\" * Follow the installation instructions EOF exit 1 fi ############################################################# # Validate options ############################################################# if [ -z \"$FASTQS\" ] || [ ! -d \"$FASTQS\" ]; then echo \"[ERROR] Path to the fastq files is not set or is not a directory\" exit 1 fi if [ \"$TRANSCRIPTOME\" ]; then case $TRANSCRIPTOME in human) TRANSCRIPTOME_S3=\"s3://genome-bucket/human/transcriptome\" ;; mouse) TRANSCRIPTOME_S3=\"s3://genome-bucket/mouse/transcriptome\" ;; human-mouse) TRANSCRIPTOME_S3=\"s3://genome-bucket/human-mouse/transcriptome\" ;; tiny) TRANSCRIPTOME_S3=\"s3://genome-bucket/tiny/transcriptome\" ;; *) echo \"[ERROR] Transcriptome name does not match the supported types: human, mouse, human-mouse, tiny\" exit 1 ;; esac else echo \"[ERROR] Transcriptome name is not set\" exit 1 fi if [ -z \"$WORKDIR\" ] || [[ \"$WORKDIR\" != \"s3://\"* ]]; then echo \"[ERROR] S3 working directory is not set or uses an unexpected schema (s3:// shall be used)\" exit 1 else WORKDIR_EXISTS=$(pipe storage ls $WORKDIR) if [ \"$WORKDIR_EXISTS\" ]; then echo \"[ERROR] S3 working directory ($WORKDIR) already exists, please specify a new location\" exit 1 fi fi EXTRA_OPTIONS=() if [ \"$INSTANCE_TYPE\" ]; then EXTRA_OPTIONS+=(\"--instance-type $INSTANCE_TYPE\") fi if [ \"$INSTANCE_TYPE\" ]; then EXTRA_OPTIONS+=(\"--instance-disk $INSTANCE_DISK\") fi ############################################################# # Transfer the local fastq files to the S3 working directory ############################################################# FASTQS_S3=\"$WORKDIR/fastq/$(basename $FASTQS)\" echo \"Transferring fastqs to the S3 working directory: $FASTQS -> $FASTQS_S3/\" pipe storage cp \"$FASTQS\" \"$FASTQS_S3/\" --recursive if [ $? -ne 0 ]; then echo \"[ERROR] Cannot upload $FASTQS to $WORKDIR\" exit 1 fi ############################################################# # Setup the paths and run options ############################################################# RESULTS_S3=\"$WORKDIR/results\" DOCKER_IMAGE=\"ngs/cellranger:latest\" ############################################################# # Launch data processing ############################################################# echo \"Launch job with parameters:\" echo \"fastqs: $FASTQS_S3\" echo \"transcriptome: $TRANSCRIPTOME_S3\" echo \"results: $RESULTS_S3\" pipe run --docker-image \"$DOCKER_IMAGE\" \\ --fastqs \"input?$FASTQS_S3\" \\ --transcriptome \"input?$TRANSCRIPTOME_S3\" \\ --results \"output?$RESULTS_S3\" \\ --cmd-template 'cellranger count --id cloud-cellranger --fastqs $fastqs --transcriptome $transcriptome' \\ --yes \\ --sync ${EXTRA_OPTIONS[@]} if [ $? -ne 0 ]; then echo \"[ERROR] Failed to process the dataset\" exit 1 fi echo \"[OK] Job has finished\" ############################################################# # Copy the results back, if requested ############################################################# if [ \"$COPY_BACK\" ]; then RESULTS_LOCAL=$(pwd)/$(basename $RESULTS_S3) echo \"Transferring results locally: $RESULTS_S3 -> $RESULTS_LOCAL\" pipe storage cp $RESULTS_S3 $RESULTS_LOCAL --recursive if [ $? -ne 0 ]; then echo \"[ERROR] Cannot download $RESULTS_S3 to $RESULTS_LOCAL\" fi echo \"[OK] Data processing results are downloaded to $RESULTS_LOCAL\" else echo \"[OK] Data processing results are available in the S3 working directory: $RESULTS_S3\" fi To launch the data processing, the script above can be launched using the following command: # Get the \"tiny\" dataset (or use your own data) cd ~ wget \"https://cloud-pipeline-oss-builds.s3.amazonaws.com/tools/cellranger/data/tiny-fastq/tiny-fastq.tgz\" tar -zxvf tiny-fastq.tgz # Run the data processing # It is assumed that cellranger.sh is stored in ~/cellranger.sh chmod +x ~/cellranger.sh ./cellranger.sh --fastqs ~/tiny-fastq \\ --transcriptome tiny \\ --workdir s3://my_bucket/tiny-example \\ --copy-back Transferring fastqs to the S3 working directory: ~/tiny-fastq -> s3://my_bucket/tiny-example/fastq/tiny-fastq/ Launch job with parameters: fastqs: s3://my_bucket/tiny-example/fastq/tiny-fastq transcriptome: s3://genome-bucket/tiny/transcriptome results: s3://my_bucket/tiny-example/results Pipeline run scheduled with RunId: 5865 Pipeline run 5865 completed with status SUCCESS Transferring results locally: s3://my_bucket/tiny-example/results -> ~/results [OK] Data processing results are downloaded to ~/results","title":"Automation via CLI"},{"location":"api/API_tutorials/Automation_via_CLI/#pipe-cli-implementation","text":"This approach does not use the HTTP/REST API directly. Instead, a command-line wrapper is used. pipe CLI is a command line utility, distributed together with the Cloud Pipeline. It offers a variety of the commands for an easy automation of the common tasks. More details on the pipe command line interface are available in the corresponding docs section This utility can be used for any backend Cloud Provider, which is enabled for the CLoud Pipeline deployment (e.g. the command are all the same if using AWS/GCP/Azure). The listed below cellranger_pipe.sh script, implements the \"Usage scenario\" using the only \"pipe\" command to communicate with the cloud platform. #!/bin/bash ############################################################# # Example dataset: # https://cloud-pipeline-oss-builds.s3.amazonaws.com/tools/cellranger/data/tiny-fastq/tiny-fastq.tgz # Example run command: # cellranger.sh --fastqs ~/tiny-fastq \\ # --transcriptome tiny \\ # --workdir s3://my_bucket/tiny-example \\ # --copy-back ############################################################# ############################################################# # Parse options ############################################################# POSITIONAL=() while [[ $# -gt 0 ]]; do key=\"$1\" case $key in -f|--fastqs) FASTQS=\"$2\" shift shift ;; -t|--transcriptome) TRANSCRIPTOME=\"$2\" shift shift ;; -w|--workdir) WORKDIR=\"$2\" shift shift ;; -i|--instance) INSTANCE_TYPE=\"$2\" shift shift ;; -d|--disk) INSTANCE_DISK=\"$2\" shift shift ;; -c|--copy-back) COPY_BACK=1 shift esac done ############################################################# # Check prerequisites ############################################################# if ! command -v pipe > /dev/null 2>&1; then cat << EOF [ERROR] `pipe` Command Line Interface is not available. Please follow the installation and configuration instructions, available in the Cloud Pipeline GUI: * Login to the GUI * Open \"Settings\" (from the left panel) * Click \"Get access key\" * Follow the installation instructions EOF exit 1 fi ############################################################# # Validate options ############################################################# if [ -z \"$FASTQS\" ] || [ ! -d \"$FASTQS\" ]; then echo \"[ERROR] Path to the fastq files is not set or is not a directory\" exit 1 fi if [ \"$TRANSCRIPTOME\" ]; then case $TRANSCRIPTOME in human) TRANSCRIPTOME_S3=\"s3://genome-bucket/human/transcriptome\" ;; mouse) TRANSCRIPTOME_S3=\"s3://genome-bucket/mouse/transcriptome\" ;; human-mouse) TRANSCRIPTOME_S3=\"s3://genome-bucket/human-mouse/transcriptome\" ;; tiny) TRANSCRIPTOME_S3=\"s3://genome-bucket/tiny/transcriptome\" ;; *) echo \"[ERROR] Transcriptome name does not match the supported types: human, mouse, human-mouse, tiny\" exit 1 ;; esac else echo \"[ERROR] Transcriptome name is not set\" exit 1 fi if [ -z \"$WORKDIR\" ] || [[ \"$WORKDIR\" != \"s3://\"* ]]; then echo \"[ERROR] S3 working directory is not set or uses an unexpected schema (s3:// shall be used)\" exit 1 else WORKDIR_EXISTS=$(pipe storage ls $WORKDIR) if [ \"$WORKDIR_EXISTS\" ]; then echo \"[ERROR] S3 working directory ($WORKDIR) already exists, please specify a new location\" exit 1 fi fi EXTRA_OPTIONS=() if [ \"$INSTANCE_TYPE\" ]; then EXTRA_OPTIONS+=(\"--instance-type $INSTANCE_TYPE\") fi if [ \"$INSTANCE_TYPE\" ]; then EXTRA_OPTIONS+=(\"--instance-disk $INSTANCE_DISK\") fi ############################################################# # Transfer the local fastq files to the S3 working directory ############################################################# FASTQS_S3=\"$WORKDIR/fastq/$(basename $FASTQS)\" echo \"Transferring fastqs to the S3 working directory: $FASTQS -> $FASTQS_S3/\" pipe storage cp \"$FASTQS\" \"$FASTQS_S3/\" --recursive if [ $? -ne 0 ]; then echo \"[ERROR] Cannot upload $FASTQS to $WORKDIR\" exit 1 fi ############################################################# # Setup the paths and run options ############################################################# RESULTS_S3=\"$WORKDIR/results\" DOCKER_IMAGE=\"ngs/cellranger:latest\" ############################################################# # Launch data processing ############################################################# echo \"Launch job with parameters:\" echo \"fastqs: $FASTQS_S3\" echo \"transcriptome: $TRANSCRIPTOME_S3\" echo \"results: $RESULTS_S3\" pipe run --docker-image \"$DOCKER_IMAGE\" \\ --fastqs \"input?$FASTQS_S3\" \\ --transcriptome \"input?$TRANSCRIPTOME_S3\" \\ --results \"output?$RESULTS_S3\" \\ --cmd-template 'cellranger count --id cloud-cellranger --fastqs $fastqs --transcriptome $transcriptome' \\ --yes \\ --sync ${EXTRA_OPTIONS[@]} if [ $? -ne 0 ]; then echo \"[ERROR] Failed to process the dataset\" exit 1 fi echo \"[OK] Job has finished\" ############################################################# # Copy the results back, if requested ############################################################# if [ \"$COPY_BACK\" ]; then RESULTS_LOCAL=$(pwd)/$(basename $RESULTS_S3) echo \"Transferring results locally: $RESULTS_S3 -> $RESULTS_LOCAL\" pipe storage cp $RESULTS_S3 $RESULTS_LOCAL --recursive if [ $? -ne 0 ]; then echo \"[ERROR] Cannot download $RESULTS_S3 to $RESULTS_LOCAL\" fi echo \"[OK] Data processing results are downloaded to $RESULTS_LOCAL\" else echo \"[OK] Data processing results are available in the S3 working directory: $RESULTS_S3\" fi To launch the data processing, the script above can be launched using the following command: # Get the \"tiny\" dataset (or use your own data) cd ~ wget \"https://cloud-pipeline-oss-builds.s3.amazonaws.com/tools/cellranger/data/tiny-fastq/tiny-fastq.tgz\" tar -zxvf tiny-fastq.tgz # Run the data processing # It is assumed that cellranger.sh is stored in ~/cellranger.sh chmod +x ~/cellranger.sh ./cellranger.sh --fastqs ~/tiny-fastq \\ --transcriptome tiny \\ --workdir s3://my_bucket/tiny-example \\ --copy-back Transferring fastqs to the S3 working directory: ~/tiny-fastq -> s3://my_bucket/tiny-example/fastq/tiny-fastq/ Launch job with parameters: fastqs: s3://my_bucket/tiny-example/fastq/tiny-fastq transcriptome: s3://genome-bucket/tiny/transcriptome results: s3://my_bucket/tiny-example/results Pipeline run scheduled with RunId: 5865 Pipeline run 5865 completed with status SUCCESS Transferring results locally: s3://my_bucket/tiny-example/results -> ~/results [OK] Data processing results are downloaded to ~/results","title":"\"pipe\" CLI implementation"},{"location":"api/API_tutorials/Direct_HTTP_API/","text":"Direct HTTP calls via curl Cloud Pipeline's HTTP/REST API can be also consumed directly. From any language/framework. Here we show to implement the Usage scenario using the curl command to query the API. All available HTTP/REST API methods are listed in the Cloud Pipeline Swagger UI (available as https://<host>/pipeline/restapi/swagger-ui.html ) In this particular example script cellranger_curl.sh , which is listed below, the following API calls are performed: GET /datastorage/find to find the ID of the \"WORKDIR\" S3 bucket POST /datastorage/tempCredentials/ to get the access token to the \"WORKDIR\" S3 bucket, which can passed to the AWS SDK for the data transfer POST /pipeline/run to submit a job to the Cloud Pipeline compute environment GET /pipeline/run to get the running job status and wait for the completion #!/bin/bash ############################################################# # Setup the parameters ############################################################# # Cloud Pipeline API entrypoint, e.g. https://<host>/pipeline API_URL=\"\" # Shall be generated in the Cloud Pipeline GUI -> Settings -> CLI -> Generate access key API_TOKEN=\"\" # Data location: # S3 bucket, that is going to be used as a \"working directory\" # Local fastq files and processing results will uploaded there # Example value: s3://my_bucket/workdir WORKDIR=\"\" # Path to a local directory, that holds the FASTQ files for processing # E.g. ~/tiny-fastq FASTQS=\"\" # Path to the cellranger transcriptome # It shall be located in the S3 bucket already # In this example we use the \"tiny\" reference, which is only 300Mb and allows to debug jobs quickly TRANSCRIPTOME_S3=\"s3://genome-bucket/tiny/transcriptome\" # Path to the S3 location, that will hold the data processing results RESULTS_S3=\"$WORKDIR/results\" # Job parameters: # The docker image, which holds a cellranger binary and environment DOCKER_IMAGE=\"single-cell/cellranger:latest\" # The size of the machine, that is going to process the data # If it's not set - the default hardware for the $DOCKER_IMAGE is going to used INSTANCE_TYPE=\"r5.xlarge\" # The size of disk volume in Gb, that will be attached to the machine defined by $INSTANCE_TYPE # If it's not set - the default hardware for the $DOCKER_IMAGE is going to used INSTANCE_DISK=\"100\" ############################################################# # Get the S3 working directory bucket ID ############################################################# BUCKET_NAME=$(cut -d/ -f 3 <<< $WORKDIR) RESPONSE=$(curl -ks -X GET -H \"Authorization: Bearer $API_TOKEN\" -H \"Accept: application/json\" \"$API_URL/restapi/datastorage/find?id=$BUCKET_NAME\") BUCKET_ID=$(jq -r \".payload.id\" <<< $RESPONSE) ############################################################# # Get the S3 working directory bucket access token by it's ID ############################################################# RESPONSE=$(curl -ks -X POST -H \"Authorization: Bearer $API_TOKEN\" -H \"Content-Type: application/json\" -H \"Accept: application/json\" -d \"[ { \\\"id\\\": $BUCKET_ID, \\\"read\\\": true, \\\"write\\\": true } ]\" \"$API_URL/restapi/datastorage/tempCredentials/\") # The resulting keys, shall be used to configure the AWS SDK for the data transfer # Here we use AWS CLI SDK, but this will work for any other (e.g. Java/JS/Go/Python/...) export AWS_ACCESS_KEY_ID=$(jq -r '.payload.keyID' <<< $RESPONSE) export AWS_SECRET_ACCESS_KEY=$(jq -r '.payload.accessKey' <<< $RESPONSE) export AWS_SESSION_TOKEN=$(jq -r '.payload.token' <<< $RESPONSE) ############################################################# # Transfer the local fastq files to the S3 working directory using the direct call to AWS SDK ############################################################# # Files will be uploaded to e.g. \"s3://my_bucket/workdir/fastq/tiny-fastq/\" FASTQS_S3=\"$WORKDIR/fastq/$(basename $FASTQS)\" aws s3 cp \"$FASTQS\" \"$FASTQS_S3/\" --recursive ############################################################# # Run processing ############################################################# RESPONSE=$(curl -ks -X POST -H \"Authorization: Bearer $API_TOKEN\" -H \"Content-Type: application/json\" -H \"Accept: application/json\" -d \"{ \\\"cmdTemplate\\\": \\\"cellranger count --id cloud-cellranger --fastqs \\$fastqs --transcriptome \\$transcriptome\\\", \\\"dockerImage\\\": \"single-cell/cellranger:latest\", \\\"hddSize\\\": $INSTANCE_DISK, \\\"instanceType\\\": \\\"$INSTANCE_TYPE\\\", \\\"params\\\": { \\\"fastqs\\\": { \\\"type\\\": \\\"input\\\", \\\"value\\\": \\\"$FASTQS_S3\\\" }, \\\"results\\\": { \\\"type\\\":\\\"output\\\", \\\"value\\\":\\\"$RESULTS_S3\\\" }, \\\"transcriptome\\\": { \\\"type\\\": \\\"input\\\", \\\"value\\\": \\\"$TRANSCRIPTOME_S3\\\" } } }\" \"$API_URL/restapi/run\") RUN_ID=$(jq -r \".payload.id\" <<< $RESPONSE) ############################################################# # Poll the run status each 30s, until it's finished ############################################################# RUN_STATUS=\"NA\" while [ \"$RUN_STATUS\" != \"SUCCESS\" ]; do sleep 30 RESPONSE=$(curl -ks -X GET -H \"Authorization: Bearer $API_TOKEN\" -H \"Accept: application/json\" \"$API_URL/restapi/run/$RUN_ID\") RUN_STATUS=$(jq -r \".payload.status\" <<< $RESPONSE) done","title":"Direct HTTP API implementation"},{"location":"api/API_tutorials/Direct_HTTP_API/#direct-http-calls-via-curl","text":"Cloud Pipeline's HTTP/REST API can be also consumed directly. From any language/framework. Here we show to implement the Usage scenario using the curl command to query the API. All available HTTP/REST API methods are listed in the Cloud Pipeline Swagger UI (available as https://<host>/pipeline/restapi/swagger-ui.html ) In this particular example script cellranger_curl.sh , which is listed below, the following API calls are performed: GET /datastorage/find to find the ID of the \"WORKDIR\" S3 bucket POST /datastorage/tempCredentials/ to get the access token to the \"WORKDIR\" S3 bucket, which can passed to the AWS SDK for the data transfer POST /pipeline/run to submit a job to the Cloud Pipeline compute environment GET /pipeline/run to get the running job status and wait for the completion #!/bin/bash ############################################################# # Setup the parameters ############################################################# # Cloud Pipeline API entrypoint, e.g. https://<host>/pipeline API_URL=\"\" # Shall be generated in the Cloud Pipeline GUI -> Settings -> CLI -> Generate access key API_TOKEN=\"\" # Data location: # S3 bucket, that is going to be used as a \"working directory\" # Local fastq files and processing results will uploaded there # Example value: s3://my_bucket/workdir WORKDIR=\"\" # Path to a local directory, that holds the FASTQ files for processing # E.g. ~/tiny-fastq FASTQS=\"\" # Path to the cellranger transcriptome # It shall be located in the S3 bucket already # In this example we use the \"tiny\" reference, which is only 300Mb and allows to debug jobs quickly TRANSCRIPTOME_S3=\"s3://genome-bucket/tiny/transcriptome\" # Path to the S3 location, that will hold the data processing results RESULTS_S3=\"$WORKDIR/results\" # Job parameters: # The docker image, which holds a cellranger binary and environment DOCKER_IMAGE=\"single-cell/cellranger:latest\" # The size of the machine, that is going to process the data # If it's not set - the default hardware for the $DOCKER_IMAGE is going to used INSTANCE_TYPE=\"r5.xlarge\" # The size of disk volume in Gb, that will be attached to the machine defined by $INSTANCE_TYPE # If it's not set - the default hardware for the $DOCKER_IMAGE is going to used INSTANCE_DISK=\"100\" ############################################################# # Get the S3 working directory bucket ID ############################################################# BUCKET_NAME=$(cut -d/ -f 3 <<< $WORKDIR) RESPONSE=$(curl -ks -X GET -H \"Authorization: Bearer $API_TOKEN\" -H \"Accept: application/json\" \"$API_URL/restapi/datastorage/find?id=$BUCKET_NAME\") BUCKET_ID=$(jq -r \".payload.id\" <<< $RESPONSE) ############################################################# # Get the S3 working directory bucket access token by it's ID ############################################################# RESPONSE=$(curl -ks -X POST -H \"Authorization: Bearer $API_TOKEN\" -H \"Content-Type: application/json\" -H \"Accept: application/json\" -d \"[ { \\\"id\\\": $BUCKET_ID, \\\"read\\\": true, \\\"write\\\": true } ]\" \"$API_URL/restapi/datastorage/tempCredentials/\") # The resulting keys, shall be used to configure the AWS SDK for the data transfer # Here we use AWS CLI SDK, but this will work for any other (e.g. Java/JS/Go/Python/...) export AWS_ACCESS_KEY_ID=$(jq -r '.payload.keyID' <<< $RESPONSE) export AWS_SECRET_ACCESS_KEY=$(jq -r '.payload.accessKey' <<< $RESPONSE) export AWS_SESSION_TOKEN=$(jq -r '.payload.token' <<< $RESPONSE) ############################################################# # Transfer the local fastq files to the S3 working directory using the direct call to AWS SDK ############################################################# # Files will be uploaded to e.g. \"s3://my_bucket/workdir/fastq/tiny-fastq/\" FASTQS_S3=\"$WORKDIR/fastq/$(basename $FASTQS)\" aws s3 cp \"$FASTQS\" \"$FASTQS_S3/\" --recursive ############################################################# # Run processing ############################################################# RESPONSE=$(curl -ks -X POST -H \"Authorization: Bearer $API_TOKEN\" -H \"Content-Type: application/json\" -H \"Accept: application/json\" -d \"{ \\\"cmdTemplate\\\": \\\"cellranger count --id cloud-cellranger --fastqs \\$fastqs --transcriptome \\$transcriptome\\\", \\\"dockerImage\\\": \"single-cell/cellranger:latest\", \\\"hddSize\\\": $INSTANCE_DISK, \\\"instanceType\\\": \\\"$INSTANCE_TYPE\\\", \\\"params\\\": { \\\"fastqs\\\": { \\\"type\\\": \\\"input\\\", \\\"value\\\": \\\"$FASTQS_S3\\\" }, \\\"results\\\": { \\\"type\\\":\\\"output\\\", \\\"value\\\":\\\"$RESULTS_S3\\\" }, \\\"transcriptome\\\": { \\\"type\\\": \\\"input\\\", \\\"value\\\": \\\"$TRANSCRIPTOME_S3\\\" } } }\" \"$API_URL/restapi/run\") RUN_ID=$(jq -r \".payload.id\" <<< $RESPONSE) ############################################################# # Poll the run status each 30s, until it's finished ############################################################# RUN_STATUS=\"NA\" while [ \"$RUN_STATUS\" != \"SUCCESS\" ]; do sleep 30 RESPONSE=$(curl -ks -X GET -H \"Authorization: Bearer $API_TOKEN\" -H \"Accept: application/json\" \"$API_URL/restapi/run/$RUN_ID\") RUN_STATUS=$(jq -r \".payload.status\" <<< $RESPONSE) done","title":"Direct HTTP calls via curl"},{"location":"api/API_tutorials/JavaScript_example/","text":"JavaScript implementation The JavaScript example implementation of the Usage scenario offers a Web form, that allows to specify the job input parameters and submit it into the Cloud Pipeline via the API. Prerequisites NodeJS 10.15.3+ npm 6.4.1+ Setup the configuration Locate the JavaScript sample application at js_example or clone the the Cloud Pipeline repository git clone https://github.com/epam/cloud-pipeline cd cloud-pipeline/docs/md/api/API_tutorials/attachments/js_example/ Open the configuration file config.js and replace the following values: <host> - set to the host of the Cloud Pipeline API <storage_id> - set to the ID of the bucket, that is going to be used as a \"working directory\" FASTQ files and processing results will placed into this bucket Start the application # Install dependencies and start the app # UI will be served on 0.0.0.0:3010 npm install npm run start Application description Once app is built and loaded in the web-browser one can perform the following operations: Setup the cellranger parameters Set the location of the FASTQ files Choose the transcriptome Specify the \"workdir\", where the job will keep the results Once the job parameters are set, user can click LAUNCH and the job is submitted to the Cloud Pipeline backend via the HTTP/REST API Application will poll the Cloud Pipeline API until the job is finished Once done, application will load the cellranger's data processing summary from the job results folder and display it in the web-browser","title":"JavaScript example"},{"location":"api/API_tutorials/JavaScript_example/#javascript-implementation","text":"The JavaScript example implementation of the Usage scenario offers a Web form, that allows to specify the job input parameters and submit it into the Cloud Pipeline via the API.","title":"JavaScript implementation"},{"location":"api/API_tutorials/JavaScript_example/#prerequisites","text":"NodeJS 10.15.3+ npm 6.4.1+","title":"Prerequisites"},{"location":"api/API_tutorials/JavaScript_example/#setup-the-configuration","text":"Locate the JavaScript sample application at js_example or clone the the Cloud Pipeline repository git clone https://github.com/epam/cloud-pipeline cd cloud-pipeline/docs/md/api/API_tutorials/attachments/js_example/ Open the configuration file config.js and replace the following values: <host> - set to the host of the Cloud Pipeline API <storage_id> - set to the ID of the bucket, that is going to be used as a \"working directory\" FASTQ files and processing results will placed into this bucket","title":"Setup the configuration"},{"location":"api/API_tutorials/JavaScript_example/#start-the-application","text":"# Install dependencies and start the app # UI will be served on 0.0.0.0:3010 npm install npm run start","title":"Start the application"},{"location":"api/API_tutorials/JavaScript_example/#application-description","text":"Once app is built and loaded in the web-browser one can perform the following operations: Setup the cellranger parameters Set the location of the FASTQ files Choose the transcriptome Specify the \"workdir\", where the job will keep the results Once the job parameters are set, user can click LAUNCH and the job is submitted to the Cloud Pipeline backend via the HTTP/REST API Application will poll the Cloud Pipeline API until the job is finished Once done, application will load the cellranger's data processing summary from the job results folder and display it in the web-browser","title":"Application description"},{"location":"installation/change_kube_certificates/","text":"Process of changing certificates on kube cluster Kubernetes v1.15 # Check the expiration kubeadm certs check-expiration # It's also possible to renew specific certs only, instead of all # Check `kubeadm certs renew --help` kubeadm certs renew all # Restart kube containers systemctl stop docker systemctl stop kubelet systemctl start docker systemctl start kubelet # Renew auth tokens for the Cloud Pipeline API \\cp /etc/kubernetes/admin.conf /root/.kube/config # Copy /root/.kube/config to all the API nodes Kubernetes v1.7.5 There is a script to backup old one and generate new one certificates for kube cluster In order to change certificates: Copy this script to some file on kube master node Replace placeholders for <KUBE_IP_HOST> , <KUBE_API_HOSTNAME> Increase DAYS_CERT if needed (365 by default) chmod +x this file Run this script, it will save old certificates in ./kubernetes-bk and generates and print to the console new one You will need manually replace client-certificate-data in several files in according to script's output Reboot kube master node Login again wait until all kube-system pod will be ready and cluster will recover to the working state Now cluster should work properly set -e KUBE_FOLDER=/etc/kubernetes KUBE_IP_HOST=<Kube master IP> KUBE_API_IP=<generally it is a 10.96.0.1> KUBE_API_HOSTNAME=$(hostname -s) KUBE_BACKUP_FOLDER=./kubernetes-bk KEYS_FOLDER=./keys TEMP_FOLDER=./temp CERT_FOLDER=./certs DAYS_CERT=365 function get-private-key { for f in $(ls $KUBE_FOLDER/*.conf) do cat $f | grep client-key-data | sed 's/client-key-data://' | sed -e 's/^[ \\t]*//' | base64 -d > $KEYS_FOLDER/$(basename $f .conf).key done cp $KUBE_FOLDER/pki/apiserver.key $KEYS_FOLDER/apiserver.key cp $KUBE_FOLDER/pki/apiserver-kubelet-client.key $KEYS_FOLDER/apiserver-kubelet-client.key cp $KUBE_FOLDER/pki/front-proxy-client.key $KEYS_FOLDER/front-proxy-client.key } function create-backup { cp -r /etc/kubernetes $KUBE_BACKUP_FOLDER } function prepare { mkdir -p $KEYS_FOLDER $TEMP_FOLDER $CERT_FOLDER } function remove-all { rm -rf $KUBE_BACKUP_FOLDER $KEYS_FOLDER $TEMP_FOLDER $CERT_FOLDER } function create_cnf () { FILENAME=$1 CN=$2 O=$3 echo \" - $FILENAME.cnf\" if [ -z \"$O\" ] then cat <<EOF > $TEMP_FOLDER/$FILENAME [req] distinguished_name = req_distinguished_name x509_extensions = v3_req prompt = no [req_distinguished_name] commonName = $CN [v3_req] keyUsage = digitalSignature, keyEncipherment extendedKeyUsage = clientAuth EOF else cat <<EOF > $TEMP_FOLDER/$FILENAME [req] distinguished_name = req_distinguished_name x509_extensions = v3_req prompt = no [req_distinguished_name] commonName = $CN O = $O [v3_req] keyUsage = digitalSignature, keyEncipherment extendedKeyUsage = clientAuth EOF fi } function create_api_cnf { echo \" - apiserver.cnf\" cat <<EOF > $TEMP_FOLDER/apiserver.cnf [req] distinguished_name = req_distinguished_name x509_extensions = v3_req prompt = no [req_distinguished_name] commonName = kube-apiserver [v3_req] keyUsage = digitalSignature, keyEncipherment extendedKeyUsage = serverAuth subjectAltName = @alt_names [alt_names] DNS.1 = $KUBE_API_HOSTNAME DNS.2 = kubernetes DNS.3 = kubernetes.default DNS.4 = kubernetes.default.svc DNS.5 = kubernetes.default.svc.cluster.local IP.1 = $KUBE_API_IP IP.2 = $KUBE_IP_HOST EOF } function create_csr { for f in $(ls $TEMP_FOLDER/*.cnf) do FN=$(basename $f .cnf) echo \" - $FN.csr\" openssl req -new -key $KEYS_FOLDER/$FN.key -out $TEMP_FOLDER/$FN.csr -config $TEMP_FOLDER/$FN.cnf done } function create_cert { for f in $(ls $TEMP_FOLDER/*.csr) do FN=$(basename $f .csr) echo \" - $FN.crt\" if [ \"$FN\" == \"front-proxy-client\" ] then openssl x509 -req -days $DAYS_CERT -in $TEMP_FOLDER/$FN.csr -CA $KUBE_FOLDER/pki/front-proxy-ca.crt -CAkey $KUBE_FOLDER/pki/front-proxy-ca.key -CAcreateserial -out $CERT_FOLDER/$FN.crt -extensions v3_req -extfile $TEMP_FOLDER/$FN.cnf else openssl x509 -req -days $DAYS_CERT -in $TEMP_FOLDER/$FN.csr -CA $KUBE_FOLDER/pki/ca.crt -CAkey $KUBE_FOLDER/pki/ca.key -CAcreateserial -out $CERT_FOLDER/$FN.crt -extensions v3_req -extfile $TEMP_FOLDER/$FN.cnf fi done } CUR_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\" cd $CUR_DIR remove-all echo \"Prepare\" prepare echo \"Create backup\" create-backup echo \"Get private key\" get-private-key echo \"Create cnf\" create_cnf scheduler.cnf system:kube-scheduler create_cnf controller-manager.cnf system:kube-controller-manager create_cnf front-proxy-client.cnf front-proxy-client create_cnf apiserver-kubelet-client.cnf kube-apiserver-kubelet-client system:masters create_cnf admin.cnf kubernetes-admin system:masters create_cnf kubelet.cnf system:node:$KUBE_API_HOSTNAME system:nodes create_api_cnf echo \"Create csr\" create_csr echo \"Create certificates\" create_cert echo \"\" echo \"manually cp $CERT_FOLDER/apiserver.crt $KUBE_FOLDER/pki/apiserver.crt\" echo \"\" echo \"manually cp $CERT_FOLDER/apiserver-kubelet-client.crt $KUBE_FOLDER/pki/apiserver-kubelet-client.crt\" echo \"\" echo \"manually cp $CERT_FOLDER/front-proxy-client.crt $KUBE_FOLDER/pki/front-proxy-client.crt\" echo \"\" admin_st=$(openssl base64 -in $CERT_FOLDER/admin.crt | tr -d '\\n') echo \"manually replace client-certificate-data in $KUBE_FOLDER/admin.conf - $admin_st\" echo \"\" kubelet_st=$(openssl base64 -in $CERT_FOLDER/kubelet.crt | tr -d '\\n') echo \"manually replace client-certificate-data in $KUBE_FOLDER/kubelet.conf - $kubelet_st\" echo \"\" scheduler_st=$(openssl base64 -in $CERT_FOLDER/scheduler.crt | tr -d '\\n') echo \"manually replace client-certificate-data in $KUBE_FOLDER/scheduler.conf - $scheduler_st\" echo \"\" controller_manager_st=$(openssl base64 -in $CERT_FOLDER/controller-manager.crt | tr -d '\\n') echo \"manually replace client-certificate-data in $KUBE_FOLDER/controller-manager.conf - $controller_manager_st\" echo \"\" echo \"manually replace client-certificate-data in ~/.kube/config - $admin_st\" echo \"\"","title":"Change Kubernetes certificates"},{"location":"installation/change_kube_certificates/#process-of-changing-certificates-on-kube-cluster","text":"","title":"Process of changing certificates on kube cluster"},{"location":"installation/change_kube_certificates/#kubernetes-v115","text":"# Check the expiration kubeadm certs check-expiration # It's also possible to renew specific certs only, instead of all # Check `kubeadm certs renew --help` kubeadm certs renew all # Restart kube containers systemctl stop docker systemctl stop kubelet systemctl start docker systemctl start kubelet # Renew auth tokens for the Cloud Pipeline API \\cp /etc/kubernetes/admin.conf /root/.kube/config # Copy /root/.kube/config to all the API nodes","title":"Kubernetes v1.15"},{"location":"installation/change_kube_certificates/#kubernetes-v175","text":"There is a script to backup old one and generate new one certificates for kube cluster In order to change certificates: Copy this script to some file on kube master node Replace placeholders for <KUBE_IP_HOST> , <KUBE_API_HOSTNAME> Increase DAYS_CERT if needed (365 by default) chmod +x this file Run this script, it will save old certificates in ./kubernetes-bk and generates and print to the console new one You will need manually replace client-certificate-data in several files in according to script's output Reboot kube master node Login again wait until all kube-system pod will be ready and cluster will recover to the working state Now cluster should work properly set -e KUBE_FOLDER=/etc/kubernetes KUBE_IP_HOST=<Kube master IP> KUBE_API_IP=<generally it is a 10.96.0.1> KUBE_API_HOSTNAME=$(hostname -s) KUBE_BACKUP_FOLDER=./kubernetes-bk KEYS_FOLDER=./keys TEMP_FOLDER=./temp CERT_FOLDER=./certs DAYS_CERT=365 function get-private-key { for f in $(ls $KUBE_FOLDER/*.conf) do cat $f | grep client-key-data | sed 's/client-key-data://' | sed -e 's/^[ \\t]*//' | base64 -d > $KEYS_FOLDER/$(basename $f .conf).key done cp $KUBE_FOLDER/pki/apiserver.key $KEYS_FOLDER/apiserver.key cp $KUBE_FOLDER/pki/apiserver-kubelet-client.key $KEYS_FOLDER/apiserver-kubelet-client.key cp $KUBE_FOLDER/pki/front-proxy-client.key $KEYS_FOLDER/front-proxy-client.key } function create-backup { cp -r /etc/kubernetes $KUBE_BACKUP_FOLDER } function prepare { mkdir -p $KEYS_FOLDER $TEMP_FOLDER $CERT_FOLDER } function remove-all { rm -rf $KUBE_BACKUP_FOLDER $KEYS_FOLDER $TEMP_FOLDER $CERT_FOLDER } function create_cnf () { FILENAME=$1 CN=$2 O=$3 echo \" - $FILENAME.cnf\" if [ -z \"$O\" ] then cat <<EOF > $TEMP_FOLDER/$FILENAME [req] distinguished_name = req_distinguished_name x509_extensions = v3_req prompt = no [req_distinguished_name] commonName = $CN [v3_req] keyUsage = digitalSignature, keyEncipherment extendedKeyUsage = clientAuth EOF else cat <<EOF > $TEMP_FOLDER/$FILENAME [req] distinguished_name = req_distinguished_name x509_extensions = v3_req prompt = no [req_distinguished_name] commonName = $CN O = $O [v3_req] keyUsage = digitalSignature, keyEncipherment extendedKeyUsage = clientAuth EOF fi } function create_api_cnf { echo \" - apiserver.cnf\" cat <<EOF > $TEMP_FOLDER/apiserver.cnf [req] distinguished_name = req_distinguished_name x509_extensions = v3_req prompt = no [req_distinguished_name] commonName = kube-apiserver [v3_req] keyUsage = digitalSignature, keyEncipherment extendedKeyUsage = serverAuth subjectAltName = @alt_names [alt_names] DNS.1 = $KUBE_API_HOSTNAME DNS.2 = kubernetes DNS.3 = kubernetes.default DNS.4 = kubernetes.default.svc DNS.5 = kubernetes.default.svc.cluster.local IP.1 = $KUBE_API_IP IP.2 = $KUBE_IP_HOST EOF } function create_csr { for f in $(ls $TEMP_FOLDER/*.cnf) do FN=$(basename $f .cnf) echo \" - $FN.csr\" openssl req -new -key $KEYS_FOLDER/$FN.key -out $TEMP_FOLDER/$FN.csr -config $TEMP_FOLDER/$FN.cnf done } function create_cert { for f in $(ls $TEMP_FOLDER/*.csr) do FN=$(basename $f .csr) echo \" - $FN.crt\" if [ \"$FN\" == \"front-proxy-client\" ] then openssl x509 -req -days $DAYS_CERT -in $TEMP_FOLDER/$FN.csr -CA $KUBE_FOLDER/pki/front-proxy-ca.crt -CAkey $KUBE_FOLDER/pki/front-proxy-ca.key -CAcreateserial -out $CERT_FOLDER/$FN.crt -extensions v3_req -extfile $TEMP_FOLDER/$FN.cnf else openssl x509 -req -days $DAYS_CERT -in $TEMP_FOLDER/$FN.csr -CA $KUBE_FOLDER/pki/ca.crt -CAkey $KUBE_FOLDER/pki/ca.key -CAcreateserial -out $CERT_FOLDER/$FN.crt -extensions v3_req -extfile $TEMP_FOLDER/$FN.cnf fi done } CUR_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\" cd $CUR_DIR remove-all echo \"Prepare\" prepare echo \"Create backup\" create-backup echo \"Get private key\" get-private-key echo \"Create cnf\" create_cnf scheduler.cnf system:kube-scheduler create_cnf controller-manager.cnf system:kube-controller-manager create_cnf front-proxy-client.cnf front-proxy-client create_cnf apiserver-kubelet-client.cnf kube-apiserver-kubelet-client system:masters create_cnf admin.cnf kubernetes-admin system:masters create_cnf kubelet.cnf system:node:$KUBE_API_HOSTNAME system:nodes create_api_cnf echo \"Create csr\" create_csr echo \"Create certificates\" create_cert echo \"\" echo \"manually cp $CERT_FOLDER/apiserver.crt $KUBE_FOLDER/pki/apiserver.crt\" echo \"\" echo \"manually cp $CERT_FOLDER/apiserver-kubelet-client.crt $KUBE_FOLDER/pki/apiserver-kubelet-client.crt\" echo \"\" echo \"manually cp $CERT_FOLDER/front-proxy-client.crt $KUBE_FOLDER/pki/front-proxy-client.crt\" echo \"\" admin_st=$(openssl base64 -in $CERT_FOLDER/admin.crt | tr -d '\\n') echo \"manually replace client-certificate-data in $KUBE_FOLDER/admin.conf - $admin_st\" echo \"\" kubelet_st=$(openssl base64 -in $CERT_FOLDER/kubelet.crt | tr -d '\\n') echo \"manually replace client-certificate-data in $KUBE_FOLDER/kubelet.conf - $kubelet_st\" echo \"\" scheduler_st=$(openssl base64 -in $CERT_FOLDER/scheduler.crt | tr -d '\\n') echo \"manually replace client-certificate-data in $KUBE_FOLDER/scheduler.conf - $scheduler_st\" echo \"\" controller_manager_st=$(openssl base64 -in $CERT_FOLDER/controller-manager.crt | tr -d '\\n') echo \"manually replace client-certificate-data in $KUBE_FOLDER/controller-manager.conf - $controller_manager_st\" echo \"\" echo \"manually replace client-certificate-data in ~/.kube/config - $admin_st\" echo \"\"","title":"Kubernetes v1.7.5"},{"location":"installation/support_windows_runs/","text":"Windows runs support Overview Build Windows ami Build Windows tool Configure deployment Configure brand-new deployment Configure existing deployment Overview The following chapters help to configure Windows runs support in Cloud Pipeline. Build Windows ami Base AMI which has to be used for new AMIs building is ami-041bf3c49db945dbb . Similar AMI in different regions can be found by this name . The following steps help to build new version of Cloud Pipeline Windows AMI. 1) Launch an instance using ami-041bf3c49db945dbb instance image, m5.large instance type, proper subnet , 100 gb disk and proper security groups . Subnet and security groups should be the same as in Cloud Pipeline deployment itself. 2) Once instance is ready generate password using default private key and connect using RDP to this machine with Administrator login and generated password. 3) Once connected open powershell console and manually execute contents of deploy/infra/aws/install-common-win-node.ps1 script excluding a single New-EC2Tag call. At this point any changes can be performed on the machine. Notice that before reboot the following command has to be executed otherwise created AMI won't work. C:\\ProgramData\\Amazon\\EC2-Windows\\Launch\\Scripts\\InitializeInstance.ps1 -Schedule 4) Create a new ami from the running instance. Build Windows tool The following steps help to build and push a new version of Cloud Pipeline Windows tool: Launch an instance using ami-041bf3c49db945dbb instance image or launch Windows run in Cloud Pipeline, connect to the instance via RDP or NoMachine, build Window docker image using deploy/docker/cp-tools/base/windows/Dockerfile and push it to Cloud Pipeline private docker registry using the commands below. Please use the actual Cloud Pipeline deployment ip. $apiIp = \"127.0.0.1\" $registryPort = \"31443\" $registry = \"${apiIp}:${registryPort}\" $apiUser = \"API_USER\" $apiToken = \"API_TOKEN\" $tag = \"20210610\" @\" { \"insecure-registries\" : [\"$registry\"], \"allow-nondistributable-artifacts\": [\"$registry\"] } \"@ | Out-File -FilePath \"c:\\programData\\docker\\config\\daemon.json\" -Encoding ascii -Force @\" $apiIp cp-docker-registry.default.svc.cluster.local $apiIp cp-api-srv.default.svc.cluster.local \"@ | Out-File -FilePath \"c:\\windows\\system32\\drivers\\etc\\hosts\" -Encoding ascii -Force docker login \"$registry\" -u \"$apiUser\" -p \"$apiToken\" docker build -t \"$registry/library/windows:$tag\" . docker tag \"$registry/library/windows:$tag\" \"$registry/library/windows:latest\" docker push \"$registry/library/windows:$tag\" docker push \"$registry/library/windows:latest\" Configure deployment Configure brand-new deployment To configure Windows runs support for brand-new Cloud Pipeline deployments the following actions have to be performed. 1) Update cluster.networks.config system preference by adding platform field to all amis and adding a new Windows ami. Before { \"regions\": [{ \"amis\": [{ \"instance_mask\": \"*\", \"ami\": \"...\", \"init_script\": \"...\" }, { \"instance_mask\": \"p*\", \"ami\": \"...\", \"init_script\": \"...\" }] }] } After { \"regions\": [{ \"amis\": [{ \"instance_mask\": \"*\", \"platform\": \"linux\", \"ami\": \"...\", \"init_script\": \"...\" }, { \"instance_mask\": \"p*\", \"platform\": \"linux\", \"ami\": \"...\", \"init_script\": \"...\" }, { \"instance_mask\": \"*\", \"platform\": \"windows\", \"ami\": \"...\", \"init_script\": \"init_multicloud.ps1\" }] }] } 2) Configure CP_REPO_ENABLED=false run parameter for Windows tool. Configure existing deployment To configure Windows runs support for existing Cloud Pipeline deployments the following actions have to be performed. 1) Two additional parameters have to be added to Kubernetes global config map. CP_KUBE_KUBEADM_CERT_HASH=\"$(openssl x509 -in /etc/kubernetes/pki/ca.crt -noout -pubkey | openssl rsa -pubin -outform DER 2>/dev/null | sha256sum | cut -d' ' -f1)\" CP_KUBE_NODE_TOKEN=\"$(kubectl --namespace=kube-system describe sa canal \\ | grep Tokens \\ | cut -d: -f2 \\ | xargs kubectl --namespace=kube-system get secret -o json \\ | jq -r '.data.token' \\ | base64 --decode)\" 2) Update cluster.networks.config system preference by adding platform field to all amis and adding a new Windows ami. Before { \"regions\": [{ \"amis\": [{ \"instance_mask\": \"*\", \"ami\": \"...\", \"init_script\": \"...\" }, { \"instance_mask\": \"p*\", \"ami\": \"...\", \"init_script\": \"...\" }] }] } After { \"regions\": [{ \"amis\": [{ \"instance_mask\": \"*\", \"platform\": \"linux\", \"ami\": \"...\", \"init_script\": \"...\" }, { \"instance_mask\": \"p*\", \"platform\": \"linux\", \"ami\": \"...\", \"init_script\": \"...\" }, { \"instance_mask\": \"*\", \"platform\": \"windows\", \"ami\": \"ami-005a8c88514c8ec1e\", \"init_script\": \"init_multicloud.ps1\" }] }] } 3) Several existing application properties have to be updated. launch.script.url --renamed-to--> launch.script.url.linux api.security.public.urls <--added-another-public-url-- launch.py 4) Several new application properties have to be added. launch.script.url.windows=https://${CP_API_SRV_INTERNAL_HOST}:${CP_API_SRV_INTERNAL_PORT}/pipeline/launch.py kube.kubeadm.cert.hash=${CP_KUBE_KUBEADM_CERT_HASH} kube.node.token=${CP_KUBE_NODE_TOKEN} 5) Kubernetes canal config map has to be updated and all canal pods have to be restarted. 6) All edge pods have to be restarted. 7) Configure CP_REPO_ENABLED=false run parameter for Windows tool.","title":"Support Windows runs"},{"location":"installation/support_windows_runs/#windows-runs-support","text":"Overview Build Windows ami Build Windows tool Configure deployment Configure brand-new deployment Configure existing deployment","title":"Windows runs support"},{"location":"installation/support_windows_runs/#overview","text":"The following chapters help to configure Windows runs support in Cloud Pipeline.","title":"Overview"},{"location":"installation/support_windows_runs/#build-windows-ami","text":"Base AMI which has to be used for new AMIs building is ami-041bf3c49db945dbb . Similar AMI in different regions can be found by this name . The following steps help to build new version of Cloud Pipeline Windows AMI. 1) Launch an instance using ami-041bf3c49db945dbb instance image, m5.large instance type, proper subnet , 100 gb disk and proper security groups . Subnet and security groups should be the same as in Cloud Pipeline deployment itself. 2) Once instance is ready generate password using default private key and connect using RDP to this machine with Administrator login and generated password. 3) Once connected open powershell console and manually execute contents of deploy/infra/aws/install-common-win-node.ps1 script excluding a single New-EC2Tag call. At this point any changes can be performed on the machine. Notice that before reboot the following command has to be executed otherwise created AMI won't work. C:\\ProgramData\\Amazon\\EC2-Windows\\Launch\\Scripts\\InitializeInstance.ps1 -Schedule 4) Create a new ami from the running instance.","title":"Build Windows ami"},{"location":"installation/support_windows_runs/#build-windows-tool","text":"The following steps help to build and push a new version of Cloud Pipeline Windows tool: Launch an instance using ami-041bf3c49db945dbb instance image or launch Windows run in Cloud Pipeline, connect to the instance via RDP or NoMachine, build Window docker image using deploy/docker/cp-tools/base/windows/Dockerfile and push it to Cloud Pipeline private docker registry using the commands below. Please use the actual Cloud Pipeline deployment ip. $apiIp = \"127.0.0.1\" $registryPort = \"31443\" $registry = \"${apiIp}:${registryPort}\" $apiUser = \"API_USER\" $apiToken = \"API_TOKEN\" $tag = \"20210610\" @\" { \"insecure-registries\" : [\"$registry\"], \"allow-nondistributable-artifacts\": [\"$registry\"] } \"@ | Out-File -FilePath \"c:\\programData\\docker\\config\\daemon.json\" -Encoding ascii -Force @\" $apiIp cp-docker-registry.default.svc.cluster.local $apiIp cp-api-srv.default.svc.cluster.local \"@ | Out-File -FilePath \"c:\\windows\\system32\\drivers\\etc\\hosts\" -Encoding ascii -Force docker login \"$registry\" -u \"$apiUser\" -p \"$apiToken\" docker build -t \"$registry/library/windows:$tag\" . docker tag \"$registry/library/windows:$tag\" \"$registry/library/windows:latest\" docker push \"$registry/library/windows:$tag\" docker push \"$registry/library/windows:latest\"","title":"Build Windows tool"},{"location":"installation/support_windows_runs/#configure-deployment","text":"","title":"Configure deployment"},{"location":"installation/support_windows_runs/#configure-brand-new-deployment","text":"To configure Windows runs support for brand-new Cloud Pipeline deployments the following actions have to be performed. 1) Update cluster.networks.config system preference by adding platform field to all amis and adding a new Windows ami. Before { \"regions\": [{ \"amis\": [{ \"instance_mask\": \"*\", \"ami\": \"...\", \"init_script\": \"...\" }, { \"instance_mask\": \"p*\", \"ami\": \"...\", \"init_script\": \"...\" }] }] } After { \"regions\": [{ \"amis\": [{ \"instance_mask\": \"*\", \"platform\": \"linux\", \"ami\": \"...\", \"init_script\": \"...\" }, { \"instance_mask\": \"p*\", \"platform\": \"linux\", \"ami\": \"...\", \"init_script\": \"...\" }, { \"instance_mask\": \"*\", \"platform\": \"windows\", \"ami\": \"...\", \"init_script\": \"init_multicloud.ps1\" }] }] } 2) Configure CP_REPO_ENABLED=false run parameter for Windows tool.","title":"Configure brand-new deployment"},{"location":"installation/support_windows_runs/#configure-existing-deployment","text":"To configure Windows runs support for existing Cloud Pipeline deployments the following actions have to be performed. 1) Two additional parameters have to be added to Kubernetes global config map. CP_KUBE_KUBEADM_CERT_HASH=\"$(openssl x509 -in /etc/kubernetes/pki/ca.crt -noout -pubkey | openssl rsa -pubin -outform DER 2>/dev/null | sha256sum | cut -d' ' -f1)\" CP_KUBE_NODE_TOKEN=\"$(kubectl --namespace=kube-system describe sa canal \\ | grep Tokens \\ | cut -d: -f2 \\ | xargs kubectl --namespace=kube-system get secret -o json \\ | jq -r '.data.token' \\ | base64 --decode)\" 2) Update cluster.networks.config system preference by adding platform field to all amis and adding a new Windows ami. Before { \"regions\": [{ \"amis\": [{ \"instance_mask\": \"*\", \"ami\": \"...\", \"init_script\": \"...\" }, { \"instance_mask\": \"p*\", \"ami\": \"...\", \"init_script\": \"...\" }] }] } After { \"regions\": [{ \"amis\": [{ \"instance_mask\": \"*\", \"platform\": \"linux\", \"ami\": \"...\", \"init_script\": \"...\" }, { \"instance_mask\": \"p*\", \"platform\": \"linux\", \"ami\": \"...\", \"init_script\": \"...\" }, { \"instance_mask\": \"*\", \"platform\": \"windows\", \"ami\": \"ami-005a8c88514c8ec1e\", \"init_script\": \"init_multicloud.ps1\" }] }] } 3) Several existing application properties have to be updated. launch.script.url --renamed-to--> launch.script.url.linux api.security.public.urls <--added-another-public-url-- launch.py 4) Several new application properties have to be added. launch.script.url.windows=https://${CP_API_SRV_INTERNAL_HOST}:${CP_API_SRV_INTERNAL_PORT}/pipeline/launch.py kube.kubeadm.cert.hash=${CP_KUBE_KUBEADM_CERT_HASH} kube.node.token=${CP_KUBE_NODE_TOKEN} 5) Kubernetes canal config map has to be updated and all canal pods have to be restarted. 6) All edge pods have to be restarted. 7) Configure CP_REPO_ENABLED=false run parameter for Windows tool.","title":"Configure existing deployment"},{"location":"installation/deployment/aws/cloud-formation/","text":"Cloud-pipeline Deployment using AWS Cloudformation step-by-step guide Overview This step-by-step guide illustrates how to deploy Cloud Pipeline using AWS CloudFormation. The solution relies on creating a CloudFormation stack that deploys the necessary infrastructure using a Terraform module and install the Cloud Pipeline on top of it. Follow the outlined steps below to execute the deployment process: Cloud-pipeline Deployment using AWS Cloudformation step-by-step guide Overview Prerequisites AWS VPC AWS Elastic IP Credentials to Create Infrastructure Resources Create DNS records Add authority signed certificates for services Integrate with an Identity Provider (IdP) Create a Zip File with Additional Assets Create Cloudformation Stack using template file Stack Parameters Description Deploy Cloud Pipeline using AWS Console Deploy Cloud Pipeline using AWS CLI Review Deployment Logs Destroy Cloud-Pipeline resources Prerequisites Before creating the CloudFormation stack, you need to prepare several requirements: AWS VPC Virtual Private Cloud (VPC) with desired network configuration should be created in advance. The deployment will create all necessary resources for Cloud Pipeline deployment during the next phases: CloudFormation stack deployment Terraform deployment AWS Elastic IP Generate an AWS Elastic IP, which will be used later to deploy an AWS Elastic Load Balancer (ELB) routing users' traffic to Cloud-Pipeline services. This EIP should also be used when requesting the creation of DNS records from your DNS provider. Credentials to Create Infrastructure Resources Credentials should be given for this deployment stack to initiate the creation of infrastructure resources. There are two ways to provide such credentials: If you have Administrator role in AWS account or role that Allowed to created resources: EC2 instance, IAM Roles/Policies, EKS cluster, security groups, cloudwatch logs, S3 buckets, EFS/FSX for Luste file systems, KMS keys, RDS etc. you can use these credentials: Put your credentials of temporary credentials into the AWS secret: Create aws secretsmanager secret using AWS console or this aws cli command(make sure you have installed aws cli): aws secretsmanager create-secret --name <secrets-name> \\ --secret-string 'export AWS_ACCESS_KEY_ID=\"<key-id>\" export AWS_SECRET_ACCESS_KEY=\"<secret acess key>\" export AWS_SESSION_TOKEN=\"<session token>\"' \\ --region <region-id> If you need to update the secret value with a new credentials (f.i. when you are using temporary credentials and decide to destroy the infrastructure. See destroy infrastructure ): aws secretsmanager put-secret-value \\ --secret-id <secrets-name> \\ --secret-string 'export AWS_ACCESS_KEY_ID=\"<key-id>\" export AWS_SECRET_ACCESS_KEY=\"<secret acess key>\" export AWS_SESSION_TOKEN=\"<session token>\"' \\ --region <region-id> Remember the secret name, you will need this value during further deployment. (To set this secret name as stack parameter AWSCredentialSecretId . For more information you can look deploy parameters ) If you don't provide secret name with the credentials to the deployment script, an additional IAM Role will be created with full administrator access and assumed by Jump-Server during deploy process. Create DNS records Since Cloud Pipeline services communicate through domain names, you need to create DNS records. The proposed records scheme: DNS record Record type Value Comment \\<cloud-pipeline-name>.\\<your-domain> A < EIP value > Required edge.<cloud-pipeline-name>.\\<your-domain> CNAME \\<cloud-pipeline-name>.\\<your-domain> Required docker. .\\<your-domain> CNAME \\<cloud-pipeline-name>.\\<your-domain> Required git.<cloud-pipeline-name>.\\<your-domain> CNAME \\<cloud-pipeline-name>.\\<your-domain> Required auth.<cloud-pipeline-name>.\\<your-domain> CNAME \\<cloud-pipeline-name>.\\<your-domain> Skip, if you use your organisation IdP Add authority signed certificates for services NOTE: This step can be skipped. By default, if no authority-signed certificates are set, Cloud Pipeline will create self-signed certificates during the deployment Field Value Subject \\<cloud-pipeline-name>.\\<your-domain> Alternative names DNS: *.\\<cloud-pipeline-name>.\\<your-domain> or DNS: edge.\\<cloud-pipeline-name>.\\<your-domain> docker.\\ .\\<your-domain> git.\\<cloud-pipeline-name>.\\<your-domain> Alternative names (optional) DNS: auth.\\ .\\<your-domain> Duration (days) 365 Alternative names (optional) - DNS: auth. . - This must be set if there is no integration with organization Identity Provider. Integrate with an Identity Provider (IdP) If you deploy Cloud-Pipeline with self-hosted IdP, you can skip this step: Cloud-pipeline creates a dummy IdP service suitable for testing, it is not recommended for production due to security concerns. For secure access, you should integrate your Cloud Pipeline with your organization's IdP Service Integration is implemented with SAML2 protocol. The following IdP connections shall be requested from the IdP team: Purpose SP URL ACS URL AML Binding Assertion information cloud-pipeline GUI https://\\<cloud-pipeline-name>.\\<your-domain>/pipeline https://\\<cloud-pipeline-name>.\\<your-domain>/pipeline/saml/SSO HTTP Redirect * NameID * email * first name * last name cloud-pipeline GitLab https://git.\\<cloud-pipeline-name>.\\<your-domain> https://git.<cloud-pipeline-name>.\\<your-domain>//users/auth/saml/callback HTTP Post * NameID * email * first name * last name As a result of the requests, the following information shall be provided: Federation metadata XML file IdP signing certificate Create a Zip File with Additional Assets This step is required if you've prepared SSL certificated and IdP integration on previous steps. It allows to provide these assets for the deployment. Once all additional certificates (and optional metadata) are ready, you should create a zip archive with a specific structure. To do this you can run script create_assets_zip.sh with your parameters, for example: bash create_assets_zip.sh ca-public-cert.pem ca-private-key.pem cp-api-srv-fed-meta.xml idp-public-cert.pem Optional: If you don't have s3 bucket you can create it in AWS Console or using AWS CLI command: aws s3api create-bucket --bucket < s3-bucket-name > Upload zip file to the s3 bucket using AWS console or aws cli command: aws s3 cp <pathtofile>/cp-assets.zip s3://<s3-bucket-name> Save this link, you will need it for further step t set it as parameter in AWS Console stack parameters(for more information you can look at parameters description ) or as additional parameter in .json file(if create stack using awc cli) : Link format should be as follows: s3:// s3-bucket-name /cp-assets.zip Create Cloudformation Stack using template file There are two methods to create a CloudFormation stack that deploys the infrastructure and Cloud-Pipeline services: via the AWS console, or the AWS Command Line Interface (CLI). Stack Parameters Description Name Description DeploymentName (Required) Name of the deployment. Will be used as resources name prefix DeploymentEnv (Required) Environment name. Will be used as resources name prefix DeploymentAWSCredentialsSecretId (Optional) Name of the aws secret with secret key and access key of the user that will be used on Jumpserver to run Terraform to deploy infrastructure. See Set access to create infrastructure JumpServerInstanceType (Optional) Jump-server EC2 instance type JumpServerAmiId (Optional) Image id that will be used for Jump-Server VpcId (Required) Id of the VPC where all resources will be created. See Create VPC SubnetIds (Required) Ids of the VCP subnets to be used for Cloud Pipeline EKS cluster, FS mount points, etc. At least one subnet id in list must be specified. See Create VPC EKSVersion (Optional) This refers to the version of the installed AWS EKS Cluster. Please note that the current version (1.29) has been tested and verified to work properly with Cloud-Pipeline. Cloud-Pipeline has not been tested with other versions of the EKS cluster and may potentially encounter issues. Users attempting to install them do so at their own risk. CPSystemSubnetId (Required) Subnet where JumpServer instance and EKS node group will be created. See Create VPC EKSAdminRoleArns (Optional) Set additional role ARNs that will be added as administartors in EKS cluster. For example in case when additional deploy role created for Jump Server and need to add additional role as EKS Administrator(By default admin role is that role which deploys EKS cluster) IAMrolePermissionsBoundaryArn (Optional) Account specific role boundaries, that can be used during creating AMI Roles with organization specific restrictions. TFstateBucketName (Required) S3 Bucket name, that will be created where terraform state file for Cloud-Pipeline Infrastructure module will be stored TFStateLockTableName (Required) Name of the DynamoDB table, that will be created, for terraform state lock CPNetworkFileSystemType (Optional) FileSystem type that will be created. Can be efs or fsx. Default efs. CPApiAccessPrefixLists (Optional) Prefix Lists to which access to Cloud Pipeline API will be granted CPExternalAccessSecurityGroupIds (Optional) List of one or more AWS Security Groups that will be used for access to Cloud Pipeline services. CPDeploymentId (Optional) Specify unique ID of the deployment. It will be used to name cloud entities (e.g. path within a docker registry object container). Must contain only letters, digits, underscore or horizontal bar. CPEdgeAwsELBSubnet (Required) The ID of the public subnet for the Load Balancer. Must be in the same Availability Zone (AZ) as the CPInfraSubnetIdNode . CPEdgeAwsELBIP (Required) Allocation ID of the Elastic IP from prerequisites in case of internet-facing ELB, or private IP in case of internal ELB. See Create AWS Elastic IP CPEdgeAwsELBSchema (Required) Type of the AWS ELB to provide access to the users to the system. Possible values 'internal', 'internet-facing'. Default 'internet-facing'. CPApiSrvHost (Required) API service domain name address. See Create DNS CPIdpHost (Optional) Self hosted IDP service domain name address. WARNING: Using self hosted IDP service in production environment strongly not recommended! If not provided CPAssetsS3Url parameter should be provided with all necessary artifacts to configure SSO authentication for Cloud-Pipeline. CPDockerHost (Required) Docker service domain name address. See Create DNS CPEdgeHost (Required) EDGE service domain name address. See Create DNS CPGitlabHost (Required) GITLAB service domain name address. See Create DNS CPAssetsS3Url (Optional) Link to zip archive with additional assets(certificates) on AWS S3 bucket. For example s3://< bucket-name >/< filename.zip > See Creating zip with additional assets CPPipectlUrl (Required) Link to the pipectl binary file that will be used to deploy Cloud Pipeline Deploy Cloud Pipeline using AWS Console Go to the CloudFormation service in the AWS Console and select the \"create stack\" option. In the \"Prerequisite - Prepare template\" section, choose \"Use an existing template\". Choose your 'jump-server.yaml' file under \"Specify a template\" and click \"Next\". On \"Specify stack details\" step provide the Stack Name and all required parameters, click \"Next\" and leave next page \"Configure stack options\" without changes, then click \"Next\" to check parameters at \"Review and create\" page. Click \"Submit\" to start stack creation. Follow the Stack Parameters Description section for a detailed explanation of each parameter. Deploy Cloud Pipeline using AWS CLI Use the AWS CLI to deploy the Cloud Pipeline. Create Cloudformation stack using AWS Console and file jump-server.yaml and set parameters (see deployment parameters ) or create file jump-server.json like in example (Not all needed parameters could be in this example): [ { \"ParameterKey\": \"DeploymentName\", \"ParameterValue\": \"xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"DeploymentEnv\", \"ParameterValue\": \"xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"VpcId\", \"ParameterValue\": \"vpc-xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"CPSystemSubnetId\", \"ParameterValue\": \"subnet-xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"SubnetIds\", \"ParameterValue\": \"subnet-xxxxxxxxxxxxxxx, subnet-xxxxxxxxxxxxxxx, subnet-xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"IAMRolePermissionsBoundaryArn\", \"ParameterValue\": \"arn:aws:iam::xxxxxxxxxxxxxxx:policy/xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"TFstateBucketName\", \"ParameterValue\": \"xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"TFStateLockTableName\", \"ParameterValue\": \"xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"CPAccessPrefixLists\", \"ParameterValue\": \"pl-xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"CPDeploymentId\", \"ParameterValue\": \"xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"CPEdgeAwsELBSubnet\", \"ParameterValue\": \"subnet-xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"CPEdgeAwsELBIP\", \"ParameterValue\": \"eipalloc-xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"CPApiSrvHost\", \"ParameterValue\": \"xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"CPDeploymentId\", \"ParameterValue\": \"xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"CPIdpHost\", \"ParameterValue\": \"auth.xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"CPDockerHost\", \"ParameterValue\": \"docker.xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"CPEdgeHost\", \"ParameterValue\": \"edge.xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"CPGitlabHost\", \"ParameterValue\": \"git.xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"DeploymentAWSCredentialsSecretId\", \"ParameterValue\": \"xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"CPNetworkFileSystemType\", \"ParameterValue\": \"xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"CPPipectlUrl\", \"ParameterValue\": \"https://cloud-pipeline-oss-builds.s3.amazonaws.com/builds/xxxxxxxxxxxxxxx/xxxxxxxxxxxxxxx\" } ] From aws cli run command: aws cloudformation create-stack --stack-name <stack-name> --template-body file://cloud-pipeline-deployer.yaml --parameters file://<user parameter filename>.json --capabilities CAPABILITY_NAMED_IAM --region <region-id> Monitor the Stack creation in the AWS Console or by running the provided AWS CLI command: aws cloudformation describe-stacks --stack-name <stack-name> --query \"Stacks[].{\\\"1.Name\\\":StackName,\\\"2.Status\\\":StackStatus,\\\"3.Output\\\":Outputs}\" --region <region-d> Review Deployment Logs To review the deployment process logs, follow these steps: Log in to the Jump Server using the AWS Console or by running the ssm start-session command from the stack creation Output. Execute the following commands to log in as the root user and navigate to the root/cloud-pipeline/infra/ directory: sudo su cd ~/cloud-pipeline/infra/ To monitor the Terraform deployment process, open the terraform_apply.log file in this directory. You can do this with the following command: tail -f terraform_apply.log Wait for the log to display \"Apply complete!\" and additional outputs. Once these appear, wait a few more minutes to allow Cloud Pipeline to start its installation and create the pipectl.log file. From ~/cloud-pipeline/pipectl-deploy/ directory monitor the Cloud-Pipeline deployment process using the following command: tail -f pipectl.log Once the installation is finished, the log will display \"Installation done\" along with some links. Wait approximately 40-50 minutes until all resources and services are deployed. Then, verify the deployment by visiting https://<service>.<user-domain-name>/ Destroy Cloud-Pipeline resources To delete all resources of the Cloud Pipeline along with the infrastructure, follow these steps: If you use AWS Secret to store temporary credentials as described in Credentials to create Infrastructure Resources , you need to first update your credentials token in your AWS Secret before proceeding with the next steps. Log in to the Jump Server instance using its Instance ID, which can be found in the CloudFormation stack output. aws ssm start-session --target <instance-id> --region <deployment-region> After a login, switch the user to root by running the command sudo su . Next, navigate to the home root directory by entering cd /root/cloud-pipeline/ . Execute the deletion script by running ./delete_all_cp_infra.sh . The script will ask for confirmation before proceeding since this action will remove all Cloud Pipeline resources. Confirm if you are sure about the deletion. Once the script finished and all resources are deleted, you can now manually delete s3 bucket for terraform state and the CloudFormation stack. Please note that these actions will delete all your resources in the Cloud Pipeline infrastructure. Be sure to back up any necessary data before starting the deletion process.","title":"AWS CloudFormation"},{"location":"installation/deployment/aws/cloud-formation/#cloud-pipeline-deployment-using-aws-cloudformation-step-by-step-guide","text":"","title":"Cloud-pipeline Deployment using AWS Cloudformation step-by-step guide"},{"location":"installation/deployment/aws/cloud-formation/#overview","text":"This step-by-step guide illustrates how to deploy Cloud Pipeline using AWS CloudFormation. The solution relies on creating a CloudFormation stack that deploys the necessary infrastructure using a Terraform module and install the Cloud Pipeline on top of it. Follow the outlined steps below to execute the deployment process: Cloud-pipeline Deployment using AWS Cloudformation step-by-step guide Overview Prerequisites AWS VPC AWS Elastic IP Credentials to Create Infrastructure Resources Create DNS records Add authority signed certificates for services Integrate with an Identity Provider (IdP) Create a Zip File with Additional Assets Create Cloudformation Stack using template file Stack Parameters Description Deploy Cloud Pipeline using AWS Console Deploy Cloud Pipeline using AWS CLI Review Deployment Logs Destroy Cloud-Pipeline resources","title":"Overview"},{"location":"installation/deployment/aws/cloud-formation/#prerequisites","text":"Before creating the CloudFormation stack, you need to prepare several requirements:","title":"Prerequisites"},{"location":"installation/deployment/aws/cloud-formation/#aws-vpc","text":"Virtual Private Cloud (VPC) with desired network configuration should be created in advance. The deployment will create all necessary resources for Cloud Pipeline deployment during the next phases: CloudFormation stack deployment Terraform deployment","title":"AWS VPC"},{"location":"installation/deployment/aws/cloud-formation/#aws-elastic-ip","text":"Generate an AWS Elastic IP, which will be used later to deploy an AWS Elastic Load Balancer (ELB) routing users' traffic to Cloud-Pipeline services. This EIP should also be used when requesting the creation of DNS records from your DNS provider.","title":"AWS Elastic IP"},{"location":"installation/deployment/aws/cloud-formation/#credentials-to-create-infrastructure-resources","text":"Credentials should be given for this deployment stack to initiate the creation of infrastructure resources. There are two ways to provide such credentials: If you have Administrator role in AWS account or role that Allowed to created resources: EC2 instance, IAM Roles/Policies, EKS cluster, security groups, cloudwatch logs, S3 buckets, EFS/FSX for Luste file systems, KMS keys, RDS etc. you can use these credentials: Put your credentials of temporary credentials into the AWS secret: Create aws secretsmanager secret using AWS console or this aws cli command(make sure you have installed aws cli): aws secretsmanager create-secret --name <secrets-name> \\ --secret-string 'export AWS_ACCESS_KEY_ID=\"<key-id>\" export AWS_SECRET_ACCESS_KEY=\"<secret acess key>\" export AWS_SESSION_TOKEN=\"<session token>\"' \\ --region <region-id> If you need to update the secret value with a new credentials (f.i. when you are using temporary credentials and decide to destroy the infrastructure. See destroy infrastructure ): aws secretsmanager put-secret-value \\ --secret-id <secrets-name> \\ --secret-string 'export AWS_ACCESS_KEY_ID=\"<key-id>\" export AWS_SECRET_ACCESS_KEY=\"<secret acess key>\" export AWS_SESSION_TOKEN=\"<session token>\"' \\ --region <region-id> Remember the secret name, you will need this value during further deployment. (To set this secret name as stack parameter AWSCredentialSecretId . For more information you can look deploy parameters ) If you don't provide secret name with the credentials to the deployment script, an additional IAM Role will be created with full administrator access and assumed by Jump-Server during deploy process.","title":"Credentials to Create Infrastructure Resources"},{"location":"installation/deployment/aws/cloud-formation/#create-dns-records","text":"Since Cloud Pipeline services communicate through domain names, you need to create DNS records. The proposed records scheme: DNS record Record type Value Comment \\<cloud-pipeline-name>.\\<your-domain> A < EIP value > Required edge.<cloud-pipeline-name>.\\<your-domain> CNAME \\<cloud-pipeline-name>.\\<your-domain> Required docker. .\\<your-domain> CNAME \\<cloud-pipeline-name>.\\<your-domain> Required git.<cloud-pipeline-name>.\\<your-domain> CNAME \\<cloud-pipeline-name>.\\<your-domain> Required auth.<cloud-pipeline-name>.\\<your-domain> CNAME \\<cloud-pipeline-name>.\\<your-domain> Skip, if you use your organisation IdP","title":"Create DNS records"},{"location":"installation/deployment/aws/cloud-formation/#add-authority-signed-certificates-for-services","text":"NOTE: This step can be skipped. By default, if no authority-signed certificates are set, Cloud Pipeline will create self-signed certificates during the deployment Field Value Subject \\<cloud-pipeline-name>.\\<your-domain> Alternative names DNS: *.\\<cloud-pipeline-name>.\\<your-domain> or DNS: edge.\\<cloud-pipeline-name>.\\<your-domain> docker.\\ .\\<your-domain> git.\\<cloud-pipeline-name>.\\<your-domain> Alternative names (optional) DNS: auth.\\ .\\<your-domain> Duration (days) 365 Alternative names (optional) - DNS: auth. . - This must be set if there is no integration with organization Identity Provider.","title":"Add authority signed certificates for services"},{"location":"installation/deployment/aws/cloud-formation/#integrate-with-an-identity-provider-idp","text":"If you deploy Cloud-Pipeline with self-hosted IdP, you can skip this step: Cloud-pipeline creates a dummy IdP service suitable for testing, it is not recommended for production due to security concerns. For secure access, you should integrate your Cloud Pipeline with your organization's IdP Service Integration is implemented with SAML2 protocol. The following IdP connections shall be requested from the IdP team: Purpose SP URL ACS URL AML Binding Assertion information cloud-pipeline GUI https://\\<cloud-pipeline-name>.\\<your-domain>/pipeline https://\\<cloud-pipeline-name>.\\<your-domain>/pipeline/saml/SSO HTTP Redirect * NameID * email * first name * last name cloud-pipeline GitLab https://git.\\<cloud-pipeline-name>.\\<your-domain> https://git.<cloud-pipeline-name>.\\<your-domain>//users/auth/saml/callback HTTP Post * NameID * email * first name * last name As a result of the requests, the following information shall be provided: Federation metadata XML file IdP signing certificate","title":"Integrate with an Identity Provider (IdP)"},{"location":"installation/deployment/aws/cloud-formation/#create-a-zip-file-with-additional-assets","text":"This step is required if you've prepared SSL certificated and IdP integration on previous steps. It allows to provide these assets for the deployment. Once all additional certificates (and optional metadata) are ready, you should create a zip archive with a specific structure. To do this you can run script create_assets_zip.sh with your parameters, for example: bash create_assets_zip.sh ca-public-cert.pem ca-private-key.pem cp-api-srv-fed-meta.xml idp-public-cert.pem Optional: If you don't have s3 bucket you can create it in AWS Console or using AWS CLI command: aws s3api create-bucket --bucket < s3-bucket-name > Upload zip file to the s3 bucket using AWS console or aws cli command: aws s3 cp <pathtofile>/cp-assets.zip s3://<s3-bucket-name> Save this link, you will need it for further step t set it as parameter in AWS Console stack parameters(for more information you can look at parameters description ) or as additional parameter in .json file(if create stack using awc cli) : Link format should be as follows: s3:// s3-bucket-name /cp-assets.zip","title":"Create a Zip File with Additional Assets"},{"location":"installation/deployment/aws/cloud-formation/#create-cloudformation-stack-using-template-file","text":"There are two methods to create a CloudFormation stack that deploys the infrastructure and Cloud-Pipeline services: via the AWS console, or the AWS Command Line Interface (CLI).","title":"Create Cloudformation Stack using template file"},{"location":"installation/deployment/aws/cloud-formation/#stack-parameters-description","text":"Name Description DeploymentName (Required) Name of the deployment. Will be used as resources name prefix DeploymentEnv (Required) Environment name. Will be used as resources name prefix DeploymentAWSCredentialsSecretId (Optional) Name of the aws secret with secret key and access key of the user that will be used on Jumpserver to run Terraform to deploy infrastructure. See Set access to create infrastructure JumpServerInstanceType (Optional) Jump-server EC2 instance type JumpServerAmiId (Optional) Image id that will be used for Jump-Server VpcId (Required) Id of the VPC where all resources will be created. See Create VPC SubnetIds (Required) Ids of the VCP subnets to be used for Cloud Pipeline EKS cluster, FS mount points, etc. At least one subnet id in list must be specified. See Create VPC EKSVersion (Optional) This refers to the version of the installed AWS EKS Cluster. Please note that the current version (1.29) has been tested and verified to work properly with Cloud-Pipeline. Cloud-Pipeline has not been tested with other versions of the EKS cluster and may potentially encounter issues. Users attempting to install them do so at their own risk. CPSystemSubnetId (Required) Subnet where JumpServer instance and EKS node group will be created. See Create VPC EKSAdminRoleArns (Optional) Set additional role ARNs that will be added as administartors in EKS cluster. For example in case when additional deploy role created for Jump Server and need to add additional role as EKS Administrator(By default admin role is that role which deploys EKS cluster) IAMrolePermissionsBoundaryArn (Optional) Account specific role boundaries, that can be used during creating AMI Roles with organization specific restrictions. TFstateBucketName (Required) S3 Bucket name, that will be created where terraform state file for Cloud-Pipeline Infrastructure module will be stored TFStateLockTableName (Required) Name of the DynamoDB table, that will be created, for terraform state lock CPNetworkFileSystemType (Optional) FileSystem type that will be created. Can be efs or fsx. Default efs. CPApiAccessPrefixLists (Optional) Prefix Lists to which access to Cloud Pipeline API will be granted CPExternalAccessSecurityGroupIds (Optional) List of one or more AWS Security Groups that will be used for access to Cloud Pipeline services. CPDeploymentId (Optional) Specify unique ID of the deployment. It will be used to name cloud entities (e.g. path within a docker registry object container). Must contain only letters, digits, underscore or horizontal bar. CPEdgeAwsELBSubnet (Required) The ID of the public subnet for the Load Balancer. Must be in the same Availability Zone (AZ) as the CPInfraSubnetIdNode . CPEdgeAwsELBIP (Required) Allocation ID of the Elastic IP from prerequisites in case of internet-facing ELB, or private IP in case of internal ELB. See Create AWS Elastic IP CPEdgeAwsELBSchema (Required) Type of the AWS ELB to provide access to the users to the system. Possible values 'internal', 'internet-facing'. Default 'internet-facing'. CPApiSrvHost (Required) API service domain name address. See Create DNS CPIdpHost (Optional) Self hosted IDP service domain name address. WARNING: Using self hosted IDP service in production environment strongly not recommended! If not provided CPAssetsS3Url parameter should be provided with all necessary artifacts to configure SSO authentication for Cloud-Pipeline. CPDockerHost (Required) Docker service domain name address. See Create DNS CPEdgeHost (Required) EDGE service domain name address. See Create DNS CPGitlabHost (Required) GITLAB service domain name address. See Create DNS CPAssetsS3Url (Optional) Link to zip archive with additional assets(certificates) on AWS S3 bucket. For example s3://< bucket-name >/< filename.zip > See Creating zip with additional assets CPPipectlUrl (Required) Link to the pipectl binary file that will be used to deploy Cloud Pipeline","title":"Stack Parameters Description"},{"location":"installation/deployment/aws/cloud-formation/#deploy-cloud-pipeline-using-aws-console","text":"Go to the CloudFormation service in the AWS Console and select the \"create stack\" option. In the \"Prerequisite - Prepare template\" section, choose \"Use an existing template\". Choose your 'jump-server.yaml' file under \"Specify a template\" and click \"Next\". On \"Specify stack details\" step provide the Stack Name and all required parameters, click \"Next\" and leave next page \"Configure stack options\" without changes, then click \"Next\" to check parameters at \"Review and create\" page. Click \"Submit\" to start stack creation. Follow the Stack Parameters Description section for a detailed explanation of each parameter.","title":"Deploy Cloud Pipeline using AWS Console"},{"location":"installation/deployment/aws/cloud-formation/#deploy-cloud-pipeline-using-aws-cli","text":"Use the AWS CLI to deploy the Cloud Pipeline. Create Cloudformation stack using AWS Console and file jump-server.yaml and set parameters (see deployment parameters ) or create file jump-server.json like in example (Not all needed parameters could be in this example): [ { \"ParameterKey\": \"DeploymentName\", \"ParameterValue\": \"xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"DeploymentEnv\", \"ParameterValue\": \"xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"VpcId\", \"ParameterValue\": \"vpc-xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"CPSystemSubnetId\", \"ParameterValue\": \"subnet-xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"SubnetIds\", \"ParameterValue\": \"subnet-xxxxxxxxxxxxxxx, subnet-xxxxxxxxxxxxxxx, subnet-xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"IAMRolePermissionsBoundaryArn\", \"ParameterValue\": \"arn:aws:iam::xxxxxxxxxxxxxxx:policy/xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"TFstateBucketName\", \"ParameterValue\": \"xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"TFStateLockTableName\", \"ParameterValue\": \"xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"CPAccessPrefixLists\", \"ParameterValue\": \"pl-xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"CPDeploymentId\", \"ParameterValue\": \"xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"CPEdgeAwsELBSubnet\", \"ParameterValue\": \"subnet-xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"CPEdgeAwsELBIP\", \"ParameterValue\": \"eipalloc-xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"CPApiSrvHost\", \"ParameterValue\": \"xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"CPDeploymentId\", \"ParameterValue\": \"xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"CPIdpHost\", \"ParameterValue\": \"auth.xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"CPDockerHost\", \"ParameterValue\": \"docker.xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"CPEdgeHost\", \"ParameterValue\": \"edge.xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"CPGitlabHost\", \"ParameterValue\": \"git.xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"DeploymentAWSCredentialsSecretId\", \"ParameterValue\": \"xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"CPNetworkFileSystemType\", \"ParameterValue\": \"xxxxxxxxxxxxxxx\" }, { \"ParameterKey\": \"CPPipectlUrl\", \"ParameterValue\": \"https://cloud-pipeline-oss-builds.s3.amazonaws.com/builds/xxxxxxxxxxxxxxx/xxxxxxxxxxxxxxx\" } ] From aws cli run command: aws cloudformation create-stack --stack-name <stack-name> --template-body file://cloud-pipeline-deployer.yaml --parameters file://<user parameter filename>.json --capabilities CAPABILITY_NAMED_IAM --region <region-id> Monitor the Stack creation in the AWS Console or by running the provided AWS CLI command: aws cloudformation describe-stacks --stack-name <stack-name> --query \"Stacks[].{\\\"1.Name\\\":StackName,\\\"2.Status\\\":StackStatus,\\\"3.Output\\\":Outputs}\" --region <region-d>","title":"Deploy Cloud Pipeline using AWS CLI"},{"location":"installation/deployment/aws/cloud-formation/#review-deployment-logs","text":"To review the deployment process logs, follow these steps: Log in to the Jump Server using the AWS Console or by running the ssm start-session command from the stack creation Output. Execute the following commands to log in as the root user and navigate to the root/cloud-pipeline/infra/ directory: sudo su cd ~/cloud-pipeline/infra/ To monitor the Terraform deployment process, open the terraform_apply.log file in this directory. You can do this with the following command: tail -f terraform_apply.log Wait for the log to display \"Apply complete!\" and additional outputs. Once these appear, wait a few more minutes to allow Cloud Pipeline to start its installation and create the pipectl.log file. From ~/cloud-pipeline/pipectl-deploy/ directory monitor the Cloud-Pipeline deployment process using the following command: tail -f pipectl.log Once the installation is finished, the log will display \"Installation done\" along with some links. Wait approximately 40-50 minutes until all resources and services are deployed. Then, verify the deployment by visiting https://<service>.<user-domain-name>/","title":"Review Deployment Logs"},{"location":"installation/deployment/aws/cloud-formation/#destroy-cloud-pipeline-resources","text":"To delete all resources of the Cloud Pipeline along with the infrastructure, follow these steps: If you use AWS Secret to store temporary credentials as described in Credentials to create Infrastructure Resources , you need to first update your credentials token in your AWS Secret before proceeding with the next steps. Log in to the Jump Server instance using its Instance ID, which can be found in the CloudFormation stack output. aws ssm start-session --target <instance-id> --region <deployment-region> After a login, switch the user to root by running the command sudo su . Next, navigate to the home root directory by entering cd /root/cloud-pipeline/ . Execute the deletion script by running ./delete_all_cp_infra.sh . The script will ask for confirmation before proceeding since this action will remove all Cloud Pipeline resources. Confirm if you are sure about the deletion. Once the script finished and all resources are deleted, you can now manually delete s3 bucket for terraform state and the CloudFormation stack. Please note that these actions will delete all your resources in the Cloud Pipeline infrastructure. Be sure to back up any necessary data before starting the deletion process.","title":"Destroy Cloud-Pipeline resources"},{"location":"installation/deployment/aws/terraform/","text":"Cloud-pipeline based on AWS EKS Deployment step-by-step guide Overview This document provides a guidance how to deploy infrastructure using terraform and install Cloud-Pipeline on top of it. The process of the deployment can be performed with the following steps: Cloud-pipeline based on AWS EKS Deployment step-by-step guide Overview Resources deployment using Terraform Prerequisites Jump-server deployment Outputs table of jump-server module Cluster-infrastructure deployment Outputs table of cluster-infrastructure module Cloud-pipeline deployment This guide assumes that you store your terraform root modules in a private git repository and able to clone it from remote instance. Resources deployment using Terraform To get started with deployment, please make sure that you satisfy requirements below. Prerequisites Name Version terraform = 1.5.0 To install terraform 1.50 on Linux amd64 type of OS you can run commands: sudo wget https://releases.hashicorp.com/terraform/1.5.0/terraform_1.5.0_linux_amd64.zip sudo unzip terraform_1.5.0_linux_amd64.zip chmod +x terraform sudo mv terraform /usr/local/bin/ sudo rm terraform_1.5.0_linux_amd64.zip To install terraform to other operating system please follow the links https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli https://developer.hashicorp.com/terraform/install Manually create S3 Bucket to store remote state of the terraform deployment. To create S3 bucket you can use AWS Console or aws cli commands: a. If region us-east-1 aws s3api create-bucket --bucket <s3-bucket-for-terraform-state-name-example> b. If another region aws s3api create-bucket --region <your deploy aws region> --bucket <s3-bucket-for-terraform-state-name-example> --create-bucket-configuration LocationConstraint=<your deploy aws region> Manually create DynamoDB table to store terraform lock records. Table schema: LockID (String) - Partition key To create DynamoDB table you can use AWS Console or aws cli command: aws dynamodb create-table --table-name <dynamobd-table-to-store-terraform-lock-name-example> --attribute-definitions AttributeName=LockID,AttributeType=S --key-schema AttributeName=LockID,KeyType=HASH --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5 --region <your deploy aws region> The following resources are dependencies and should be created in advance: * VPC in your AWS account * Private subnets (where all infrastructure will be created: EKS cluster, RDS instance, FS, etc.) * Mechanism to have inbound access to the VPC (IGW, transit gateway, VPN from corporate network etc.) AWS Elastic IP allocation. Create AWS Elastic IP allocation to provide this value further during Cloud-Pipeline installation. This value will be used to deploy AWS ELB in your account to route the traffic from users to Cloud-Pipeline services This EIP also should be used to request DNS records creation from you DNS provider. The following scheme of the records is proposed: DNS record Record type Value . A < EIP value > edge. . CNAME . docker. . CNAME . git. . CNAME . Jump-server deployment To deploy required resources in your environment with terraform, please follow these steps: In the Git repo where you would like to store you terraform code: create directory named, for example, jump-server and place Terraform files there: main.tf and output.tf. Templates of the deployment Jump-server files: main.tf terraform { backend \"s3\" { bucket = \"xxxxxxxxxxxx-infra\" key = \"xxxxxx-jumpbox/terraform.tfstate\" region = \"<region>\" encrypt = true dynamodb_table = \"xxxxxxxxxxxx-infra\" } required_version = \"1.5.0\" } provider \"aws\" { region = \"<region>\" } module jump-server { source = \"git::https://github.com/epam/cloud-pipeline//deploy/infra/aws/terraform/cloud-native/jump-server?ref=<branch-tag-or-commit>\" project_name = \"xxxxxxxxxxxx\" env = \"xxxxxxx\" vpc_id = \"vpc-xxxxxxxxxxxx\" subnet_id = \"subnet-xxxxxxxxxxxx\" iam_role_permissions_boundary_arn = \"arn:aws:iam::xxxxxxxxxxxx:policy/eo_role_boundary\" } output.tf output \"instance_connection\" { value = module.jump-server.output_message } output \"instance_id\" { value = module.jump-server.jump_sever_id } output \"instance_role\" { value = module.jump-server.jump_server_role } Change xxxxxxxxxxxx to values that described is list of the variables: Name Description bucket Name of the created S3 bucket to store terraform state file. See the prerequisites dynamodb_table Name of the created DynamoDB table for terraform. See the prerequisites deployment_name Name of the deployment. Will be used as resource name prefix of the created resources (security groups, iam roles etc.) deployment_env Environment name for the deployment. Will be used as resource name prefix of the created resources (security groups, IAM roles etc.) vpc_id Id of the VCP to be used for deployment of the bastion instance. subnet_id Id of the VCP subnet to be used to launch an instance ami_id (Optional) AMI to be used for bastion ec2 instance. If empty - eks-optimized will be used. iam_role_permissions_boundary_arn (Optional) Account specific role boundaries which will be applied during jump-server instance profile creation Push created configuration in to your git repository. From jump-server directory run terraform init command, output of command must be like this: Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. After successful output of the terraform init command run terraform apply and when it shows list of the planned for creation resources submit with yes . Example of the apply output: Apply complete! Resources: ..... Outputs: instance_connection = \"Login to Jump Server with command: aws ssm start-session --target i-xxxxxxxxxxx --region <region>\" instance_id = \"i-xxxxxxxxxxxx\" instance_role = \"arn:aws:iam::xxxxxxxxxxx:role/xxxxxxxxxxx_BastionExecutionRole\" Outputs table of jump-server module Name Description instance_connection Id of created Jump Server instance. instance_id Login to Jump Server with command: aws ssm start-session --target ${module.ec2_instance.id} --region ${data.aws_region.current.name}. instance_role ARN of bastion execution role that must be set in EKS deployment module User can call terraform output again by run command: terraform output <output name from table above> Note: In most cases this command will only show output after resources were deployed with terraform apply command. Cluster-infrastructure deployment In the Git repo where you would like to store you terraform code: create directory named, for example, cluster-infrastructure and place your Terraform files: main.tf, output.tf and if you would like to deploy cloud-pipeline databases configuration - versions.tf. Template of the cluster-infrastructure files deployment: main.tf terraform { backend \"s3\" { bucket = \"xxxxxxxxxxxx-infra\" key = \"eks/terraform.tfstate\" region = \"<region>\" encrypt = true dynamodb_table = \"xxxxxxxxxxxx-infra\" } required_version = \"1.5.0\" } provider \"aws\" { region = \"<region>\" } provider \"kubernetes\" { host = module.cluster-infra.cluster_endpoint cluster_ca_certificate = base64decode(module.cluster-infra.cluster_certificate_authority_data) exec { api_version = \"client.authentication.k8s.io/v1beta1\" command = \"aws\" # This requires the awscli to be installed locally where Terraform is executed args = [\"eks\", \"get-token\", \"--cluster-name\", module.cluster-infra.cluster_name] } } provider \"helm\" { kubernetes { host = module.cluster-infra.cluster_endpoint cluster_ca_certificate = base64decode(module.cluster-infra.cluster_certificate_authority_data) exec { api_version = \"client.authentication.k8s.io/v1beta1\" command = \"aws\" # This requires the awscli to be installed locally where Terraform is executed args = [\"eks\", \"get-token\", \"--cluster-name\", module.cluster-infra.cluster_name] } } } provider \"postgresql\" { host = module.cluster-infra.rds_address port = module.cluster-infra.rds_port username = module.cluster-infra.rds_root_username password = module.cluster-infra.rds_root_pass_secret superuser = false } module \"cluster-infra\" { source = \"git::https://github.com/epam/cloud-pipeline//deploy/infra/aws/terraform/cloud-native/cluster-infra?ref=<branch-tag-or-commit>\" deployment_name = \"xxxxxxxxxxxx\" deployment_env = \"xxxxxxxxxxxx\" vpc_id = \"vpc-xxxxxxxxxxxx\" cp_api_access_prefix_lists = [\"pl-xxxxxxxxxxxx\"] external_access_security_group_ids = [\"xxxxxxxxxxxx\"] subnet_ids = [\"subnet-xxxxxxxxxxxx\", \"subnet-xxxxxxxxxxxx\", \"subnet-xxxxxxxxxxxx\"] iam_role_permissions_boundary_arn = \"arn:aws:iam::xxxxxxxxxxxx:policy/eo_role_boundary\" eks_system_node_group_subnet_ids = [\"subnet-xxxxxxxxxxxx\"] deploy_filesystem_type = \"xxxxxxxxxxxx\" cp_deployment_id = \"xxxxxxxxxxxx\" cp_edge_elb_schema = \"xxxxxxxxxxxx\" cp_edge_elb_subnet = \"xxxxxxxxxxxx\" cp_edge_elb_ip = \"xxxxxxxxxxxx\" cp_api_srv_host = \"xxxxxxxxxxxx\" cp_idp_host = \"xxxxxxxxxxxx\" cp_docker_host = \"xxxxxxxxxxxx\" cp_edge_host = \"xxxxxxxxxxxx\" cp_gitlab_host = \"xxxxxxxxxxxx\" eks_additional_role_mapping = [ { iam_role_arn = \"arn:aws:iam::xxxxxxxxxxxx:role/xxxxxxxxxx-BastionExecutionRole\" eks_role_name = \"system:node:{{EC2PrivateDNSName}}\" eks_groups = [\"system:bootstrappers\", \"system:nodes\"] } ] } versions.tf (if Cloud-Pipepline database configuration should be deployed): terraform { required_providers { postgresql = { source = \"cyrilgdn/postgresql\" version = \"1.21.0\" } } } output.tf output \"filesystem_mount\" { value = module.cluster-infra.cp_filesystem_mount_point } output \"filesystem_type\" { value = module.cluster-infra.deploy_filesystem_type } output \"cp_pipectl_script\" { value = module.cluster-infra.cp_deploy_script } To configure cluster-infrastructure deployment, there is a list of variables that need to be specified: Name Description bucket Name of the created S3 bucket to store terraform state file. See the prerequisites dynamodb_table Name of the created DynamoDB table for terraform. See the prerequisites deployment_name Name of the deployment. Will be used as resource name prefix of the created resources (security groups, IAM roles etc.) deployment_env Environment name for the deployment. Will be used as resource name prefix of the created resources (security groups, IAM roles etc.) vpc_id Id of the VCP to be used for deployment of the infrastructure. subnet_ids Ids of the VCP subnets to be used for Cloud Pipeline EKS cluster, FS mount points, etc. eks_system_node_group_subnet_ids Ids of the VCP subnets to be used for EKS cluster Cloud Pipeline system node group. eks_additional_role_mapping List of additional roles mapping for aws_auth map. deploy_filesystem_type (Optional) Option to create EFS or FSx Lustre filesystem: must be set efs or fsx. If empty, no FS will be created. Default efs. cloud_pipeline_db_configuration (Optional) Username with password and database, which will be created. Username will be owner of the database. Additional settings with Postgresql provider and versions.tf file must be set. For example see main.tf of the cluster deployment iam_role_permissions_boundary_arn (Optional) Account specific role boundaries which will be applied during jump-server instance profile creation deploy_rds (Optional) You can disable deployment of the RDS instance by setting deploy_rds = false. In this case no db configuration will be created regardless the value of create_cloud_pipeline_db_configuration create_cloud_pipeline_db_configuration (Optional) You can disable creation of the cloud-pipeline database configuration by setting to false cp_deployment_id (Optional) Specify unique ID of the Cloud-Pipeline deployment. It will be used to name cloud entities (e.g. path within a docker registry object container).Must contain only letters, digits, underscore or dash cp_edge_elb_schema (Required) Type of the AWS ELB to provide access to the users to the system. Possible values 'internal', 'internet-facing'. Default 'internet-facing'. cp_edge_elb_subnet (Required) The ID of the public subnet for the Load Balancer to be created. Must be in the same Availability Zone (AZ) as the CPSystemSubnetId cp_edge_elb_ip (Required) Allocation ID of the Elastic IP from prerequisites in case of internet-facing ELB, or private IP in case of internal ELB. cp_api_srv_host (Required) API service domain name address. cp_idp_host (Optional) Self hosted IDP service domain name address. WARNING: Using self hosted IDP service in production environment strongly not recommended! cp_docker_host (Required) Docker service domain name address. cp_edge_host (Required) EDGE service domain name address. cp_gitlab_host (Required) GITLAB service domain name address. Push created configuration in to your git repository. Install aws ssm manager: https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-working-with-install-plugin.html Connect to created jump-server instance using command like: aws ssm start-session --target i-xxxxxxxxxxxxx --region <region> Where xxxxxxxx is your jump-server instance ID that could be found(also with full command) from output of the terraform apply jump-server deployment. Clone from your git repository pushed previously configuration. From cluster-infrastructure directory run terraform init command, output of command must be like this: Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. After successful output of the init command run terraform apply and when it shows list of the planned for creation resources submit with yes . The output can be different depending on terraform options like cp_idp_host or enable_aws_omics_integration. Example of the apply output: Apply complete! Resources: ..... Outputs: cp_pipectl_script = <<EOT ./pipectl install \\ -dt aws-native \\ -jc \\ -env CP_MAIN_SERVICE_ROLE=\"arn:aws:iam::xxxxxxxxxxxxxxx:role/xxxxxxxxxxxxxxxCPExecutionRole\" \\ -env CP_CSI_DRIVER_TYPE=\"efs\" \\ -env CP_SYSTEM_FILESYSTEM_ID=\"fs-xxxxxxxxxxxxxxx\" \\ -env CP_SYSTEM_FILESYSTEM_MOUNTNAME=\"\" \\ -env CP_CSI_EXECUTION_ROLE=\"arn:aws:iam::xxxxxxxxxxxxxxx:role/xxxxxxxxxxxxxxx-ExecutionRole\" \\ -env CP_DOCKER_DIST_SRV=\"quay.io/\" \\ -env CP_AWS_KMS_ARN=\"arn:aws:kms:xxxxxxxxxxxxxxx:xxxxxxxxxxxxxxx:key/xxxxxxxxxxxxxxx\" \\ -env CP_PREF_CLUSTER_SSH_KEY_NAME=\"xxxxxxxxxxxxxxx\" \\ -env CP_PREF_CLUSTER_INSTANCE_SECURITY_GROUPS=\"sg-xxxxxxxxxxxxxxx\" \\ -env CP_PREF_STORAGE_TEMP_CREDENTIALS_ROLE=\"arn:aws:iam::xxxxxxxxxxxxxxx:role/xxxxxxxxxxxxxxxS3viaSTSRole\" \\ -env CP_CLUSTER_SSH_KEY=\"/opt/root/ssh/ssh-key.pem\" \\ -env CP_DOCKER_STORAGE_TYPE=\"obj\" \\ -env CP_DOCKER_STORAGE_CONTAINER=\"xxxxxxxxxxxxxxx-docker\" \\ -env CP_DEPLOYMENT_ID=\"<users deployment id>\" \\ -env CP_PREF_UI_PIPELINE_DEPLOYMENT_NAME=\"<users deployment name>\" \\ -env CP_CLOUD_REGION_ID=\"xxxxxxxxxxxxxxx\" \\ -env CP_KUBE_CLUSTER_NAME=\"xxxxxxxxxxxxxxx-eks-cluster\" \\ -env CP_KUBE_EXTERNAL_HOST=\"https://xxxxxxxxxxxxxxx.gr7.xxxxxxxxxxxxxxx.eks.amazonaws.com\" \\ -env CP_KUBE_SERVICES_TYPE=\"ingress\" \\ --external-host-dns \\ -env PSG_HOST=\"xxxxxxxxxxxxxxx-rds.xxxxxxxxxxxxxxx.xxxxxxxxxxxxxxx.rds.amazonaws.com\" \\ -env PSG_PASS=\"pipeline\" \\ -env PSG_CONNECT_PARAMS=\"?ssl=true&sslfactory=org.postgresql.ssl.NonValidatingFactory\" \\ -s cp-api-srv \\ -env CP_API_SRV_EXTERNAL_PORT=443 \\ -env CP_API_SRV_INTERNAL_PORT=443 \\ -env CP_API_SRV_EXTERNAL_HOST=\"<user-domain-name>\" \\ -env CP_API_SRV_INTERNAL_HOST=\"<user-domain-name>\" \\ -env CP_API_SRV_IDP_CERT_PATH=\"/opt/idp/pki\" \\ -env CP_PREF_UI_PIPELINE_DEPLOYMENT_NAME=\"<users deployment name>\" \\ -env CP_PREF_STORAGE_SYSTEM_STORAGE_NAME=\"xxxxxxxxxxxxxxx\" \\ -env CP_API_SRV_SSO_BINDING=\"urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST\" \\ -env CP_API_SRV_SAML_ALLOW_ANONYMOUS_USER=\"true\" \\ -env CP_API_SRV_SAML_AUTO_USER_CREATE=\"EXPLICIT\" \\ -env CP_API_SRV_SAML_GROUPS_ATTRIBUTE_NAME=\"Group\" \\ -env CP_HA_DEPLOY_ENABLED=\"true\" \\ -s cp-docker-registry \\ -env CP_DOCKER_EXTERNAL_PORT=443 \\ -env CP_DOCKER_INTERNAL_PORT=443 \\ -env CP_DOCKER_EXTERNAL_HOST=\"docker.<user-domain-name>\" \\ -env CP_DOCKER_INTERNAL_HOST=\"docker.<user-domain-name>\" \\ -env CP_DOCKER_STORAGE_ROOT_DIR=\"/docker-pub/\" \\ -s cp-edge \\ -env CP_EDGE_EXTERNAL_PORT=443 \\ -env CP_EDGE_INTERNAL_PORT=443 \\ -env CP_EDGE_EXTERNAL_HOST=\"edge.<user-domain-name>\" \\ -env CP_EDGE_INTERNAL_HOST=\"edge.<user-domain-name>\" \\ -env CP_EDGE_WEB_CLIENT_MAX_SIZE=0 \\ -s cp-clair \\ -env CP_CLAIR_DATABASE_HOST=\"xxxxxxxxxxxxxxx-rds.xxxxxxxxxxxxxxx.xxxxxxxxxxxxxxx.rds.amazonaws.com\" \\ -env CP_CLAIR_DATABASE_PASSWORD=\"clair\" \\ -env CP_CLAIR_DATABASE_SSL_MODE=\"require\" \\ -s cp-docker-comp \\ -env CP_DOCKER_COMP_WORKING_DIR=\"/cloud-pipeline/docker-comp/wd\" \\ -s cp-search \\ -s cp-heapster \\ -s cp-dav \\ -env CP_DAV_AUTH_URL_PATH=\"webdav/auth-sso\" \\ -env CP_DAV_MOUNT_POINT=\"/dav-mount\" \\ -env CP_DAV_SERVE_DIR=\"/dav-serve\" \\ -env CP_DAV_URL_PATH=\"webdav\" \\ -s cp-gitlab-db \\ -env GITLAB_DATABASE_VERSION=\"12.18\" \\ -s cp-git \\ -env CP_GITLAB_VERSION=15 \\ -env CP_GITLAB_SESSION_API_DISABLE=\"true\" \\ -env CP_GITLAB_API_VERSION=v4 \\ -env CP_GITLAB_EXTERNAL_PORT=443 \\ -env CP_GITLAB_INTERNAL_PORT=443 \\ -env CP_GITLAB_EXTERNAL_HOST=\"git.<user-domain-name>\" \\ -env CP_GITLAB_INTERNAL_HOST=\"git.<user-domain-name>\" \\ -env CP_GITLAB_EXTERNAL_URL=\"https://git.<user-domain-name>\" \\ -env CP_GITLAB_IDP_CERT_PATH=\"/opt/idp/pki\" \\ -s cp-git-sync \\ -s cp-billing-srv \\ -env CP_BILLING_DISABLE_GS=\"true\" \\ -env CP_BILLING_DISABLE_AZURE_BLOB=\"true\" \\ -env CP_BILLING_CENTER_KEY=\"billing-group\" \\ -s cp-idp \\ -env CP_IDP_EXTERNAL_HOST=\"auth.<user-domain-name>\" \\ -env CP_IDP_INTERNAL_HOST=\"auth.<user-domain-name>\" \\ -env CP_IDP_EXTERNAL_PORT=443 \\ -env CP_IDP_INTERNAL_PORT=443 \\ -env CP_PREF_AWS_OMICS_SERVICE_ROLE=arn:aws:iam::xxxxxxxxxxxxxxx:role/xxxxxxxxxxxxxxxOmicsServiceRole \\ -env CP_PREF_AWS_OMICS_ECR_REGISTRY=xxxxxxxxxxxxxxx.dkr.ecr.xxxxxxxxxxxxxxx.amazonaws.com \\ -env CP_EDGE_AWS_ELB_SCHEME=\"internet-facing\" \\ -env CP_EDGE_AWS_ELB_SUBNETS=\"subnet-xxxxxxxxxxxxxxx\" \\ -env CP_EDGE_AWS_ELB_SG=\",sg-xxxxxxxxxxxxxxx,sg-xxxxxxxxxxxxxxx\" \\ -env CP_EDGE_AWS_ELB_EIPALLOCS=\"eipalloc-xxxxxxxxxxxxxxx\" \\ -env CP_EDGE_KUBE_SERVICES_TYPE=elb EOT filesystem_mount = \"fs-xxxxxxxxxxxxxxx.efs.xxxxxxxxxxxxxxx.amazonaws.com:/\" filesystem_type = \"efs\" Outputs table of cluster-infrastructure module Name Description | filesystem_mount | Filesystem mount endpoint that can be used to mount ElasticFileSystem in EC2 JumpServer | | filesystem_type | Type of the created internet file system | | cp_pipectl_script | Example of the pipeline install script with all necessary values from infrastructure deployment | User can call terraform output again by run command: terraform output <output name from table above> Note: You should deploy infrastructure first, to see output values. Cloud-pipeline deployment Download latest pipectl binary file. Mount created file system into instance. For EFS run commands (https://docs.aws.amazon.com/efs/latest/ug/mounting-fs-mount-cmd-dns-name.html): fs_mount=$(terraform output -raw filesystem_mount) sudo yum install amazon-efs-utils -y sudo mount -t efs -o tls $fs_mount /opt For FSx for Lustre (https://docs.aws.amazon.com/fsx/latest/LustreGuide/mounting-ec2-instance.html): fs_mount=$(terraform output -raw filesystem_mount) sudo amazon-linux-extras install -y lustre sudo mount -t lustre -o relatime,flock $fs_mount /opt Create ssh key from cluster-infrastructure deployment: sudo mkdir -p /opt/root/ssh terraform show -json | jq -r \".values.root_module.child_modules[].resources[] | select(.address==\\\"$(terraform state list | grep ssh_tls_key)\\\") |.values.private_key_pem\" > /opt/root/ssh/ssh-key.pem Take script from the cluster-infrastructure deployment output and run it by using bash commands. For example: CP_PIPECTL_URL=https://cloud-pipeline-oss-builds.s3.amazonaws.com/builds/<link-to-the-desired-pipectl-version> \\ wget -c $CP_PIPECTL_URL -O pipectl && chmod +x pipectl \\ terraform output -raw cp_pipectl_script > \"deploy_cloud_pipeline.sh\" && \\ chmod +x deploy_cloud_pipeline.sh \\ ./deploy_cloud_pipeline.sh &> pipectl.log Wait until deployment finishes. Your Cloud-Pipeline environment should be available on the provided DNS name provided during deployment ( cp_api_srv_host ).","title":"Terraform"},{"location":"installation/deployment/aws/terraform/#cloud-pipeline-based-on-aws-eks-deployment-step-by-step-guide","text":"","title":"Cloud-pipeline based on AWS EKS Deployment step-by-step guide"},{"location":"installation/deployment/aws/terraform/#overview","text":"This document provides a guidance how to deploy infrastructure using terraform and install Cloud-Pipeline on top of it. The process of the deployment can be performed with the following steps: Cloud-pipeline based on AWS EKS Deployment step-by-step guide Overview Resources deployment using Terraform Prerequisites Jump-server deployment Outputs table of jump-server module Cluster-infrastructure deployment Outputs table of cluster-infrastructure module Cloud-pipeline deployment This guide assumes that you store your terraform root modules in a private git repository and able to clone it from remote instance.","title":"Overview"},{"location":"installation/deployment/aws/terraform/#resources-deployment-using-terraform","text":"To get started with deployment, please make sure that you satisfy requirements below.","title":"Resources deployment using Terraform"},{"location":"installation/deployment/aws/terraform/#prerequisites","text":"Name Version terraform = 1.5.0 To install terraform 1.50 on Linux amd64 type of OS you can run commands: sudo wget https://releases.hashicorp.com/terraform/1.5.0/terraform_1.5.0_linux_amd64.zip sudo unzip terraform_1.5.0_linux_amd64.zip chmod +x terraform sudo mv terraform /usr/local/bin/ sudo rm terraform_1.5.0_linux_amd64.zip To install terraform to other operating system please follow the links https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli https://developer.hashicorp.com/terraform/install Manually create S3 Bucket to store remote state of the terraform deployment. To create S3 bucket you can use AWS Console or aws cli commands: a. If region us-east-1 aws s3api create-bucket --bucket <s3-bucket-for-terraform-state-name-example> b. If another region aws s3api create-bucket --region <your deploy aws region> --bucket <s3-bucket-for-terraform-state-name-example> --create-bucket-configuration LocationConstraint=<your deploy aws region> Manually create DynamoDB table to store terraform lock records. Table schema: LockID (String) - Partition key To create DynamoDB table you can use AWS Console or aws cli command: aws dynamodb create-table --table-name <dynamobd-table-to-store-terraform-lock-name-example> --attribute-definitions AttributeName=LockID,AttributeType=S --key-schema AttributeName=LockID,KeyType=HASH --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5 --region <your deploy aws region> The following resources are dependencies and should be created in advance: * VPC in your AWS account * Private subnets (where all infrastructure will be created: EKS cluster, RDS instance, FS, etc.) * Mechanism to have inbound access to the VPC (IGW, transit gateway, VPN from corporate network etc.) AWS Elastic IP allocation. Create AWS Elastic IP allocation to provide this value further during Cloud-Pipeline installation. This value will be used to deploy AWS ELB in your account to route the traffic from users to Cloud-Pipeline services This EIP also should be used to request DNS records creation from you DNS provider. The following scheme of the records is proposed: DNS record Record type Value . A < EIP value > edge. . CNAME . docker. . CNAME . git. . CNAME .","title":"Prerequisites"},{"location":"installation/deployment/aws/terraform/#jump-server-deployment","text":"To deploy required resources in your environment with terraform, please follow these steps: In the Git repo where you would like to store you terraform code: create directory named, for example, jump-server and place Terraform files there: main.tf and output.tf. Templates of the deployment Jump-server files: main.tf terraform { backend \"s3\" { bucket = \"xxxxxxxxxxxx-infra\" key = \"xxxxxx-jumpbox/terraform.tfstate\" region = \"<region>\" encrypt = true dynamodb_table = \"xxxxxxxxxxxx-infra\" } required_version = \"1.5.0\" } provider \"aws\" { region = \"<region>\" } module jump-server { source = \"git::https://github.com/epam/cloud-pipeline//deploy/infra/aws/terraform/cloud-native/jump-server?ref=<branch-tag-or-commit>\" project_name = \"xxxxxxxxxxxx\" env = \"xxxxxxx\" vpc_id = \"vpc-xxxxxxxxxxxx\" subnet_id = \"subnet-xxxxxxxxxxxx\" iam_role_permissions_boundary_arn = \"arn:aws:iam::xxxxxxxxxxxx:policy/eo_role_boundary\" } output.tf output \"instance_connection\" { value = module.jump-server.output_message } output \"instance_id\" { value = module.jump-server.jump_sever_id } output \"instance_role\" { value = module.jump-server.jump_server_role } Change xxxxxxxxxxxx to values that described is list of the variables: Name Description bucket Name of the created S3 bucket to store terraform state file. See the prerequisites dynamodb_table Name of the created DynamoDB table for terraform. See the prerequisites deployment_name Name of the deployment. Will be used as resource name prefix of the created resources (security groups, iam roles etc.) deployment_env Environment name for the deployment. Will be used as resource name prefix of the created resources (security groups, IAM roles etc.) vpc_id Id of the VCP to be used for deployment of the bastion instance. subnet_id Id of the VCP subnet to be used to launch an instance ami_id (Optional) AMI to be used for bastion ec2 instance. If empty - eks-optimized will be used. iam_role_permissions_boundary_arn (Optional) Account specific role boundaries which will be applied during jump-server instance profile creation Push created configuration in to your git repository. From jump-server directory run terraform init command, output of command must be like this: Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. After successful output of the terraform init command run terraform apply and when it shows list of the planned for creation resources submit with yes . Example of the apply output: Apply complete! Resources: ..... Outputs: instance_connection = \"Login to Jump Server with command: aws ssm start-session --target i-xxxxxxxxxxx --region <region>\" instance_id = \"i-xxxxxxxxxxxx\" instance_role = \"arn:aws:iam::xxxxxxxxxxx:role/xxxxxxxxxxx_BastionExecutionRole\"","title":"Jump-server deployment"},{"location":"installation/deployment/aws/terraform/#outputs-table-of-jump-server-module","text":"Name Description instance_connection Id of created Jump Server instance. instance_id Login to Jump Server with command: aws ssm start-session --target ${module.ec2_instance.id} --region ${data.aws_region.current.name}. instance_role ARN of bastion execution role that must be set in EKS deployment module User can call terraform output again by run command: terraform output <output name from table above> Note: In most cases this command will only show output after resources were deployed with terraform apply command.","title":"Outputs table of jump-server module"},{"location":"installation/deployment/aws/terraform/#cluster-infrastructure-deployment","text":"In the Git repo where you would like to store you terraform code: create directory named, for example, cluster-infrastructure and place your Terraform files: main.tf, output.tf and if you would like to deploy cloud-pipeline databases configuration - versions.tf. Template of the cluster-infrastructure files deployment: main.tf terraform { backend \"s3\" { bucket = \"xxxxxxxxxxxx-infra\" key = \"eks/terraform.tfstate\" region = \"<region>\" encrypt = true dynamodb_table = \"xxxxxxxxxxxx-infra\" } required_version = \"1.5.0\" } provider \"aws\" { region = \"<region>\" } provider \"kubernetes\" { host = module.cluster-infra.cluster_endpoint cluster_ca_certificate = base64decode(module.cluster-infra.cluster_certificate_authority_data) exec { api_version = \"client.authentication.k8s.io/v1beta1\" command = \"aws\" # This requires the awscli to be installed locally where Terraform is executed args = [\"eks\", \"get-token\", \"--cluster-name\", module.cluster-infra.cluster_name] } } provider \"helm\" { kubernetes { host = module.cluster-infra.cluster_endpoint cluster_ca_certificate = base64decode(module.cluster-infra.cluster_certificate_authority_data) exec { api_version = \"client.authentication.k8s.io/v1beta1\" command = \"aws\" # This requires the awscli to be installed locally where Terraform is executed args = [\"eks\", \"get-token\", \"--cluster-name\", module.cluster-infra.cluster_name] } } } provider \"postgresql\" { host = module.cluster-infra.rds_address port = module.cluster-infra.rds_port username = module.cluster-infra.rds_root_username password = module.cluster-infra.rds_root_pass_secret superuser = false } module \"cluster-infra\" { source = \"git::https://github.com/epam/cloud-pipeline//deploy/infra/aws/terraform/cloud-native/cluster-infra?ref=<branch-tag-or-commit>\" deployment_name = \"xxxxxxxxxxxx\" deployment_env = \"xxxxxxxxxxxx\" vpc_id = \"vpc-xxxxxxxxxxxx\" cp_api_access_prefix_lists = [\"pl-xxxxxxxxxxxx\"] external_access_security_group_ids = [\"xxxxxxxxxxxx\"] subnet_ids = [\"subnet-xxxxxxxxxxxx\", \"subnet-xxxxxxxxxxxx\", \"subnet-xxxxxxxxxxxx\"] iam_role_permissions_boundary_arn = \"arn:aws:iam::xxxxxxxxxxxx:policy/eo_role_boundary\" eks_system_node_group_subnet_ids = [\"subnet-xxxxxxxxxxxx\"] deploy_filesystem_type = \"xxxxxxxxxxxx\" cp_deployment_id = \"xxxxxxxxxxxx\" cp_edge_elb_schema = \"xxxxxxxxxxxx\" cp_edge_elb_subnet = \"xxxxxxxxxxxx\" cp_edge_elb_ip = \"xxxxxxxxxxxx\" cp_api_srv_host = \"xxxxxxxxxxxx\" cp_idp_host = \"xxxxxxxxxxxx\" cp_docker_host = \"xxxxxxxxxxxx\" cp_edge_host = \"xxxxxxxxxxxx\" cp_gitlab_host = \"xxxxxxxxxxxx\" eks_additional_role_mapping = [ { iam_role_arn = \"arn:aws:iam::xxxxxxxxxxxx:role/xxxxxxxxxx-BastionExecutionRole\" eks_role_name = \"system:node:{{EC2PrivateDNSName}}\" eks_groups = [\"system:bootstrappers\", \"system:nodes\"] } ] } versions.tf (if Cloud-Pipepline database configuration should be deployed): terraform { required_providers { postgresql = { source = \"cyrilgdn/postgresql\" version = \"1.21.0\" } } } output.tf output \"filesystem_mount\" { value = module.cluster-infra.cp_filesystem_mount_point } output \"filesystem_type\" { value = module.cluster-infra.deploy_filesystem_type } output \"cp_pipectl_script\" { value = module.cluster-infra.cp_deploy_script } To configure cluster-infrastructure deployment, there is a list of variables that need to be specified: Name Description bucket Name of the created S3 bucket to store terraform state file. See the prerequisites dynamodb_table Name of the created DynamoDB table for terraform. See the prerequisites deployment_name Name of the deployment. Will be used as resource name prefix of the created resources (security groups, IAM roles etc.) deployment_env Environment name for the deployment. Will be used as resource name prefix of the created resources (security groups, IAM roles etc.) vpc_id Id of the VCP to be used for deployment of the infrastructure. subnet_ids Ids of the VCP subnets to be used for Cloud Pipeline EKS cluster, FS mount points, etc. eks_system_node_group_subnet_ids Ids of the VCP subnets to be used for EKS cluster Cloud Pipeline system node group. eks_additional_role_mapping List of additional roles mapping for aws_auth map. deploy_filesystem_type (Optional) Option to create EFS or FSx Lustre filesystem: must be set efs or fsx. If empty, no FS will be created. Default efs. cloud_pipeline_db_configuration (Optional) Username with password and database, which will be created. Username will be owner of the database. Additional settings with Postgresql provider and versions.tf file must be set. For example see main.tf of the cluster deployment iam_role_permissions_boundary_arn (Optional) Account specific role boundaries which will be applied during jump-server instance profile creation deploy_rds (Optional) You can disable deployment of the RDS instance by setting deploy_rds = false. In this case no db configuration will be created regardless the value of create_cloud_pipeline_db_configuration create_cloud_pipeline_db_configuration (Optional) You can disable creation of the cloud-pipeline database configuration by setting to false cp_deployment_id (Optional) Specify unique ID of the Cloud-Pipeline deployment. It will be used to name cloud entities (e.g. path within a docker registry object container).Must contain only letters, digits, underscore or dash cp_edge_elb_schema (Required) Type of the AWS ELB to provide access to the users to the system. Possible values 'internal', 'internet-facing'. Default 'internet-facing'. cp_edge_elb_subnet (Required) The ID of the public subnet for the Load Balancer to be created. Must be in the same Availability Zone (AZ) as the CPSystemSubnetId cp_edge_elb_ip (Required) Allocation ID of the Elastic IP from prerequisites in case of internet-facing ELB, or private IP in case of internal ELB. cp_api_srv_host (Required) API service domain name address. cp_idp_host (Optional) Self hosted IDP service domain name address. WARNING: Using self hosted IDP service in production environment strongly not recommended! cp_docker_host (Required) Docker service domain name address. cp_edge_host (Required) EDGE service domain name address. cp_gitlab_host (Required) GITLAB service domain name address. Push created configuration in to your git repository. Install aws ssm manager: https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-working-with-install-plugin.html Connect to created jump-server instance using command like: aws ssm start-session --target i-xxxxxxxxxxxxx --region <region> Where xxxxxxxx is your jump-server instance ID that could be found(also with full command) from output of the terraform apply jump-server deployment. Clone from your git repository pushed previously configuration. From cluster-infrastructure directory run terraform init command, output of command must be like this: Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. After successful output of the init command run terraform apply and when it shows list of the planned for creation resources submit with yes . The output can be different depending on terraform options like cp_idp_host or enable_aws_omics_integration. Example of the apply output: Apply complete! Resources: ..... Outputs: cp_pipectl_script = <<EOT ./pipectl install \\ -dt aws-native \\ -jc \\ -env CP_MAIN_SERVICE_ROLE=\"arn:aws:iam::xxxxxxxxxxxxxxx:role/xxxxxxxxxxxxxxxCPExecutionRole\" \\ -env CP_CSI_DRIVER_TYPE=\"efs\" \\ -env CP_SYSTEM_FILESYSTEM_ID=\"fs-xxxxxxxxxxxxxxx\" \\ -env CP_SYSTEM_FILESYSTEM_MOUNTNAME=\"\" \\ -env CP_CSI_EXECUTION_ROLE=\"arn:aws:iam::xxxxxxxxxxxxxxx:role/xxxxxxxxxxxxxxx-ExecutionRole\" \\ -env CP_DOCKER_DIST_SRV=\"quay.io/\" \\ -env CP_AWS_KMS_ARN=\"arn:aws:kms:xxxxxxxxxxxxxxx:xxxxxxxxxxxxxxx:key/xxxxxxxxxxxxxxx\" \\ -env CP_PREF_CLUSTER_SSH_KEY_NAME=\"xxxxxxxxxxxxxxx\" \\ -env CP_PREF_CLUSTER_INSTANCE_SECURITY_GROUPS=\"sg-xxxxxxxxxxxxxxx\" \\ -env CP_PREF_STORAGE_TEMP_CREDENTIALS_ROLE=\"arn:aws:iam::xxxxxxxxxxxxxxx:role/xxxxxxxxxxxxxxxS3viaSTSRole\" \\ -env CP_CLUSTER_SSH_KEY=\"/opt/root/ssh/ssh-key.pem\" \\ -env CP_DOCKER_STORAGE_TYPE=\"obj\" \\ -env CP_DOCKER_STORAGE_CONTAINER=\"xxxxxxxxxxxxxxx-docker\" \\ -env CP_DEPLOYMENT_ID=\"<users deployment id>\" \\ -env CP_PREF_UI_PIPELINE_DEPLOYMENT_NAME=\"<users deployment name>\" \\ -env CP_CLOUD_REGION_ID=\"xxxxxxxxxxxxxxx\" \\ -env CP_KUBE_CLUSTER_NAME=\"xxxxxxxxxxxxxxx-eks-cluster\" \\ -env CP_KUBE_EXTERNAL_HOST=\"https://xxxxxxxxxxxxxxx.gr7.xxxxxxxxxxxxxxx.eks.amazonaws.com\" \\ -env CP_KUBE_SERVICES_TYPE=\"ingress\" \\ --external-host-dns \\ -env PSG_HOST=\"xxxxxxxxxxxxxxx-rds.xxxxxxxxxxxxxxx.xxxxxxxxxxxxxxx.rds.amazonaws.com\" \\ -env PSG_PASS=\"pipeline\" \\ -env PSG_CONNECT_PARAMS=\"?ssl=true&sslfactory=org.postgresql.ssl.NonValidatingFactory\" \\ -s cp-api-srv \\ -env CP_API_SRV_EXTERNAL_PORT=443 \\ -env CP_API_SRV_INTERNAL_PORT=443 \\ -env CP_API_SRV_EXTERNAL_HOST=\"<user-domain-name>\" \\ -env CP_API_SRV_INTERNAL_HOST=\"<user-domain-name>\" \\ -env CP_API_SRV_IDP_CERT_PATH=\"/opt/idp/pki\" \\ -env CP_PREF_UI_PIPELINE_DEPLOYMENT_NAME=\"<users deployment name>\" \\ -env CP_PREF_STORAGE_SYSTEM_STORAGE_NAME=\"xxxxxxxxxxxxxxx\" \\ -env CP_API_SRV_SSO_BINDING=\"urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST\" \\ -env CP_API_SRV_SAML_ALLOW_ANONYMOUS_USER=\"true\" \\ -env CP_API_SRV_SAML_AUTO_USER_CREATE=\"EXPLICIT\" \\ -env CP_API_SRV_SAML_GROUPS_ATTRIBUTE_NAME=\"Group\" \\ -env CP_HA_DEPLOY_ENABLED=\"true\" \\ -s cp-docker-registry \\ -env CP_DOCKER_EXTERNAL_PORT=443 \\ -env CP_DOCKER_INTERNAL_PORT=443 \\ -env CP_DOCKER_EXTERNAL_HOST=\"docker.<user-domain-name>\" \\ -env CP_DOCKER_INTERNAL_HOST=\"docker.<user-domain-name>\" \\ -env CP_DOCKER_STORAGE_ROOT_DIR=\"/docker-pub/\" \\ -s cp-edge \\ -env CP_EDGE_EXTERNAL_PORT=443 \\ -env CP_EDGE_INTERNAL_PORT=443 \\ -env CP_EDGE_EXTERNAL_HOST=\"edge.<user-domain-name>\" \\ -env CP_EDGE_INTERNAL_HOST=\"edge.<user-domain-name>\" \\ -env CP_EDGE_WEB_CLIENT_MAX_SIZE=0 \\ -s cp-clair \\ -env CP_CLAIR_DATABASE_HOST=\"xxxxxxxxxxxxxxx-rds.xxxxxxxxxxxxxxx.xxxxxxxxxxxxxxx.rds.amazonaws.com\" \\ -env CP_CLAIR_DATABASE_PASSWORD=\"clair\" \\ -env CP_CLAIR_DATABASE_SSL_MODE=\"require\" \\ -s cp-docker-comp \\ -env CP_DOCKER_COMP_WORKING_DIR=\"/cloud-pipeline/docker-comp/wd\" \\ -s cp-search \\ -s cp-heapster \\ -s cp-dav \\ -env CP_DAV_AUTH_URL_PATH=\"webdav/auth-sso\" \\ -env CP_DAV_MOUNT_POINT=\"/dav-mount\" \\ -env CP_DAV_SERVE_DIR=\"/dav-serve\" \\ -env CP_DAV_URL_PATH=\"webdav\" \\ -s cp-gitlab-db \\ -env GITLAB_DATABASE_VERSION=\"12.18\" \\ -s cp-git \\ -env CP_GITLAB_VERSION=15 \\ -env CP_GITLAB_SESSION_API_DISABLE=\"true\" \\ -env CP_GITLAB_API_VERSION=v4 \\ -env CP_GITLAB_EXTERNAL_PORT=443 \\ -env CP_GITLAB_INTERNAL_PORT=443 \\ -env CP_GITLAB_EXTERNAL_HOST=\"git.<user-domain-name>\" \\ -env CP_GITLAB_INTERNAL_HOST=\"git.<user-domain-name>\" \\ -env CP_GITLAB_EXTERNAL_URL=\"https://git.<user-domain-name>\" \\ -env CP_GITLAB_IDP_CERT_PATH=\"/opt/idp/pki\" \\ -s cp-git-sync \\ -s cp-billing-srv \\ -env CP_BILLING_DISABLE_GS=\"true\" \\ -env CP_BILLING_DISABLE_AZURE_BLOB=\"true\" \\ -env CP_BILLING_CENTER_KEY=\"billing-group\" \\ -s cp-idp \\ -env CP_IDP_EXTERNAL_HOST=\"auth.<user-domain-name>\" \\ -env CP_IDP_INTERNAL_HOST=\"auth.<user-domain-name>\" \\ -env CP_IDP_EXTERNAL_PORT=443 \\ -env CP_IDP_INTERNAL_PORT=443 \\ -env CP_PREF_AWS_OMICS_SERVICE_ROLE=arn:aws:iam::xxxxxxxxxxxxxxx:role/xxxxxxxxxxxxxxxOmicsServiceRole \\ -env CP_PREF_AWS_OMICS_ECR_REGISTRY=xxxxxxxxxxxxxxx.dkr.ecr.xxxxxxxxxxxxxxx.amazonaws.com \\ -env CP_EDGE_AWS_ELB_SCHEME=\"internet-facing\" \\ -env CP_EDGE_AWS_ELB_SUBNETS=\"subnet-xxxxxxxxxxxxxxx\" \\ -env CP_EDGE_AWS_ELB_SG=\",sg-xxxxxxxxxxxxxxx,sg-xxxxxxxxxxxxxxx\" \\ -env CP_EDGE_AWS_ELB_EIPALLOCS=\"eipalloc-xxxxxxxxxxxxxxx\" \\ -env CP_EDGE_KUBE_SERVICES_TYPE=elb EOT filesystem_mount = \"fs-xxxxxxxxxxxxxxx.efs.xxxxxxxxxxxxxxx.amazonaws.com:/\" filesystem_type = \"efs\"","title":"Cluster-infrastructure deployment"},{"location":"installation/deployment/aws/terraform/#outputs-table-of-cluster-infrastructure-module","text":"Name Description | filesystem_mount | Filesystem mount endpoint that can be used to mount ElasticFileSystem in EC2 JumpServer | | filesystem_type | Type of the created internet file system | | cp_pipectl_script | Example of the pipeline install script with all necessary values from infrastructure deployment | User can call terraform output again by run command: terraform output <output name from table above> Note: You should deploy infrastructure first, to see output values.","title":"Outputs table of cluster-infrastructure module"},{"location":"installation/deployment/aws/terraform/#cloud-pipeline-deployment","text":"Download latest pipectl binary file. Mount created file system into instance. For EFS run commands (https://docs.aws.amazon.com/efs/latest/ug/mounting-fs-mount-cmd-dns-name.html): fs_mount=$(terraform output -raw filesystem_mount) sudo yum install amazon-efs-utils -y sudo mount -t efs -o tls $fs_mount /opt For FSx for Lustre (https://docs.aws.amazon.com/fsx/latest/LustreGuide/mounting-ec2-instance.html): fs_mount=$(terraform output -raw filesystem_mount) sudo amazon-linux-extras install -y lustre sudo mount -t lustre -o relatime,flock $fs_mount /opt Create ssh key from cluster-infrastructure deployment: sudo mkdir -p /opt/root/ssh terraform show -json | jq -r \".values.root_module.child_modules[].resources[] | select(.address==\\\"$(terraform state list | grep ssh_tls_key)\\\") |.values.private_key_pem\" > /opt/root/ssh/ssh-key.pem Take script from the cluster-infrastructure deployment output and run it by using bash commands. For example: CP_PIPECTL_URL=https://cloud-pipeline-oss-builds.s3.amazonaws.com/builds/<link-to-the-desired-pipectl-version> \\ wget -c $CP_PIPECTL_URL -O pipectl && chmod +x pipectl \\ terraform output -raw cp_pipectl_script > \"deploy_cloud_pipeline.sh\" && \\ chmod +x deploy_cloud_pipeline.sh \\ ./deploy_cloud_pipeline.sh &> pipectl.log Wait until deployment finishes. Your Cloud-Pipeline environment should be available on the provided DNS name provided during deployment ( cp_api_srv_host ).","title":"Cloud-pipeline deployment"},{"location":"installation/management/environments_sync/","text":"Environments synchronization In some cases, you may need to synchronize two different environments of the Cloud Pipeline. Special routine in the pipectl utility shall be used for that - pipectl sync . It allows to synchronize from the source environment to the destination one the following objects: users / user groups / user roles docker registry / tool groups / tools Synchronization can be performed with or without synchronization of attributes (metadata) for the specified Platform objects. During the synchronization, changes are being performed only in the destination environment, the source environment remains the same. Prerequisites For the correct work, launch pipectl sync utility only at Linux machine, in the separate terminal, not from the run. If you are going to synchronize tools between environments, Docker shall be installed at the machine as well. Users synchronization process The format of the command to synchronize users between environments: ./pipectl sync --users --source-url <source_url> --source-token <source_token> --target-url <target_url> --target-token <target_token> Note : the command above shall be performed with admin rights in the the same directory where pipectl utility was downloaded. Details: <source_url> is the source environment address. Shall be specified in the following format: <protocol> :// <IP/hostname> : <port> (e.g. https://cloud-pipeline.epam.com:443 ) <source_token> is the source environment's access token. It can be obtained via the GUI (at the \"Pipe CLI\" settings form ) or via pipe token CLI command <target_url> is the destination environment address. Shall be specified in the following format: <protocol> :// <IP/hostname> : <port> (e.g. https://cloud-pipeline-dev.epam.com:443 ) <target_token> is the destination environment's access token. It can be obtained via the GUI (at the \"Pipe CLI\" settings form ) or via pipe token CLI command During the --users synchronization, the following processes are being performed: Roles/groups synchronization roles/groups from the source environment, that are missing in the destination environment, are being created in the destination environment (\"blocked\" statuses are being set according to the source environment as well) for roles/groups from the destination environment matching roles/groups from the source environment, their blocked status are being overridden: if a group/role is blocked in the source environment - that status will be set for the group/role in the destination environment too if a group/role is not blocked in the source environment but blocked in the destination environment - that status will be removed in the destination environment Users synchronization users from the source environment, that are missing in the destination environment, are being created in the destination environment (\"blocked\" statuses, groups/roles lists are being set according to the source environment) for users from the destination environment matching users from the source environment, their blocked statuses are being overridden: if a user is blocked in the source environment - that status is being set for the user in the destination environment too if a user is not blocked in the source environment but blocked in the destination environment - that status will be removed in the destination environment for users from the destination environment matching users from the source environment, their groups/roles lists are being merged with the corresponding lists of the source environment Metadata synchronization by default, metadata (attributes) for users/groups/roles are being created/merged too. For users/groups/roles from the destination environment matching users/groups/roles from the source environment, if metadata keys for the source and destination are the same - the values of such metadata will be overridden by the source environment values to exclude overriding of the matching metadata, the specific environment variable shall be set before the running of the pipectl utility - CP_SYNC_USERS_METADATA_SKIP_KEYS : for that variable, the comma-separated list of users' metadata keys to skip during the synchronization shall be specified example of usage: export CP_SYNC_USERS_METADATA_SKIP_KEYS=\"ssh_pub,ssh_prv\" - to exclude public and private keys merging during the synchronization Command example: Example of the output (part): Tools synchronization process The format of the command to synchronize tools between environments: ./pipectl sync --tools --source-url <source_url> --source-token <source_token> --target-url <target_url> --target-token <target_token> Note : the command above shall be performed with admin rights in the the same directory where pipectl utility was downloaded. Details: <source_url> is the source environment address. Shall be specified in the following format: <protocol> :// <IP/hostname> : <port> (e.g. https://cloud-pipeline.epam.com:443 ) <source_token> is the source environment's access token. It could be gotten via the GUI (at the \"Pipe CLI\" settings form ) or via pipe token CLI command <target_url> is the destination environment address. Shall be specified in the following format: <protocol> :// <IP/hostname> : <port> (e.g. https://cloud-pipeline-dev.epam.com:443 ) <target_token> is the destination environment's access token. It could be gotten via the GUI (at the \"Pipe CLI\" settings form ) or via pipe token CLI command During the --tools synchronization, the following processes are being performed: If the default registry is not attached to the destination environment - this will be performed Tools groups' synchronization if a group doesn't exist in the destination environment registry - a new group will be created with all the properties of a source one for tool groups from the destination environment matching tool groups from the source environment, their permissions/OWNER property will be overridden according to corresponding source tool groups' settings (in case, when these settings are different) Tools'synchronization tools from the source environment, that are missing in the destination environment, will be transferred to the destination environment registry (with their versions, settings, descriptions according to the source environment) for tools from the destination environment matching tools from the source environment: the same identical versions (that exist in both environments) remain in the destination versions from the destination not matching the source ones will be removed versions from the source not matching the destination will be transferred to the destination environment registry settings and descriptions will be set according to the source environment \"Symlinked\" tools' synchronization Metadata synchronization by default, metadata (attributes) for tools/tool groups/registry are being created/merged too. For tools/tool groups/registry from the destination environment matching tools/tool groups/registry from the source environment, if metadata keys for the source and destination are the same - the values of such metadata will be overridden according to the source environment values to exclude overriding of the matching metadata, the specific environment variable shall be set before the running of the pipectl utility - CP_SYNC_TOOLS_METADATA_SKIP_KEYS : for that variable, the comma-separated list of tools' metadata keys to skip during the synchronization shall be specified example of usage: export CP_SYNC_TOOLS_METADATA_SKIP_KEYS=\"ke1,key2\" - to exclude merging of \" key1 \" and \" key2 \" metadata keys during the synchronization Command example: Example of the output (part):","title":"Environments sync"},{"location":"installation/management/environments_sync/#environments-synchronization","text":"In some cases, you may need to synchronize two different environments of the Cloud Pipeline. Special routine in the pipectl utility shall be used for that - pipectl sync . It allows to synchronize from the source environment to the destination one the following objects: users / user groups / user roles docker registry / tool groups / tools Synchronization can be performed with or without synchronization of attributes (metadata) for the specified Platform objects. During the synchronization, changes are being performed only in the destination environment, the source environment remains the same.","title":"Environments synchronization"},{"location":"installation/management/environments_sync/#prerequisites","text":"For the correct work, launch pipectl sync utility only at Linux machine, in the separate terminal, not from the run. If you are going to synchronize tools between environments, Docker shall be installed at the machine as well.","title":"Prerequisites"},{"location":"installation/management/environments_sync/#users-synchronization-process","text":"The format of the command to synchronize users between environments: ./pipectl sync --users --source-url <source_url> --source-token <source_token> --target-url <target_url> --target-token <target_token> Note : the command above shall be performed with admin rights in the the same directory where pipectl utility was downloaded. Details: <source_url> is the source environment address. Shall be specified in the following format: <protocol> :// <IP/hostname> : <port> (e.g. https://cloud-pipeline.epam.com:443 ) <source_token> is the source environment's access token. It can be obtained via the GUI (at the \"Pipe CLI\" settings form ) or via pipe token CLI command <target_url> is the destination environment address. Shall be specified in the following format: <protocol> :// <IP/hostname> : <port> (e.g. https://cloud-pipeline-dev.epam.com:443 ) <target_token> is the destination environment's access token. It can be obtained via the GUI (at the \"Pipe CLI\" settings form ) or via pipe token CLI command During the --users synchronization, the following processes are being performed: Roles/groups synchronization roles/groups from the source environment, that are missing in the destination environment, are being created in the destination environment (\"blocked\" statuses are being set according to the source environment as well) for roles/groups from the destination environment matching roles/groups from the source environment, their blocked status are being overridden: if a group/role is blocked in the source environment - that status will be set for the group/role in the destination environment too if a group/role is not blocked in the source environment but blocked in the destination environment - that status will be removed in the destination environment Users synchronization users from the source environment, that are missing in the destination environment, are being created in the destination environment (\"blocked\" statuses, groups/roles lists are being set according to the source environment) for users from the destination environment matching users from the source environment, their blocked statuses are being overridden: if a user is blocked in the source environment - that status is being set for the user in the destination environment too if a user is not blocked in the source environment but blocked in the destination environment - that status will be removed in the destination environment for users from the destination environment matching users from the source environment, their groups/roles lists are being merged with the corresponding lists of the source environment Metadata synchronization by default, metadata (attributes) for users/groups/roles are being created/merged too. For users/groups/roles from the destination environment matching users/groups/roles from the source environment, if metadata keys for the source and destination are the same - the values of such metadata will be overridden by the source environment values to exclude overriding of the matching metadata, the specific environment variable shall be set before the running of the pipectl utility - CP_SYNC_USERS_METADATA_SKIP_KEYS : for that variable, the comma-separated list of users' metadata keys to skip during the synchronization shall be specified example of usage: export CP_SYNC_USERS_METADATA_SKIP_KEYS=\"ssh_pub,ssh_prv\" - to exclude public and private keys merging during the synchronization Command example: Example of the output (part):","title":"Users synchronization process"},{"location":"installation/management/environments_sync/#tools-synchronization-process","text":"The format of the command to synchronize tools between environments: ./pipectl sync --tools --source-url <source_url> --source-token <source_token> --target-url <target_url> --target-token <target_token> Note : the command above shall be performed with admin rights in the the same directory where pipectl utility was downloaded. Details: <source_url> is the source environment address. Shall be specified in the following format: <protocol> :// <IP/hostname> : <port> (e.g. https://cloud-pipeline.epam.com:443 ) <source_token> is the source environment's access token. It could be gotten via the GUI (at the \"Pipe CLI\" settings form ) or via pipe token CLI command <target_url> is the destination environment address. Shall be specified in the following format: <protocol> :// <IP/hostname> : <port> (e.g. https://cloud-pipeline-dev.epam.com:443 ) <target_token> is the destination environment's access token. It could be gotten via the GUI (at the \"Pipe CLI\" settings form ) or via pipe token CLI command During the --tools synchronization, the following processes are being performed: If the default registry is not attached to the destination environment - this will be performed Tools groups' synchronization if a group doesn't exist in the destination environment registry - a new group will be created with all the properties of a source one for tool groups from the destination environment matching tool groups from the source environment, their permissions/OWNER property will be overridden according to corresponding source tool groups' settings (in case, when these settings are different) Tools'synchronization tools from the source environment, that are missing in the destination environment, will be transferred to the destination environment registry (with their versions, settings, descriptions according to the source environment) for tools from the destination environment matching tools from the source environment: the same identical versions (that exist in both environments) remain in the destination versions from the destination not matching the source ones will be removed versions from the source not matching the destination will be transferred to the destination environment registry settings and descriptions will be set according to the source environment \"Symlinked\" tools' synchronization Metadata synchronization by default, metadata (attributes) for tools/tool groups/registry are being created/merged too. For tools/tool groups/registry from the destination environment matching tools/tool groups/registry from the source environment, if metadata keys for the source and destination are the same - the values of such metadata will be overridden according to the source environment values to exclude overriding of the matching metadata, the specific environment variable shall be set before the running of the pipectl utility - CP_SYNC_TOOLS_METADATA_SKIP_KEYS : for that variable, the comma-separated list of tools' metadata keys to skip during the synchronization shall be specified example of usage: export CP_SYNC_TOOLS_METADATA_SKIP_KEYS=\"ke1,key2\" - to exclude merging of \" key1 \" and \" key2 \" metadata keys during the synchronization Command example: Example of the output (part):","title":"Tools synchronization process"},{"location":"installation/prerequisites/aws/","text":"AWS Note : Account specific information shall be updated in the JSONs below: Account name: <account-name> Account ID: <account-id> Region: <region-id> VPC A new VPC in the <region-id> region with the default configuration Subnets A routable subnet with /26 CIDR or a subnet with the Elastic IPs allowed. It will be used to deploy user-facing services Non routable subnets with any CIDR range (e.g. /16) in each of the available Availability Zones. These subnets will be used to launch the worker nodes. E.g. for us-east-1 region, the following subnets/CIDRs can be created: us-east-1a: 10.0.0.0/23 us-east-1b: 10.0.2.0/23 us-east-1c: 10.0.4.0/23 us-east-1d: 10.0.6.0/23 us-east-1e: 10.0.8.0/23 us-east-1f: 10.0.10.0/23 Security Groups CP-Cluster-Internal: Traffic type: ALL Ports: ALL Inbound: from Outbound: to CP-HTTPS-Access: Traffic type: HTTPS Port: 443 Inbound: from Internal networks or 0.0.0.0 (for the Elastic IPs usage) CP-Internet-Access: Traffic type: Any Port: 3128 Outbound: to Egress HTTP proxy, if applicable AMI The following AMIs shall be white-listed for the AWS Account: CPU-only: ami-0383ffd4e3eea1784 GPU/CUDA: ami-0b1141f33ec5cf631 VPC S3 Endpoint A new VPC endpoint shall be created for the S3 service. * Policy: { \"Sid\": \"CP-S3-Endpoint-Policy\", \"Effect\": \"Allow\", \"Principal\": \"*\", \"Action\": \"*\", \"Resource\": \"*\" } NOTE: Resource \" \" is used here because CP need to make API calls to \"s3:CreateJob\" and \"s3:DescribeJob\" to perform archive and restore actions with s3 objects, when Cloud-Pipeline Storage Lifecycle Service * is used. SSH Key A new SSH key named CP-SSH-Key IAM Policies Name: CP-Service-Policy { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"S3Allow\", \"Effect\": \"Allow\", \"Action\": [ \"s3:GetLifecycleConfiguration\", \"s3:GetBucketTagging\", \"s3:DeleteObjectVersion\", \"s3:GetObjectVersionTagging\", \"s3:ListBucketVersions\", \"s3:RestoreObject\", \"s3:CreateBucket\", \"s3:ListBucket\", \"s3:GetBucketPolicy\", \"s3:GetObjectAcl\", \"s3:AbortMultipartUpload\", \"s3:PutBucketTagging\", \"s3:PutLifecycleConfiguration\", \"s3:PutBucketAcl\", \"s3:GetObjectTagging\", \"s3:PutObjectTagging\", \"s3:*Delete*\", \"s3:PutBucketVersioning\", \"s3:PutObjectAcl\", \"s3:DeleteObjectTagging\", \"s3:ListBucketMultipartUploads\", \"s3:PutObjectVersionTagging\", \"s3:DeleteObjectVersionTagging\", \"s3:GetBucketVersioning\", \"s3:PutBucketCORS\", \"s3:GetBucketAcl\", \"s3:ListMultipartUploadParts\", \"s3:PutObject\", \"s3:GetObject\", \"s3:GetBucketCORS\", \"s3:PutBucketPolicy\", \"s3:GetBucketLocation\", \"s3:GetObjectVersion\", \"s3:PutEncryptionConfiguration\", \"s3:GetEncryptionConfiguration\", \"s3:ListAllMyBuckets\" ], \"Resource\": [ \"arn:aws:s3:::*\" ] }, { \"Sid\": \"EFSAllow\", \"Effect\": \"Allow\", \"Action\": [ \"elasticfilesystem:*\" ], \"Resource\": [ \"arn:aws:elasticfilesystem:<region-id>:*:file-system/*\" ] }, { \"Sid\": \"OtherAllow\", \"Effect\": \"Allow\", \"Action\": [ \"ec2:AttachVolume\", \"ec2:DeregisterImage\", \"kms:Decrypt\", \"ec2:DeleteSnapshot\", \"ec2:RequestSpotInstances\", \"ec2:DeleteTags\", \"elasticfilesystem:CreateFileSystem\", \"ec2:CancelSpotFleetRequests\", \"ec2:CreateKeyPair\", \"ec2:CreateImage\", \"ec2:RequestSpotFleet\", \"ec2:DeleteVolume\", \"ec2:DescribeNetworkInterfaces\", \"ec2:StartInstances\", \"kms:Encrypt\", \"ec2:CreateSnapshot\", \"kms:ReEncryptTo\", \"kms:DescribeKey\", \"ec2:ModifyInstanceAttribute\", \"ec2:DescribeInstanceStatus\", \"ec2:DetachVolume\", \"ec2:ReleaseAddress\", \"ec2:RebootInstances\", \"iam:GetRole\", \"ec2:TerminateInstances\", \"ec2:CreateTags\", \"ec2:RegisterImage\", \"iam:ListRoles\", \"ec2:RunInstances\", \"ec2:StopInstances\", \"kms:ReEncryptFrom\", \"ec2:CreateVolume\", \"ec2:CreateNetworkInterface\", \"ec2:CancelSpotInstanceRequests\", \"ec2:Describe*\", \"kms:GenerateDataKey\", \"ec2:DescribeSubnets\", \"ec2:DeleteKeyPair\", \"cloudwatch:GetMetricStatistics\", \"cloudwatch:ListMetrics\", \"cloudwatch:PutMetricData\", \"cloudwatch:GetMetricData\" ], \"Resource\": \"*\" }, { \"Sid\": \"STSAllow\", \"Effect\": \"Allow\", \"Action\": [ \"sts:AssumeRole\" ], \"Resource\": [ \"arn:aws:iam::<account-id>:role/CP-S3viaSTS\" ] }, { \"Sid\": \"RunS3BatchOperationsAllow\", \"Effect\": \"Allow\", \"Action\": [ \"s3:CreateJob\", \"s3:DescribeJob\", \"s3:GetLifecycleConfiguration\", \"s3:PutLifecycleConfiguration\" ], \"Resource\": \"*\" }, { \"Sid\": \"PassSLSRoleAllow\", \"Effect\": \"Allow\", \"Action\": [ \"iam:GetRole\", \"iam:PassRole\" ], \"Resource\": \"arn:aws:iam::<account-id>:role/CP-SLS-Role\" } ] } Name: CP-KMS-Assume-Policy { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"KMSAllowOps\", \"Effect\": \"Allow\", \"Action\": [ \"kms:Decrypt\", \"kms:Encrypt\", \"kms:DescribeKey\", \"kms:ReEncrypt*\", \"kms:GenerateDataKey*\" \"kms:ListKeys\" ], \"Resource\": \"*\" } ] } Name: CP-S3viaSTS-Policy { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"s3:ListBucket\", \"s3:GetObject\", \"s3:PutObject\", \"s3:DeleteObject\", \"s3:GetObjectTagging\", \"s3:PutObjectTagging\", \"s3:DeleteObjectTagging\", \"s3:GetObjectVersionTagging\", \"s3:PutObjectVersionTagging\", \"s3:DeleteObjectVersionTagging\", \"s3:GetObjectVersion\", \"s3:DeleteObjectVersion\", \"s3:ListBucketVersions\", \"s3:GetBucketTagging\", \"s3:PutBucketTagging\" ], \"Resource\": [ \"arn:aws:s3:::*\" ] } ] } Name: CP-SLS-Policy { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"s3:DeleteObject\", \"s3:GetObject\", \"s3:PutObjectTagging\", \"s3:GetObjectTagging\", \"s3:DeleteObjectTagging\", \"s3:ListBucket\", \"s3:PutObject\", \"s3:ListBucketVersions\", \"s3:DeleteObjectVersion\", \"s3:GetObjectVersion\", \"s3:PutObjectVersionTagging\", \"s3:GetObjectVersionTagging\", \"s3:DeleteObjectVersionTagging\", \"s3:RestoreObject\" ], \"Resource\": [ \"arn:aws:s3:::*\" ] }, ] } Roles AWSServiceRoleForEC2Spot : policies according to the AWS Documentation Manually create the AWSServiceRoleForEC2Spot service-linked role CP-Service : CP-Service-Policy and CP-KMS-Assume-Policy CP-SLS-Service : CP-SLS-Policy Trust relationship: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"batchoperations.s3.amazonaws.com\" }, \"Action\": \"sts:AssumeRole\" } ] } NOTE : If access to some storages would be performed through different role that CP-Service , for such roles policy statements RunS3BatchOperationsAllow and PassSLSRoleAllow also should be attached. CP-S3viaSTS : Policies: CP-S3viaSTS-Policy and CP-KMS-Assume-Policy Trust relationship: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::<account-id>:role/CP-Service\" }, \"Action\": \"sts:AssumeRole\" } ] } KMS The following AWS KMS key shall be created: * Region: <region-id> * Name: CP-KMS- <region-id> * Description: Cloud Pipeline KMS encryption key * Key Material: AWS_KMS The following policy shall be attached to the key: { \"Id\": \"CP-KMS-Key-Policy\", \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Enable IAM User Permissions\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::<account-id>:root\" }, \"Action\": \"kms:*\", \"Resource\": \"*\" }, { \"Sid\": \"Allow access for Key Administrators\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::<account-id>:role/CP-Service\", \"arn:aws:iam::<account-id>:role/aws-service-role/spot.amazonaws.com/AWSServiceRoleForEC2Spot\" ] }, \"Action\": [ \"kms:Create*\", \"kms:Describe*\", \"kms:Enable*\", \"kms:List*\", \"kms:Put*\", \"kms:Update*\", \"kms:Revoke*\", \"kms:Disable*\", \"kms:Get*\", \"kms:Delete*\", \"kms:TagResource\", \"kms:UntagResource\", \"kms:ScheduleKeyDeletion\", \"kms:CancelKeyDeletion\" ], \"Resource\": \"*\" }, { \"Sid\": \"Allow use of the key\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::<account-id>:role/CP-Service\", \"arn:aws:iam::<account-id>:role/aws-service-role/spot.amazonaws.com/AWSServiceRoleForEC2Spot\" ] }, \"Action\": [ \"kms:Encrypt\", \"kms:Decrypt\", \"kms:ReEncrypt*\", \"kms:GenerateDataKey*\", \"kms:DescribeKey\" ], \"Resource\": \"*\" }, { \"Sid\": \"Allow attachment of persistent resources\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::<account-id>:role/CP-Service\", \"arn:aws:iam::<account-id>:role/aws-service-role/spot.amazonaws.com/AWSServiceRoleForEC2Spot\" ] }, \"Action\": [ \"kms:CreateGrant\", \"kms:ListGrants\", \"kms:RevokeGrant\" ], \"Resource\": \"*\", \"Condition\": { \"Bool\": { \"kms:GrantIsForAWSResource\": \"true\" } } } ] }","title":"AWS"},{"location":"installation/prerequisites/aws/#aws","text":"Note : Account specific information shall be updated in the JSONs below: Account name: <account-name> Account ID: <account-id> Region: <region-id>","title":"AWS"},{"location":"installation/prerequisites/aws/#vpc","text":"A new VPC in the <region-id> region with the default configuration","title":"VPC"},{"location":"installation/prerequisites/aws/#subnets","text":"A routable subnet with /26 CIDR or a subnet with the Elastic IPs allowed. It will be used to deploy user-facing services Non routable subnets with any CIDR range (e.g. /16) in each of the available Availability Zones. These subnets will be used to launch the worker nodes. E.g. for us-east-1 region, the following subnets/CIDRs can be created: us-east-1a: 10.0.0.0/23 us-east-1b: 10.0.2.0/23 us-east-1c: 10.0.4.0/23 us-east-1d: 10.0.6.0/23 us-east-1e: 10.0.8.0/23 us-east-1f: 10.0.10.0/23","title":"Subnets"},{"location":"installation/prerequisites/aws/#security-groups","text":"CP-Cluster-Internal: Traffic type: ALL Ports: ALL Inbound: from Outbound: to CP-HTTPS-Access: Traffic type: HTTPS Port: 443 Inbound: from Internal networks or 0.0.0.0 (for the Elastic IPs usage) CP-Internet-Access: Traffic type: Any Port: 3128 Outbound: to Egress HTTP proxy, if applicable","title":"Security Groups"},{"location":"installation/prerequisites/aws/#ami","text":"The following AMIs shall be white-listed for the AWS Account: CPU-only: ami-0383ffd4e3eea1784 GPU/CUDA: ami-0b1141f33ec5cf631","title":"AMI"},{"location":"installation/prerequisites/aws/#vpc-s3-endpoint","text":"A new VPC endpoint shall be created for the S3 service. * Policy: { \"Sid\": \"CP-S3-Endpoint-Policy\", \"Effect\": \"Allow\", \"Principal\": \"*\", \"Action\": \"*\", \"Resource\": \"*\" } NOTE: Resource \" \" is used here because CP need to make API calls to \"s3:CreateJob\" and \"s3:DescribeJob\" to perform archive and restore actions with s3 objects, when Cloud-Pipeline Storage Lifecycle Service * is used.","title":"VPC S3 Endpoint"},{"location":"installation/prerequisites/aws/#ssh-key","text":"A new SSH key named CP-SSH-Key","title":"SSH Key"},{"location":"installation/prerequisites/aws/#iam","text":"","title":"IAM"},{"location":"installation/prerequisites/aws/#policies","text":"Name: CP-Service-Policy { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"S3Allow\", \"Effect\": \"Allow\", \"Action\": [ \"s3:GetLifecycleConfiguration\", \"s3:GetBucketTagging\", \"s3:DeleteObjectVersion\", \"s3:GetObjectVersionTagging\", \"s3:ListBucketVersions\", \"s3:RestoreObject\", \"s3:CreateBucket\", \"s3:ListBucket\", \"s3:GetBucketPolicy\", \"s3:GetObjectAcl\", \"s3:AbortMultipartUpload\", \"s3:PutBucketTagging\", \"s3:PutLifecycleConfiguration\", \"s3:PutBucketAcl\", \"s3:GetObjectTagging\", \"s3:PutObjectTagging\", \"s3:*Delete*\", \"s3:PutBucketVersioning\", \"s3:PutObjectAcl\", \"s3:DeleteObjectTagging\", \"s3:ListBucketMultipartUploads\", \"s3:PutObjectVersionTagging\", \"s3:DeleteObjectVersionTagging\", \"s3:GetBucketVersioning\", \"s3:PutBucketCORS\", \"s3:GetBucketAcl\", \"s3:ListMultipartUploadParts\", \"s3:PutObject\", \"s3:GetObject\", \"s3:GetBucketCORS\", \"s3:PutBucketPolicy\", \"s3:GetBucketLocation\", \"s3:GetObjectVersion\", \"s3:PutEncryptionConfiguration\", \"s3:GetEncryptionConfiguration\", \"s3:ListAllMyBuckets\" ], \"Resource\": [ \"arn:aws:s3:::*\" ] }, { \"Sid\": \"EFSAllow\", \"Effect\": \"Allow\", \"Action\": [ \"elasticfilesystem:*\" ], \"Resource\": [ \"arn:aws:elasticfilesystem:<region-id>:*:file-system/*\" ] }, { \"Sid\": \"OtherAllow\", \"Effect\": \"Allow\", \"Action\": [ \"ec2:AttachVolume\", \"ec2:DeregisterImage\", \"kms:Decrypt\", \"ec2:DeleteSnapshot\", \"ec2:RequestSpotInstances\", \"ec2:DeleteTags\", \"elasticfilesystem:CreateFileSystem\", \"ec2:CancelSpotFleetRequests\", \"ec2:CreateKeyPair\", \"ec2:CreateImage\", \"ec2:RequestSpotFleet\", \"ec2:DeleteVolume\", \"ec2:DescribeNetworkInterfaces\", \"ec2:StartInstances\", \"kms:Encrypt\", \"ec2:CreateSnapshot\", \"kms:ReEncryptTo\", \"kms:DescribeKey\", \"ec2:ModifyInstanceAttribute\", \"ec2:DescribeInstanceStatus\", \"ec2:DetachVolume\", \"ec2:ReleaseAddress\", \"ec2:RebootInstances\", \"iam:GetRole\", \"ec2:TerminateInstances\", \"ec2:CreateTags\", \"ec2:RegisterImage\", \"iam:ListRoles\", \"ec2:RunInstances\", \"ec2:StopInstances\", \"kms:ReEncryptFrom\", \"ec2:CreateVolume\", \"ec2:CreateNetworkInterface\", \"ec2:CancelSpotInstanceRequests\", \"ec2:Describe*\", \"kms:GenerateDataKey\", \"ec2:DescribeSubnets\", \"ec2:DeleteKeyPair\", \"cloudwatch:GetMetricStatistics\", \"cloudwatch:ListMetrics\", \"cloudwatch:PutMetricData\", \"cloudwatch:GetMetricData\" ], \"Resource\": \"*\" }, { \"Sid\": \"STSAllow\", \"Effect\": \"Allow\", \"Action\": [ \"sts:AssumeRole\" ], \"Resource\": [ \"arn:aws:iam::<account-id>:role/CP-S3viaSTS\" ] }, { \"Sid\": \"RunS3BatchOperationsAllow\", \"Effect\": \"Allow\", \"Action\": [ \"s3:CreateJob\", \"s3:DescribeJob\", \"s3:GetLifecycleConfiguration\", \"s3:PutLifecycleConfiguration\" ], \"Resource\": \"*\" }, { \"Sid\": \"PassSLSRoleAllow\", \"Effect\": \"Allow\", \"Action\": [ \"iam:GetRole\", \"iam:PassRole\" ], \"Resource\": \"arn:aws:iam::<account-id>:role/CP-SLS-Role\" } ] } Name: CP-KMS-Assume-Policy { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"KMSAllowOps\", \"Effect\": \"Allow\", \"Action\": [ \"kms:Decrypt\", \"kms:Encrypt\", \"kms:DescribeKey\", \"kms:ReEncrypt*\", \"kms:GenerateDataKey*\" \"kms:ListKeys\" ], \"Resource\": \"*\" } ] } Name: CP-S3viaSTS-Policy { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"s3:ListBucket\", \"s3:GetObject\", \"s3:PutObject\", \"s3:DeleteObject\", \"s3:GetObjectTagging\", \"s3:PutObjectTagging\", \"s3:DeleteObjectTagging\", \"s3:GetObjectVersionTagging\", \"s3:PutObjectVersionTagging\", \"s3:DeleteObjectVersionTagging\", \"s3:GetObjectVersion\", \"s3:DeleteObjectVersion\", \"s3:ListBucketVersions\", \"s3:GetBucketTagging\", \"s3:PutBucketTagging\" ], \"Resource\": [ \"arn:aws:s3:::*\" ] } ] } Name: CP-SLS-Policy { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"s3:DeleteObject\", \"s3:GetObject\", \"s3:PutObjectTagging\", \"s3:GetObjectTagging\", \"s3:DeleteObjectTagging\", \"s3:ListBucket\", \"s3:PutObject\", \"s3:ListBucketVersions\", \"s3:DeleteObjectVersion\", \"s3:GetObjectVersion\", \"s3:PutObjectVersionTagging\", \"s3:GetObjectVersionTagging\", \"s3:DeleteObjectVersionTagging\", \"s3:RestoreObject\" ], \"Resource\": [ \"arn:aws:s3:::*\" ] }, ] }","title":"Policies"},{"location":"installation/prerequisites/aws/#roles","text":"AWSServiceRoleForEC2Spot : policies according to the AWS Documentation Manually create the AWSServiceRoleForEC2Spot service-linked role CP-Service : CP-Service-Policy and CP-KMS-Assume-Policy CP-SLS-Service : CP-SLS-Policy Trust relationship: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"batchoperations.s3.amazonaws.com\" }, \"Action\": \"sts:AssumeRole\" } ] } NOTE : If access to some storages would be performed through different role that CP-Service , for such roles policy statements RunS3BatchOperationsAllow and PassSLSRoleAllow also should be attached. CP-S3viaSTS : Policies: CP-S3viaSTS-Policy and CP-KMS-Assume-Policy Trust relationship: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::<account-id>:role/CP-Service\" }, \"Action\": \"sts:AssumeRole\" } ] }","title":"Roles"},{"location":"installation/prerequisites/aws/#kms","text":"The following AWS KMS key shall be created: * Region: <region-id> * Name: CP-KMS- <region-id> * Description: Cloud Pipeline KMS encryption key * Key Material: AWS_KMS The following policy shall be attached to the key: { \"Id\": \"CP-KMS-Key-Policy\", \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Enable IAM User Permissions\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::<account-id>:root\" }, \"Action\": \"kms:*\", \"Resource\": \"*\" }, { \"Sid\": \"Allow access for Key Administrators\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::<account-id>:role/CP-Service\", \"arn:aws:iam::<account-id>:role/aws-service-role/spot.amazonaws.com/AWSServiceRoleForEC2Spot\" ] }, \"Action\": [ \"kms:Create*\", \"kms:Describe*\", \"kms:Enable*\", \"kms:List*\", \"kms:Put*\", \"kms:Update*\", \"kms:Revoke*\", \"kms:Disable*\", \"kms:Get*\", \"kms:Delete*\", \"kms:TagResource\", \"kms:UntagResource\", \"kms:ScheduleKeyDeletion\", \"kms:CancelKeyDeletion\" ], \"Resource\": \"*\" }, { \"Sid\": \"Allow use of the key\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::<account-id>:role/CP-Service\", \"arn:aws:iam::<account-id>:role/aws-service-role/spot.amazonaws.com/AWSServiceRoleForEC2Spot\" ] }, \"Action\": [ \"kms:Encrypt\", \"kms:Decrypt\", \"kms:ReEncrypt*\", \"kms:GenerateDataKey*\", \"kms:DescribeKey\" ], \"Resource\": \"*\" }, { \"Sid\": \"Allow attachment of persistent resources\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::<account-id>:role/CP-Service\", \"arn:aws:iam::<account-id>:role/aws-service-role/spot.amazonaws.com/AWSServiceRoleForEC2Spot\" ] }, \"Action\": [ \"kms:CreateGrant\", \"kms:ListGrants\", \"kms:RevokeGrant\" ], \"Resource\": \"*\", \"Condition\": { \"Bool\": { \"kms:GrantIsForAWSResource\": \"true\" } } } ] }","title":"KMS"},{"location":"installation/prerequisites/azure/","text":"","title":"Azure"},{"location":"installation/prerequisites/common/","text":"Cloud Pipeline installation prerequisites Cloud Provider specific configuration A number of the resources shall be created/configured in the underlying Cloud Provider. Please refer to the corresponding section for the details: AWS Azure GCP Domain names The following DNS entries are considered to access the Cloud Pipeline GUI and route the requests to the other services. Cloud Pipeline GUI/API Record type: A Record value: <Cloud Pipeline Core Instance IP> Example: cloud-pipeline.epam.com Embedded git access for the \"well-established\" pipelines Record type: CNAME Record value: <Cloud Pipeline GUI/API> Example: git.cloud-pipeline.epam.com Access to the docker registry for pull/push operation Record type: CNAME Record value: <Cloud Pipeline GUI/API> Example: docker.cloud-pipeline.epam.com Access to the \"interactive\" services, controlled by the Cloud Pipeline Record type: CNAME Record value: <Cloud Pipeline GUI/API> Example: edge.cloud-pipeline.epam.com SSL/TLS certificates SSL/TLS certificate shall be issued using an available CA (shall be trusted by the users' workstations) or purchased from the external CA. SAML/SSO configuration The following IdP configuration is required: Cloud Pipeline GUI Service Provider URL: https:// <Cloud Pipeline GUI/API> /pipeline Service Provider ACS URL: https:// <Cloud Pipeline GUI/API> /pipeline/saml SAML Binding: HTTP Redirect Assertion information: NameID Email FirstName LastName Cloud Pipeline Git Service Provider URL: https:// <Embedded git> Service Provider ACS URL: https:// <Embedded git> /users/auth/saml/callback SAML Binding: HTTP POST Assertion information: NameID Email FirstName LastName","title":"General requirements"},{"location":"installation/prerequisites/common/#cloud-pipeline-installation-prerequisites","text":"","title":"Cloud Pipeline installation prerequisites"},{"location":"installation/prerequisites/common/#cloud-provider-specific-configuration","text":"A number of the resources shall be created/configured in the underlying Cloud Provider. Please refer to the corresponding section for the details: AWS Azure GCP","title":"Cloud Provider specific configuration"},{"location":"installation/prerequisites/common/#domain-names","text":"The following DNS entries are considered to access the Cloud Pipeline GUI and route the requests to the other services. Cloud Pipeline GUI/API Record type: A Record value: <Cloud Pipeline Core Instance IP> Example: cloud-pipeline.epam.com Embedded git access for the \"well-established\" pipelines Record type: CNAME Record value: <Cloud Pipeline GUI/API> Example: git.cloud-pipeline.epam.com Access to the docker registry for pull/push operation Record type: CNAME Record value: <Cloud Pipeline GUI/API> Example: docker.cloud-pipeline.epam.com Access to the \"interactive\" services, controlled by the Cloud Pipeline Record type: CNAME Record value: <Cloud Pipeline GUI/API> Example: edge.cloud-pipeline.epam.com","title":"Domain names"},{"location":"installation/prerequisites/common/#ssltls-certificates","text":"SSL/TLS certificate shall be issued using an available CA (shall be trusted by the users' workstations) or purchased from the external CA.","title":"SSL/TLS certificates"},{"location":"installation/prerequisites/common/#samlsso-configuration","text":"The following IdP configuration is required: Cloud Pipeline GUI Service Provider URL: https:// <Cloud Pipeline GUI/API> /pipeline Service Provider ACS URL: https:// <Cloud Pipeline GUI/API> /pipeline/saml SAML Binding: HTTP Redirect Assertion information: NameID Email FirstName LastName Cloud Pipeline Git Service Provider URL: https:// <Embedded git> Service Provider ACS URL: https:// <Embedded git> /users/auth/saml/callback SAML Binding: HTTP POST Assertion information: NameID Email FirstName LastName","title":"SAML/SSO configuration"},{"location":"installation/prerequisites/gcp/","text":"GCP Note : Account specific information shall be updated according to the environment: Project ID: <project-id> Region: <region-id> VPC A new VPC in the <project-id> project with the default configuration Subnets A routable subnet with /26 CIDR or a subnet with the Public IPs allowed. It will be used to deploy user-facing services Non routable subnets(s) with any private CIDR range (e.g. /16) in the . These subnets will be used to launch the worker nodes Note: for small deployments or POCs - a single small routable subnet is enough Firewall rules CP-Cluster-Internal: Targets: ALL Ports: TCP: 0-65535 UDP: 0-65535 ICMP IP Ranges: VPC Subnets CIDR Type: Ingress CP-HTTPS-Access: Targets: ALL Ports: TCP: 443 IP Ranges: Internal (on-prem) networks or 0.0.0.0 (for the Public IPs usage) Type: Ingress CP-Internet-Access: Targets: ALL Ports: TCP: 3128 IP Ranges: Egress HTTP proxy, if applicable Type: Egress IAM The following service accounts shall be created: cp-service Description: This account is used by the Cloud Pipeline to communicate to the GCP API (create VMs, manage data, etc.) Roles: Compute Admin Service Account Token Creator Storage Admin cp-storage Description: This account is used by the end-users to communicate to the GCS. Users are not granted access to the account directly, instead - temporary tokens are generated to perform CLI/GUI operations Roles: Storage Object Admin","title":"GCP"},{"location":"installation/prerequisites/gcp/#gcp","text":"Note : Account specific information shall be updated according to the environment: Project ID: <project-id> Region: <region-id>","title":"GCP"},{"location":"installation/prerequisites/gcp/#vpc","text":"A new VPC in the <project-id> project with the default configuration","title":"VPC"},{"location":"installation/prerequisites/gcp/#subnets","text":"A routable subnet with /26 CIDR or a subnet with the Public IPs allowed. It will be used to deploy user-facing services Non routable subnets(s) with any private CIDR range (e.g. /16) in the . These subnets will be used to launch the worker nodes Note: for small deployments or POCs - a single small routable subnet is enough","title":"Subnets"},{"location":"installation/prerequisites/gcp/#firewall-rules","text":"CP-Cluster-Internal: Targets: ALL Ports: TCP: 0-65535 UDP: 0-65535 ICMP IP Ranges: VPC Subnets CIDR Type: Ingress CP-HTTPS-Access: Targets: ALL Ports: TCP: 443 IP Ranges: Internal (on-prem) networks or 0.0.0.0 (for the Public IPs usage) Type: Ingress CP-Internet-Access: Targets: ALL Ports: TCP: 3128 IP Ranges: Egress HTTP proxy, if applicable Type: Egress","title":"Firewall rules"},{"location":"installation/prerequisites/gcp/#iam","text":"The following service accounts shall be created: cp-service Description: This account is used by the Cloud Pipeline to communicate to the GCP API (create VMs, manage data, etc.) Roles: Compute Admin Service Account Token Creator Storage Admin cp-storage Description: This account is used by the end-users to communicate to the GCS. Users are not granted access to the account directly, instead - temporary tokens are generated to perform CLI/GUI operations Roles: Storage Object Admin","title":"IAM"},{"location":"manual/Cloud_Pipeline_-_Manual/","text":"Cloud Pipeline - Manual 1. Quick start 2. Getting started 3. Overview 4. Manage Folder 4.1. Create an object in Folder 4.2. Rename folder 4.3. Delete Folder 4.4. Clone a folder 4.5. Lock a folder 5. Manage Metadata 5.1. Add/Delete metadata items 5.2. Upload metadata 5.3. Customize view of the entity instance table 5.4. Launch a run configuration on metadata 5.5. Download data from external resources to the cloud data storage 6. Manage Pipeline 6.1. Create and configure pipeline 6.1.1. Building WDL pipeline with graphical PipelineBuilder 6.2. Launch a pipeline 6.3. Delete and unregister pipeline 6.4. Work with AWS HealthOmics Workflow 7. Manage Detached configuration 7.1. Create and customize Detached configuration 7.2. Launch Detached Configuration 7.3. Expansion Expressions 7.4. Remove Detached configuration 8. Manage Data Storage 8.1. Create and edit storage 8.2. Upload/Download data 8.3. Create and Edit text files 8.4. Control File versions 8.5. Delete and unregister Data Storage 8.6. Delete Files and Folders from Storage 8.7. Create shared file system 8.8. Data sharing 8.9. Mapping storages 8.10. Storage lifecycle management 8.11. Sensitive storages 8.12. Cloud Data application 8.13. Versioned storages 8.14. Omics storages 9. Manage Cluster nodes 9.1. Manage hot node pools 10. Manage Tools 10.1. Add/Edit a Docker registry 10.2. Add/Edit a Tool group 10.3. Add a Tool 10.4. Edit a Tool 10.5. Launch a Tool 10.6. Tool security check 10.7. Tool version menu 10.8. \"Symlinked\" tools 10.9. Run capabilities 11. Manage Runs 11.1. Manage runs lifecycles 11.2. Auto-commit Docker image 11.3. Sharing with other users or groups of users 11.4. Automatic labels and actions for the runs 12. Manage Settings 12.1. Add a new system event 12.2. Edit a system event 12.3. Create a new user 12.4. Edit/delete a user 12.5. Create a group 12.6. Edit a group/role 12.7. Delete a group 12.8. Change a set of roles/groups for a user 12.9. Change email notification 12.10. Manage system-level settings 12.11. Advanced features 12.12. System logs 12.13. System dictionaries 12.14. NAT gateway 13. Permissions 14. Command-line interface (CLI) 14.1. Install and setup CLI 14.2. View and manage Attributes via CLI 14.3. Manage Storage via CLI 14.4. View pipeline definitions via CLI 14.5. Manage pipeline executions via CLI 14.6. View cluster nodes via CLI 14.7. View and manage Permissions via CLI 14.8. View tools definitions via CLI 14.9. User management via CLI 14.10. SSH tunnel to the running compute instance 15. Interactive services 15.1. Starting an Interactive application 15.2. Using Terminal access 15.3. Expose node filesystem 15.4. Interactive service examples 16. Issues 17. CP objects tagging by additional attributes 17.1. Faceted filters search using tags 18. Home page 19. Global search Appendix A. Instance and Docker container lifecycles Appendix B. Working with a Project Appendix C. Working with autoscaled cluster runs Appendix D. Costs management Appendix E. Pipeline objects concept Appendix F. \u0421omparison of using different FS storage types 1. Quick start This chapter will give you a basic knowledge of pipeline running procedure. 2. Getting started Find Thesaurus, list of supported browsers and authentication details. 3. Overview Get familiar with a Cloud Pipeline Graphical User Interface (GUI). 4. Manage Folder Learn how you can create your own hierarchical structured space. 5. Manage Metadata Learn how you can create a complex analysis environment using custom data entities. 6. Manage Pipeline Get details on pipeline creation, configuration, and launching. Get descriptions of \"Launch pipeline\" page. There is also a description of graphical \"Pipeline builder\" for WDL pipelines. 7. Manage Detached configuration Learn how to run a cluster of differently configurated instances. \"Launch cluster\" box vs LaunchofClusterconfiguration. 8. Manage Data Storage Create a new storage and learn how to manage it: download and upload, copy, move data, delete and unregister storages. 9. Manage Cluster Nodes Learn about active nodes, how to terminate and refresh them. Get familiar with a related dashboard. 10. Manage Tools Add and edit Docker registry. Learn how to add, modify and launch Tools. 11. Manage Runs Learn about Runs space and what information you can get from it. 12. Manage Settings Here you can find information about: how to install CLI, add a new system event, managing roles. 13. Permissions Learn how to manage permissions for Cloud Pipeline objects. 14. Command-line interface (CLI) Get familiar with a Cloud Pipeline Command Line Interface (CLI). 15. Interactive services Learn how to setup non-batch application in a cloud infrastructure and access it via a web interface. 16. Issues Learn how to share results with other users or get feedback. 17. CP objects tagging by additional attributes Learn how to manage custom sets of \"key-values\" attributes for data storage and files. 18. Home page Get details about homepage widgets, how to configure homepage view. 19. Global search How to search objects over the Platform. Appendix A. Instance and Docker container lifecycles Learn basics about instance and Docker container lifecycle. Appendix B. Working with a Project Learn basics about working with a Project. Appendix C. Working with autoscaled cluster runs Learn basics about working with autoscaled cluster runs. Appendix D. Costs management Get details about costs management concept. Appendix E. Pipeline objects concept Get details about the Pipeline objects concept in Cloud Pipeline environment. Appendix F. \u0421omparison of using different FS storage types Get details about the comparison of using different FS storage types ( FSx for Lustre / EFS in AWS / BTRFS on EBS / LizardFS on EBS ) in Cloud Pipeline environment.","title":"Contents"},{"location":"manual/Cloud_Pipeline_-_Manual/#cloud-pipeline-manual","text":"1. Quick start 2. Getting started 3. Overview 4. Manage Folder 4.1. Create an object in Folder 4.2. Rename folder 4.3. Delete Folder 4.4. Clone a folder 4.5. Lock a folder 5. Manage Metadata 5.1. Add/Delete metadata items 5.2. Upload metadata 5.3. Customize view of the entity instance table 5.4. Launch a run configuration on metadata 5.5. Download data from external resources to the cloud data storage 6. Manage Pipeline 6.1. Create and configure pipeline 6.1.1. Building WDL pipeline with graphical PipelineBuilder 6.2. Launch a pipeline 6.3. Delete and unregister pipeline 6.4. Work with AWS HealthOmics Workflow 7. Manage Detached configuration 7.1. Create and customize Detached configuration 7.2. Launch Detached Configuration 7.3. Expansion Expressions 7.4. Remove Detached configuration 8. Manage Data Storage 8.1. Create and edit storage 8.2. Upload/Download data 8.3. Create and Edit text files 8.4. Control File versions 8.5. Delete and unregister Data Storage 8.6. Delete Files and Folders from Storage 8.7. Create shared file system 8.8. Data sharing 8.9. Mapping storages 8.10. Storage lifecycle management 8.11. Sensitive storages 8.12. Cloud Data application 8.13. Versioned storages 8.14. Omics storages 9. Manage Cluster nodes 9.1. Manage hot node pools 10. Manage Tools 10.1. Add/Edit a Docker registry 10.2. Add/Edit a Tool group 10.3. Add a Tool 10.4. Edit a Tool 10.5. Launch a Tool 10.6. Tool security check 10.7. Tool version menu 10.8. \"Symlinked\" tools 10.9. Run capabilities 11. Manage Runs 11.1. Manage runs lifecycles 11.2. Auto-commit Docker image 11.3. Sharing with other users or groups of users 11.4. Automatic labels and actions for the runs 12. Manage Settings 12.1. Add a new system event 12.2. Edit a system event 12.3. Create a new user 12.4. Edit/delete a user 12.5. Create a group 12.6. Edit a group/role 12.7. Delete a group 12.8. Change a set of roles/groups for a user 12.9. Change email notification 12.10. Manage system-level settings 12.11. Advanced features 12.12. System logs 12.13. System dictionaries 12.14. NAT gateway 13. Permissions 14. Command-line interface (CLI) 14.1. Install and setup CLI 14.2. View and manage Attributes via CLI 14.3. Manage Storage via CLI 14.4. View pipeline definitions via CLI 14.5. Manage pipeline executions via CLI 14.6. View cluster nodes via CLI 14.7. View and manage Permissions via CLI 14.8. View tools definitions via CLI 14.9. User management via CLI 14.10. SSH tunnel to the running compute instance 15. Interactive services 15.1. Starting an Interactive application 15.2. Using Terminal access 15.3. Expose node filesystem 15.4. Interactive service examples 16. Issues 17. CP objects tagging by additional attributes 17.1. Faceted filters search using tags 18. Home page 19. Global search Appendix A. Instance and Docker container lifecycles Appendix B. Working with a Project Appendix C. Working with autoscaled cluster runs Appendix D. Costs management Appendix E. Pipeline objects concept Appendix F. \u0421omparison of using different FS storage types 1. Quick start This chapter will give you a basic knowledge of pipeline running procedure. 2. Getting started Find Thesaurus, list of supported browsers and authentication details. 3. Overview Get familiar with a Cloud Pipeline Graphical User Interface (GUI). 4. Manage Folder Learn how you can create your own hierarchical structured space. 5. Manage Metadata Learn how you can create a complex analysis environment using custom data entities. 6. Manage Pipeline Get details on pipeline creation, configuration, and launching. Get descriptions of \"Launch pipeline\" page. There is also a description of graphical \"Pipeline builder\" for WDL pipelines. 7. Manage Detached configuration Learn how to run a cluster of differently configurated instances. \"Launch cluster\" box vs LaunchofClusterconfiguration. 8. Manage Data Storage Create a new storage and learn how to manage it: download and upload, copy, move data, delete and unregister storages. 9. Manage Cluster Nodes Learn about active nodes, how to terminate and refresh them. Get familiar with a related dashboard. 10. Manage Tools Add and edit Docker registry. Learn how to add, modify and launch Tools. 11. Manage Runs Learn about Runs space and what information you can get from it. 12. Manage Settings Here you can find information about: how to install CLI, add a new system event, managing roles. 13. Permissions Learn how to manage permissions for Cloud Pipeline objects. 14. Command-line interface (CLI) Get familiar with a Cloud Pipeline Command Line Interface (CLI). 15. Interactive services Learn how to setup non-batch application in a cloud infrastructure and access it via a web interface. 16. Issues Learn how to share results with other users or get feedback. 17. CP objects tagging by additional attributes Learn how to manage custom sets of \"key-values\" attributes for data storage and files. 18. Home page Get details about homepage widgets, how to configure homepage view. 19. Global search How to search objects over the Platform. Appendix A. Instance and Docker container lifecycles Learn basics about instance and Docker container lifecycle. Appendix B. Working with a Project Learn basics about working with a Project. Appendix C. Working with autoscaled cluster runs Learn basics about working with autoscaled cluster runs. Appendix D. Costs management Get details about costs management concept. Appendix E. Pipeline objects concept Get details about the Pipeline objects concept in Cloud Pipeline environment. Appendix F. \u0421omparison of using different FS storage types Get details about the comparison of using different FS storage types ( FSx for Lustre / EFS in AWS / BTRFS on EBS / LizardFS on EBS ) in Cloud Pipeline environment.","title":"Cloud Pipeline - Manual"},{"location":"manual/01_Quick_start/1._Quick_start/","text":"1. Quick start To launch a Pipeline you need to have EXECUTE permissions for that Pipeline . For more information see 13. Permissions . This quick start will help you to get a general idea of data processing via the Cloud Pipeline platform. Pipelines are set of data processing steps that are dependent on each other. Here we will describe an example of a pipeline launch. Note : Pipeline launch steps remain the same for different pipelines. Pipeline running procedure for basic users The first step you need to perform is to upload the data that you will work with. Generally, data in the Cloud Pipeline is kept in data storages . So, the first thing you need to do is to find a data storage in the Library tab on the left side of the screen. Then upload data to it. In this example, we will choose the data storage named \" INPUT \" ( 1 ) and then create a folder ( 2 ) named \" gromacs_benchmark \" ( 3 ). Then we will navigate to this folder by clicking on its name and click \" Upload \" button to upload your input data to the storage. Note : make sure that the size of data doesn't exceed 5 Gb. To upload more than 5 Gb you shall use CLI (see details here ). Then decide where to put results of pipeline processing. A good practice is to not mix input and output data in a one data storage, so we will use another data storage for output data. In this example, we will use data storage named \" ANALYSIS \" to store pipeline output data . In the \" Search \" field of the Library tab, find a pipeline that will process the input data . In this example, we will use the \" Gromacs \" pipeline that performs molecular modeling. Choose the pipeline version and click the Run button. On the \" Launch a pipeline \" page you'll see pipeline execution parameters . On the bottom, you'll see a \" Parameter \" section with the input parameter and output parameter. Here you can specify the paths to your folders in storages, which you've created in step 1 and step 2 . Click the controls to choose your folders. Check your folder in the list and click \"OK\" . Tip: If you want to create a new folder for the analysis, you can put /$RUN_ID at the end of the output string. The system will create a new folder with Run ID name. Click the Launch button to run the pipeline. You'll be redirected to the Runs tab of the CP . Here you can find launched \"Gromacs\" pipeline. Click a run with the pipeline name to see run logs . When all tasks of pipeline finish their execution, you'll be able to find the pipeline run in the Completed Runs section of the Runs tab. To see the output data, you shall navigate to the data storage with output data - ANALYSIS - and find the results of the pipeline run.","title":"1. Quick start"},{"location":"manual/01_Quick_start/1._Quick_start/#1-quick-start","text":"To launch a Pipeline you need to have EXECUTE permissions for that Pipeline . For more information see 13. Permissions . This quick start will help you to get a general idea of data processing via the Cloud Pipeline platform. Pipelines are set of data processing steps that are dependent on each other. Here we will describe an example of a pipeline launch. Note : Pipeline launch steps remain the same for different pipelines.","title":"1. Quick start"},{"location":"manual/01_Quick_start/1._Quick_start/#pipeline-running-procedure-for-basic-users","text":"The first step you need to perform is to upload the data that you will work with. Generally, data in the Cloud Pipeline is kept in data storages . So, the first thing you need to do is to find a data storage in the Library tab on the left side of the screen. Then upload data to it. In this example, we will choose the data storage named \" INPUT \" ( 1 ) and then create a folder ( 2 ) named \" gromacs_benchmark \" ( 3 ). Then we will navigate to this folder by clicking on its name and click \" Upload \" button to upload your input data to the storage. Note : make sure that the size of data doesn't exceed 5 Gb. To upload more than 5 Gb you shall use CLI (see details here ). Then decide where to put results of pipeline processing. A good practice is to not mix input and output data in a one data storage, so we will use another data storage for output data. In this example, we will use data storage named \" ANALYSIS \" to store pipeline output data . In the \" Search \" field of the Library tab, find a pipeline that will process the input data . In this example, we will use the \" Gromacs \" pipeline that performs molecular modeling. Choose the pipeline version and click the Run button. On the \" Launch a pipeline \" page you'll see pipeline execution parameters . On the bottom, you'll see a \" Parameter \" section with the input parameter and output parameter. Here you can specify the paths to your folders in storages, which you've created in step 1 and step 2 . Click the controls to choose your folders. Check your folder in the list and click \"OK\" . Tip: If you want to create a new folder for the analysis, you can put /$RUN_ID at the end of the output string. The system will create a new folder with Run ID name. Click the Launch button to run the pipeline. You'll be redirected to the Runs tab of the CP . Here you can find launched \"Gromacs\" pipeline. Click a run with the pipeline name to see run logs . When all tasks of pipeline finish their execution, you'll be able to find the pipeline run in the Completed Runs section of the Runs tab. To see the output data, you shall navigate to the data storage with output data - ANALYSIS - and find the results of the pipeline run.","title":"Pipeline running procedure for basic users"},{"location":"manual/02_Getting_started/2._Getting_started/","text":"2. Getting started Thesaurus Cloud Provider - supported cloud computing provider. Cloud Region - specific geographical location where compute cloud resources could be hosted. Each Cloud Provider has its own Cloud Regions. Objects - Cloud Pipeline (CP) entities such as Folder, Pipeline, Data Storage, Run configuration, Cluster node, Docker Registry, Tool group, Tool. Folder , or directory - CP object. It is an entity similar to the directories in the file system. Folders are used to structure other CP objects. Project - a special type of Folder. It might be used to organize data and metadata (see below) and simplify analysis runs for a large data set. Pipeline - CP object. Represents a workflow script with versioned source code, documentation, and configuration. Under the hood, it is a git repository. Data Storage - CP object. It is a cloud storage (e.g. S3 bucket for AWS or Blob Storage for MS Azure) representation in a folder hierarchy. FS Mount - a data storage based on network FS (File System). It is a distributed file system that can be used by several nodes during High-performance computing jobs. Docker Registry - CP object. It is a Docker registry representation in the CP. Docker registry stores docker images. For more details refer to https://docs.docker.com/registry/ . Tool - CP object. It is a Docker image representation in CP. Tool group - CP object. It allows organizing different Tools into groups. Detached configuration - Run configuration that is not attached to any particular pipeline. Run configuration - CP object specifying parameters of the run: what pipeline or tool, type of instance to run with what parameters. There are Run Configurations for Detached configuration and for Pipeline configuration. History - CP object. It shows all runs that are related to a project. Cluster configuration - CP object. It is Run Configuration of Detached Configuration. Represents a set of pipelines or tools that run as one cluster. Also is used to associate pipeline parameters with some Metadata Entity. Metadata - CP object that defines custom data entities associated with raw data files (fastq, bcl, etc.) or data parameters. Batch job - an automated job that doesn't require interaction with a human. Interactive application - type of an application that requires an interaction with a human to achieve certain results. Docker image - a stand-alone, executable package that includes everything needed to run a piece of software. Containerized software will always run the same, regardless of the environment. Docker container - a runtime instance of docker image - what the image turns into when actually executed. Run - Executed \"Pipeline\" or \"Tool\" object, contains log information and parameters of an execution process. Instance - virtual computing environment (e.g. EC2 instance for AWS or Azure Virtual Machines for MS Azure). On-Demand Instances \u2013 \"Stable\" type of instances that can't be overbought while in use. Spot Instances \u2013 instances, which has significantly lower costs. This type of instances are rented and can be overbought and shut down by cloud provider. Instances may have any type (number of CPU cores, processor type, amount of RAM and disk space, etc.) that describes optimization and available features for the instance. Each Cloud Provider calls these instances differently: spot instances for AWS, low priority instances for Azure, preemptible instances for GCP. Cluster node (Calculation node) - instance used for a \"Run\" object. Task - a discrete Pipeline step. Attributes - data that provides information about other data. CP allows creating a custom metadata set of \"key=values\" (attributes) for its objects. It helps to maintain traceability and use this information for search queries. Cloud Pipeline CLI - command line interface to the CP. It allows interaction with CP via command line instead of GUI. STS (Short-Term Storage) - data storage that is used for frequently accessed files. LTS (Long-Term Storage) - data storage that is used for infrequently accessed files. Access to data becomes more complicated but it's cheaper. Pipeline template - it is a template structure for creating a pipeline. Typically it includes source code folder structure with the executable script, doc file, and configuration file. Token - authentication key to getting access to an application. Endpoint - link to access a launched application. Pipeline version - A particular version of pipeline source code, documentation and configuration. Log - automatically produced and time-stamped documentation of events relevant to a particular system. Cluster - a collection of instances which are connected so that they can be used together on a task. Access Control List - a list of pairs of attributes - user id and permissions: read/write/execute. Supported Cloud Providers The following Cloud Providers are supported at the moment: Amazon Web Services\u200e ( AWS ) Microsoft Azure services ( MS Azure ) Google Cloud Platform ( GCP ) Supported Browsers The following web-browsers are supported at the moment: Google Chrome (best option) Firefox Microsoft EDGE Internet Explorer 11 Authentication SAML Authentication protocol is currently used in a Cloud Pipeline.","title":"2. Getting started"},{"location":"manual/02_Getting_started/2._Getting_started/#2-getting-started","text":"","title":"2. Getting started"},{"location":"manual/02_Getting_started/2._Getting_started/#thesaurus","text":"Cloud Provider - supported cloud computing provider. Cloud Region - specific geographical location where compute cloud resources could be hosted. Each Cloud Provider has its own Cloud Regions. Objects - Cloud Pipeline (CP) entities such as Folder, Pipeline, Data Storage, Run configuration, Cluster node, Docker Registry, Tool group, Tool. Folder , or directory - CP object. It is an entity similar to the directories in the file system. Folders are used to structure other CP objects. Project - a special type of Folder. It might be used to organize data and metadata (see below) and simplify analysis runs for a large data set. Pipeline - CP object. Represents a workflow script with versioned source code, documentation, and configuration. Under the hood, it is a git repository. Data Storage - CP object. It is a cloud storage (e.g. S3 bucket for AWS or Blob Storage for MS Azure) representation in a folder hierarchy. FS Mount - a data storage based on network FS (File System). It is a distributed file system that can be used by several nodes during High-performance computing jobs. Docker Registry - CP object. It is a Docker registry representation in the CP. Docker registry stores docker images. For more details refer to https://docs.docker.com/registry/ . Tool - CP object. It is a Docker image representation in CP. Tool group - CP object. It allows organizing different Tools into groups. Detached configuration - Run configuration that is not attached to any particular pipeline. Run configuration - CP object specifying parameters of the run: what pipeline or tool, type of instance to run with what parameters. There are Run Configurations for Detached configuration and for Pipeline configuration. History - CP object. It shows all runs that are related to a project. Cluster configuration - CP object. It is Run Configuration of Detached Configuration. Represents a set of pipelines or tools that run as one cluster. Also is used to associate pipeline parameters with some Metadata Entity. Metadata - CP object that defines custom data entities associated with raw data files (fastq, bcl, etc.) or data parameters. Batch job - an automated job that doesn't require interaction with a human. Interactive application - type of an application that requires an interaction with a human to achieve certain results. Docker image - a stand-alone, executable package that includes everything needed to run a piece of software. Containerized software will always run the same, regardless of the environment. Docker container - a runtime instance of docker image - what the image turns into when actually executed. Run - Executed \"Pipeline\" or \"Tool\" object, contains log information and parameters of an execution process. Instance - virtual computing environment (e.g. EC2 instance for AWS or Azure Virtual Machines for MS Azure). On-Demand Instances \u2013 \"Stable\" type of instances that can't be overbought while in use. Spot Instances \u2013 instances, which has significantly lower costs. This type of instances are rented and can be overbought and shut down by cloud provider. Instances may have any type (number of CPU cores, processor type, amount of RAM and disk space, etc.) that describes optimization and available features for the instance. Each Cloud Provider calls these instances differently: spot instances for AWS, low priority instances for Azure, preemptible instances for GCP. Cluster node (Calculation node) - instance used for a \"Run\" object. Task - a discrete Pipeline step. Attributes - data that provides information about other data. CP allows creating a custom metadata set of \"key=values\" (attributes) for its objects. It helps to maintain traceability and use this information for search queries. Cloud Pipeline CLI - command line interface to the CP. It allows interaction with CP via command line instead of GUI. STS (Short-Term Storage) - data storage that is used for frequently accessed files. LTS (Long-Term Storage) - data storage that is used for infrequently accessed files. Access to data becomes more complicated but it's cheaper. Pipeline template - it is a template structure for creating a pipeline. Typically it includes source code folder structure with the executable script, doc file, and configuration file. Token - authentication key to getting access to an application. Endpoint - link to access a launched application. Pipeline version - A particular version of pipeline source code, documentation and configuration. Log - automatically produced and time-stamped documentation of events relevant to a particular system. Cluster - a collection of instances which are connected so that they can be used together on a task. Access Control List - a list of pairs of attributes - user id and permissions: read/write/execute.","title":"Thesaurus"},{"location":"manual/02_Getting_started/2._Getting_started/#supported-cloud-providers","text":"The following Cloud Providers are supported at the moment: Amazon Web Services\u200e ( AWS ) Microsoft Azure services ( MS Azure ) Google Cloud Platform ( GCP )","title":"Supported Cloud Providers"},{"location":"manual/02_Getting_started/2._Getting_started/#supported-browsers","text":"The following web-browsers are supported at the moment: Google Chrome (best option) Firefox Microsoft EDGE Internet Explorer 11","title":"Supported Browsers"},{"location":"manual/02_Getting_started/2._Getting_started/#authentication","text":"SAML Authentication protocol is currently used in a Cloud Pipeline.","title":"Authentication"},{"location":"manual/03_Overview/3._Overview/","text":"3. Overview User Journey in a nutshell GUI menu tab bar Home Library Cluster Nodes Tools Runs Settings Search Logout User Journey in a nutshell Cloud Pipeline (CP) is a cloud-based web application which allows users to solve a wide range of analytical tasks and includes: Data processing : you can create data processing pipelines and run them in the cloud in an automated way. Data storing : create your data storage, download or upload data from it or edit text files within CP UI. File version control is supported. Tool management : you can create and deploy your own calculation environment using Docker's container concept. This Manual is mostly around data processing lifecycle which, in a nutshell, can be described in these several steps: To run a user's calculation script it shall be registered in CP as a pipeline . The script could be created in CP environment or uploaded from the local machine. See more details 6. Manage Pipeline . Note : If you need to run a pipeline in different environments simultaneously or set a specific type of data, you can use detach configuration object. See more 7. Manage Detached configuration . To store pipeline's inputs and outputs data files the Data Storage shall be determined in CP . Learn more 8. Manage Data Storage . Almost every pipeline requires a specific package of software to run it, which is defined in a docker image. So when a user starts a pipeline, CP starts a new cloud instance (nodes) and runs a docker image at it. See more details 9. Manage Cluster nodes and 10. Manage Tools . When the environment is set, pipeline starts execution. A user in CP can change and save configurations of the run. Learn more 6.2. Launch a pipeline . A user can monitor the status of active and completed runs and usage of active instances in CP . Learn more 11. Manage Runs and 9. Manage Cluster nodes . Note : CP can run a docker image at instance without any pipeline at all if needed. There just will be an instance with some installed and running software. A user can SSH to it or use it in the interactive mode. Also, CP supports CLI, which duplicates some of GUI features and has extra features unavailable via GUI, such as automation of interaction with CP during the pipeline script running, or uploading considerable amount of data (more than 5 Gb), etc. The basics of CLI you can learn here . GUI menu tab bar There are several menu tabs at the left edge of CP window. Home Home space is opened by default when Cloud Pipeline is loaded. It provides \"personal\" and often-used information for a user. By default, this dashboard shows 3 widgets: Active Runs (see picture below, 1 ) displays a list of user's active runs. See more details 11. Manage Runs . Note : press Explore all active runs to see a list of all runs. Tools (see picture below, 2 ) shows a list of Tools in a user's personal repository and Tools available to your group. Learn more 10. Manage Tools and 13. Permissions . Note : group-level Tools will be shown on the top of the Tools list. Note : user can search for a particular Tool by using the Search tools text box. System will suggest Tools based on a user's query. Data (see picture below, 3 ) displays available Data Storages for a user - user shall have OWNER rights or READ/WRITE access to a Data Storage. See more 8. Manage Data Storage and 13. Permissions . Note : personal Data storages (i.e. a user is an OWNER of this Storage) will be shown on top, WRITE - second priority, READ - third priority. Each widget has the Help icon . Hover this icon to get a brief description of a widget. Learn more about Home tab 18. Home page . Library Library space supports a hierarchical view of its content: Pipelines and its versions Data Storages Folders Configurations Metadata objects . The tab consists of two panels: \"Hierarchy\" view (see the picture below, 2 ) displays a hierarchical-structured list of pipelines, folders, storages and machine configurations, etc. Use the \" Search \" field to find a CP object by a name. \" Collapse/Expand \" button (see the picture below, 1 ) at the bottom-left corner of the screen: use it to collapse or expand \"Hierarchy\" view. \"Details\" view (see the picture below, 3 ) shows details of a selected item in hierarchy panel. Depends on a selected type of object it has a very different view. You can learn about each on respective pages of the manual. Note : Each time \"Hierarchy\" view loads, the first Folder in the hierarchy that has more than 1 child object (Folder, Pipeline, Data Storage, etc) is automatically selected. Contents of that Folder are automatically expanded. Cluster Nodes This space provides a list of working nodes. You can get information on their usage and terminate them in this tab. See more details 9. Manage Cluster nodes . Tools Tools space displays available Docker images and their tools, organized in tool groups and docker registries. You can edit information about them and run nodes with any Docker image in this tab. See more details 10. Manage Tools . Runs This space helps you monitor the state of your run instances. You can get parameters and logs of a specific run, stop a run, rerun a completed run. Learn more 11. Manage Runs . Settings This tab opens a Settings window which allows: generate a CLI installation and configuration commands to set CLI for CP , manage system events notifications, manage roles and permissions. See more details 12. Manage Settings . Search It's a search field which allows searching specific runs by its \"Run ID\" or \"Parameters\". Logout This is a Logout button which logs you out. Note : if automatic logging-in is configured, you will be re-logged at once. Note : if any changes occur in the CP application during an authorized session, the changes are applied after re-login.","title":"3. Overview"},{"location":"manual/03_Overview/3._Overview/#3-overview","text":"User Journey in a nutshell GUI menu tab bar Home Library Cluster Nodes Tools Runs Settings Search Logout","title":"3. Overview"},{"location":"manual/03_Overview/3._Overview/#user-journey-in-a-nutshell","text":"Cloud Pipeline (CP) is a cloud-based web application which allows users to solve a wide range of analytical tasks and includes: Data processing : you can create data processing pipelines and run them in the cloud in an automated way. Data storing : create your data storage, download or upload data from it or edit text files within CP UI. File version control is supported. Tool management : you can create and deploy your own calculation environment using Docker's container concept. This Manual is mostly around data processing lifecycle which, in a nutshell, can be described in these several steps: To run a user's calculation script it shall be registered in CP as a pipeline . The script could be created in CP environment or uploaded from the local machine. See more details 6. Manage Pipeline . Note : If you need to run a pipeline in different environments simultaneously or set a specific type of data, you can use detach configuration object. See more 7. Manage Detached configuration . To store pipeline's inputs and outputs data files the Data Storage shall be determined in CP . Learn more 8. Manage Data Storage . Almost every pipeline requires a specific package of software to run it, which is defined in a docker image. So when a user starts a pipeline, CP starts a new cloud instance (nodes) and runs a docker image at it. See more details 9. Manage Cluster nodes and 10. Manage Tools . When the environment is set, pipeline starts execution. A user in CP can change and save configurations of the run. Learn more 6.2. Launch a pipeline . A user can monitor the status of active and completed runs and usage of active instances in CP . Learn more 11. Manage Runs and 9. Manage Cluster nodes . Note : CP can run a docker image at instance without any pipeline at all if needed. There just will be an instance with some installed and running software. A user can SSH to it or use it in the interactive mode. Also, CP supports CLI, which duplicates some of GUI features and has extra features unavailable via GUI, such as automation of interaction with CP during the pipeline script running, or uploading considerable amount of data (more than 5 Gb), etc. The basics of CLI you can learn here .","title":"User Journey in a nutshell"},{"location":"manual/03_Overview/3._Overview/#gui-menu-tab-bar","text":"There are several menu tabs at the left edge of CP window.","title":"GUI menu tab bar"},{"location":"manual/03_Overview/3._Overview/#home","text":"Home space is opened by default when Cloud Pipeline is loaded. It provides \"personal\" and often-used information for a user. By default, this dashboard shows 3 widgets: Active Runs (see picture below, 1 ) displays a list of user's active runs. See more details 11. Manage Runs . Note : press Explore all active runs to see a list of all runs. Tools (see picture below, 2 ) shows a list of Tools in a user's personal repository and Tools available to your group. Learn more 10. Manage Tools and 13. Permissions . Note : group-level Tools will be shown on the top of the Tools list. Note : user can search for a particular Tool by using the Search tools text box. System will suggest Tools based on a user's query. Data (see picture below, 3 ) displays available Data Storages for a user - user shall have OWNER rights or READ/WRITE access to a Data Storage. See more 8. Manage Data Storage and 13. Permissions . Note : personal Data storages (i.e. a user is an OWNER of this Storage) will be shown on top, WRITE - second priority, READ - third priority. Each widget has the Help icon . Hover this icon to get a brief description of a widget. Learn more about Home tab 18. Home page .","title":"Home"},{"location":"manual/03_Overview/3._Overview/#library","text":"Library space supports a hierarchical view of its content: Pipelines and its versions Data Storages Folders Configurations Metadata objects . The tab consists of two panels: \"Hierarchy\" view (see the picture below, 2 ) displays a hierarchical-structured list of pipelines, folders, storages and machine configurations, etc. Use the \" Search \" field to find a CP object by a name. \" Collapse/Expand \" button (see the picture below, 1 ) at the bottom-left corner of the screen: use it to collapse or expand \"Hierarchy\" view. \"Details\" view (see the picture below, 3 ) shows details of a selected item in hierarchy panel. Depends on a selected type of object it has a very different view. You can learn about each on respective pages of the manual. Note : Each time \"Hierarchy\" view loads, the first Folder in the hierarchy that has more than 1 child object (Folder, Pipeline, Data Storage, etc) is automatically selected. Contents of that Folder are automatically expanded.","title":"Library"},{"location":"manual/03_Overview/3._Overview/#cluster-nodes","text":"This space provides a list of working nodes. You can get information on their usage and terminate them in this tab. See more details 9. Manage Cluster nodes .","title":"Cluster Nodes"},{"location":"manual/03_Overview/3._Overview/#tools","text":"Tools space displays available Docker images and their tools, organized in tool groups and docker registries. You can edit information about them and run nodes with any Docker image in this tab. See more details 10. Manage Tools .","title":"Tools"},{"location":"manual/03_Overview/3._Overview/#runs","text":"This space helps you monitor the state of your run instances. You can get parameters and logs of a specific run, stop a run, rerun a completed run. Learn more 11. Manage Runs .","title":"Runs"},{"location":"manual/03_Overview/3._Overview/#settings","text":"This tab opens a Settings window which allows: generate a CLI installation and configuration commands to set CLI for CP , manage system events notifications, manage roles and permissions. See more details 12. Manage Settings .","title":"Settings"},{"location":"manual/03_Overview/3._Overview/#search","text":"It's a search field which allows searching specific runs by its \"Run ID\" or \"Parameters\".","title":"Search"},{"location":"manual/03_Overview/3._Overview/#logout","text":"This is a Logout button which logs you out. Note : if automatic logging-in is configured, you will be re-logged at once. Note : if any changes occur in the CP application during an authorized session, the changes are applied after re-login.","title":"Logout"},{"location":"manual/04_Manage_Folder/4.1._Create_an_object_in_Folder/","text":"4.1. Create an object in Folder How to create Example: Create Folder To create new object in the Folder you need WRITE permissions for that folder and an appropriate ROLE, e.g. to create new folder you need to have the ROLE_FOLDER_MANAGER role. For more information see 13. Permissions . You can create Pipelines , Storages , Detached configurations and Folders in a Folder. How to create Navigate to the Folder . Click \" Create \". Choose a type of the object to be created from a drop-down list. Set properties of the object (such as a name for the object). See Create pipeline , Create storage , Create Detached Configuration pages. Click OK . Example: Create Folder Navigate to a folder. Click Create . Choose Folder from the drop-down list. Enter the name of the folder. Note : Folder name must be unique in its hierarchical layer. Click OK .","title":"4.1. Create an object in Folder"},{"location":"manual/04_Manage_Folder/4.1._Create_an_object_in_Folder/#41-create-an-object-in-folder","text":"How to create Example: Create Folder To create new object in the Folder you need WRITE permissions for that folder and an appropriate ROLE, e.g. to create new folder you need to have the ROLE_FOLDER_MANAGER role. For more information see 13. Permissions . You can create Pipelines , Storages , Detached configurations and Folders in a Folder.","title":"4.1. Create an object in Folder"},{"location":"manual/04_Manage_Folder/4.1._Create_an_object_in_Folder/#how-to-create","text":"Navigate to the Folder . Click \" Create \". Choose a type of the object to be created from a drop-down list. Set properties of the object (such as a name for the object). See Create pipeline , Create storage , Create Detached Configuration pages. Click OK .","title":"How to create"},{"location":"manual/04_Manage_Folder/4.1._Create_an_object_in_Folder/#example-create-folder","text":"Navigate to a folder. Click Create . Choose Folder from the drop-down list. Enter the name of the folder. Note : Folder name must be unique in its hierarchical layer. Click OK .","title":"Example: Create Folder"},{"location":"manual/04_Manage_Folder/4.2._Rename_folder/","text":"4.2. Rename folder To edit name of a Folder you need WRITE permission for that folder. For more information see 13. Permissions . You can rename a Folder . To do that: Navigate to a Folder . Click icon and choose Edit folder - the settings window will open. Enter the new name. Note : Folder name must be unique in its hierarchical layer. Click OK . Note : another way to rename folder is: Hover over the title of the folder - the editing icon will appear. Click on the highlight field - it's switched to edit-mode. Type a name. Click out of the active field - the name will be saved.","title":"4.2. Rename Folder"},{"location":"manual/04_Manage_Folder/4.2._Rename_folder/#42-rename-folder","text":"To edit name of a Folder you need WRITE permission for that folder. For more information see 13. Permissions . You can rename a Folder . To do that: Navigate to a Folder . Click icon and choose Edit folder - the settings window will open. Enter the new name. Note : Folder name must be unique in its hierarchical layer. Click OK . Note : another way to rename folder is: Hover over the title of the folder - the editing icon will appear. Click on the highlight field - it's switched to edit-mode. Type a name. Click out of the active field - the name will be saved.","title":"4.2. Rename folder"},{"location":"manual/04_Manage_Folder/4.3._Delete_Folder/","text":"4.3. Delete Folder To delete a Folder you need WRITE permissions for that folder and the ROLE_FOLDER_MANAGER role. For more information see 13. Permissions . Note : If the folder is not empty it will not be deleted. If a folder contains only metadata, the folder will be deleted. Navigate to the folder contains the folder to delete. Click the icon in the line with desired object to delete. To delete current (selected folder): Click button. Click Delete . Click OK .","title":"4.3. Delete Folder"},{"location":"manual/04_Manage_Folder/4.3._Delete_Folder/#43-delete-folder","text":"To delete a Folder you need WRITE permissions for that folder and the ROLE_FOLDER_MANAGER role. For more information see 13. Permissions . Note : If the folder is not empty it will not be deleted. If a folder contains only metadata, the folder will be deleted. Navigate to the folder contains the folder to delete. Click the icon in the line with desired object to delete. To delete current (selected folder): Click button. Click Delete . Click OK .","title":"4.3. Delete Folder"},{"location":"manual/04_Manage_Folder/4.4._Clone_a_folder/","text":"4.4. Clone a folder This feature allows a user to clone any folder to a specific destination: to user's personal folder, or user's project. It would be helpful to create a new project way faster due to copying metadata, configurations, storages from another project. To copy a folder, you need READ permissions for the copied folder and WRITE permissions for a folder selected as a destination. For more information see 13. Permissions . Note : learn more about metadata here . To clone a folder, the following steps shall be performed: Navigate to the desired folder page. Click icon and choose \"Clone\" menu item. The \"Select destination folder\" will pop-up. Note : The pop-up window will be open with parent folder of a copied folder as a default destination. Choose the destination if it needed by classical navigation actions. Note : The \"Clone\" button ( 1 ) displays which destination is selected at the moment. Name the new clone of the folder ( 2 ). Note : The name shall not break the uniqueness principal: there shouldn't be two folders with the same name in one destination. Click \"Clone\" button - and the folder will be cloned. The page of the clone of the folder will be open automatically. All child of the copied folder will be copied. Note : The exception is pipelines. Pipelines won't be copied as far as it may cause the collision. Note : The storages are copied by creating a new empty one with a unique name and path. No file will be copied from a copied storage to a new one.","title":"4.4. Clone a Folder"},{"location":"manual/04_Manage_Folder/4.4._Clone_a_folder/#44-clone-a-folder","text":"This feature allows a user to clone any folder to a specific destination: to user's personal folder, or user's project. It would be helpful to create a new project way faster due to copying metadata, configurations, storages from another project. To copy a folder, you need READ permissions for the copied folder and WRITE permissions for a folder selected as a destination. For more information see 13. Permissions . Note : learn more about metadata here . To clone a folder, the following steps shall be performed: Navigate to the desired folder page. Click icon and choose \"Clone\" menu item. The \"Select destination folder\" will pop-up. Note : The pop-up window will be open with parent folder of a copied folder as a default destination. Choose the destination if it needed by classical navigation actions. Note : The \"Clone\" button ( 1 ) displays which destination is selected at the moment. Name the new clone of the folder ( 2 ). Note : The name shall not break the uniqueness principal: there shouldn't be two folders with the same name in one destination. Click \"Clone\" button - and the folder will be cloned. The page of the clone of the folder will be open automatically. All child of the copied folder will be copied. Note : The exception is pipelines. Pipelines won't be copied as far as it may cause the collision. Note : The storages are copied by creating a new empty one with a unique name and path. No file will be copied from a copied storage to a new one.","title":"4.4. Clone a folder"},{"location":"manual/04_Manage_Folder/4.5._Lock_a_folder/","text":"4.5. Lock a folder To lock or unlock a folder, a user shall have OWNER permissions. For more permissions details see 13. Permissions . If you want to save your folder from the changes, you may lock it. Other users won't be able to make any changes in a locked folder and its children. To lock a folder the following steps shall be performed: Navigate to a folder you want to lock. Click icon \u2192 Lock . Confirm you decision in the dialog window. The folder will be locked and marked with a specific symbol . To unlock the folder, you just need to click icon \u2192 Unlock and confirm it in the dialog box.","title":"4.5. Lock a Folder"},{"location":"manual/04_Manage_Folder/4.5._Lock_a_folder/#45-lock-a-folder","text":"To lock or unlock a folder, a user shall have OWNER permissions. For more permissions details see 13. Permissions . If you want to save your folder from the changes, you may lock it. Other users won't be able to make any changes in a locked folder and its children. To lock a folder the following steps shall be performed: Navigate to a folder you want to lock. Click icon \u2192 Lock . Confirm you decision in the dialog window. The folder will be locked and marked with a specific symbol . To unlock the folder, you just need to click icon \u2192 Unlock and confirm it in the dialog box.","title":"4.5. Lock a folder"},{"location":"manual/04_Manage_Folder/4._Manage_Folder/","text":"4. Manage Folder \"Details\" view Controls Upload metadata + Create \"Displays\" icon \"Gear\" icon Child objects controls One of the key objects represented in the \" Library \" space is Folder . It is an entity similar to the directories in the file system. Folders are used to structure other CP objects. Folders can be arranged into a tree like a file system, which helps to store information divided by departments, users, programs or other logical groups. Note : there is a special type of Folder - a Project . For details see here . \"Details\" view \" Details \" panel shows contents of the selected folder: subfolders, files, pipelines etc. Controls The following buttons are available at the top of the \" Details \" view of the folder: Upload metadata \" Upload metadata \" ( 1 ) control allows a user to upload new metadata entities (e.g. Samples, Participants) from the .csv/.tsv/.tdf files in a project or another folder selected by a user. It helps to organize and manage metadata in a user's workspace (project or some sandbox folder for collecting metadata). Learn more 5.2. Upload metadata . + Create A user can add: a CP objects to the selected folder with the Create button ( 2 ). See 4.1. Create an object in Folder ; create a CP object from a template. See Appendix B. Working with a Project . Note : The list of templates could be extended. \"Displays\" icon The control (3) includes options to change view of the page: Feature Description Descriptions This feature makes visible addition description for child objects: child objects' attributes description in creating form if it exists. Attributes Attributes control opens/closes attributes pane. Folder's attributes - keys (a) and values (b) will be represented in the metadata pane on the right: Note : If a selected folder has any defined attribute, Attributes pane is shown by default. See how to edit attributes here . Issues This feature shows/hides the issues of the current folder to discuss. To learn more see here \"Gear\" icon There are the following features in \"Gear\" (4) icon: Feature Description Edit folder Rename a folder and set permissions for the folder. See 4.2. Rename folder and 13. Permissions . Clone This feature helps to copy the current folder and its child objects. See more 4.4. Clone a folder . Lock/Unlock You can save your folder and its content from changes by locking it. Learn more 4.5. Lock a folder . Delete A user can delete a folder with the Delete icon. A folder will be deleted if metadata is stored only. See 4.3. Delete Folder . Child objects controls Control CP objects Description Delete Folder Delete ( 1 ) a current folder. Learn more 4.3. Delete Folder . Discussion Folder, Pipeline This icon ( 2 ) allows to create discussion threads in child objects: you can create a topic, leave comments and address them to a specific user. To learn more see here . Run Pipeline Allows running a pipeline from the parent folder. Click on the icon ( 3 ) and the Launch form page will be open. See 6.2. Launch a pipeline . Edit Pipeline, Data storage, Run configuration Click this icon ( 4 ) to edit basic CP object's information: name, description, etc.","title":"4.0. Overview"},{"location":"manual/04_Manage_Folder/4._Manage_Folder/#4-manage-folder","text":"\"Details\" view Controls Upload metadata + Create \"Displays\" icon \"Gear\" icon Child objects controls One of the key objects represented in the \" Library \" space is Folder . It is an entity similar to the directories in the file system. Folders are used to structure other CP objects. Folders can be arranged into a tree like a file system, which helps to store information divided by departments, users, programs or other logical groups. Note : there is a special type of Folder - a Project . For details see here .","title":"4. Manage Folder"},{"location":"manual/04_Manage_Folder/4._Manage_Folder/#details-view","text":"\" Details \" panel shows contents of the selected folder: subfolders, files, pipelines etc.","title":"\"Details\"\u00a0view"},{"location":"manual/04_Manage_Folder/4._Manage_Folder/#controls","text":"The following buttons are available at the top of the \" Details \" view of the folder:","title":"Controls"},{"location":"manual/04_Manage_Folder/4._Manage_Folder/#upload-metadata","text":"\" Upload metadata \" ( 1 ) control allows a user to upload new metadata entities (e.g. Samples, Participants) from the .csv/.tsv/.tdf files in a project or another folder selected by a user. It helps to organize and manage metadata in a user's workspace (project or some sandbox folder for collecting metadata). Learn more 5.2. Upload metadata .","title":"Upload metadata"},{"location":"manual/04_Manage_Folder/4._Manage_Folder/#create","text":"A user can add: a CP objects to the selected folder with the Create button ( 2 ). See 4.1. Create an object in Folder ; create a CP object from a template. See Appendix B. Working with a Project . Note : The list of templates could be extended.","title":"+ Create"},{"location":"manual/04_Manage_Folder/4._Manage_Folder/#displays-icon","text":"The control (3) includes options to change view of the page: Feature Description Descriptions This feature makes visible addition description for child objects: child objects' attributes description in creating form if it exists. Attributes Attributes control opens/closes attributes pane. Folder's attributes - keys (a) and values (b) will be represented in the metadata pane on the right: Note : If a selected folder has any defined attribute, Attributes pane is shown by default. See how to edit attributes here . Issues This feature shows/hides the issues of the current folder to discuss. To learn more see here","title":"\"Displays\" icon"},{"location":"manual/04_Manage_Folder/4._Manage_Folder/#gear-icon","text":"There are the following features in \"Gear\" (4) icon: Feature Description Edit folder Rename a folder and set permissions for the folder. See 4.2. Rename folder and 13. Permissions . Clone This feature helps to copy the current folder and its child objects. See more 4.4. Clone a folder . Lock/Unlock You can save your folder and its content from changes by locking it. Learn more 4.5. Lock a folder . Delete A user can delete a folder with the Delete icon. A folder will be deleted if metadata is stored only. See 4.3. Delete Folder .","title":"\"Gear\" icon"},{"location":"manual/04_Manage_Folder/4._Manage_Folder/#child-objects-controls","text":"Control CP objects Description Delete Folder Delete ( 1 ) a current folder. Learn more 4.3. Delete Folder . Discussion Folder, Pipeline This icon ( 2 ) allows to create discussion threads in child objects: you can create a topic, leave comments and address them to a specific user. To learn more see here . Run Pipeline Allows running a pipeline from the parent folder. Click on the icon ( 3 ) and the Launch form page will be open. See 6.2. Launch a pipeline . Edit Pipeline, Data storage, Run configuration Click this icon ( 4 ) to edit basic CP object's information: name, description, etc.","title":"Child objects controls"},{"location":"manual/05_Manage_Metadata/5.1._Add_Delete_metadata_items/","text":"5.1. Add/Delete metadata items To manage metadata items, a user shall have WRITE permission for the parent folder and the ROLE_ENTITY_MANAGER role. For more information see 13. Permissions . Add metadata item Add SampleSet item Edit metadata items Field values autofill Copy/move metadata items Delete metadata item A user is able to add metadata item manually. Add metadata item To add metadata item the following steps shall be performed: Navigate to Metadata of the desired folder or project. Note : or navigate to specific metadata entity folder, e.g. Participant , Sample , etc. Click the Additional options button and select + Add instance item: The pop-up will be opened, e.g.: Fill up the fields: Instance type ( required ). Choose the metadata item type, e.g. Participant , Sample , etc. Note : if you create an instance from the specific metadata entity folder, the type would be set by default, but you would be able to change it. Note : this dropdown list contains all available instance types (from all Platform metadata entities) Instance ID ( optional ). It should be a unique identification for a new metadata item. If it is not specified, it will be autogenerated during the item creation in the UUID4 format values of the specific entity parameters ( non-required ) - at the picture above, there are: R1_Fastq , R2_Fastq , SampleName Example of the filled pop-up: Click Add parameter to set a new attribute for the creating metadata instance. It could be: String attribute. You can add an attribute with any name and value. Link to a metadata entity. You can choose a link to what a metadata entity you want to add as an attribute, e.g. set a link to a participant existing in the Platform as a sample's attribute. Click the Create button - the new metadata item will be created and shown in the chosen metadata entity table: Note : the created date for each entity is added automatically (see the column \" Created Date \") Add SampleSet item Additionally to described above, users may create SampleSets (or other \"Container-like\") entities from the GUI. To create a new SampleSet: Click the + Add instance button in the Metadata section: Choose the SampleSet instance type: Additional fields for the creating SampleSet will appear automatically. Provide the information for the new SampleSet: InstanceID ( optional ) - if it is not specified, it will be autogenerated during the item creation in the UUID4 format Name - name of the SampleSet object Samples - Click the Browse button to select a list of Samples, which will be associated with the creating SampleSet Click the Create button to add the new SampleSet: Open the SampleSet metadata class to view the new SampleSet: Edit metadata items To edit any metadata item - click its row in the table, e.g.: Values of the clicked metadata instance will be displayed in the \"Attributes\" panel, where columns and metadata instance values represent corresponding keys and values. Here, you can edit any instance value except ID and Created Date . To edit existing value: Click the desired value in the \"Attributes\" panel, e.g.: Specify a new value to the field. Click any space out of that field or press Enter key. Specified value will be saved automatically and will appear in the metadata table, e.g.: To remove existing value: Click the remove button near the desired key-value pair in the \"Attributes\" panel, e.g.: Confirm the deletion: This attribute will be removed for the selected instance: Note : if attributes of the same key will be removed in the described way for all metadata instances - the corresponding column will disappear from the table. To create a new instance field: Click \" + Add \" button in the \"Attributes\" panel: Specify key (name of the field) and value for a new attribute, e.g.: Click the Add button to confirm. If the specified key doesn't match any existing field name of the current metadata entity - a new field will be created and the corresponding column will automatically appear in the metadata table: For more information about working with the \"Attributes\" panel see 17. CP objects tagging by additional attributes . Field values autofill You can use an autofill feature to faster fill metadata instances with data that are based on data in other instances in the same column/row. To use an autofill feature: Hover over the cell in the metadata table, which value you wish to use in other cells, e.g.: Click the right-bottom corner of the cell (on the small blue rectangle) and move the mouse holding the left button - vertically (if you wish to autofill cells in the column) or horizontally (if you wish to autofill cells in the row), e.g.: Once you will release the mouse button - selected cells will be autofilled by the value of the cell that you've dragged (the cell value will be copied to all selected cells): In the right-bottom corner of the selected area, the additional control will appear. Click it: If you want to cancel autofilling - click the Revert item from the list. If you want to perform autofilling again - click the Copy cells item from the list. Click any cell out of the selected area or press Esc key to confirm changes: Note : if the cell that will be dragged has the format ending by the number - then during the autofilling, the number series will be atomatically continued for the whole selected area (to increasing), textual part of the value format will stay the same. I.e. if the cell has value SN_001 , then via the autofilling the new appeared values will be SN_002 , SN_003 , etc.: Copy/move metadata items You can copy or move metadata items from one folder to other: Navigate to Metadata of the desired folder or project. Open specific metadata entity folder (e.g. Participant , Sample , etc.): Select items you want to copy/move. The Bulk operation panel is enabled. Click the V button next the \" Show only selected items \" control and select the Copy item in the list: The pop-up appears: Here the Folder/Project should be selected where selected Metadata items will be copied/moved. Select the Folder/Project you want using the tree on the left panel, e.g.: Click the Copy button to create a copy of the Metadata items in the selected folder or the Move button if you wish to move items to selected folder from the original one. The pop-up will be closed. Open the folder selected at step 6, see that Metadata items were copied/moved to it: Delete metadata item To delete metadata item the following steps shall be performed: Navigate to the metadata entity table that contains the metadata item you want to delete. Tick one metadata item or more - the Bulk operation panel is enabled. Click the V button next the \" Show only selected items \" control and select the Delete item in the list: Confirm your choice in the dialog window: The items are removed.","title":"5.1. Add/delete Metadata items"},{"location":"manual/05_Manage_Metadata/5.1._Add_Delete_metadata_items/#51-adddelete-metadata-items","text":"To manage metadata items, a user shall have WRITE permission for the parent folder and the ROLE_ENTITY_MANAGER role. For more information see 13. Permissions . Add metadata item Add SampleSet item Edit metadata items Field values autofill Copy/move metadata items Delete metadata item A user is able to add metadata item manually.","title":"5.1. Add/Delete metadata items"},{"location":"manual/05_Manage_Metadata/5.1._Add_Delete_metadata_items/#add-metadata-item","text":"To add metadata item the following steps shall be performed: Navigate to Metadata of the desired folder or project. Note : or navigate to specific metadata entity folder, e.g. Participant , Sample , etc. Click the Additional options button and select + Add instance item: The pop-up will be opened, e.g.: Fill up the fields: Instance type ( required ). Choose the metadata item type, e.g. Participant , Sample , etc. Note : if you create an instance from the specific metadata entity folder, the type would be set by default, but you would be able to change it. Note : this dropdown list contains all available instance types (from all Platform metadata entities) Instance ID ( optional ). It should be a unique identification for a new metadata item. If it is not specified, it will be autogenerated during the item creation in the UUID4 format values of the specific entity parameters ( non-required ) - at the picture above, there are: R1_Fastq , R2_Fastq , SampleName Example of the filled pop-up: Click Add parameter to set a new attribute for the creating metadata instance. It could be: String attribute. You can add an attribute with any name and value. Link to a metadata entity. You can choose a link to what a metadata entity you want to add as an attribute, e.g. set a link to a participant existing in the Platform as a sample's attribute. Click the Create button - the new metadata item will be created and shown in the chosen metadata entity table: Note : the created date for each entity is added automatically (see the column \" Created Date \")","title":"Add metadata item"},{"location":"manual/05_Manage_Metadata/5.1._Add_Delete_metadata_items/#add-sampleset-item","text":"Additionally to described above, users may create SampleSets (or other \"Container-like\") entities from the GUI. To create a new SampleSet: Click the + Add instance button in the Metadata section: Choose the SampleSet instance type: Additional fields for the creating SampleSet will appear automatically. Provide the information for the new SampleSet: InstanceID ( optional ) - if it is not specified, it will be autogenerated during the item creation in the UUID4 format Name - name of the SampleSet object Samples - Click the Browse button to select a list of Samples, which will be associated with the creating SampleSet Click the Create button to add the new SampleSet: Open the SampleSet metadata class to view the new SampleSet:","title":"Add SampleSet item"},{"location":"manual/05_Manage_Metadata/5.1._Add_Delete_metadata_items/#edit-metadata-items","text":"To edit any metadata item - click its row in the table, e.g.: Values of the clicked metadata instance will be displayed in the \"Attributes\" panel, where columns and metadata instance values represent corresponding keys and values. Here, you can edit any instance value except ID and Created Date . To edit existing value: Click the desired value in the \"Attributes\" panel, e.g.: Specify a new value to the field. Click any space out of that field or press Enter key. Specified value will be saved automatically and will appear in the metadata table, e.g.: To remove existing value: Click the remove button near the desired key-value pair in the \"Attributes\" panel, e.g.: Confirm the deletion: This attribute will be removed for the selected instance: Note : if attributes of the same key will be removed in the described way for all metadata instances - the corresponding column will disappear from the table. To create a new instance field: Click \" + Add \" button in the \"Attributes\" panel: Specify key (name of the field) and value for a new attribute, e.g.: Click the Add button to confirm. If the specified key doesn't match any existing field name of the current metadata entity - a new field will be created and the corresponding column will automatically appear in the metadata table: For more information about working with the \"Attributes\" panel see 17. CP objects tagging by additional attributes .","title":"Edit metadata items"},{"location":"manual/05_Manage_Metadata/5.1._Add_Delete_metadata_items/#field-values-autofill","text":"You can use an autofill feature to faster fill metadata instances with data that are based on data in other instances in the same column/row. To use an autofill feature: Hover over the cell in the metadata table, which value you wish to use in other cells, e.g.: Click the right-bottom corner of the cell (on the small blue rectangle) and move the mouse holding the left button - vertically (if you wish to autofill cells in the column) or horizontally (if you wish to autofill cells in the row), e.g.: Once you will release the mouse button - selected cells will be autofilled by the value of the cell that you've dragged (the cell value will be copied to all selected cells): In the right-bottom corner of the selected area, the additional control will appear. Click it: If you want to cancel autofilling - click the Revert item from the list. If you want to perform autofilling again - click the Copy cells item from the list. Click any cell out of the selected area or press Esc key to confirm changes: Note : if the cell that will be dragged has the format ending by the number - then during the autofilling, the number series will be atomatically continued for the whole selected area (to increasing), textual part of the value format will stay the same. I.e. if the cell has value SN_001 , then via the autofilling the new appeared values will be SN_002 , SN_003 , etc.:","title":"Field values autofill"},{"location":"manual/05_Manage_Metadata/5.1._Add_Delete_metadata_items/#copymove-metadata-items","text":"You can copy or move metadata items from one folder to other: Navigate to Metadata of the desired folder or project. Open specific metadata entity folder (e.g. Participant , Sample , etc.): Select items you want to copy/move. The Bulk operation panel is enabled. Click the V button next the \" Show only selected items \" control and select the Copy item in the list: The pop-up appears: Here the Folder/Project should be selected where selected Metadata items will be copied/moved. Select the Folder/Project you want using the tree on the left panel, e.g.: Click the Copy button to create a copy of the Metadata items in the selected folder or the Move button if you wish to move items to selected folder from the original one. The pop-up will be closed. Open the folder selected at step 6, see that Metadata items were copied/moved to it:","title":"Copy/move metadata items"},{"location":"manual/05_Manage_Metadata/5.1._Add_Delete_metadata_items/#delete-metadata-item","text":"To delete metadata item the following steps shall be performed: Navigate to the metadata entity table that contains the metadata item you want to delete. Tick one metadata item or more - the Bulk operation panel is enabled. Click the V button next the \" Show only selected items \" control and select the Delete item in the list: Confirm your choice in the dialog window: The items are removed.","title":"Delete metadata item"},{"location":"manual/05_Manage_Metadata/5.2._Upload_metadata/","text":"5.2. Upload metadata To upload a Metadata to a Folder you need to have WRITE permission for that folder and the ROLE_ENTITY_MANAGER role. For more information see 13. Permissions . Each folder allows uploading metadata to its space. But to get the full set of features it is advised to upload the metadata into the special types of folders called Projects . How to create a project described here . Uploading could be executed from the csv or tsv file. The structure of load file should be as in Table 1 . Table 1 - Load file structure (referenced_entity_type):ID membership:MembershipAttributeName:(referenced_entity_type):ID AttributeName referenced_uid_value referenced_uid_value attribute_value referenced_uid_value referenced_uid_value attribute_value referenced_uid_value referenced_uid_value attribute_value Examples: wes-11-rep-samples.csv , wes-11-rep-set.csv (referenced_entity_type):ID - here you set up what type of entity you are uploading and what is an ID of it, e.g. Sample:ID = FZ700059549 . membership:MembershipAttributeName:(referenced_entity_type):ID - this is a reference to an instance of another entity. If you have a sample owned by a participant, you can show this connection - membership:Participants:Participant:ID = TY90000044343 . Note : make sure, that the reference entity id already exists in the system. AttributeName - any attribute name of an instance, e.g. RNA or DNA type, Blood or Tissue, etc. Note : specified instance ID should be a unique identification for an each metadata item. If it is not specified in the file (the field is left empty), it will be autogenerated during the item creation in the UUID4 format. Upload instances of an entity For the example, we will use the simple metadata file with the following content: Click the button in the right upper corner of the folder you want to add metadata. OR click the Additional options button and select the Upload metadata item for existing metadata object: The Uploading metadata window is open. Select the appropriate file. Confirm choice - click the OK button. Note : if your file contains a link to the instance that doesn't exist, the whole metadata file won't be uploaded. For example, you set a link to a participant ID that doesn't exist in the file with samples . Note : while uploading the progress bar will be shown, e.g.: The new metadata is uploaded in the folder of a specific entity in the project: See that: for the ID field that has empty value in the uploaded file, instance ID was autogenerated the \"Created date\" attribute is being added automatically Note : the instances will be uploaded to the entity with a name, related to the first column if exists (e.g. Sample ). Otherwise, the new entity folder will be created. Note : if the file contains instances already exist in the system, the attributes of it will be updated.","title":"5.2. Upload Metadata"},{"location":"manual/05_Manage_Metadata/5.2._Upload_metadata/#52-upload-metadata","text":"To upload a Metadata to a Folder you need to have WRITE permission for that folder and the ROLE_ENTITY_MANAGER role. For more information see 13. Permissions . Each folder allows uploading metadata to its space. But to get the full set of features it is advised to upload the metadata into the special types of folders called Projects . How to create a project described here . Uploading could be executed from the csv or tsv file. The structure of load file should be as in Table 1 . Table 1 - Load file structure (referenced_entity_type):ID membership:MembershipAttributeName:(referenced_entity_type):ID AttributeName referenced_uid_value referenced_uid_value attribute_value referenced_uid_value referenced_uid_value attribute_value referenced_uid_value referenced_uid_value attribute_value Examples: wes-11-rep-samples.csv , wes-11-rep-set.csv (referenced_entity_type):ID - here you set up what type of entity you are uploading and what is an ID of it, e.g. Sample:ID = FZ700059549 . membership:MembershipAttributeName:(referenced_entity_type):ID - this is a reference to an instance of another entity. If you have a sample owned by a participant, you can show this connection - membership:Participants:Participant:ID = TY90000044343 . Note : make sure, that the reference entity id already exists in the system. AttributeName - any attribute name of an instance, e.g. RNA or DNA type, Blood or Tissue, etc. Note : specified instance ID should be a unique identification for an each metadata item. If it is not specified in the file (the field is left empty), it will be autogenerated during the item creation in the UUID4 format.","title":"5.2. Upload metadata"},{"location":"manual/05_Manage_Metadata/5.2._Upload_metadata/#upload-instances-of-an-entity","text":"For the example, we will use the simple metadata file with the following content: Click the button in the right upper corner of the folder you want to add metadata. OR click the Additional options button and select the Upload metadata item for existing metadata object: The Uploading metadata window is open. Select the appropriate file. Confirm choice - click the OK button. Note : if your file contains a link to the instance that doesn't exist, the whole metadata file won't be uploaded. For example, you set a link to a participant ID that doesn't exist in the file with samples . Note : while uploading the progress bar will be shown, e.g.: The new metadata is uploaded in the folder of a specific entity in the project: See that: for the ID field that has empty value in the uploaded file, instance ID was autogenerated the \"Created date\" attribute is being added automatically Note : the instances will be uploaded to the entity with a name, related to the first column if exists (e.g. Sample ). Otherwise, the new entity folder will be created. Note : if the file contains instances already exist in the system, the attributes of it will be updated.","title":"Upload instances of an entity"},{"location":"manual/05_Manage_Metadata/5.3._Customize_view_of_the_entity_instance_table/","text":"5.3. Customize view of the entity instance table Change attributes view Filter instances Sorting To customize view of the table with instances of an entity you need to have READ permissions for the folder with that metadata. For more information see 13. Permissions . This page describes how a user can customize an entity instance table and make it more easy to read. Any user, who has permissions for reading, will be able to customize the view of the table: change set of viewed attributes, change the order of attributes, filter instances and reset the default settings. Change attributes view Press the \"Change view\" control. A list of accessible attributes shows up. The list contains all attributes of entities, uploaded in the current project. The attributes of current view are ticked. Choose desired attributes by ticking. Note : The last tick will be disabled, so you can't clear all checkboxes. To change the order, click a control in front of the desired attribute and pulling it up or down. The changes are applied instantly. The table view has only selected attributes. Note : This customization saved only for logged in user. To leave the form a user shall click outside the form. For example, attributes were customized as shown: And the table automatically changed its view to: Note : you are able to restore the initial order of columns. To do that, click the Reset Columns control in the Change view panel. Filter instances Date filter User can restrict shown metadata instances by filtering them according to their created date. For that: Click the filter icon next to the \" Created Date \" column header The date filter panel will appear: Specify dates \" From \" and \" To \", e.g.: Click the Apply button to confirm: Filter will be applied and in the table, only instances with the \" Created Date \" according to a specified date range will be shown To reset applied date filter, specified for the \" Created date \" column, click the filter icon next to the column header and click the Reset button: Note : if only \" From \" date was specified - filter will be applied for all instances with the \" Created Date \" from specified date till today. Note : if only \" To \" date was specified - filter will be applied for all instances with the \" Created Date \" till specified date. Column filter Also user can filter instances of an entity in a table by their attribute values. To filter a table by any column value(s): Click the filter icon next to the column header (for the column \" Created Date \", see the section above ) The column filter panel will appear: Specify filter text and press Enter key, e.g.: You can specify another one or several filters in way as described at step 3, e.g.: Several filters will combine with logical OR operation. If you want to cancel the certain specified filter - click the cross-button next to filter text - . After all desired filters are specified, click the Apply button. Filters will be applied and in the table, only instances matching the specified filters will be shown. To reset applied filters, specified for the column, click the filter icon next to the column header and click the Reset button: Common notes for both filter types: user can apply filters for any count of columns when any filter is set for the column, the filter icon next to the column header is being highlighted in blue, e.g.: to reset all filters, click the Clear filters button above the metadata table - Metadata table sorting To sort instances of an entity in a table, click a header of the desired column, e.g.: This action will sort a list in a descending order by this column (see the corresponding icon near the column header). Next click by the same header will sort a list in an ascending order: Next click by the same header will reset sorting by this column. To sort instances of an entity in a table by several columns: Sort a list by the column you wish to sort in the first order (as described above). Click a header you wish to sort in the second order, e.g.: Near each header of the sorted columns, there will be a mark with the sorting order. You may sort a list in such way by any count of columns. You may customize sorting for any column independently to others - by additional clicks on its header.","title":"5.3. Customize view of the entity instance table"},{"location":"manual/05_Manage_Metadata/5.3._Customize_view_of_the_entity_instance_table/#53-customize-view-of-the-entity-instance-table","text":"Change attributes view Filter instances Sorting To customize view of the table with instances of an entity you need to have READ permissions for the folder with that metadata. For more information see 13. Permissions . This page describes how a user can customize an entity instance table and make it more easy to read. Any user, who has permissions for reading, will be able to customize the view of the table: change set of viewed attributes, change the order of attributes, filter instances and reset the default settings.","title":"5.3. Customize view of the entity instance table"},{"location":"manual/05_Manage_Metadata/5.3._Customize_view_of_the_entity_instance_table/#change-attributes-view","text":"Press the \"Change view\" control. A list of accessible attributes shows up. The list contains all attributes of entities, uploaded in the current project. The attributes of current view are ticked. Choose desired attributes by ticking. Note : The last tick will be disabled, so you can't clear all checkboxes. To change the order, click a control in front of the desired attribute and pulling it up or down. The changes are applied instantly. The table view has only selected attributes. Note : This customization saved only for logged in user. To leave the form a user shall click outside the form. For example, attributes were customized as shown: And the table automatically changed its view to: Note : you are able to restore the initial order of columns. To do that, click the Reset Columns control in the Change view panel.","title":"Change attributes view"},{"location":"manual/05_Manage_Metadata/5.3._Customize_view_of_the_entity_instance_table/#filter-instances","text":"","title":"Filter instances"},{"location":"manual/05_Manage_Metadata/5.3._Customize_view_of_the_entity_instance_table/#date-filter","text":"User can restrict shown metadata instances by filtering them according to their created date. For that: Click the filter icon next to the \" Created Date \" column header The date filter panel will appear: Specify dates \" From \" and \" To \", e.g.: Click the Apply button to confirm: Filter will be applied and in the table, only instances with the \" Created Date \" according to a specified date range will be shown To reset applied date filter, specified for the \" Created date \" column, click the filter icon next to the column header and click the Reset button: Note : if only \" From \" date was specified - filter will be applied for all instances with the \" Created Date \" from specified date till today. Note : if only \" To \" date was specified - filter will be applied for all instances with the \" Created Date \" till specified date.","title":"Date filter"},{"location":"manual/05_Manage_Metadata/5.3._Customize_view_of_the_entity_instance_table/#column-filter","text":"Also user can filter instances of an entity in a table by their attribute values. To filter a table by any column value(s): Click the filter icon next to the column header (for the column \" Created Date \", see the section above ) The column filter panel will appear: Specify filter text and press Enter key, e.g.: You can specify another one or several filters in way as described at step 3, e.g.: Several filters will combine with logical OR operation. If you want to cancel the certain specified filter - click the cross-button next to filter text - . After all desired filters are specified, click the Apply button. Filters will be applied and in the table, only instances matching the specified filters will be shown. To reset applied filters, specified for the column, click the filter icon next to the column header and click the Reset button: Common notes for both filter types: user can apply filters for any count of columns when any filter is set for the column, the filter icon next to the column header is being highlighted in blue, e.g.: to reset all filters, click the Clear filters button above the metadata table -","title":"Column filter"},{"location":"manual/05_Manage_Metadata/5.3._Customize_view_of_the_entity_instance_table/#metadata-table-sorting","text":"To sort instances of an entity in a table, click a header of the desired column, e.g.: This action will sort a list in a descending order by this column (see the corresponding icon near the column header). Next click by the same header will sort a list in an ascending order: Next click by the same header will reset sorting by this column. To sort instances of an entity in a table by several columns: Sort a list by the column you wish to sort in the first order (as described above). Click a header you wish to sort in the second order, e.g.: Near each header of the sorted columns, there will be a mark with the sorting order. You may sort a list in such way by any count of columns. You may customize sorting for any column independently to others - by additional clicks on its header.","title":"Metadata table sorting"},{"location":"manual/05_Manage_Metadata/5.4._Launch_a_run_configuration_on_metadata/","text":"5.4. Launch a run configuration on metadata To launch a run configuration on metadata a user shall have the following permissions: READ permissions a for folder contains metadata items; READ and EXECUTE permissions for a run configuration. For more information see 13. Permissions . A user could launch a run configuration on selected metadata from the metadata space. To launch a run configuration, the following steps shall be performed: Navigate to the desired folder with metadata items, e.g.: Tick desired metadata items (see the picture below, 1 ). The Bulk operation panel will be enabled. Click the Run button (see the picture above, 2 ) - and the \"Select configuration\" pop-up window will be open. Note : Clicking on a configuration item, you'll see configuration's parameters. The second click will hide the parameters. Note : if you want to choose a run configuration with Root entity that differs from selected metadata items, please, specify it in \"Define expression\" field. To learn more about expressions, see here . Click \"OK\" button - and the runs will be scheduled. You'll be redirected into Run space automatically.","title":"5.4. Launch a Run configuration"},{"location":"manual/05_Manage_Metadata/5.4._Launch_a_run_configuration_on_metadata/#54-launch-a-run-configuration-on-metadata","text":"To launch a run configuration on metadata a user shall have the following permissions: READ permissions a for folder contains metadata items; READ and EXECUTE permissions for a run configuration. For more information see 13. Permissions . A user could launch a run configuration on selected metadata from the metadata space. To launch a run configuration, the following steps shall be performed: Navigate to the desired folder with metadata items, e.g.: Tick desired metadata items (see the picture below, 1 ). The Bulk operation panel will be enabled. Click the Run button (see the picture above, 2 ) - and the \"Select configuration\" pop-up window will be open. Note : Clicking on a configuration item, you'll see configuration's parameters. The second click will hide the parameters. Note : if you want to choose a run configuration with Root entity that differs from selected metadata items, please, specify it in \"Define expression\" field. To learn more about expressions, see here . Click \"OK\" button - and the runs will be scheduled. You'll be redirected into Run space automatically.","title":"5.4. Launch a run configuration on metadata"},{"location":"manual/05_Manage_Metadata/5.5._Download_data_from_external_resources_to_the_cloud_data_storage/","text":"5.5. Download data from external resources to the cloud data storage Overview Upload metadata with list of external resources from CSV/TSV file Download data from external resources Overview Users often get the raw datasets from the external partners for processing. CP provides comfortable way to load a list of such files as external links to the CP GUI and launch a data load procedure to the cloud storage in background mode. Users can provide CSV/TSV files with the external links and submit a data transfer job, so that the files will be moved to the cloud storage in the background. Upload metadata with list of external resources from CSV/TSV file Note : To upload a Metadata to a Folder you need to have WRITE permission for that folder and the ROLE_ENTITY_MANAGER role. For more information see 13. Permissions . Note : file with list of external resources should have PATH column and http/ftp links in this column (example: sample.csv ) Navigate on folder. Click on \"Upload metadata\" button. Navigate to CSV/TSV file with list of external resources in appeared dialog and select file. Click \" Open \" button. Click on \"Metadata\" object: Click on \"Sample\" class: List of samples with the external links will be appeared: Download data from external resources Click the Additional options button in the right-upper corner and select Transfer to the cloud item: The pop-up window for preparing for transferring will appear: In this window fill fields: a . Input data storage which will be use as a destination for downloading data. Click on button. In opened pop-up window select required storage (on the left panel with folder-tree) ( 1 ). In selected storage set the checkbox opposite the folder name, where external data will be downloaded ( 2 ). Click \" Ok \" button ( 3 ): Note : you need to have READ and WRITE permissions for folder and data storage, that contain folder for downloading. b . Select CSV/TSV columns (only from PATH columns), which shall be used to get external URLs (if several columns contain URLs - all can be used). c . ( optionally ) Select whether to rename resulting path to some other value (can be specified as another column cell, e.g. sample name). For do that click on button and select from dropdown list. d . ( optionally ) Input max threads count, if needs to limit. e . ( optionally ) Set if needs to create new folders within destination in case when several columns are selected for \" Path fields \" option ( b ). E.g. if two columns contain URLs and both are selected - then folders will be created for the corresponding column name and used for appropriate files storage. f . ( optionally ) Set if needs to update external URL within a table to the new location of the files. If set - http/ftp URLs will be changed to data storage path. Such data structure can be then used for a processing by a pipeline: URLs are changed to the clickable storage-hyperlinks (checkbox is set): URLs aren't changed and not-clickable hyperlinks (checkbox isn't set): Once you filled the form, click \" Start download \" button: System pipeline (transfer job) will be started automatically: Once the transfer job will be finished successfully, files will be located into the selected data storage:","title":"5.5. Download data from external resources"},{"location":"manual/05_Manage_Metadata/5.5._Download_data_from_external_resources_to_the_cloud_data_storage/#55-download-data-from-external-resources-to-the-cloud-data-storage","text":"Overview Upload metadata with list of external resources from CSV/TSV file Download data from external resources","title":"5.5. Download data from external resources to the cloud data storage"},{"location":"manual/05_Manage_Metadata/5.5._Download_data_from_external_resources_to_the_cloud_data_storage/#overview","text":"Users often get the raw datasets from the external partners for processing. CP provides comfortable way to load a list of such files as external links to the CP GUI and launch a data load procedure to the cloud storage in background mode. Users can provide CSV/TSV files with the external links and submit a data transfer job, so that the files will be moved to the cloud storage in the background.","title":"Overview"},{"location":"manual/05_Manage_Metadata/5.5._Download_data_from_external_resources_to_the_cloud_data_storage/#upload-metadata-with-list-of-external-resources-from-csvtsv-file","text":"Note : To upload a Metadata to a Folder you need to have WRITE permission for that folder and the ROLE_ENTITY_MANAGER role. For more information see 13. Permissions . Note : file with list of external resources should have PATH column and http/ftp links in this column (example: sample.csv ) Navigate on folder. Click on \"Upload metadata\" button. Navigate to CSV/TSV file with list of external resources in appeared dialog and select file. Click \" Open \" button. Click on \"Metadata\" object: Click on \"Sample\" class: List of samples with the external links will be appeared:","title":"Upload metadata with list of external resources from CSV/TSV file"},{"location":"manual/05_Manage_Metadata/5.5._Download_data_from_external_resources_to_the_cloud_data_storage/#download-data-from-external-resources","text":"Click the Additional options button in the right-upper corner and select Transfer to the cloud item: The pop-up window for preparing for transferring will appear: In this window fill fields: a . Input data storage which will be use as a destination for downloading data. Click on button. In opened pop-up window select required storage (on the left panel with folder-tree) ( 1 ). In selected storage set the checkbox opposite the folder name, where external data will be downloaded ( 2 ). Click \" Ok \" button ( 3 ): Note : you need to have READ and WRITE permissions for folder and data storage, that contain folder for downloading. b . Select CSV/TSV columns (only from PATH columns), which shall be used to get external URLs (if several columns contain URLs - all can be used). c . ( optionally ) Select whether to rename resulting path to some other value (can be specified as another column cell, e.g. sample name). For do that click on button and select from dropdown list. d . ( optionally ) Input max threads count, if needs to limit. e . ( optionally ) Set if needs to create new folders within destination in case when several columns are selected for \" Path fields \" option ( b ). E.g. if two columns contain URLs and both are selected - then folders will be created for the corresponding column name and used for appropriate files storage. f . ( optionally ) Set if needs to update external URL within a table to the new location of the files. If set - http/ftp URLs will be changed to data storage path. Such data structure can be then used for a processing by a pipeline: URLs are changed to the clickable storage-hyperlinks (checkbox is set): URLs aren't changed and not-clickable hyperlinks (checkbox isn't set): Once you filled the form, click \" Start download \" button: System pipeline (transfer job) will be started automatically: Once the transfer job will be finished successfully, files will be located into the selected data storage:","title":"Download data\u00a0from external resources"},{"location":"manual/05_Manage_Metadata/5._Manage_Metadata/","text":"5. Manage Metadata Overview \"Details\" view Controls Search field \"Customize view\" Additional options Sorting control Filter control Bulk operation panel Overview Metadata is a CP object that defines custom data entities (see the definition below) associated with raw data files (fastq, bcl, etc.) or data parameters (see the picture below, arrow 1 ). By using this object a user can create a complex analysis environment. For example, you can customize your analysis to work with a subset of your data. Two important concepts of the metadata object is an Entity and an Instance of an entity . Entity - abstract category of comparable objects. For example, entity \"Sample\" can contain sequencing data from different people (see the picture below, arrow 2 ). An Instance of an entity - a specific representation of an entity. For example, sequencing data from a particular patient in the \"Sample\" entity is an instance of that entity (see the picture below, arrow 3 ). \"Details\" view \"Details\" panel displays content as a table of entity instances. Each column is an attribute of an instance, which is duplicated in the \"Attribute\" panel. Note : more about managing instance's attribute you can learn here . Controls The following controls (buttons and fields) are available in the metadata entity space: Search field To find a particular instance of an entity a user shall use the Search field (see the picture above , 1 ), which is searching for the occurrence of entered text in any column of the table (except \" Created Date \"), e.g.: The search field supports multiple terms search - in this case, multiple terms for the search should be specified space separated. E.g. sample1 sample2 - will be found all instances containing sample1 or sample2 in any attribute value. The search field supports a key:value search, where key is an attribute name (column header) and value is a term that shall be searched in that attribute values. E.g. ID:D703 - will be found all instances which ID contain D703 string. \"Customize view\" This control (see the picture above , 2 ) allows customizing the view of the table with instances of an entity. For more information see 5.3. Customize view of the entity instance table . Additional options This button (see the picture above , 3 ) expand the menu with additional options: Here: + Add instance item - to add a new instance in the current metadata container. For more information see 5.1. Add/Delete metadata items . Upload metadata item - use this item to create the metadata object/to add entities to the existing metadata object/to add instances of the existing metadata entity. See here for more information - 5.2. Upload metadata . Transfer to the cloud item - to download files from the external ftp / http resources. See here for more information - 5.5. Transfer data to the cloud . Delete class item - to delete entity class (with all its instances) from the current metadata object. Show attributes / Hide attributes item - this button allows to view or edit attributes of a particular instance of an entity. For more information see 17. CP objects tagging by additional attributes . When this item is enabled - values of the clicked metadata instance are displaying in the \"Attributes\" panel, where columns and metadata instance values represent corresponding keys and values, e.g.: User can edit any instance value here except ID and Created Date . Sorting control To sort instances of an entity in a table, user shall click a header of the desired column (e.g., see the picture above , 4 ): first click sorts a list in an descending order, the next click sorts a list in a ascending order, the next click reset sorting. Filter control To filter instances of an entity in a table, user shall click a special control near a header of the desired column (see the picture above , 5 ). The panel will appear, where one or several filter conditions for that column can be specified and then applied, e.g.: For more information see 5.3. Customize view of the entity instance table . Bulk operation panel At this panel, there are controls that allow to execute operations for one and more instances. User can tick desired items and the panel switches to active mode, e.g.: Here: \" Show only selected items \"/\" Show all metadata items \" control - to show separately only selected items. All unselected items will be hidden, e.g.: This feature can be useful if the metadata object has lots of instances, these instances span multiple pages, but user has interest to view/work only with several instances from different pages. For shown selected items, all functionality as for the general table is available except filtering. To return to the previous view (all metadata items), user shall click this control again. \" Manage selection \" menu - this menu appears by V button next the \" Show only selected items \" control: From this menu, user can: Clear selection - clears all selected items. The \" Bulk operation panel \" is deactivated. Copy - allows to copy/move selected items to another metadata object (in other Project/Folder). See more details here . Delete - removes selected items. See more details here . RUN button - allows to execute run configurations for the selected items. For more details see 5.4. Launch a run configuration on metadata .","title":"5.0. Overview"},{"location":"manual/05_Manage_Metadata/5._Manage_Metadata/#5-manage-metadata","text":"Overview \"Details\" view Controls Search field \"Customize view\" Additional options Sorting control Filter control Bulk operation panel","title":"5. Manage Metadata"},{"location":"manual/05_Manage_Metadata/5._Manage_Metadata/#overview","text":"Metadata is a CP object that defines custom data entities (see the definition below) associated with raw data files (fastq, bcl, etc.) or data parameters (see the picture below, arrow 1 ). By using this object a user can create a complex analysis environment. For example, you can customize your analysis to work with a subset of your data. Two important concepts of the metadata object is an Entity and an Instance of an entity . Entity - abstract category of comparable objects. For example, entity \"Sample\" can contain sequencing data from different people (see the picture below, arrow 2 ). An Instance of an entity - a specific representation of an entity. For example, sequencing data from a particular patient in the \"Sample\" entity is an instance of that entity (see the picture below, arrow 3 ).","title":"Overview"},{"location":"manual/05_Manage_Metadata/5._Manage_Metadata/#details-view","text":"\"Details\" panel displays content as a table of entity instances. Each column is an attribute of an instance, which is duplicated in the \"Attribute\" panel. Note : more about managing instance's attribute you can learn here .","title":"\"Details\"\u00a0view"},{"location":"manual/05_Manage_Metadata/5._Manage_Metadata/#controls","text":"The following controls (buttons and fields) are available in the metadata entity space:","title":"Controls"},{"location":"manual/05_Manage_Metadata/5._Manage_Metadata/#search-field","text":"To find a particular instance of an entity a user shall use the Search field (see the picture above , 1 ), which is searching for the occurrence of entered text in any column of the table (except \" Created Date \"), e.g.: The search field supports multiple terms search - in this case, multiple terms for the search should be specified space separated. E.g. sample1 sample2 - will be found all instances containing sample1 or sample2 in any attribute value. The search field supports a key:value search, where key is an attribute name (column header) and value is a term that shall be searched in that attribute values. E.g. ID:D703 - will be found all instances which ID contain D703 string.","title":"Search field"},{"location":"manual/05_Manage_Metadata/5._Manage_Metadata/#customize-view","text":"This control (see the picture above , 2 ) allows customizing the view of the table with instances of an entity. For more information see 5.3. Customize view of the entity instance table .","title":"\"Customize view\""},{"location":"manual/05_Manage_Metadata/5._Manage_Metadata/#additional-options","text":"This button (see the picture above , 3 ) expand the menu with additional options: Here: + Add instance item - to add a new instance in the current metadata container. For more information see 5.1. Add/Delete metadata items . Upload metadata item - use this item to create the metadata object/to add entities to the existing metadata object/to add instances of the existing metadata entity. See here for more information - 5.2. Upload metadata . Transfer to the cloud item - to download files from the external ftp / http resources. See here for more information - 5.5. Transfer data to the cloud . Delete class item - to delete entity class (with all its instances) from the current metadata object. Show attributes / Hide attributes item - this button allows to view or edit attributes of a particular instance of an entity. For more information see 17. CP objects tagging by additional attributes . When this item is enabled - values of the clicked metadata instance are displaying in the \"Attributes\" panel, where columns and metadata instance values represent corresponding keys and values, e.g.: User can edit any instance value here except ID and Created Date .","title":"Additional options"},{"location":"manual/05_Manage_Metadata/5._Manage_Metadata/#sorting-control","text":"To sort instances of an entity in a table, user shall click a header of the desired column (e.g., see the picture above , 4 ): first click sorts a list in an descending order, the next click sorts a list in a ascending order, the next click reset sorting.","title":"Sorting control"},{"location":"manual/05_Manage_Metadata/5._Manage_Metadata/#filter-control","text":"To filter instances of an entity in a table, user shall click a special control near a header of the desired column (see the picture above , 5 ). The panel will appear, where one or several filter conditions for that column can be specified and then applied, e.g.: For more information see 5.3. Customize view of the entity instance table .","title":"Filter control"},{"location":"manual/05_Manage_Metadata/5._Manage_Metadata/#bulk-operation-panel","text":"At this panel, there are controls that allow to execute operations for one and more instances. User can tick desired items and the panel switches to active mode, e.g.: Here: \" Show only selected items \"/\" Show all metadata items \" control - to show separately only selected items. All unselected items will be hidden, e.g.: This feature can be useful if the metadata object has lots of instances, these instances span multiple pages, but user has interest to view/work only with several instances from different pages. For shown selected items, all functionality as for the general table is available except filtering. To return to the previous view (all metadata items), user shall click this control again. \" Manage selection \" menu - this menu appears by V button next the \" Show only selected items \" control: From this menu, user can: Clear selection - clears all selected items. The \" Bulk operation panel \" is deactivated. Copy - allows to copy/move selected items to another metadata object (in other Project/Folder). See more details here . Delete - removes selected items. See more details here . RUN button - allows to execute run configurations for the selected items. For more details see 5.4. Launch a run configuration on metadata .","title":"Bulk operation panel"},{"location":"manual/06_Manage_Pipeline/6.1.1._Building_WDL_pipeline_with_graphical_PipelineBuilder/","text":"6.1.1. Building WDL pipeline with graphical PipelineBuilder Overview Creating a new pipeline with a Pipeline Builder Overriding docker image for a specific task Example Pipeline Search over the Graph To create a new WDL pipeline in a Folder you need to have WRITE permissions for that folder and the ROLE_PIPELINE_MANAGER role. For more information see 13. Permissions . Overview Cloud Pipeline allows creating pipelines using graphical IDE called \" PipelineBuilder \". \" PipelineBuilder \" provides GUI approach to construct WDL pipeline workflow supported dependencies, loops, etc without programming. \" PipelineBuilder \" is based on WDL language (by Broad Institute, https://github.com/openwdl/wdl ) that is executed by \"Cromwell\" service. Creating a new pipeline with a Pipeline Builder To start using Pipeline Builder - create a new pipeline from \" WDL \" template: + Create \u2192 Pipeline \u2192 WDL . Name it (e.g. \"pipeline-builder-test\"). This will create a new pipeline with a draft version. Click on the created pipeline and open the pipeline draft version. Navigate to the GRAPH tab. Default pipeline will be generated with a single task \" HelloWorld_print \". Auxiliary controls (\"Save\", \"Revert changes\", \"Layout\", \"Fit to screen\", \"Show links\", \"Zoom out\", \"Zoom in\", \"Search\", \"Fullscreen\") are in the left side of the WDL GRAPH: To add more tasks: click PROPERTIES in the top right corner to open \" Properties \" panel then click ADD TASK button Note : The ADD SCATTER button allows adding scatters . This will create a new task in the main workflow: Click on the just-created task. It will become highlighted and the task editor will appear at \" Properties \" panel: That panel contains the following controls: Name ( a ) - a name of a task (it will be used for visualizing in a workflow and logging). If you want to change it - click on that field and input a new task name. Valid names are marked with green \"OK\" icon: invalid - with red \"cross\" icon: Inputs (collapsed header) ( b ) - a list of parameters that a task can accept from upstream tasks. To add parameter - click on ADD button. The header will be expanded automatically: In appeared fields input attributes of a new input parameter. Outputs (collapsed header) ( c ) - a list of parameters that a task will pass to the downstream tasks. To add parameter - click on ADD button. The header will be expanded automatically: In appeared fields input attributes of a new output parameter. Use another docker image ( d ) - if ticked - docker image, that is used within a task, can be overridden (i.e. different tools/images can be used for each task of the workflow). If you want to use another docker image - set that checkbox and then click upon an appeared field to select docker image: See more info below . Use another compute node ( e ) - if ticked - instance type, that is used within a task, can be changed (e.g. more productive node can be used for the specific task). If you want to use another instance type - set that checkbox and then select instance from an appeared dropdown list: Command ( f ) - a shell script that will be executed within a task. DELETE task button ( g ) - to delete a task. Inputted changes are automatically being represented at the graph. The following picture presents an example of a basic task creating: After input values of a new task, click Save and Commit . The visualization with a new \"task1\" with one output will be displayed. To create a \"real\" workflow - create a second task with one input: Then click Save and Commit . Link task1 output with task2 input with a mouse cursor (click \"output1\" and slide to \"input1\"). Then click Save and Commit . Note : to remove link hover mouse pointer over it and then click on \"cross\" button: Note : \" HelloWorld_print \" task isn't linked to other tasks. When code generates from a graph, tasks without links to other tasks can be executed in any order (e.g. task1 \u2192 HelloWorld_print \u2192 task2 or HelloWorld_print \u2192 task1 \u2192 task2 , etc). If you want to delete a task - click upon it, open \"Properties\" panel and click DELETE task button in the bottom of the panel: To add skatter: click PROPERTIES in the top right corner to open \" Properties \" panel then click ADD SCATTER button This will create a new scatter in the main workflow: Click on the just-created scatter. It will become highlighted and the scatter editor will appear at \" Properties \" panel: That panel contains the following controls: ADD TASK button ( a ) - to add a task into the scatter item. Creation of a task in the scatter is the same as described above. You can add inputs/outputs for that scatter task, links them with outputs/inputs of other tasks. Inputs (collapsed header) ( b ) - a mandatory parameter of a ScatterItem type that a scatter can accept from upstream task. User couldn't remove it or change its type, only change its name. For that - click upon the header and specify a new name: DELETE scatter button ( c ) - to delete a scatter. Inputted changes are automatically being represented at the graph. The following picture presents an example with a scatter, that has a task, which output is linked to \"HelloWorld_print\" task's input: Overriding docker image for a specific task By default, all tasks (and their commands) will run within a docker image that is specified for the initial run. This is useful when all tools/libraries are packed into a single docker image. But if a specific step requires tools that are not packed into the same docker image - \" PipelineBuilder \" allows to specify another docker image: Open any task details and check the Use another docker image option. This will bring a docker image selector. In an appeared pop-up choose Registry , Tool group . Select an image and its version. Then click OK button. Specified docker image will be used instead of the initial one. This means that a command specified for a task will be executed in another docker container. Example Pipeline As an example - R-based scRNA secondary analysis script. This script uses 10xGenomics matrix as input. Workflow diagram: Note : this workflow uses \"overridden\" docker image for the last task to show how it behaves (as described in Overriding docker image for a specific task section). Search over the Graph To search some element over the Graph: Click the Search button in the auxiliary controls menu: In the appeared field input the name or type of the element you want to find. Search results will appear immediately while typing: In the list, click the element you want to find (e.g. task3 ) Found element will be highlighted and placed into the focus:","title":"6.1.1. Building WDL Pipeline with graphical PipelineBuilder"},{"location":"manual/06_Manage_Pipeline/6.1.1._Building_WDL_pipeline_with_graphical_PipelineBuilder/#611-building-wdl-pipeline-with-graphical-pipelinebuilder","text":"Overview Creating a new pipeline with a Pipeline Builder Overriding docker image for a specific task Example Pipeline Search over the Graph To create a new WDL pipeline in a Folder you need to have WRITE permissions for that folder and the ROLE_PIPELINE_MANAGER role. For more information see 13. Permissions .","title":"6.1.1. Building WDL pipeline with graphical PipelineBuilder"},{"location":"manual/06_Manage_Pipeline/6.1.1._Building_WDL_pipeline_with_graphical_PipelineBuilder/#overview","text":"Cloud Pipeline allows creating pipelines using graphical IDE called \" PipelineBuilder \". \" PipelineBuilder \" provides GUI approach to construct WDL pipeline workflow supported dependencies, loops, etc without programming. \" PipelineBuilder \" is based on WDL language (by Broad Institute, https://github.com/openwdl/wdl ) that is executed by \"Cromwell\" service.","title":"Overview"},{"location":"manual/06_Manage_Pipeline/6.1.1._Building_WDL_pipeline_with_graphical_PipelineBuilder/#creating-a-new-pipeline-with-a-pipeline-builder","text":"To start using Pipeline Builder - create a new pipeline from \" WDL \" template: + Create \u2192 Pipeline \u2192 WDL . Name it (e.g. \"pipeline-builder-test\"). This will create a new pipeline with a draft version. Click on the created pipeline and open the pipeline draft version. Navigate to the GRAPH tab. Default pipeline will be generated with a single task \" HelloWorld_print \". Auxiliary controls (\"Save\", \"Revert changes\", \"Layout\", \"Fit to screen\", \"Show links\", \"Zoom out\", \"Zoom in\", \"Search\", \"Fullscreen\") are in the left side of the WDL GRAPH: To add more tasks: click PROPERTIES in the top right corner to open \" Properties \" panel then click ADD TASK button Note : The ADD SCATTER button allows adding scatters . This will create a new task in the main workflow: Click on the just-created task. It will become highlighted and the task editor will appear at \" Properties \" panel: That panel contains the following controls: Name ( a ) - a name of a task (it will be used for visualizing in a workflow and logging). If you want to change it - click on that field and input a new task name. Valid names are marked with green \"OK\" icon: invalid - with red \"cross\" icon: Inputs (collapsed header) ( b ) - a list of parameters that a task can accept from upstream tasks. To add parameter - click on ADD button. The header will be expanded automatically: In appeared fields input attributes of a new input parameter. Outputs (collapsed header) ( c ) - a list of parameters that a task will pass to the downstream tasks. To add parameter - click on ADD button. The header will be expanded automatically: In appeared fields input attributes of a new output parameter. Use another docker image ( d ) - if ticked - docker image, that is used within a task, can be overridden (i.e. different tools/images can be used for each task of the workflow). If you want to use another docker image - set that checkbox and then click upon an appeared field to select docker image: See more info below . Use another compute node ( e ) - if ticked - instance type, that is used within a task, can be changed (e.g. more productive node can be used for the specific task). If you want to use another instance type - set that checkbox and then select instance from an appeared dropdown list: Command ( f ) - a shell script that will be executed within a task. DELETE task button ( g ) - to delete a task. Inputted changes are automatically being represented at the graph. The following picture presents an example of a basic task creating: After input values of a new task, click Save and Commit . The visualization with a new \"task1\" with one output will be displayed. To create a \"real\" workflow - create a second task with one input: Then click Save and Commit . Link task1 output with task2 input with a mouse cursor (click \"output1\" and slide to \"input1\"). Then click Save and Commit . Note : to remove link hover mouse pointer over it and then click on \"cross\" button: Note : \" HelloWorld_print \" task isn't linked to other tasks. When code generates from a graph, tasks without links to other tasks can be executed in any order (e.g. task1 \u2192 HelloWorld_print \u2192 task2 or HelloWorld_print \u2192 task1 \u2192 task2 , etc). If you want to delete a task - click upon it, open \"Properties\" panel and click DELETE task button in the bottom of the panel: To add skatter: click PROPERTIES in the top right corner to open \" Properties \" panel then click ADD SCATTER button This will create a new scatter in the main workflow: Click on the just-created scatter. It will become highlighted and the scatter editor will appear at \" Properties \" panel: That panel contains the following controls: ADD TASK button ( a ) - to add a task into the scatter item. Creation of a task in the scatter is the same as described above. You can add inputs/outputs for that scatter task, links them with outputs/inputs of other tasks. Inputs (collapsed header) ( b ) - a mandatory parameter of a ScatterItem type that a scatter can accept from upstream task. User couldn't remove it or change its type, only change its name. For that - click upon the header and specify a new name: DELETE scatter button ( c ) - to delete a scatter. Inputted changes are automatically being represented at the graph. The following picture presents an example with a scatter, that has a task, which output is linked to \"HelloWorld_print\" task's input:","title":"Creating a new pipeline with a Pipeline Builder"},{"location":"manual/06_Manage_Pipeline/6.1.1._Building_WDL_pipeline_with_graphical_PipelineBuilder/#overriding-docker-image-for-a-specific-task","text":"By default, all tasks (and their commands) will run within a docker image that is specified for the initial run. This is useful when all tools/libraries are packed into a single docker image. But if a specific step requires tools that are not packed into the same docker image - \" PipelineBuilder \" allows to specify another docker image: Open any task details and check the Use another docker image option. This will bring a docker image selector. In an appeared pop-up choose Registry , Tool group . Select an image and its version. Then click OK button. Specified docker image will be used instead of the initial one. This means that a command specified for a task will be executed in another docker container.","title":"Overriding docker image for a specific task"},{"location":"manual/06_Manage_Pipeline/6.1.1._Building_WDL_pipeline_with_graphical_PipelineBuilder/#example-pipeline","text":"As an example - R-based scRNA secondary analysis script. This script uses 10xGenomics matrix as input. Workflow diagram: Note : this workflow uses \"overridden\" docker image for the last task to show how it behaves (as described in Overriding docker image for a specific task section).","title":"Example Pipeline"},{"location":"manual/06_Manage_Pipeline/6.1.1._Building_WDL_pipeline_with_graphical_PipelineBuilder/#search-over-the-graph","text":"To search some element over the Graph: Click the Search button in the auxiliary controls menu: In the appeared field input the name or type of the element you want to find. Search results will appear immediately while typing: In the list, click the element you want to find (e.g. task3 ) Found element will be highlighted and placed into the focus:","title":"Search over the Graph"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/","text":"6.1. Create and configure pipeline Create a pipeline in a Library space Customize a pipeline version Edit documentation (optional) Edit code section Edit pipeline configuration (optional) Add/delete storage rules (optional) Edit a pipeline info Example: Create Pipeline Pipeline input data Pipeline output folder Configure the main_file Configure pipeline input/output parameters via GUI Check the results of pipeline execution Example: Add pipeline configuration Example: Create a configuration that uses system parameter Example: Limit mounted storages Example: Configure a custom node image To create a Pipeline in a Folder you need to have WRITE permission for that folder and the ROLE_PIPELINE_MANAGER role. To edit pipeline you need just WRITE permissions for a pipeline. For more information see 13. Permissions . To create a working pipeline version you need: Create a pipeline in a Library space Customize a pipeline version: Edit documentation (optional) Edit Code file Edit Configuration, Add new configuration (optional) Add storage rules (optional) . Create a pipeline in a Library space Go to the \"Library\" tab and select a folder. Click + Create \u2192 Pipeline and choose one of the built-in pipeline templates ( Python , Shell , Snakemake , Luigi , WDL , Nextflow ) or choose DEFAULT item to create a pipeline without a template. Pipeline template defines the programming language for a pipeline. As templates are empty user shall write pipeline logic on his own. Enter pipeline's name (pipeline description is optional) in the popped-up form. Click the Create button. A new pipeline will appear in the folder. Note : To configure repository where to store pipeline versions click the Edit repository settings button. Click on the button and two additional fields will appear: Repository (repository address) and Token (password to access a repository). The new pipeline will appear in a Library space. Customize a pipeline version Click a pipeline version to start its configuration process. Edit documentation (optional) This option allows you to make a detailed description of your pipelines. Navigate to the Documents tab and: Click Edit . Change the document using a markdown language . Click the Save button. Enter a description of the change and click Commit . Changes are saved. Edit code section It is not optional because you need to create a pipeline that will be tailored to your specific needs. For that purpose, you need to extend basic pipeline templates/add new files. Navigate to the Code tab. Click on any file you want to edit. Note : each pipeline version has a default code file: it named after a pipeline and has a respective extension. A new window with file contents will open. Click the Edit button and change the code file in the desired way. When you are done, click the Save button. You'll be asked to write a Commit message (e.g. 'added second \"echo\" command'). Then click the Commit button. After that changes will be applied to your file. Note : all code files are downloaded to the node to run the pipeline. Just adding a new file to the Code section doesn't change anything. You need to specify the order of scripts execution by yourself. E.g. you have three files in your pipeline: first.sh ( main_file ), second.sh and config.json . cmd_template parameter is chmod +x $SCRIPTS_DIR/src/* && $SCRIPTS_DIR/src/[main_file] . So in the first.sh file you need to explicitly specify execution of second.sh script for them both to run inside your pipeline, otherwise this file will be ignored. Edit pipeline configuration (optional) See details about pipeline configuration parameters here . Every pipeline has default pipeline configuration from the moment it was created. To change default pipeline configuration: Navigate to the Configuration tab. Expand \"Exec environment\" and \"Advanced\" tabs to see a full list of pipeline parameters. \"Parameters\" tab is opened by default. Change any parameter you need. In this example, we will set Cloud Region to Europe Ireland, Disk to 40 Gb and set the Timeout to 400 mins. Click the Save button. Now this will be the default pipeline configuration for the pipeline execution. Add/delete storage rules (optional) This section allows configuring what data will be transferred to an STS after pipeline execution. To add a new rule: Click the Add new rule button. A pop-up will appear. Enter File mask and then tick the box \"Move to STS\" to move pipeline output data to STS after pipeline execution. Note : If many rules with different Masks are present all of them are checked one by one. If a file corresponds to any of rules - it will be uploaded to the bucket. To delete storage rule click the Delete button in the right part of the storage rule's row. Edit a pipeline info To edit a pipeline info: Click the Gear icon in the right upper corner of the pipeline page The popup with the pipeline info will be opened: Here you can edit pipeline name ( a ) and description ( b ) To edit repository settings click the corresponding button ( c ): Here you can edit access token to a repository ( d ) Note : the \"Repository\" field is disabled for the existing pipelines Click the SAVE button to save changes Note : if you rename a pipeline the corresponding GitLab repo will be automatically renamed too. So, the clone/pull/push URL will change. Make sure to change the remote address, if this pipeline is used somewhere. How it works: Open the pipeline: Click the GIT REPOSITORY button in the right upper corner of the page: Pipeline name and repository name are identical Click the Gear icon in the right upper corner. In the popup change pipeline name and click the SAVE button: Click the GIT REPOSITORY button again: Pipeline name and repository name are identical Also, if you want just rename a pipeline without changing its other info fields: Hover over the pipeline name at the \"breadcrumbs\" control in the top of the pipeline page - the \"edit\" symbol will appear: Click the pipeline name - the field will become available to edit. Rename the pipeline: Press the Enter key or click any empty space - a new pipeline name will be saved: Example: Create Pipeline We will create a simple Shell pipeline (Shell template used). For that purpose, we will click + Create \u2192 Pipeline \u2192 SHELL . Then we will write Pipeline name ( 1 ), Pipeline description ( 2 ) and click Create ( 3 ). This pipeline will: Download a file. Rename it. Upload renamed the file to the bucket. Pipeline input data This is where pipeline input data is stored. About storages see here . This path will be used in pipeline parameters later on. Pipeline output folder This is where pipeline output data will be stored after pipeline execution. About storages see here . This path will be used in pipeline parameters later on. Configure the main_file The pipeline will consist of 2 files: main_file and config.json . Let's extend the main_file so that it renames the input file and puts it into the $ANALYSIS_DIR folder on the node from which data will be uploaded to the bucket. To do that click the main_file name and click the Edit button. Then type all the pipeline instructions. Click the Save button, input a commit message and click the Commit button. Configure pipeline input/output parameters via GUI Click the Run button. In the pipeline run configuration select the arrow near the Add parameter button and select the \"Input path parameter\" option from the drop-down list. Name the parameter (e.g. \"input\") and click on the grey \"download\" icon to select the path to the pipeline input data (we described pipeline input data above ). For pipeline output folder parameter choose the \"Output path parameter\" option from the drop-down list, name it and click on the grey \"upload\" icon to select the path to the pipeline output folder (we described pipeline output data above ). This is how everything looks after these parameters are set: Leave all other parameters default and click the Launch button. Check the results of pipeline execution After pipeline finished its execution, you can find the renamed file in the output folder: Example: Add pipeline configuration In this example, we will create a new pipeline configuration for the example pipeline and set it as default one. To add new pipeline configuration perform the following steps: Select a pipeline Select a pipeline version Navigate to the CONFIGURATION tab Click the + ADD button in the upper-right corner of the screen Specify Configuration name , Description (optionally) and the Template - this is a pipeline configuration, from which the new pipeline configuration will inherit its parameters (right now only the \"default\" template is available). Click the Create button. As you can see, the new configuration has the same parameters as the default configuration. Use Delete ( 1 ), Set as default ( 2 ) or Save ( 3 ) buttons to delete, set as default or save this configuration respectively. Expand the Exec environment section ( 1 ) and then Specify 30 GB Disk size ( 2 ), click the control to choose another Docker image ( 3 ). Click the Save button ( 4 ). Set \"new-configuration\" as default with the Set as default button. Navigate to the CODE tab. As you can see, config.json file now contains information about two configurations: \"default\" and \"new-configuration\". \"new-configuration\" is default one for pipeline execution. Example: Create a configuration that uses system parameter Users can specify system parameters (per run), that change/configure special behavior for the current run. In the example below we will use the system parameter, that installs and allows using of the DIND (Docker in Docker) in the launched container: Select a pipeline Select a pipeline version Navigate to the CONFIGURATION tab In the CONFIGURATION tab expand the Advanced section, set \"Start idle\" checkbox and click the Add system parameter button: Click the CP_CAP_DIND_CONTAINER option and then click the OK button: This option will enable docker engine for a run using a containerized approach. Added system parameter appears on the configuration page. Save the configuration - now it will use \"Docker inside Docker\" technology while running: To see it, click the Run button in the upper-right corner to launch the configuration. Edit or add any parameters you want on the Launch page and click the Launch button in the upper-right corner: Confirm the launch in the appeared pop-up. In the ACTIVE RUNS tab press the just-launched pipeline name. Wait until the SSH hyperlink will appear in the upper-right corner, click it: On the opened tab specify the command docker version and then press \"Enter\" key: As you can see, DinD works correctly. Example: Limit mounted storages By default, all available to a user storages are mounted to the launched container during the run initialization. User could have access to them via /cloud-data or ~/cloud-data folder using the interactive sessions (SSH/Web endpoints/Desktop) or pipeline runs. Note : to a user only storages are available for which he has READ permission. For more information see 13. Permissions . To limit the number of data storages being mounted to a specific pipeline run: Select a pipeline Select a pipeline version Navigate to the CONFIGURATION tab In the CONFIGURATION tab expand the Advanced section, click the field next to the \"Limit mounts\" label: In the pop-up select storages you want to mount during the run, e.g.: Confirm your choise by click the OK button Selected storages will appear in the field next to the \"Limit mounts\" label: Set \"Start idle\" checkbox and click the Save button: Click the Run button in the upper-right corner to launch the configuration. Edit or add any parameters you want on the Launch page and click the Launch button in the upper-right corner: Confirm the launch in the appeared pop-up. In the ACTIVE RUNS tab press the just-launched pipeline name. At the Run logs page expand the \"Parameters\" section: Here you can see a list of datastorages that will be mounted to the job. Wait until the SSH hyperlink will appear in the upper-right corner, click it. On the opened tab specify the command ls cloud-data/ and then press \"Enter\" key: Here you can see the list of the mounted storages that is equal to the list of the selected storages at step 5. Other storages were not mounted. Each mounted storage is available for the interactive/batch jobs using the path /cloud-data/{storage_name} . If you wish to not mount any storage to the run, you should set the checkbox \" Do not mount storages \" at step 5: In this case, no storages will be mounted at all: Example: Configure a custom node image Users can configure a custom image for the node - i.e. base image from which the cloud instance itself is being initialized and launched. Please don't confuse this setting with the \"docker image\" setting Node image can be forsibly specified in the pipeline configuration, in the following format: \"instance_image\": \"<custom_node_image>\" , where <custom_node_image> is the name of the custom image. Example of usage: Select a pipeline Select a pipeline version Navigate to the CODE tab Click config.json file: Click the EDIT button: In the configuration, specify custom node image in the format as described above, e.g.: Click the SAVE button In the popup, specify the valid commit message and confirm, e.g.: Run the pipeline (see more detais in 6.2. Launch a Pipeline ) and open the Run logs page: Wait until the task \" InitializeNode \" will be performed. Click it Check that for the run the custom node image specified at step 6 is used:","title":"6.1. Create and configure Pipeline"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#61-create-and-configure-pipeline","text":"Create a pipeline in a Library space Customize a pipeline version Edit documentation (optional) Edit code section Edit pipeline configuration (optional) Add/delete storage rules (optional) Edit a pipeline info Example: Create Pipeline Pipeline input data Pipeline output folder Configure the main_file Configure pipeline input/output parameters via GUI Check the results of pipeline execution Example: Add pipeline configuration Example: Create a configuration that uses system parameter Example: Limit mounted storages Example: Configure a custom node image To create a Pipeline in a Folder you need to have WRITE permission for that folder and the ROLE_PIPELINE_MANAGER role. To edit pipeline you need just WRITE permissions for a pipeline. For more information see 13. Permissions . To create a working pipeline version you need: Create a pipeline in a Library space Customize a pipeline version: Edit documentation (optional) Edit Code file Edit Configuration, Add new configuration (optional) Add storage rules (optional) .","title":"6.1. Create and configure pipeline"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#create-a-pipeline-in-a-library-space","text":"Go to the \"Library\" tab and select a folder. Click + Create \u2192 Pipeline and choose one of the built-in pipeline templates ( Python , Shell , Snakemake , Luigi , WDL , Nextflow ) or choose DEFAULT item to create a pipeline without a template. Pipeline template defines the programming language for a pipeline. As templates are empty user shall write pipeline logic on his own. Enter pipeline's name (pipeline description is optional) in the popped-up form. Click the Create button. A new pipeline will appear in the folder. Note : To configure repository where to store pipeline versions click the Edit repository settings button. Click on the button and two additional fields will appear: Repository (repository address) and Token (password to access a repository). The new pipeline will appear in a Library space.","title":"Create a pipeline in a Library space"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#customize-a-pipeline-version","text":"Click a pipeline version to start its configuration process.","title":"Customize a pipeline version"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#edit-documentation-optional","text":"This option allows you to make a detailed description of your pipelines. Navigate to the Documents tab and: Click Edit . Change the document using a markdown language . Click the Save button. Enter a description of the change and click Commit . Changes are saved.","title":"Edit documentation (optional)"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#edit-code-section","text":"It is not optional because you need to create a pipeline that will be tailored to your specific needs. For that purpose, you need to extend basic pipeline templates/add new files. Navigate to the Code tab. Click on any file you want to edit. Note : each pipeline version has a default code file: it named after a pipeline and has a respective extension. A new window with file contents will open. Click the Edit button and change the code file in the desired way. When you are done, click the Save button. You'll be asked to write a Commit message (e.g. 'added second \"echo\" command'). Then click the Commit button. After that changes will be applied to your file. Note : all code files are downloaded to the node to run the pipeline. Just adding a new file to the Code section doesn't change anything. You need to specify the order of scripts execution by yourself. E.g. you have three files in your pipeline: first.sh ( main_file ), second.sh and config.json . cmd_template parameter is chmod +x $SCRIPTS_DIR/src/* && $SCRIPTS_DIR/src/[main_file] . So in the first.sh file you need to explicitly specify execution of second.sh script for them both to run inside your pipeline, otherwise this file will be ignored.","title":"Edit code section"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#edit-pipeline-configuration-optional","text":"See details about pipeline configuration parameters here . Every pipeline has default pipeline configuration from the moment it was created. To change default pipeline configuration: Navigate to the Configuration tab. Expand \"Exec environment\" and \"Advanced\" tabs to see a full list of pipeline parameters. \"Parameters\" tab is opened by default. Change any parameter you need. In this example, we will set Cloud Region to Europe Ireland, Disk to 40 Gb and set the Timeout to 400 mins. Click the Save button. Now this will be the default pipeline configuration for the pipeline execution.","title":"Edit pipeline configuration (optional)"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#adddelete-storage-rules-optional","text":"This section allows configuring what data will be transferred to an STS after pipeline execution. To add a new rule: Click the Add new rule button. A pop-up will appear. Enter File mask and then tick the box \"Move to STS\" to move pipeline output data to STS after pipeline execution. Note : If many rules with different Masks are present all of them are checked one by one. If a file corresponds to any of rules - it will be uploaded to the bucket. To delete storage rule click the Delete button in the right part of the storage rule's row.","title":"Add/delete storage rules (optional)"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#edit-a-pipeline-info","text":"To edit a pipeline info: Click the Gear icon in the right upper corner of the pipeline page The popup with the pipeline info will be opened: Here you can edit pipeline name ( a ) and description ( b ) To edit repository settings click the corresponding button ( c ): Here you can edit access token to a repository ( d ) Note : the \"Repository\" field is disabled for the existing pipelines Click the SAVE button to save changes Note : if you rename a pipeline the corresponding GitLab repo will be automatically renamed too. So, the clone/pull/push URL will change. Make sure to change the remote address, if this pipeline is used somewhere. How it works: Open the pipeline: Click the GIT REPOSITORY button in the right upper corner of the page: Pipeline name and repository name are identical Click the Gear icon in the right upper corner. In the popup change pipeline name and click the SAVE button: Click the GIT REPOSITORY button again: Pipeline name and repository name are identical Also, if you want just rename a pipeline without changing its other info fields: Hover over the pipeline name at the \"breadcrumbs\" control in the top of the pipeline page - the \"edit\" symbol will appear: Click the pipeline name - the field will become available to edit. Rename the pipeline: Press the Enter key or click any empty space - a new pipeline name will be saved:","title":"Edit a pipeline info"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#example-create-pipeline","text":"We will create a simple Shell pipeline (Shell template used). For that purpose, we will click + Create \u2192 Pipeline \u2192 SHELL . Then we will write Pipeline name ( 1 ), Pipeline description ( 2 ) and click Create ( 3 ). This pipeline will: Download a file. Rename it. Upload renamed the file to the bucket.","title":"Example: Create Pipeline"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#pipeline-input-data","text":"This is where pipeline input data is stored. About storages see here . This path will be used in pipeline parameters later on.","title":"Pipeline input data"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#pipeline-output-folder","text":"This is where pipeline output data will be stored after pipeline execution. About storages see here . This path will be used in pipeline parameters later on.","title":"Pipeline output folder"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#configure-the-main_file","text":"The pipeline will consist of 2 files: main_file and config.json . Let's extend the main_file so that it renames the input file and puts it into the $ANALYSIS_DIR folder on the node from which data will be uploaded to the bucket. To do that click the main_file name and click the Edit button. Then type all the pipeline instructions. Click the Save button, input a commit message and click the Commit button.","title":"Configure the main_file"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#configure-pipeline-inputoutput-parameters-via-gui","text":"Click the Run button. In the pipeline run configuration select the arrow near the Add parameter button and select the \"Input path parameter\" option from the drop-down list. Name the parameter (e.g. \"input\") and click on the grey \"download\" icon to select the path to the pipeline input data (we described pipeline input data above ). For pipeline output folder parameter choose the \"Output path parameter\" option from the drop-down list, name it and click on the grey \"upload\" icon to select the path to the pipeline output folder (we described pipeline output data above ). This is how everything looks after these parameters are set: Leave all other parameters default and click the Launch button.","title":"Configure pipeline input/output parameters via GUI"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#check-the-results-of-pipeline-execution","text":"After pipeline finished its execution, you can find the renamed file in the output folder:","title":"Check the results of pipeline execution"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#example-add-pipeline-configuration","text":"In this example, we will create a new pipeline configuration for the example pipeline and set it as default one. To add new pipeline configuration perform the following steps: Select a pipeline Select a pipeline version Navigate to the CONFIGURATION tab Click the + ADD button in the upper-right corner of the screen Specify Configuration name , Description (optionally) and the Template - this is a pipeline configuration, from which the new pipeline configuration will inherit its parameters (right now only the \"default\" template is available). Click the Create button. As you can see, the new configuration has the same parameters as the default configuration. Use Delete ( 1 ), Set as default ( 2 ) or Save ( 3 ) buttons to delete, set as default or save this configuration respectively. Expand the Exec environment section ( 1 ) and then Specify 30 GB Disk size ( 2 ), click the control to choose another Docker image ( 3 ). Click the Save button ( 4 ). Set \"new-configuration\" as default with the Set as default button. Navigate to the CODE tab. As you can see, config.json file now contains information about two configurations: \"default\" and \"new-configuration\". \"new-configuration\" is default one for pipeline execution.","title":"Example: Add pipeline configuration"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#example-create-a-configuration-that-uses-system-parameter","text":"Users can specify system parameters (per run), that change/configure special behavior for the current run. In the example below we will use the system parameter, that installs and allows using of the DIND (Docker in Docker) in the launched container: Select a pipeline Select a pipeline version Navigate to the CONFIGURATION tab In the CONFIGURATION tab expand the Advanced section, set \"Start idle\" checkbox and click the Add system parameter button: Click the CP_CAP_DIND_CONTAINER option and then click the OK button: This option will enable docker engine for a run using a containerized approach. Added system parameter appears on the configuration page. Save the configuration - now it will use \"Docker inside Docker\" technology while running: To see it, click the Run button in the upper-right corner to launch the configuration. Edit or add any parameters you want on the Launch page and click the Launch button in the upper-right corner: Confirm the launch in the appeared pop-up. In the ACTIVE RUNS tab press the just-launched pipeline name. Wait until the SSH hyperlink will appear in the upper-right corner, click it: On the opened tab specify the command docker version and then press \"Enter\" key: As you can see, DinD works correctly.","title":"Example: Create a configuration that\u00a0uses system parameter"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#example-limit-mounted-storages","text":"By default, all available to a user storages are mounted to the launched container during the run initialization. User could have access to them via /cloud-data or ~/cloud-data folder using the interactive sessions (SSH/Web endpoints/Desktop) or pipeline runs. Note : to a user only storages are available for which he has READ permission. For more information see 13. Permissions . To limit the number of data storages being mounted to a specific pipeline run: Select a pipeline Select a pipeline version Navigate to the CONFIGURATION tab In the CONFIGURATION tab expand the Advanced section, click the field next to the \"Limit mounts\" label: In the pop-up select storages you want to mount during the run, e.g.: Confirm your choise by click the OK button Selected storages will appear in the field next to the \"Limit mounts\" label: Set \"Start idle\" checkbox and click the Save button: Click the Run button in the upper-right corner to launch the configuration. Edit or add any parameters you want on the Launch page and click the Launch button in the upper-right corner: Confirm the launch in the appeared pop-up. In the ACTIVE RUNS tab press the just-launched pipeline name. At the Run logs page expand the \"Parameters\" section: Here you can see a list of datastorages that will be mounted to the job. Wait until the SSH hyperlink will appear in the upper-right corner, click it. On the opened tab specify the command ls cloud-data/ and then press \"Enter\" key: Here you can see the list of the mounted storages that is equal to the list of the selected storages at step 5. Other storages were not mounted. Each mounted storage is available for the interactive/batch jobs using the path /cloud-data/{storage_name} . If you wish to not mount any storage to the run, you should set the checkbox \" Do not mount storages \" at step 5: In this case, no storages will be mounted at all:","title":"Example: Limit mounted storages"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#example-configure-a-custom-node-image","text":"Users can configure a custom image for the node - i.e. base image from which the cloud instance itself is being initialized and launched. Please don't confuse this setting with the \"docker image\" setting Node image can be forsibly specified in the pipeline configuration, in the following format: \"instance_image\": \"<custom_node_image>\" , where <custom_node_image> is the name of the custom image. Example of usage: Select a pipeline Select a pipeline version Navigate to the CODE tab Click config.json file: Click the EDIT button: In the configuration, specify custom node image in the format as described above, e.g.: Click the SAVE button In the popup, specify the valid commit message and confirm, e.g.: Run the pipeline (see more detais in 6.2. Launch a Pipeline ) and open the Run logs page: Wait until the task \" InitializeNode \" will be performed. Click it Check that for the run the custom node image specified at step 6 is used:","title":"Example: Configure a custom node image"},{"location":"manual/06_Manage_Pipeline/6.2._Launch_a_pipeline/","text":"6.2. Launch a pipeline To launch a pipeline you need to have EXECUTE permissions for the pipeline. For more information see 13. Permissions . Also you can launch a pipeline via CLI. See 14.5 Manage pipeline executions via CLI . Select a pipeline in the \" Library \" menu ( 3. Overview ). Select a pipeline version to run. Click the Run button. Launch pipeline page will be opened: Feel free to change settings of run configuration if you need to. See an example of editing configuration here . If the Price type is set as \" On-demand \" - at the Launch page , an additional checkbox Auto pause appears: This checkbox allows to enable automatic pausing on-demand instance if it is not used. Such behavior could be controlled by Administrators using a set of parameters at System Preferences (see here ). Please note, this checkbox will not be displayed if any cluster is configured (\"Static\" or \"Autoscaled\"). If the Price type is set as \" On-demand \" - at the Launch page , an additional control Maintenance appears. It allows to configure schedule for automatical pause/resume a pipeline run. It could be useful when the pipeline is launched for a long time (several days/weeks) but it shall not stand idle, just increasing costs, in weekends and holidays, for example. Schedule is defined as a list of rules (user is able to specify any number of them). For each rule in the list the user is able to set: the action: PAUSE or RESUME the recurrence: Daily : every N days, time or Weekly : every weekday(s) , time Conflicting rules are not allowed (i.e. rules that are configured on the same execution time). If any schedule rule is configured for the launched active run - that run will be paused/restarted accordingly in the scheduled day and time. To set a schedule for pause/restart a job: Click the Configure button: The \"Maintenance\" popup will appear: Click the Add rule button. The first rule will appear: Using available controls configure the rule according to your wish, e.g. to automatically pause a job every 2 days at 15:00 : To add another rule click the Add rule button. Configure a new rule using available controls, e.g. to automatically restart (resume) a job every monday and friday at 18:30 : Click the OK button to save specified rules Saved rules will be displayed at the Launch form: Please note, the Maintenance control will not be displayed if any cluster is configured (\"Static\" or \"Autoscaled\"). Users (who have permissions to pause/resume a run) can create/view/modify/delete schedule rules anytime launched run is active via the Run logs page - for more details see 11. Manage runs . Click Launch . Please note, that the current user can launch a pipeline only if he/his group has corresponding permissions on that pipeline (for more information see 13. Permissions ), but the Launch button may be disabled also for one of the following reasons: execution isn't allowed for specified docker image; read operations aren't allowed for specified input or common path parameters; write operations aren't allowed for specified output path parameters. In such cases, hover over the Launch button to view warning notification with a reason of a run forbiddance, e.g.: Note : you can also launch a pipeline with the same settings via the CLI command or API request. To generate the corresponding command/request click the button near the \"Launch\" button. For more details see here . Confirm launch in the appeared popup. You'll be redirected to the \"Runs\" area. Here you'll find your pipeline running. You can monitor status of your run and see additional information (see 11. Manage Runs ). Note : after some initialization time, a new node will appear in the \" Cluster nodes \" tab. See 9. Manage Cluster nodes . Note : to learn about launching a pipeline as an Interactive service, refer to 15. Interactive services .","title":"6.2. Launch a Pipeline"},{"location":"manual/06_Manage_Pipeline/6.2._Launch_a_pipeline/#62-launch-a-pipeline","text":"To launch a pipeline you need to have EXECUTE permissions for the pipeline. For more information see 13. Permissions . Also you can launch a pipeline via CLI. See 14.5 Manage pipeline executions via CLI . Select a pipeline in the \" Library \" menu ( 3. Overview ). Select a pipeline version to run. Click the Run button. Launch pipeline page will be opened: Feel free to change settings of run configuration if you need to. See an example of editing configuration here . If the Price type is set as \" On-demand \" - at the Launch page , an additional checkbox Auto pause appears: This checkbox allows to enable automatic pausing on-demand instance if it is not used. Such behavior could be controlled by Administrators using a set of parameters at System Preferences (see here ). Please note, this checkbox will not be displayed if any cluster is configured (\"Static\" or \"Autoscaled\"). If the Price type is set as \" On-demand \" - at the Launch page , an additional control Maintenance appears. It allows to configure schedule for automatical pause/resume a pipeline run. It could be useful when the pipeline is launched for a long time (several days/weeks) but it shall not stand idle, just increasing costs, in weekends and holidays, for example. Schedule is defined as a list of rules (user is able to specify any number of them). For each rule in the list the user is able to set: the action: PAUSE or RESUME the recurrence: Daily : every N days, time or Weekly : every weekday(s) , time Conflicting rules are not allowed (i.e. rules that are configured on the same execution time). If any schedule rule is configured for the launched active run - that run will be paused/restarted accordingly in the scheduled day and time. To set a schedule for pause/restart a job: Click the Configure button: The \"Maintenance\" popup will appear: Click the Add rule button. The first rule will appear: Using available controls configure the rule according to your wish, e.g. to automatically pause a job every 2 days at 15:00 : To add another rule click the Add rule button. Configure a new rule using available controls, e.g. to automatically restart (resume) a job every monday and friday at 18:30 : Click the OK button to save specified rules Saved rules will be displayed at the Launch form: Please note, the Maintenance control will not be displayed if any cluster is configured (\"Static\" or \"Autoscaled\"). Users (who have permissions to pause/resume a run) can create/view/modify/delete schedule rules anytime launched run is active via the Run logs page - for more details see 11. Manage runs . Click Launch . Please note, that the current user can launch a pipeline only if he/his group has corresponding permissions on that pipeline (for more information see 13. Permissions ), but the Launch button may be disabled also for one of the following reasons: execution isn't allowed for specified docker image; read operations aren't allowed for specified input or common path parameters; write operations aren't allowed for specified output path parameters. In such cases, hover over the Launch button to view warning notification with a reason of a run forbiddance, e.g.: Note : you can also launch a pipeline with the same settings via the CLI command or API request. To generate the corresponding command/request click the button near the \"Launch\" button. For more details see here . Confirm launch in the appeared popup. You'll be redirected to the \"Runs\" area. Here you'll find your pipeline running. You can monitor status of your run and see additional information (see 11. Manage Runs ). Note : after some initialization time, a new node will appear in the \" Cluster nodes \" tab. See 9. Manage Cluster nodes . Note : to learn about launching a pipeline as an Interactive service, refer to 15. Interactive services .","title":"6.2. Launch a pipeline"},{"location":"manual/06_Manage_Pipeline/6.3._Delete_a_pipeline/","text":"6.3. Delete and unregister Pipeline Delete pipeline Unregister pipeline To delete a Pipeline you need to have WRITE permission for that pipeline and the ROLE_PIPELINE_MANAGER role. For more details see 13. Permissions . Delete pipeline Select a pipeline. Click the Gear icon in the right upper corner of the pipeline page: In the popup click the DELETE button: You will be offered to unregister or delete a pipeline. Click the Delete button to remove the pipeline permanently: Unregister pipeline A user can unregister pipeline. Git repository neither will be deleted nor will be accessible in Cloud Pipeline. Do the same actions as in the Delete pipeline section but click the Unregister button at step 4 : If you want to register in the Cloud Pipeline the pipeline that was previously unregistered: Start create a new pipeline. For details see here . At the \"Create pipeline\" popup click the \"Edit repository settings\" button. Into the \"Repository\" field specify the git repository address of the unregistered pipeline: Specify a pipeline name. Click the CREATE button. The re-registered pipeline will appear in the library.","title":"6.3. Delete and unregister Pipeline"},{"location":"manual/06_Manage_Pipeline/6.3._Delete_a_pipeline/#63-delete-and-unregister-pipeline","text":"Delete pipeline Unregister pipeline To delete a Pipeline you need to have WRITE permission for that pipeline and the ROLE_PIPELINE_MANAGER role. For more details see 13. Permissions .","title":"6.3. Delete and unregister Pipeline"},{"location":"manual/06_Manage_Pipeline/6.3._Delete_a_pipeline/#delete-pipeline","text":"Select a pipeline. Click the Gear icon in the right upper corner of the pipeline page: In the popup click the DELETE button: You will be offered to unregister or delete a pipeline. Click the Delete button to remove the pipeline permanently:","title":"Delete pipeline"},{"location":"manual/06_Manage_Pipeline/6.3._Delete_a_pipeline/#unregister-pipeline","text":"A user can unregister pipeline. Git repository neither will be deleted nor will be accessible in Cloud Pipeline. Do the same actions as in the Delete pipeline section but click the Unregister button at step 4 : If you want to register in the Cloud Pipeline the pipeline that was previously unregistered: Start create a new pipeline. For details see here . At the \"Create pipeline\" popup click the \"Edit repository settings\" button. Into the \"Repository\" field specify the git repository address of the unregistered pipeline: Specify a pipeline name. Click the CREATE button. The re-registered pipeline will appear in the library.","title":"Unregister pipeline"},{"location":"manual/06_Manage_Pipeline/6.4._Work_with_aws_healthomics_workflow/","text":"6.4. AWS HealthOmics Workflow Cloud Pipeline has built-in pipeline template that can be used to run your Nextflow workflow with AWS HealthOmics Workflow https://docs.aws.amazon.com/omics/latest/dev/workflows.html . Note : Works only for AWS based Cloud Pipeline deployment Currently only Nextflow workflows are supported This document provides you with necessary information on how to use existing workflows or write your own in Cloud Pipeline , to be able to utilize AWS HealthOmics Workflow. 1. Create pipeline from AWS HealthOmics pipeline template To start developing and Nextflow workflow or reuse existing one, first of all you will need to create Pipeline inside Cloud Pipeline platform. For this reason: Navigate to the Cloud Pipeline UI: In the catalog tree choose a location (folder) where you would like to create you pipeline: Click + Create button in the upper right corner and choose Pipeline -> AWS-HEALTHOMICS-WORKFLOW : Specify desired name and click CREATE: Wait while pipeline being created and after that click on its name in the folder content listing: Then click on the newly created revision of this pipeline and choose CODE tab at the top menu: Now, lets take a look on the structure of it: - workflow/ # Directory where all workflow related code should be placed - main.tf # Workflow file with dummy pipeline - nextflow.config # Worklow configuration file - run-omics-workflow.sh # Entry of the Cloud Pipeline pipeline, this script will be run on start up of the pipeline and will take care of creation and run of the AWS HealthOmics Workflow - config.json # Standard Cloud Pipeline pipeline configuration file where all pipeline related settings are stored Now you are ready to proceed develop your own / utilize existing workflow. 2. Integrate existing workflow with AWS HealthOmics on Cloud Pipeline Process of developing your own workflow hasn't actual difference from using existing one, in terms of integration this workflow with AWS HealthOmics on Cloud Pipeline . Here we describe the process of how to reuse existing workflow with AWS HealthOmics on Cloud Pipeline , but you can also use this documentation wen developing it from scratch. Obtain gitlab token for the authentication Each pipeline object in Cloud Pipeline Platform is a git repository hosted on gitlab, so it is possible and essential to use it as a git repository. In the pipeline view page click GITLAB REPOSITORY: Copy the url and navigate to the URL, if it asks to do it, authenticate with you credentials in Cloud Pipeline platform: Navigate to the Settings -> Access Tokens and fill required fields as shown on the screenshot, adjust expiration date and click Create project access token : Copy new token and save it somewhere safe, you will need it: Clone gitlab repository locally Open terminal and perform the following command: git clone < repo-name >.git When it asks for the credentials - specify you Cloud Pipeline username as a Username and for the password use newly generated access token from previous step. Modify git repository and ingest workflow code in it Here we will use nf-core/rnaseq v3.11.1 Navigate to the src/workflow directory of your pipeline repository: Remove dummy sample pipeline files: rm -rf ./* Clone rnaseq pipeline to the directory: git clone https://github.com/nf-core/rnaseq.git . Checkout to the needed version and clean up: git checkout 3.11.1 rm -rf ./git* This workflow need one small improvements to be made to work correctly, otherwise it will fail during execution. Provide additional flag -p for the mkdir command to not fail if directory already exists: sed -i 's|mkdir|mkdir -p|g' modules/nf-core/qualimap/rnaseq/main.nf Commit changes: git add . git commit -m \"nf-core/rnaseq workflow code added\" Push changes to the remote repository: git push origin master When it asks for the credentials - specify you Cloud Pipeline username as a Username and for the password use newly generated access token from previous step. Navigate to the Cloud Pipeline UI -> Pipeline view and check that nf-core/rnaseq workflow is in the workflow directory: During each new run of the pipeline, Cloud Pipeline will check if there is an inclusion of omics.conf file in nextflow.config file, which means that this pipeline is preconfigured for omics workflow. If configuration is there, Cloud Pipeline will simply pack the code and register as AWS HealthOmics Workflow, Otherwise, it will try to configure it by inspecting the code with help of inspect_nf.py and prepare necessary configs files. Provide parameter values to the workflow during pipeline execution Most of the workflows will expect some input parameters to be provided on startup, so we need some mechanism to provide these parameters to the underlying workflow during Cloud Pipeline pipeline execution. For nf-code/rnaseq there is a page where you can inspect all available values that can be provided. For this nf-core/rnaseq pipeline we specify the next parameters: input fasta gtf Go to CONFIGURATION tab of the pipeline: There is OUTPUT_DIR parameter that should be filled in. S3 bucket prefix is expected here. This is parameter with path type, so you are able to navigate through Cloud Pipeline storages, to specify the value: Click on folder icon near the parameter name. Choose the bucket prefix and click OK: You can also add some additional postfix for the selected path (for example if you would like to this postfix to be created during pipeline execution), and you also are able to use special keywords such as ${RUN_ID} , this one will be resolved during pipeline execution to the read run id value, so it will guarantee unique output prefix for each run. Click near to Add parameter button and choose Path parameter . New placeholder will appear: Specify name as fasta . Create another two parameters gtf and input with the same path type as follows and click Save button at the right corner of this view: Running the pipeline with a workflow Test data location: https://github.com/nf-core/test-datasets branch rnaseq fasta: https://raw.githubusercontent.com/nf-core/test-datasets/rnaseq/reference/genome.fasta samplesheet: https://raw.githubusercontent.com/nf-core/test-datasets/rnaseq/samplesheet/samplesheet.csv fastqs: https://github.com/nf-core/test-datasets/tree/rnaseq/testdata/GSE110004 From the pipeline view click RUN button: You will see the next view: Choose desire path to the fasta file (it could be s3 or AWS HealthOmics store path). Click on folder icon near the parameter name. Here we choose path from AWS HealthOmics Reference Store: Final value would be: For gtf parameter specify path to the desire gtf file on s3: The last parameter would be a input with a path to the samplesheet on s3 bucket: Samplesheet can refer files from the s3 or AWS HealthOmics Sequence Store. In our case it is an AWS HealthOmics Sequence Store. Format of the file as follows: sample,fastq_1,fastq_2,strandedness SRR6357070_2,omics://332551323846.storage.eu-west-1.amazonaws.com/7077728170/readSet/9082704952/source1.fastq.gz,omics://332551323846.storage.eu-west-1.amazonaws.com/7077728170/readSet/9082704952/source2.fastq.gz,reverse SRR6357071_2,omics://332551323846.storage.eu-west-1.amazonaws.com/7077728170/readSet/2230602162/source1.fastq.gz,omics://332551323846.storage.eu-west-1.amazonaws.com/7077728170/readSet/2230602162/source2.fastq.gz,reverse SRR6357072_2,omics://332551323846.storage.eu-west-1.amazonaws.com/7077728170/readSet/4337925538/source1.fastq.gz,omics://332551323846.storage.eu-west-1.amazonaws.com/7077728170/readSet/4337925538/source2.fastq.gz,reverse SRR6357073_1,omics://332551323846.storage.eu-west-1.amazonaws.com/7077728170/readSet/7316328213/source1.fastq.gz,,reverse SRR6357074_1,omics://332551323846.storage.eu-west-1.amazonaws.com/7077728170/readSet/9152942427/source1.fastq.gz,,reverse SRR6357075_1,omics://332551323846.storage.eu-west-1.amazonaws.com/7077728170/readSet/1700045600/source1.fastq.gz,,reverse SRR6357076_1,omics://332551323846.storage.eu-west-1.amazonaws.com/7077728170/readSet/2959595296/source1.fastq.gz,omics://332551323846.storage.eu-west-1.amazonaws.com/7077728170/readSet/2959595296/source2.fastq.gz,reverse All these files were preregistered in AWS HealthOmics Sequence Store. You can find our the path to the specific file by navigating to the ReadSet file in your AWS HealthOmics Sequence Store in Cloud Pipeline UI and copy the path: Also Note that for each file there should be a .fastq.gz extension specified at the end of the path. Click the LAUNCH button at the right upper corner: Approve launch: As result, you will see the next view: Click on the newly created run to see its logs: When pipeline will finish you will find its output under the path from OUTPUT_DIR parameter value:","title":"6.4. Work with AWS HealthOmics Workflow"},{"location":"manual/06_Manage_Pipeline/6.4._Work_with_aws_healthomics_workflow/#64-aws-healthomics-workflow","text":"Cloud Pipeline has built-in pipeline template that can be used to run your Nextflow workflow with AWS HealthOmics Workflow https://docs.aws.amazon.com/omics/latest/dev/workflows.html . Note : Works only for AWS based Cloud Pipeline deployment Currently only Nextflow workflows are supported This document provides you with necessary information on how to use existing workflows or write your own in Cloud Pipeline , to be able to utilize AWS HealthOmics Workflow.","title":"6.4. AWS HealthOmics Workflow"},{"location":"manual/06_Manage_Pipeline/6.4._Work_with_aws_healthomics_workflow/#1-create-pipeline-from-aws-healthomics-pipeline-template","text":"To start developing and Nextflow workflow or reuse existing one, first of all you will need to create Pipeline inside Cloud Pipeline platform. For this reason: Navigate to the Cloud Pipeline UI: In the catalog tree choose a location (folder) where you would like to create you pipeline: Click + Create button in the upper right corner and choose Pipeline -> AWS-HEALTHOMICS-WORKFLOW : Specify desired name and click CREATE: Wait while pipeline being created and after that click on its name in the folder content listing: Then click on the newly created revision of this pipeline and choose CODE tab at the top menu: Now, lets take a look on the structure of it: - workflow/ # Directory where all workflow related code should be placed - main.tf # Workflow file with dummy pipeline - nextflow.config # Worklow configuration file - run-omics-workflow.sh # Entry of the Cloud Pipeline pipeline, this script will be run on start up of the pipeline and will take care of creation and run of the AWS HealthOmics Workflow - config.json # Standard Cloud Pipeline pipeline configuration file where all pipeline related settings are stored Now you are ready to proceed develop your own / utilize existing workflow.","title":"1. Create pipeline from AWS HealthOmics pipeline template"},{"location":"manual/06_Manage_Pipeline/6.4._Work_with_aws_healthomics_workflow/#2-integrate-existing-workflow-with-aws-healthomics-on-cloud-pipeline","text":"Process of developing your own workflow hasn't actual difference from using existing one, in terms of integration this workflow with AWS HealthOmics on Cloud Pipeline . Here we describe the process of how to reuse existing workflow with AWS HealthOmics on Cloud Pipeline , but you can also use this documentation wen developing it from scratch.","title":"2. Integrate existing workflow with AWS HealthOmics on Cloud Pipeline"},{"location":"manual/06_Manage_Pipeline/6.4._Work_with_aws_healthomics_workflow/#obtain-gitlab-token-for-the-authentication","text":"Each pipeline object in Cloud Pipeline Platform is a git repository hosted on gitlab, so it is possible and essential to use it as a git repository. In the pipeline view page click GITLAB REPOSITORY: Copy the url and navigate to the URL, if it asks to do it, authenticate with you credentials in Cloud Pipeline platform: Navigate to the Settings -> Access Tokens and fill required fields as shown on the screenshot, adjust expiration date and click Create project access token : Copy new token and save it somewhere safe, you will need it:","title":"Obtain gitlab token for the authentication"},{"location":"manual/06_Manage_Pipeline/6.4._Work_with_aws_healthomics_workflow/#clone-gitlab-repository-locally","text":"Open terminal and perform the following command: git clone < repo-name >.git When it asks for the credentials - specify you Cloud Pipeline username as a Username and for the password use newly generated access token from previous step.","title":"Clone gitlab repository locally"},{"location":"manual/06_Manage_Pipeline/6.4._Work_with_aws_healthomics_workflow/#modify-git-repository-and-ingest-workflow-code-in-it","text":"Here we will use nf-core/rnaseq v3.11.1 Navigate to the src/workflow directory of your pipeline repository: Remove dummy sample pipeline files: rm -rf ./* Clone rnaseq pipeline to the directory: git clone https://github.com/nf-core/rnaseq.git . Checkout to the needed version and clean up: git checkout 3.11.1 rm -rf ./git* This workflow need one small improvements to be made to work correctly, otherwise it will fail during execution. Provide additional flag -p for the mkdir command to not fail if directory already exists: sed -i 's|mkdir|mkdir -p|g' modules/nf-core/qualimap/rnaseq/main.nf Commit changes: git add . git commit -m \"nf-core/rnaseq workflow code added\" Push changes to the remote repository: git push origin master When it asks for the credentials - specify you Cloud Pipeline username as a Username and for the password use newly generated access token from previous step. Navigate to the Cloud Pipeline UI -> Pipeline view and check that nf-core/rnaseq workflow is in the workflow directory: During each new run of the pipeline, Cloud Pipeline will check if there is an inclusion of omics.conf file in nextflow.config file, which means that this pipeline is preconfigured for omics workflow. If configuration is there, Cloud Pipeline will simply pack the code and register as AWS HealthOmics Workflow, Otherwise, it will try to configure it by inspecting the code with help of inspect_nf.py and prepare necessary configs files.","title":"Modify git repository and ingest workflow code in it"},{"location":"manual/06_Manage_Pipeline/6.4._Work_with_aws_healthomics_workflow/#provide-parameter-values-to-the-workflow-during-pipeline-execution","text":"Most of the workflows will expect some input parameters to be provided on startup, so we need some mechanism to provide these parameters to the underlying workflow during Cloud Pipeline pipeline execution. For nf-code/rnaseq there is a page where you can inspect all available values that can be provided. For this nf-core/rnaseq pipeline we specify the next parameters: input fasta gtf Go to CONFIGURATION tab of the pipeline: There is OUTPUT_DIR parameter that should be filled in. S3 bucket prefix is expected here. This is parameter with path type, so you are able to navigate through Cloud Pipeline storages, to specify the value: Click on folder icon near the parameter name. Choose the bucket prefix and click OK: You can also add some additional postfix for the selected path (for example if you would like to this postfix to be created during pipeline execution), and you also are able to use special keywords such as ${RUN_ID} , this one will be resolved during pipeline execution to the read run id value, so it will guarantee unique output prefix for each run. Click near to Add parameter button and choose Path parameter . New placeholder will appear: Specify name as fasta . Create another two parameters gtf and input with the same path type as follows and click Save button at the right corner of this view:","title":"Provide parameter values to the workflow during pipeline execution"},{"location":"manual/06_Manage_Pipeline/6.4._Work_with_aws_healthomics_workflow/#running-the-pipeline-with-a-workflow","text":"Test data location: https://github.com/nf-core/test-datasets branch rnaseq fasta: https://raw.githubusercontent.com/nf-core/test-datasets/rnaseq/reference/genome.fasta samplesheet: https://raw.githubusercontent.com/nf-core/test-datasets/rnaseq/samplesheet/samplesheet.csv fastqs: https://github.com/nf-core/test-datasets/tree/rnaseq/testdata/GSE110004 From the pipeline view click RUN button: You will see the next view: Choose desire path to the fasta file (it could be s3 or AWS HealthOmics store path). Click on folder icon near the parameter name. Here we choose path from AWS HealthOmics Reference Store: Final value would be: For gtf parameter specify path to the desire gtf file on s3: The last parameter would be a input with a path to the samplesheet on s3 bucket: Samplesheet can refer files from the s3 or AWS HealthOmics Sequence Store. In our case it is an AWS HealthOmics Sequence Store. Format of the file as follows: sample,fastq_1,fastq_2,strandedness SRR6357070_2,omics://332551323846.storage.eu-west-1.amazonaws.com/7077728170/readSet/9082704952/source1.fastq.gz,omics://332551323846.storage.eu-west-1.amazonaws.com/7077728170/readSet/9082704952/source2.fastq.gz,reverse SRR6357071_2,omics://332551323846.storage.eu-west-1.amazonaws.com/7077728170/readSet/2230602162/source1.fastq.gz,omics://332551323846.storage.eu-west-1.amazonaws.com/7077728170/readSet/2230602162/source2.fastq.gz,reverse SRR6357072_2,omics://332551323846.storage.eu-west-1.amazonaws.com/7077728170/readSet/4337925538/source1.fastq.gz,omics://332551323846.storage.eu-west-1.amazonaws.com/7077728170/readSet/4337925538/source2.fastq.gz,reverse SRR6357073_1,omics://332551323846.storage.eu-west-1.amazonaws.com/7077728170/readSet/7316328213/source1.fastq.gz,,reverse SRR6357074_1,omics://332551323846.storage.eu-west-1.amazonaws.com/7077728170/readSet/9152942427/source1.fastq.gz,,reverse SRR6357075_1,omics://332551323846.storage.eu-west-1.amazonaws.com/7077728170/readSet/1700045600/source1.fastq.gz,,reverse SRR6357076_1,omics://332551323846.storage.eu-west-1.amazonaws.com/7077728170/readSet/2959595296/source1.fastq.gz,omics://332551323846.storage.eu-west-1.amazonaws.com/7077728170/readSet/2959595296/source2.fastq.gz,reverse All these files were preregistered in AWS HealthOmics Sequence Store. You can find our the path to the specific file by navigating to the ReadSet file in your AWS HealthOmics Sequence Store in Cloud Pipeline UI and copy the path: Also Note that for each file there should be a .fastq.gz extension specified at the end of the path. Click the LAUNCH button at the right upper corner: Approve launch: As result, you will see the next view: Click on the newly created run to see its logs: When pipeline will finish you will find its output under the path from OUTPUT_DIR parameter value:","title":"Running the pipeline with a workflow"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/","text":"6. Manage Pipeline Pipeline object GUI \"Details\" view pane \"Details\" controls Pipeline versions GUI Pipeline controls Pipeline launching page Pipeline version tabs DOCUMENTS CODE CONFIGURATION HISTORY STORAGE RULES GRAPH Default environment variables Pipelines represent a sequence of tasks that are executed along with each other in order to process some data. They help to automate complex tasks that consist of many sub-tasks. This chapter describes Pipeline space GUI and the main working scenarios. Pipeline object GUI As far as the pipeline is one of CP objects which stored in \" Library \" space, the Pipeline workspace is separated into two panes: \"Hierarchy\" view pane \"Details\" view pane. Note : also you can view general information and some details about the specific pipeline via CLI. See 14.4 View pipeline definitions via CLI . \"Details\" view pane The \"Details\" view pane displays content of a selected object. In case of a pipeline, you will see: a list of pipeline versions with a description of last update and date of the last update; specific space's controls . \"Details\" controls Control Description Displays icon This icon includes: \" Attributes \" control ( 1 ) opens Attributes pane. Here you can see a list of \"key=value\" attributes of the pipeline. For more info see here . Note : If the selected pipeline has any defined attribute, Attributes pane is shown by default. Issues shows/hides the issues of the current pipeline to discuss. To learn more see here . \"Gear\" icon This control ( 2 ) allows to rename, change the description, delete and edit repository settings i.e. Repository address and Token Git repository \" Git repository \" control ( 3 ) shows a git repository address where pipeline versions are stored, which could be copied and pasted into a browser address field: If for your purposes ssh protocol is required, you may click the HTTPS/SSH selector and choose the SSH item: In that case, you will get a reformatted SSH address: Also a user can work directly with git from the console on the running node. For more information, how to configure Git client to work with the Cloud Pipeline, see here . To clone a pipeline a user shall have READ permissions, to push WRITE permission are also needed. For more info see 13. Permissions . Release \" Release \" control ( 4 ) is used to tag a particular pipeline version with a name. A draft pipeline version has the control only. Note : you can edit the last pipeline version only. Run Each pipeline version item of the selected pipeline's list has a \" Run \" control ( 5 ) to launch a pipeline version. Pipeline versions GUI Pipeline version interface displays full information about a pipeline version: supporting documentation, code files, and configurations, history of version runnings, etc. Pipeline controls The following buttons are available to manage this space. Control Description Run This button launch a pipeline version. When a user clicks the button, the \"Launch a pipeline\" page opens. \"Gear\" icon This control allows to rename, change the description, delete and edit repository settings i.e. Repository address and Token . Git repository Shows a git repository address where pipeline versions are stored, which could be copied and pasted in a browser line. Also a user can work directly with git from the console on the running node. For more information, how to configure Git client to work with the Cloud Pipeline, see here . To clone a pipeline a user shall have READ permissions, to push WRITE permission is also needed. For more info see 13. Permissions . Pipeline launching page \" Launch a pipeline \" page shows parameters of a default configuration the pipeline version. This page has the same view as a \"Configuration\" tab of a pipeline version. Here you can select any other configuration from the list and/or change parameters for this specific run (changes in configuration will be applied only to this specific run). Pipeline version tabs Pipeline version space dramatically differs from the Pipeline space. You can open it: just click on it. The whole information is organized into the following tabs in \"Details\" view pane. DOCUMENTS The \"Documents\" tab contains documentation associated with the pipeline, e.g. README, pipeline description, etc. See an example here . Note : README.md file is created automatically and contains default text which could be easily edited by a user. Documents tab controls The following buttons are available to manage this section: Control Description Upload ( a ) This control ( a ) allows to upload documentation files. Delete ( b ) \"Delete\" control ( b ) helps to delete a file. Rename ( c ) To rename a file a user shall use a \"Rename\" control ( c ). Download ( d ) This control ( d ) allows downloading pipeline documentation file to your local machine. Edit ( e ) \"Edit\" control ( e ) helps a user to edit any text files (e.g. README) here in a text editor using a markdown language . CODE This section contains a list of scripts to run a pipeline. Here you can create new files, folders and upload files here. Each script file could be edited (see details here ). Note : .json configuration file can also be edited in the Configuration tab via GUI. Code tab controls The following controls are available: Control Description Plus button ( a ) This control is to create a new folder in a pipeline version. The folder's name shall be specified. + New file ( b ) To create a new file in the current folder. Upload ( c ) To upload files from your local file system to a pipeline version. Rename ( d ) Each file or folder has a \"Rename\" control which allows renaming a file/folder. Delete ( e ) Each file or folder has a \"Delete\" control which deletes a file/folder. The list of system files All newly created pipelines have at least 2 starting files no matter what pipeline template you've chosen. Only newly created DEFAULT pipeline has 1 starting file ( config.json ). main_file This file contains a pipeline scenario. By default, it is named after a pipeline, but this may be changed in the configuration file. Note : the main_file is usually an entry point to start pipeline execution. To create your own scenario the default template of the main file shall be edited (see details here ). Example : below is the piece of the main_file of the Gromacs pipeline: config.json This file contains pipeline execution parameters. You can not rename or delete it because of it's used in pipeline scripts and they will not work without it. Note : it is advised that pipeline execution settings are modified via CONFIGURATION tab (e.g. if you want to change default settings for pipeline execution) or via Launch pipeline page (e.g. if you want to change pipeline settings for a current run). Manual config.json editing should be used only for advanced users (primarily developers) since json format is not validated in this case. Note : all attributes from config.json are available as environment variables for pipeline execution. The config.json file for every pipeline template have the following settings: Setting Description main_file A name of the main file for that pipeline. instance_size Instance type in terms of the specific Cloud Provider that specifies an amount of RAM in Gb, CPU and GPU cores number (e.g. m4.xlarge for AWS EC2 instance). instance_disk An instance's disk size in Gb. docker_image A name of the Docker image that will be used in the current pipeline. cmd_template Command line template that will be executed at the running instance in the pipeline. cmd_template can use environment variables: - to address the main_file parameter value, use the following construction - [main_file] - to address all other parameters, usual Linux environment variables style shall be used (e.g. $docker_image ) parameters Pipeline execution parameters (e.g. path to the data storage with input data). A parameter has a name and set of attributes. There are possible keys for each parameter: - \"type\" - key specifies a type for current parameter, - \"value\" - key specifies default value for parameter, - \"required\" - key specifies whether this parameter must be set ( \"required\": true ) or might not ( \"required\": false ) - \"no_override\" - key specifies whether the parameter default value be immutable in the detached configuration that uses this pipeline: - if a parameter has a default value and \"no_override\" is true - that parameter field will be read-only - if a parameter has a default value and \"no_override\" is false or not set - that parameter field will be writable - if a parameter has no default value - \"no_override\" option is ignored and that parameter field will be writable Example : config.json file of the Gromacs pipeline: Note : In addition to main_file and config.json you can add any number of files to the CODE section and combine it in one whole scenario. CONFIGURATION This section represents pipeline execution parameters which are set in config.json file. The parameters can be changed here and config.json file will be changed respectively. See how to edit configuration here . A configuration specifies: Section Control Description Name Pipeline and its configuration names. Estimated price per hour Control shows machine hours prices. If you navigate mouse to \"info\" icon, you'll see the maximum, minimum and average price for a particular pipeline version run as well as the price per hour. Exec environment This section lists execution environment parameters. Docker image A name of a Docker image to use for a pipeline execution (e.g. \"library/gromacs-gpu\"). Node type An instance type in terms of the specific Cloud Provider: CPU, RAM, GPU (e.g. 2 CPU cores, 8 Gb RAM, 0 GPU cores). Disk Size of a disk in gigabytes, that will be attached to the instance in Gb. Configure cluster button On-click, pop-up window will be shown: Here you can configure cluster or auto-scaled cluster. Cluster is a collection of instances which are connected so that they can be used together on a task. In both cases, a number of additional worker nodes with some main node as cluster head are launching (total number of pipelines = \"number of working nodes\" + 1). See v.0.14 - 7.2. Launch Detached Configuration for details. In case of using cluster , an exact count of worker nodes is directly defined by the user before launching the task and could not changing during the run. In case of using auto-scaled cluster , a max count of worker nodes is defined by the user before launching the task but really used count of worker nodes can change during the run depending on the jobs queue load. See Appendix C. Working with autoscaled cluster runs for details. For configure cluster: in opened window click Cluster button specify a number of child nodes (workers' count) if you want to use GridEngine server for the cluster, tick the Enable GridEngine checkbox. Setting of that checkbox automatically adds the CP_CAP_SGE system parameter with value true . Note : you may set this and other system parameters manually - see the example of using system parameters here . if you want to use Apache Spark for the cluster, tick the Enable Apache Spark checkbox. Setting of that checkbox automatically adds the CP_CAP_SPARK system parameter with value true . See the example of using Apache Spark here . if you want to use Slurm for the cluster, tick the Enable Slurm checkbox. Setting of that checkbox automatically adds the CP_CAP_SLURM system parameter with value true . See the example of using Slurm here . click OK button: When user selects Cluster option, information on total cluster resources is shown. Resources are calculated as (CPU/RAM/GPU)*(NUMBER_OF_WORKERS+1) : For configure auto-scaled cluster : in opened window click Auto-scaled cluster button specify a number of child nodes (workers' count) in field Auto-scaled up to and click OK button: Note : that number is meaning total count of \"auto-scaled\" nodes - it is the max count of worker nodes that could be attached to the main node to work together as cluster. These nodes will be attached to the cluster only in case if some jobs are in waiting state longer than a specific time. Also these nodes will be dropped from the cluster in case when jobs queue is empty or all jobs are running and there are some idle nodes longer than a specific time. Note : about timeout periods for scale-up and scale-down of auto-scaled cluster see here . additionally you may enable hybrid mode for the auto-scaled cluster - it allows to scale-up the cluster (attach a worker node) with the instance type distinct of the master - worker is being picked up based on the amount of unsatisfied CPU requirements of all pending jobs. For that behavior, set the \" Enable Hybrid cluster \" checkbox. For more details see here . additionally you may specify a number of \"persistent\" child nodes (workers' count) - click Setup default child nodes count , input Default child nodes number and click Ok button: These default child nodes will be never \"scaled-down\" during the run regardless of jobs queue load. In the example above, total count of \"auto-scaled\" nodes - 3, and 1 of them is \"persistent\". Note : total count of child nodes always must be greater than count of default (\"persistent\") child nodes. if you don't want to use default (\"persistent\") child nodes in your auto-scaled cluster - click Reset button opposite the Default child nodes field. additionally you may choose a price type for workers that will be attached during the run - via the \" Workers price type \" dropdown list - workers' price type can be automatically the same as the master node type (by default) or forcibly specified regardless on the master's type: When user selects Auto-scaled cluster , information on total cluster resources is shown as interval - from the \"min\" configuration to \"max\" configuration: \"min\" configuration resources are calculated as (CPU/RAM/GPU)*(NUMBER_OF_DEFAULT_WORKERS+1) \"max\" configuration resources are calculated as (CPU/RAM/GPU)*(TOTAL_NUMBER_OF_WORKERS+1) E.g. for auto-scaled cluster with 2 child nodes and without default (\"persistent\") child nodes (NUMBER_OF_DEFAULT_WORKERS = 0; TOTAL_NUMBER_OF_WORKERS = 2) : E.g. for auto-scaled cluster with 2 child nodes and 1 default (\"persistent\") child node (NUMBER_OF_DEFAULT_WORKERS = 1; TOTAL_NUMBER_OF_WORKERS = 2) : Note : in some specific configurations such as hybrid autoscaling clusters amount of resources can vary beyond the shown interval. Note : if you don't want to use any cluster - click Single node button and then click OK button. Cloud Region A specific region for a compute node placement. Please note, if a non-default region is selected - certain CP features may be unavailable: FS mounts usage from the another region (e.g. \" EU West \" region cannot use FS mounts from the \" US East \"). Regular storages will be still available If a specific tool, used for a run, requires an on-premise license server (e.g. monolix, matlab, schrodinger, etc.) - such instances shall be run in a region, that hosts those license servers. Note : if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - in that control Cloud Provider auxiliary icons also will be displayed, e.g.: For a single-Provider deployments only Cloud Region icons are displayed. Run capabilities Allows to launch a pipeline with a pre-configured additional software package(s), e.g. Docker-In-Docker , Singularity and others. For details see here . Advanced Price type Choose Spot or On-demand type of instance. You can look information about price types hovering \"Info\" icon and based on it make your choice. Timeout (min) After this time pipeline will shut down (optional). Before the shut down, all the contents of the $ANALYSIS_DIR directory will be copied to output storages. Limit mounts Allow to specify storages that should be mounted. For details see here . Cmd template A shell command that will be executed to start a pipeline. \"Start idle\" The flag sets cmd_template to sleep infinity . For more information about starting a job in this mode refer to 15. Interactive services . Parameters This section lists pipeline specific parameters that can be used during a pipeline run. Pipeline parameters can be assigned the following types: String - generic scalar value (e.g. Sample name). Boolean - boolean value. Path - path in a data storage hierarchy. Input - path in a data storage hierarchy. During pipeline initialization, this path will be used to download input data on the calculation node for processing from a storage. Output - path in a data storage hierarchy. During pipeline finalization, this path will be used to upload resulting data to a storage. Common - path in a data storage hierarchy. Similar to \"Input\" type, but this data will not be erased from a calculation node, when a pipeline is finished (this is useful for reference data, as it can be reused by further pipeline runs that share the same reference). Note : You can use Project attribute values as parameters for the Run : Click an empty parameter value field. Enter \" project. \" In the drop-down list select the Project attribute value: Add parameter This control helps to add an additional parameter to a configuration. Configuration tab controls Control Description Add To create a customized configuration for the pipeline, click the + ADD button in the upper-right corner of the screen. For more details see here . Save This button saves changes in a configuration. HISTORY This section contains information about all the current pipeline version's runs. Runs info is organized into a table with the following columns: Run - each record of that column contains two rows: in upper - run name that consists of pipeline name and run id, in bottom - Cloud Region. Note : if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - corresponding text information also has a Provider name, e.g.: Parent-run - id of the run that executed current run (this field is non-empty only for runs that are executed by other runs). Pipeline - each record of that column contains two rows: in upper - pipeline name, in bottom - pipeline version. Docker image - base docker image name. Started - time pipeline started running. Completed - time pipeline finished execution. Elapsed - each record of that column contains two rows: in upper - pipeline running time, in bottom - run's estimated price, which is calculated based on the run duration, region and instance type. Owner - user who launched run. You can filter runs by clicking the filter icon . By using the filter control you can choose whether display runs for current pipeline version or display runs for all pipeline versions. History tab controls Control Description PAUSE (a) To pause running pipeline press this control. This control is available only for on-demand instances. STOP (b) To stop running pipeline press this control. LOG (c) \"Log\" control opens detailed information about the run. You'll be redirected to \" Runs \" space (see 11. Manage Runs ). RESUME (d) To resume pausing pipeline press this control. This control is available only for on-demand instances. TERMINATE (e) To terminate node without waiting of the pipeline resuming. This control is available only for on-demand instances, which were paused. RERUN (f) This control reruns completed pipeline's runs. Pipeline run's states Icons at the left represent the current state of the pipeline runs: - Queued state (\"sandglass\" icon) - a run is waiting in the queue for the available compute node. - Initializing state (\"rotating\" icon) - a run is being initialized. - Pulling state (\"download\" icon) - now pipeline Docker image is downloaded to the node. - Running state (\"play\" icon) - a pipeline is running. The node is appearing and pipeline input data is being downloaded to the node before the \" InitializeEnvironment \" service task appears. - Paused state (\"pause\" icon) - a run is paused. At this moment compute node is already stopped but keeps it's state. Such run may be resumed. - Success state (\"OK\" icon) - successful pipeline execution. - Failed state (\"caution\" icon) - unsuccessful pipeline execution. - Stopped state (\"clock\" icon) - a pipeline manually stopped. Also, help tooltips are provided when hovering a run state icon, e.g.: STORAGE RULES This section displays a list of rules used to upload data to the output data storage, once pipeline finished. It helps to store only data you need and minimize the amount of interim data in data storages. Info is organized into a table with the following columns: Mask column contains a relative path from the $ANALYSIS_DIR folder (see Default environment variables section below for more information). Mask uses bash syntax to specify the data that you want to upload from the $ANALYSIS_DIR. Data from the specified path will be uploaded to the bucket from the pipeline node. Note : by default whole $ANALYSIS_DIR folder is uploaded to the cloud bucket (default Mask is - \"*\"). For example, \"*.txt*\" mask specifies that all files with .txt extension need to be uploaded from the $ANALYSIS_DIR to the data storage. Note : Be accurate when specifying masks - if wildcard mask (\"*\") is specified, all files will be uploaded, no matter what additional masks are specified. The Created column shows date and time of rules creation. Move to Short-Term Storage column indicates whether pipeline output data will be moved to a short-term storage. Storage rules tab control Control Description Add new rule (a) This control allows adding a new data managing rule. Delete (b) To delete a data managing rule press this control. GRAPH This section represents the sequence of pipeline tasks as a directed graph. Tasks are graph vertices, edges represent execution order. A task can be executed only when all input edges - associated tasks - are completed (see more information about creating a pipeline with GRAPH section here ). Note : only for Luigi and WDL pipelines. Note : If main_file has mistakes, pipeline workflow won't be visualized. Graph tab controls When a PipelineBuilder graph is loaded, the following layout controls become available to the user. Control Description Save saves changes. Revert reverts all changes to the last saving. Layout performs graph linearization, make it more readable. Fit zooms graph to fit the screen. Show links enables/disables workflow level links to the tasks. It is disabled by default, as for large workflows it overwhelms the visualization. Zoom out zooms graph out. Zoom in zooms graph in. Search element allows to find specific object at the graph. Fullscreen expands graph to the full screen. Default environment variables Pipeline scripts (e.g. main_file ) use default environmental variables for pipeline execution. These variables are set in internal CP scripts: RUN_ID - pipeline run ID. PIPELINE_NAME - pipeline name. COMMON_DIR - directory where pipeline common data (parameter with \"type\": \"common\" ) will be stored. ANALYSIS_DIR - directory where output data of the pipeline (parameter with \"type\": \"output\" ) will be stored. INPUT_DIR - directory where input data of the pipeline (parameter with \"type\": \"input\" ) will be stored. SCRIPTS_DIR - directory where all pipeline scripts and config.json file will be stored.","title":"6.0. Overview"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#6-manage-pipeline","text":"Pipeline object GUI \"Details\" view pane \"Details\" controls Pipeline versions GUI Pipeline controls Pipeline launching page Pipeline version tabs DOCUMENTS CODE CONFIGURATION HISTORY STORAGE RULES GRAPH Default environment variables Pipelines represent a sequence of tasks that are executed along with each other in order to process some data. They help to automate complex tasks that consist of many sub-tasks. This chapter describes Pipeline space GUI and the main working scenarios.","title":"6. Manage Pipeline"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#pipeline-object-gui","text":"As far as the pipeline is one of CP objects which stored in \" Library \" space, the Pipeline workspace is separated into two panes: \"Hierarchy\" view pane \"Details\" view pane. Note : also you can view general information and some details about the specific pipeline via CLI. See 14.4 View pipeline definitions via CLI .","title":"Pipeline object GUI"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#details-view-pane","text":"The \"Details\" view pane displays content of a selected object. In case of a pipeline, you will see: a list of pipeline versions with a description of last update and date of the last update; specific space's controls .","title":"\"Details\" view pane"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#details-controls","text":"Control Description Displays icon This icon includes: \" Attributes \" control ( 1 ) opens Attributes pane. Here you can see a list of \"key=value\" attributes of the pipeline. For more info see here . Note : If the selected pipeline has any defined attribute, Attributes pane is shown by default. Issues shows/hides the issues of the current pipeline to discuss. To learn more see here . \"Gear\" icon This control ( 2 ) allows to rename, change the description, delete and edit repository settings i.e. Repository address and Token Git repository \" Git repository \" control ( 3 ) shows a git repository address where pipeline versions are stored, which could be copied and pasted into a browser address field: If for your purposes ssh protocol is required, you may click the HTTPS/SSH selector and choose the SSH item: In that case, you will get a reformatted SSH address: Also a user can work directly with git from the console on the running node. For more information, how to configure Git client to work with the Cloud Pipeline, see here . To clone a pipeline a user shall have READ permissions, to push WRITE permission are also needed. For more info see 13. Permissions . Release \" Release \" control ( 4 ) is used to tag a particular pipeline version with a name. A draft pipeline version has the control only. Note : you can edit the last pipeline version only. Run Each pipeline version item of the selected pipeline's list has a \" Run \" control ( 5 ) to launch a pipeline version.","title":"\"Details\" controls"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#pipeline-versions-gui","text":"Pipeline version interface displays full information about a pipeline version: supporting documentation, code files, and configurations, history of version runnings, etc.","title":"Pipeline versions GUI"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#pipeline-controls","text":"The following buttons are available to manage this space. Control Description Run This button launch a pipeline version. When a user clicks the button, the \"Launch a pipeline\" page opens. \"Gear\" icon This control allows to rename, change the description, delete and edit repository settings i.e. Repository address and Token . Git repository Shows a git repository address where pipeline versions are stored, which could be copied and pasted in a browser line. Also a user can work directly with git from the console on the running node. For more information, how to configure Git client to work with the Cloud Pipeline, see here . To clone a pipeline a user shall have READ permissions, to push WRITE permission is also needed. For more info see 13. Permissions .","title":"Pipeline controls"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#pipeline-launching-page","text":"\" Launch a pipeline \" page shows parameters of a default configuration the pipeline version. This page has the same view as a \"Configuration\" tab of a pipeline version. Here you can select any other configuration from the list and/or change parameters for this specific run (changes in configuration will be applied only to this specific run).","title":"Pipeline launching page"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#pipeline-version-tabs","text":"Pipeline version space dramatically differs from the Pipeline space. You can open it: just click on it. The whole information is organized into the following tabs in \"Details\" view pane.","title":"Pipeline version tabs"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#documents","text":"The \"Documents\" tab contains documentation associated with the pipeline, e.g. README, pipeline description, etc. See an example here . Note : README.md file is created automatically and contains default text which could be easily edited by a user.","title":"DOCUMENTS"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#documents-tab-controls","text":"The following buttons are available to manage this section: Control Description Upload ( a ) This control ( a ) allows to upload documentation files. Delete ( b ) \"Delete\" control ( b ) helps to delete a file. Rename ( c ) To rename a file a user shall use a \"Rename\" control ( c ). Download ( d ) This control ( d ) allows downloading pipeline documentation file to your local machine. Edit ( e ) \"Edit\" control ( e ) helps a user to edit any text files (e.g. README) here in a text editor using a markdown language .","title":"Documents tab controls"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#code","text":"This section contains a list of scripts to run a pipeline. Here you can create new files, folders and upload files here. Each script file could be edited (see details here ). Note : .json configuration file can also be edited in the Configuration tab via GUI.","title":"CODE"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#code-tab-controls","text":"The following controls are available: Control Description Plus button ( a ) This control is to create a new folder in a pipeline version. The folder's name shall be specified. + New file ( b ) To create a new file in the current folder. Upload ( c ) To upload files from your local file system to a pipeline version. Rename ( d ) Each file or folder has a \"Rename\" control which allows renaming a file/folder. Delete ( e ) Each file or folder has a \"Delete\" control which deletes a file/folder.","title":"Code tab controls"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#the-list-of-system-files","text":"All newly created pipelines have at least 2 starting files no matter what pipeline template you've chosen. Only newly created DEFAULT pipeline has 1 starting file ( config.json ).","title":"The list of system files"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#main_file","text":"This file contains a pipeline scenario. By default, it is named after a pipeline, but this may be changed in the configuration file. Note : the main_file is usually an entry point to start pipeline execution. To create your own scenario the default template of the main file shall be edited (see details here ). Example : below is the piece of the main_file of the Gromacs pipeline:","title":"main_file"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#configjson","text":"This file contains pipeline execution parameters. You can not rename or delete it because of it's used in pipeline scripts and they will not work without it. Note : it is advised that pipeline execution settings are modified via CONFIGURATION tab (e.g. if you want to change default settings for pipeline execution) or via Launch pipeline page (e.g. if you want to change pipeline settings for a current run). Manual config.json editing should be used only for advanced users (primarily developers) since json format is not validated in this case. Note : all attributes from config.json are available as environment variables for pipeline execution. The config.json file for every pipeline template have the following settings: Setting Description main_file A name of the main file for that pipeline. instance_size Instance type in terms of the specific Cloud Provider that specifies an amount of RAM in Gb, CPU and GPU cores number (e.g. m4.xlarge for AWS EC2 instance). instance_disk An instance's disk size in Gb. docker_image A name of the Docker image that will be used in the current pipeline. cmd_template Command line template that will be executed at the running instance in the pipeline. cmd_template can use environment variables: - to address the main_file parameter value, use the following construction - [main_file] - to address all other parameters, usual Linux environment variables style shall be used (e.g. $docker_image ) parameters Pipeline execution parameters (e.g. path to the data storage with input data). A parameter has a name and set of attributes. There are possible keys for each parameter: - \"type\" - key specifies a type for current parameter, - \"value\" - key specifies default value for parameter, - \"required\" - key specifies whether this parameter must be set ( \"required\": true ) or might not ( \"required\": false ) - \"no_override\" - key specifies whether the parameter default value be immutable in the detached configuration that uses this pipeline: - if a parameter has a default value and \"no_override\" is true - that parameter field will be read-only - if a parameter has a default value and \"no_override\" is false or not set - that parameter field will be writable - if a parameter has no default value - \"no_override\" option is ignored and that parameter field will be writable Example : config.json file of the Gromacs pipeline: Note : In addition to main_file and config.json you can add any number of files to the CODE section and combine it in one whole scenario.","title":"config.json"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#configuration","text":"This section represents pipeline execution parameters which are set in config.json file. The parameters can be changed here and config.json file will be changed respectively. See how to edit configuration here . A configuration specifies: Section Control Description Name Pipeline and its configuration names. Estimated price per hour Control shows machine hours prices. If you navigate mouse to \"info\" icon, you'll see the maximum, minimum and average price for a particular pipeline version run as well as the price per hour. Exec environment This section lists execution environment parameters. Docker image A name of a Docker image to use for a pipeline execution (e.g. \"library/gromacs-gpu\"). Node type An instance type in terms of the specific Cloud Provider: CPU, RAM, GPU (e.g. 2 CPU cores, 8 Gb RAM, 0 GPU cores). Disk Size of a disk in gigabytes, that will be attached to the instance in Gb. Configure cluster button On-click, pop-up window will be shown: Here you can configure cluster or auto-scaled cluster. Cluster is a collection of instances which are connected so that they can be used together on a task. In both cases, a number of additional worker nodes with some main node as cluster head are launching (total number of pipelines = \"number of working nodes\" + 1). See v.0.14 - 7.2. Launch Detached Configuration for details. In case of using cluster , an exact count of worker nodes is directly defined by the user before launching the task and could not changing during the run. In case of using auto-scaled cluster , a max count of worker nodes is defined by the user before launching the task but really used count of worker nodes can change during the run depending on the jobs queue load. See Appendix C. Working with autoscaled cluster runs for details. For configure cluster: in opened window click Cluster button specify a number of child nodes (workers' count) if you want to use GridEngine server for the cluster, tick the Enable GridEngine checkbox. Setting of that checkbox automatically adds the CP_CAP_SGE system parameter with value true . Note : you may set this and other system parameters manually - see the example of using system parameters here . if you want to use Apache Spark for the cluster, tick the Enable Apache Spark checkbox. Setting of that checkbox automatically adds the CP_CAP_SPARK system parameter with value true . See the example of using Apache Spark here . if you want to use Slurm for the cluster, tick the Enable Slurm checkbox. Setting of that checkbox automatically adds the CP_CAP_SLURM system parameter with value true . See the example of using Slurm here . click OK button: When user selects Cluster option, information on total cluster resources is shown. Resources are calculated as (CPU/RAM/GPU)*(NUMBER_OF_WORKERS+1) : For configure auto-scaled cluster : in opened window click Auto-scaled cluster button specify a number of child nodes (workers' count) in field Auto-scaled up to and click OK button: Note : that number is meaning total count of \"auto-scaled\" nodes - it is the max count of worker nodes that could be attached to the main node to work together as cluster. These nodes will be attached to the cluster only in case if some jobs are in waiting state longer than a specific time. Also these nodes will be dropped from the cluster in case when jobs queue is empty or all jobs are running and there are some idle nodes longer than a specific time. Note : about timeout periods for scale-up and scale-down of auto-scaled cluster see here . additionally you may enable hybrid mode for the auto-scaled cluster - it allows to scale-up the cluster (attach a worker node) with the instance type distinct of the master - worker is being picked up based on the amount of unsatisfied CPU requirements of all pending jobs. For that behavior, set the \" Enable Hybrid cluster \" checkbox. For more details see here . additionally you may specify a number of \"persistent\" child nodes (workers' count) - click Setup default child nodes count , input Default child nodes number and click Ok button: These default child nodes will be never \"scaled-down\" during the run regardless of jobs queue load. In the example above, total count of \"auto-scaled\" nodes - 3, and 1 of them is \"persistent\". Note : total count of child nodes always must be greater than count of default (\"persistent\") child nodes. if you don't want to use default (\"persistent\") child nodes in your auto-scaled cluster - click Reset button opposite the Default child nodes field. additionally you may choose a price type for workers that will be attached during the run - via the \" Workers price type \" dropdown list - workers' price type can be automatically the same as the master node type (by default) or forcibly specified regardless on the master's type: When user selects Auto-scaled cluster , information on total cluster resources is shown as interval - from the \"min\" configuration to \"max\" configuration: \"min\" configuration resources are calculated as (CPU/RAM/GPU)*(NUMBER_OF_DEFAULT_WORKERS+1) \"max\" configuration resources are calculated as (CPU/RAM/GPU)*(TOTAL_NUMBER_OF_WORKERS+1) E.g. for auto-scaled cluster with 2 child nodes and without default (\"persistent\") child nodes (NUMBER_OF_DEFAULT_WORKERS = 0; TOTAL_NUMBER_OF_WORKERS = 2) : E.g. for auto-scaled cluster with 2 child nodes and 1 default (\"persistent\") child node (NUMBER_OF_DEFAULT_WORKERS = 1; TOTAL_NUMBER_OF_WORKERS = 2) : Note : in some specific configurations such as hybrid autoscaling clusters amount of resources can vary beyond the shown interval. Note : if you don't want to use any cluster - click Single node button and then click OK button. Cloud Region A specific region for a compute node placement. Please note, if a non-default region is selected - certain CP features may be unavailable: FS mounts usage from the another region (e.g. \" EU West \" region cannot use FS mounts from the \" US East \"). Regular storages will be still available If a specific tool, used for a run, requires an on-premise license server (e.g. monolix, matlab, schrodinger, etc.) - such instances shall be run in a region, that hosts those license servers. Note : if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - in that control Cloud Provider auxiliary icons also will be displayed, e.g.: For a single-Provider deployments only Cloud Region icons are displayed. Run capabilities Allows to launch a pipeline with a pre-configured additional software package(s), e.g. Docker-In-Docker , Singularity and others. For details see here . Advanced Price type Choose Spot or On-demand type of instance. You can look information about price types hovering \"Info\" icon and based on it make your choice. Timeout (min) After this time pipeline will shut down (optional). Before the shut down, all the contents of the $ANALYSIS_DIR directory will be copied to output storages. Limit mounts Allow to specify storages that should be mounted. For details see here . Cmd template A shell command that will be executed to start a pipeline. \"Start idle\" The flag sets cmd_template to sleep infinity . For more information about starting a job in this mode refer to 15. Interactive services . Parameters This section lists pipeline specific parameters that can be used during a pipeline run. Pipeline parameters can be assigned the following types: String - generic scalar value (e.g. Sample name). Boolean - boolean value. Path - path in a data storage hierarchy. Input - path in a data storage hierarchy. During pipeline initialization, this path will be used to download input data on the calculation node for processing from a storage. Output - path in a data storage hierarchy. During pipeline finalization, this path will be used to upload resulting data to a storage. Common - path in a data storage hierarchy. Similar to \"Input\" type, but this data will not be erased from a calculation node, when a pipeline is finished (this is useful for reference data, as it can be reused by further pipeline runs that share the same reference). Note : You can use Project attribute values as parameters for the Run : Click an empty parameter value field. Enter \" project. \" In the drop-down list select the Project attribute value: Add parameter This control helps to add an additional parameter to a configuration.","title":"CONFIGURATION"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#configuration-tab-controls","text":"Control Description Add To create a customized configuration for the pipeline, click the + ADD button in the upper-right corner of the screen. For more details see here . Save This button saves changes in a configuration.","title":"Configuration tab controls"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#history","text":"This section contains information about all the current pipeline version's runs. Runs info is organized into a table with the following columns: Run - each record of that column contains two rows: in upper - run name that consists of pipeline name and run id, in bottom - Cloud Region. Note : if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - corresponding text information also has a Provider name, e.g.: Parent-run - id of the run that executed current run (this field is non-empty only for runs that are executed by other runs). Pipeline - each record of that column contains two rows: in upper - pipeline name, in bottom - pipeline version. Docker image - base docker image name. Started - time pipeline started running. Completed - time pipeline finished execution. Elapsed - each record of that column contains two rows: in upper - pipeline running time, in bottom - run's estimated price, which is calculated based on the run duration, region and instance type. Owner - user who launched run. You can filter runs by clicking the filter icon . By using the filter control you can choose whether display runs for current pipeline version or display runs for all pipeline versions.","title":"HISTORY"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#history-tab-controls","text":"Control Description PAUSE (a) To pause running pipeline press this control. This control is available only for on-demand instances. STOP (b) To stop running pipeline press this control. LOG (c) \"Log\" control opens detailed information about the run. You'll be redirected to \" Runs \" space (see 11. Manage Runs ). RESUME (d) To resume pausing pipeline press this control. This control is available only for on-demand instances. TERMINATE (e) To terminate node without waiting of the pipeline resuming. This control is available only for on-demand instances, which were paused. RERUN (f) This control reruns completed pipeline's runs.","title":"History tab controls"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#pipeline-runs-states","text":"Icons at the left represent the current state of the pipeline runs: - Queued state (\"sandglass\" icon) - a run is waiting in the queue for the available compute node. - Initializing state (\"rotating\" icon) - a run is being initialized. - Pulling state (\"download\" icon) - now pipeline Docker image is downloaded to the node. - Running state (\"play\" icon) - a pipeline is running. The node is appearing and pipeline input data is being downloaded to the node before the \" InitializeEnvironment \" service task appears. - Paused state (\"pause\" icon) - a run is paused. At this moment compute node is already stopped but keeps it's state. Such run may be resumed. - Success state (\"OK\" icon) - successful pipeline execution. - Failed state (\"caution\" icon) - unsuccessful pipeline execution. - Stopped state (\"clock\" icon) - a pipeline manually stopped. Also, help tooltips are provided when hovering a run state icon, e.g.:","title":"Pipeline run's states"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#storage-rules","text":"This section displays a list of rules used to upload data to the output data storage, once pipeline finished. It helps to store only data you need and minimize the amount of interim data in data storages. Info is organized into a table with the following columns: Mask column contains a relative path from the $ANALYSIS_DIR folder (see Default environment variables section below for more information). Mask uses bash syntax to specify the data that you want to upload from the $ANALYSIS_DIR. Data from the specified path will be uploaded to the bucket from the pipeline node. Note : by default whole $ANALYSIS_DIR folder is uploaded to the cloud bucket (default Mask is - \"*\"). For example, \"*.txt*\" mask specifies that all files with .txt extension need to be uploaded from the $ANALYSIS_DIR to the data storage. Note : Be accurate when specifying masks - if wildcard mask (\"*\") is specified, all files will be uploaded, no matter what additional masks are specified. The Created column shows date and time of rules creation. Move to Short-Term Storage column indicates whether pipeline output data will be moved to a short-term storage.","title":"STORAGE RULES"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#storage-rules-tab-control","text":"Control Description Add new rule (a) This control allows adding a new data managing rule. Delete (b) To delete a data managing rule press this control.","title":"Storage rules tab control"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#graph","text":"This section represents the sequence of pipeline tasks as a directed graph. Tasks are graph vertices, edges represent execution order. A task can be executed only when all input edges - associated tasks - are completed (see more information about creating a pipeline with GRAPH section here ). Note : only for Luigi and WDL pipelines. Note : If main_file has mistakes, pipeline workflow won't be visualized.","title":"GRAPH"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#graph-tab-controls","text":"When a PipelineBuilder graph is loaded, the following layout controls become available to the user. Control Description Save saves changes. Revert reverts all changes to the last saving. Layout performs graph linearization, make it more readable. Fit zooms graph to fit the screen. Show links enables/disables workflow level links to the tasks. It is disabled by default, as for large workflows it overwhelms the visualization. Zoom out zooms graph out. Zoom in zooms graph in. Search element allows to find specific object at the graph. Fullscreen expands graph to the full screen.","title":"Graph tab controls"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#default-environment-variables","text":"Pipeline scripts (e.g. main_file ) use default environmental variables for pipeline execution. These variables are set in internal CP scripts: RUN_ID - pipeline run ID. PIPELINE_NAME - pipeline name. COMMON_DIR - directory where pipeline common data (parameter with \"type\": \"common\" ) will be stored. ANALYSIS_DIR - directory where output data of the pipeline (parameter with \"type\": \"output\" ) will be stored. INPUT_DIR - directory where input data of the pipeline (parameter with \"type\": \"input\" ) will be stored. SCRIPTS_DIR - directory where all pipeline scripts and config.json file will be stored.","title":"Default environment variables"},{"location":"manual/07_Manage_Detached_configuration/7.1._Create_and_customize_Detached_configuration/","text":"7.1. Create and customize Detached configuration Create Detached configuration Customize Detached configuration Edit detached configuration name and description Edit detached configuration permissions Add run configuration to detached configuration Edit run configuration in the Detached configuration Set a root entity and map configuration parameters Delete run configuration from the Detached configuration Create Detached configuration To create Detached configuration in a Folder you need to have WRITE permission for that folder and the ROLE_CONFIGURATION_MANAGER role. For more information see 13. Permissions . Note : you can create a specific type of a run configuration which could be used only for a specific type of data. In such type of a run configuration, you can link type of data (e.g. Sample, Participant, etc) and the algorithm - a pipeline. To do that, you shall create your run configuration in a Project folder. Learn how to create a project here . To create a Detached configuration: Navigate to the folder where you want to create. Click + Create \u2192 Configuration . Enter Configuration name and Configuration description in pop-up window. Click Create . The configuration will be shown in the Library. Customize Detached configuration To edit Detached configuration you need WRITE permissions for it. For more information see 13. Permissions . Edit detached configuration name and description Navigate to the Folder where the Detached configuration is stored. Click icon. The \"Edit configuration info\" pop-up window will be open. Change Detached configuration name and description. Click Save . Edit detached configuration permissions Navigate to the Detached configuration and click icon. Note: Also you can navigate to the Folder where the Detached configuration is stored and click \"Pencil\" icon. Go to Permissions tab. Click Add user or Add user group . In the example screenshots, we grant permissions to a user. Enter user's name. Auto-filling will help you. A user will be added to the list. Click User's name to manage user's permissions. Tick appropriate permissions. For more details see 13. Permissions . Add run configuration to detached configuration Select Detached configuration in the Library . Add new Run configuration via + ADD button. Enter the name, description of the new Run configuration . If the Detached configuration already has more than one Run configuration , select the template for the new one. The new configuration will be based on the template . Click Create . New Run Configuration will be represented at Detached configuration details pane. Edit run configuration in the Detached configuration Select Detached configuration in the Library . Go to the tab with the Run configuration that you want to change. Change parameters of the Run configuration. Click Save . Set a root entity and map configuration parameters If your configuration stored in a folder with a Project type, when, to set a \"Root entity\" field, you shall add metadata to your project. After that, you'll be able to select metadata entity type from the drop-down list. Note : learn how to create a project here and about managing metadata here . Click \" Root entity \" combo-box. Choose the object from default values: Participants, Samples, Pairs, Sets of Participants, Sets of Samples, Sets of Pairs. When you select \" Root entity \", you'll be able to map configuration parameters to the root entity metadata attributes. You can set it using expansion expressions. Click an empty parameter value field. Enter \" this. \". \" this \" means that value is attributed to the selected Root entity type . In the drop-down list select the metadata value. Delete run configuration from the Detached configuration Select Detached configuration in the Library . Go to the tab with the Run configuration that you want to delete. Click Remove . Confirm removal.","title":"7.1. Create and customize Detached configuration"},{"location":"manual/07_Manage_Detached_configuration/7.1._Create_and_customize_Detached_configuration/#71-create-and-customize-detached-configuration","text":"Create Detached configuration Customize Detached configuration Edit detached configuration name and description Edit detached configuration permissions Add run configuration to detached configuration Edit run configuration in the Detached configuration Set a root entity and map configuration parameters Delete run configuration from the Detached configuration","title":"7.1. Create and customize Detached configuration"},{"location":"manual/07_Manage_Detached_configuration/7.1._Create_and_customize_Detached_configuration/#create-detached-configuration","text":"To create Detached configuration in a Folder you need to have WRITE permission for that folder and the ROLE_CONFIGURATION_MANAGER role. For more information see 13. Permissions . Note : you can create a specific type of a run configuration which could be used only for a specific type of data. In such type of a run configuration, you can link type of data (e.g. Sample, Participant, etc) and the algorithm - a pipeline. To do that, you shall create your run configuration in a Project folder. Learn how to create a project here . To create a Detached configuration: Navigate to the folder where you want to create. Click + Create \u2192 Configuration . Enter Configuration name and Configuration description in pop-up window. Click Create . The configuration will be shown in the Library.","title":"Create Detached configuration"},{"location":"manual/07_Manage_Detached_configuration/7.1._Create_and_customize_Detached_configuration/#customize-detached-configuration","text":"To edit Detached configuration you need WRITE permissions for it. For more information see 13. Permissions .","title":"Customize Detached configuration"},{"location":"manual/07_Manage_Detached_configuration/7.1._Create_and_customize_Detached_configuration/#edit-detached-configuration-name-and-description","text":"Navigate to the Folder where the Detached configuration is stored. Click icon. The \"Edit configuration info\" pop-up window will be open. Change Detached configuration name and description. Click Save .","title":"Edit\u00a0detached configuration name and description"},{"location":"manual/07_Manage_Detached_configuration/7.1._Create_and_customize_Detached_configuration/#edit-detached-configuration-permissions","text":"Navigate to the Detached configuration and click icon. Note: Also you can navigate to the Folder where the Detached configuration is stored and click \"Pencil\" icon. Go to Permissions tab. Click Add user or Add user group . In the example screenshots, we grant permissions to a user. Enter user's name. Auto-filling will help you. A user will be added to the list. Click User's name to manage user's permissions. Tick appropriate permissions. For more details see 13. Permissions .","title":"Edit\u00a0detached configuration permissions"},{"location":"manual/07_Manage_Detached_configuration/7.1._Create_and_customize_Detached_configuration/#add-run-configuration-to-detached-configuration","text":"Select Detached configuration in the Library . Add new Run configuration via + ADD button. Enter the name, description of the new Run configuration . If the Detached configuration already has more than one Run configuration , select the template for the new one. The new configuration will be based on the template . Click Create . New Run Configuration will be represented at Detached configuration details pane.","title":"Add\u00a0run configuration\u00a0to detached configuration"},{"location":"manual/07_Manage_Detached_configuration/7.1._Create_and_customize_Detached_configuration/#edit-run-configuration-in-the-detached-configuration","text":"Select Detached configuration in the Library . Go to the tab with the Run configuration that you want to change. Change parameters of the Run configuration. Click Save .","title":"Edit run configuration in the Detached configuration"},{"location":"manual/07_Manage_Detached_configuration/7.1._Create_and_customize_Detached_configuration/#set-a-root-entity-and-map-configuration-parameters","text":"If your configuration stored in a folder with a Project type, when, to set a \"Root entity\" field, you shall add metadata to your project. After that, you'll be able to select metadata entity type from the drop-down list. Note : learn how to create a project here and about managing metadata here . Click \" Root entity \" combo-box. Choose the object from default values: Participants, Samples, Pairs, Sets of Participants, Sets of Samples, Sets of Pairs. When you select \" Root entity \", you'll be able to map configuration parameters to the root entity metadata attributes. You can set it using expansion expressions. Click an empty parameter value field. Enter \" this. \". \" this \" means that value is attributed to the selected Root entity type . In the drop-down list select the metadata value.","title":"Set a root entity and map configuration parameters"},{"location":"manual/07_Manage_Detached_configuration/7.1._Create_and_customize_Detached_configuration/#delete-run-configuration-from-the-detached-configuration","text":"Select Detached configuration in the Library . Go to the tab with the Run configuration that you want to delete. Click Remove . Confirm removal.","title":"Delete\u00a0run configuration\u00a0from the Detached configuration"},{"location":"manual/07_Manage_Detached_configuration/7.2._Launch_Detached_Configuration/","text":"7.2. Launch Detached Configuration Launch detached cluster configuration as a cluster Schedule a launch from the detached configuration Create and configure a schedule rule Delete a schedule rule To launch a Detached configuration you need to have EXECUTE permissions for it. For more information see 13. Permissions . Detached configuration represents a configuration for running instances as a cluster. For example, when you need different instances running at one task: master machine and one or several worker machines are configured. They may or may not use one docker image and run different scripts. There are different options to run the cluster in Cloud Pipeline : To use Configure cluster button at Launch pipeline page or at Detached configuration page. Click the button, configure cluster - you will be offered to start current configuration at several machines (\"working nodes\"). I.e. there will start several identically configured machines having network file system within each. It is impossible to run all configurations for the pipeline at once as a cluster. For more details see 6. Manage Pipeline . To use Detached configuration . Cluster configuration is a set of configurations for nodes that may or may not start some pipeline. Configurations in Detached configuration page are typically different. From Detached configuration page, you can launch all configurations at once as a cluster or launch configurations one by one as via Launch pipeline tab. Launch detached cluster configuration as a cluster Navigate to Detached Configuration details page. Click Run \u2192 Run cluster . All the Run configurations of the Detached configuration will start execution. Note : Select Run selected to launch only the opened Run configuration. Please note, that the current user can launch a detached configuration only if he/his group has corresponding permissions on that configuration (for more information see 13. Permissions ), but the Run button may be disabled also for one of the following reasons: execution isn't allowed for specified docker image; read operations aren't allowed for specified input or common path parameters; write operations aren't allowed for specified output path parameters. In such cases, hover over the Run button to view warning notification with a reason of a run forbiddance, e.g.: In case of launching Root entity configuration , a pop-up window emerges. Select an appropriate metadata in correspondence with root entity . If root entity is an attribute of the selected metadata, use expansion expression in the Define expression field . Click OK . Schedule a launch from the detached configuration In some cases, the ability to configure a schedule for detached configuration running is beneficial. User is able to set a schedule for launch a run from the detached configuration: Schedule is defined as a list of rules (user is able to specify any number of them) For each rule in the list user is able to set the recurrence: Daily : every N days, time or Weekly : every weekday(s) , time Conflicting rules are not allowed (i.e. rules that are configured on the same launching time) If any schedule rule is configured for the detached configuration - a corresponding job (a pipeline (if specified) or a plain container) will be started accordingly in the scheduled day and time Configuration will be run from the user who created or updated the corresponding schedule If detached configuration has several entries (configs) only default one will be launched by the schedule Create and configure a schedule rule Open the detached configuration Click the Run schedule button in the right upper corner: The \"Run schedule\" popup will appear: Click the Add rule button. The first rule in the list will appear: Let's configure the launch of the detached configuration every 3 days at 12:30. For that: click the field next to \"Every\" label and specify the corresponding number in it click the time field, select hours in the left dropdown list and minutes in the right Let's add another rule and configure the launch of the detached configuration every monday and wednesday at 15:00. For that: click the Add rule button for the appeared new rule select the \"Weekly\" item in the recurrence dropdown list click the field next to the recurrence dropdown and select the corresponding weekdays click the time field, select the corresponding time values Click the OK button to save created rules: From now, the run from that detached configuration will be automatically launching according to the created schedule rules. Delete a schedule rule Click the Run schedule button in the right upper corner of the detached configuration: The \"Run schedule\" popup will appear. Click the Delete button in the row of the rule you want to remove, e.g.: The row with removing rule will become pink: Note : you may easily revert the removal by button Click the OK button to save performed changes and permanently remove the rule","title":"7.2. Launch Detached configuration"},{"location":"manual/07_Manage_Detached_configuration/7.2._Launch_Detached_Configuration/#72-launch-detached-configuration","text":"Launch detached cluster configuration as a cluster Schedule a launch from the detached configuration Create and configure a schedule rule Delete a schedule rule To launch a Detached configuration you need to have EXECUTE permissions for it. For more information see 13. Permissions . Detached configuration represents a configuration for running instances as a cluster. For example, when you need different instances running at one task: master machine and one or several worker machines are configured. They may or may not use one docker image and run different scripts. There are different options to run the cluster in Cloud Pipeline : To use Configure cluster button at Launch pipeline page or at Detached configuration page. Click the button, configure cluster - you will be offered to start current configuration at several machines (\"working nodes\"). I.e. there will start several identically configured machines having network file system within each. It is impossible to run all configurations for the pipeline at once as a cluster. For more details see 6. Manage Pipeline . To use Detached configuration . Cluster configuration is a set of configurations for nodes that may or may not start some pipeline. Configurations in Detached configuration page are typically different. From Detached configuration page, you can launch all configurations at once as a cluster or launch configurations one by one as via Launch pipeline tab.","title":"7.2. Launch Detached Configuration"},{"location":"manual/07_Manage_Detached_configuration/7.2._Launch_Detached_Configuration/#launch-detached-cluster-configuration-as-a-cluster","text":"Navigate to Detached Configuration details page. Click Run \u2192 Run cluster . All the Run configurations of the Detached configuration will start execution. Note : Select Run selected to launch only the opened Run configuration. Please note, that the current user can launch a detached configuration only if he/his group has corresponding permissions on that configuration (for more information see 13. Permissions ), but the Run button may be disabled also for one of the following reasons: execution isn't allowed for specified docker image; read operations aren't allowed for specified input or common path parameters; write operations aren't allowed for specified output path parameters. In such cases, hover over the Run button to view warning notification with a reason of a run forbiddance, e.g.: In case of launching Root entity configuration , a pop-up window emerges. Select an appropriate metadata in correspondence with root entity . If root entity is an attribute of the selected metadata, use expansion expression in the Define expression field . Click OK .","title":"Launch detached\u00a0cluster\u00a0configuration as a cluster"},{"location":"manual/07_Manage_Detached_configuration/7.2._Launch_Detached_Configuration/#schedule-a-launch-from-the-detached-configuration","text":"In some cases, the ability to configure a schedule for detached configuration running is beneficial. User is able to set a schedule for launch a run from the detached configuration: Schedule is defined as a list of rules (user is able to specify any number of them) For each rule in the list user is able to set the recurrence: Daily : every N days, time or Weekly : every weekday(s) , time Conflicting rules are not allowed (i.e. rules that are configured on the same launching time) If any schedule rule is configured for the detached configuration - a corresponding job (a pipeline (if specified) or a plain container) will be started accordingly in the scheduled day and time Configuration will be run from the user who created or updated the corresponding schedule If detached configuration has several entries (configs) only default one will be launched by the schedule","title":"Schedule a launch from the detached configuration"},{"location":"manual/07_Manage_Detached_configuration/7.2._Launch_Detached_Configuration/#create-and-configure-a-schedule-rule","text":"Open the detached configuration Click the Run schedule button in the right upper corner: The \"Run schedule\" popup will appear: Click the Add rule button. The first rule in the list will appear: Let's configure the launch of the detached configuration every 3 days at 12:30. For that: click the field next to \"Every\" label and specify the corresponding number in it click the time field, select hours in the left dropdown list and minutes in the right Let's add another rule and configure the launch of the detached configuration every monday and wednesday at 15:00. For that: click the Add rule button for the appeared new rule select the \"Weekly\" item in the recurrence dropdown list click the field next to the recurrence dropdown and select the corresponding weekdays click the time field, select the corresponding time values Click the OK button to save created rules: From now, the run from that detached configuration will be automatically launching according to the created schedule rules.","title":"Create and configure a schedule rule"},{"location":"manual/07_Manage_Detached_configuration/7.2._Launch_Detached_Configuration/#delete-a-schedule-rule","text":"Click the Run schedule button in the right upper corner of the detached configuration: The \"Run schedule\" popup will appear. Click the Delete button in the row of the rule you want to remove, e.g.: The row with removing rule will become pink: Note : you may easily revert the removal by button Click the OK button to save performed changes and permanently remove the rule","title":"Delete a schedule rule"},{"location":"manual/07_Manage_Detached_configuration/7.3._Expansion_Expressions/","text":"7.3. Expansion Expressions As the Root entity has a number of attributes and the entity is connected with other entities and their attributes, we need a special way to define them as parameters of configuration or be able to launch one entity metadata under a configuration with a different type of root entity. For this purpose we use expansion expressions : \"this. ...\" - \"this\" keyword references a specific instance of an entity. It shall be followed by an attribute of an instance. Additionally, set of attributes of corresponding entities can be chained (see examples below). \"this.attribute\" - \"this.fastq\" \"this.attribute.attribute. ...\" - chaining allows creating more complex pipeline runs. In this case, each \"attribute\" keyword will be expanded and used as an input for the next \"attribute\". For example, \" this.control_sample.r1_fastq \". \"this.attribute\" construction is also useful at Root entity configuration launching step when a user selects metadata: Example 1 . The root entity for the configuration is \" Samples \", but user selects a \" Pair \" in the pop-up window. The \" Pair \" has a link to two samples as attributes . A user here should define the desired attribute ( sample ) to launch pipeline with the specified root entity. For example, type \" this.control_sample \" in Define expression field. Example 2 . The root entity is \" Samples \", but user selects \" Sample Set \" and wants to run the analysis for all the samples in the set . A user here should type the following expression \" this.Samples \" in Define expression field.","title":"7.3. Expansion expressions"},{"location":"manual/07_Manage_Detached_configuration/7.3._Expansion_Expressions/#73-expansion-expressions","text":"As the Root entity has a number of attributes and the entity is connected with other entities and their attributes, we need a special way to define them as parameters of configuration or be able to launch one entity metadata under a configuration with a different type of root entity. For this purpose we use expansion expressions : \"this. ...\" - \"this\" keyword references a specific instance of an entity. It shall be followed by an attribute of an instance. Additionally, set of attributes of corresponding entities can be chained (see examples below). \"this.attribute\" - \"this.fastq\" \"this.attribute.attribute. ...\" - chaining allows creating more complex pipeline runs. In this case, each \"attribute\" keyword will be expanded and used as an input for the next \"attribute\". For example, \" this.control_sample.r1_fastq \". \"this.attribute\" construction is also useful at Root entity configuration launching step when a user selects metadata: Example 1 . The root entity for the configuration is \" Samples \", but user selects a \" Pair \" in the pop-up window. The \" Pair \" has a link to two samples as attributes . A user here should define the desired attribute ( sample ) to launch pipeline with the specified root entity. For example, type \" this.control_sample \" in Define expression field. Example 2 . The root entity is \" Samples \", but user selects \" Sample Set \" and wants to run the analysis for all the samples in the set . A user here should type the following expression \" this.Samples \" in Define expression field.","title":"7.3. Expansion Expressions"},{"location":"manual/07_Manage_Detached_configuration/7.4._Remove_Detached_configuration/","text":"7.4. Remove Detached configuration To delete Detached configuration you need to have WRITE permission for that configuration and the ROLE_CONFIGURATION_MANAGER . For more information see 13. Permissions . You can remove the Detached configuration . Navigate to the Folder where Cluster configuration is stored. Click icon in the detached configuration line to delete. Click Delete . Confirm the action.","title":"7.4. Remove Detached configuration"},{"location":"manual/07_Manage_Detached_configuration/7.4._Remove_Detached_configuration/#74-remove-detached-configuration","text":"To delete Detached configuration you need to have WRITE permission for that configuration and the ROLE_CONFIGURATION_MANAGER . For more information see 13. Permissions . You can remove the Detached configuration . Navigate to the Folder where Cluster configuration is stored. Click icon in the detached configuration line to delete. Click Delete . Confirm the action.","title":"7.4. Remove Detached configuration"},{"location":"manual/07_Manage_Detached_configuration/7._Manage_Detached_configuration/","text":"7. Manage Detached configuration \"Details\" view pane Controls Run schedule \"Gear\" icon Add Run Remove Save Detached configuration is a run configuration or a set of run configurations that allows running tools and pipelines. Note : In comparison with pipeline configurations, detached configurations do not require a pipeline. \"Details\" view pane At the \" Details view \" pane you can find: Section Control Description Name Detach configuration (a) and its Run configuration (b) names Estimated price per hour Control shows machine hours prices. If you navigate mouse to \" info \" icon (c) , you'll see the maximum, minimum and average price for a particular pipeline version run as well as the price per hour. Exec environment This section lists execution environment parameters. Pipeline (d) A name of the pipeline to be executed (optional). Click on the field to select a pipeline in the pop-up. Execution environment (e) An environment platform for execution the pipeline. Click for select from the list. Docker image (f) A name of a Docker image to use for a pipeline execution (e.g. \"base-generic-centos7\"). Click on the field to select an image in the pop-up. Node type (g) An instance type in terms of Cloud Provider with specifying amounts of CPU, RAM and GPU (e.g. 4 CPU cores, 16 Gb RAM and 0 GPU cores). Disk (h) Size of a disk, that will be attached to the instance in Gb. Configure cluster button (i) By clicking on this button you can configure cluster or auto-scaled cluster. Cluster is a collection of instances which are connected so that they can be used together on a task. See here and here for more information. Cloud Region (j) A specific region for a compute node placement. Please note, if a non-default region is selected - certain CP features may be unavailable: FS mounts usage from the another region (e.g. \" EU West \" region cannot use FS mounts from the \" US East \"). Regular storages will be still available If a specific tool, used for a run, requires an on-premise license server (e.g. monolix, matlab, schrodinger, etc.) - such instances shall be run in a region, that hosts those license servers. Note : if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - in that control Cloud Provider auxiliary icons also will be displayed, e.g.: For a single-Provider deployments only Cloud Region icons are displayed. Total resources (q) Information about total resources that will be used for running pipeline with specified parameters (depends on node type and cluster configuration). See here for more details. Advanced Price type (k) Choose spot or on-demand type of instance. The \"Info\" icon can give you additional information, which helps you to make choice. Timeout (min) (l) After this time pipeline will shut down (optional). Before the shut down, all the contents of the $ANALYSIS_DIR directory will be copied to output storages. Limit mounts (m) Restricts available storages for the tools or pipelines. See here . Cmd template (n) A shell command that will be executed on the running node. \"Start idle\" The flag sets Cmd template to \"sleep infinity\". For more information about starting a job in this mode refer to 15. Interactive services . Parameters This section lists pipeline specific parameters that can be used during a pipeline run. Pipeline parameters can be assigned the following types (p) : String - generic scalar value (e.g. Sample name). Boolean - boolean value. Path - path in a data storage hierarchy. Input - path in a data storage hierarchy. During pipeline initialization, this path will be used to get data from a storage. Output - path in a data storage hierarchy. During pipeline finalization, this path will be used to upload resulting data to a storage. Common - path in a data storage hierarchy. Similar to \"Input\" type, but this data will not be erased from a calculation node, when a pipeline is finished (this is useful for reference data, as it can be reused by further pipeline runs that share the same reference). Note : You can use Project attribute values as parameters for the Run : Click an empty parameter value field. Enter \"project.\" In the drop-down list select the Project attribute value. Add parameter (o) This control helps to add an additional parameter to a configuration. Root entity type Note : This parameter is only available for configurations that are stored in Project type of the Folder and the Project has to store metadata object(s) within. See 7.1. Create and customize Detached configuration . It defines an entity which metadata will be used to process data. Default values: Participants, Samples, Pairs, Sets of Participants, Sets of Samples, Sets of Pairs. Controls There are buttons at the top of the \"Details\" view: Run schedule Allows creating of schedule rules to launch runs from the default configuration in the scheduled day and time in automatic regimen (a) . See 7.2. Launch Detached Configuration . \"Gear\" icon Allows changing a name, description of the configuration and permissions for it (b) . See 7.1. Create and customize Detached configuration . Add Allows adding machine configuration (c) . See 7.1. Create and customize Detached configuration . Run Allows launching one machine or all machines as a cluster (d) . See 7.2. Launch Detached Configuration . Remove Allows removing machine configuration (e) . See 7.4. Remove Detached configuration . Save Allow saving machine configuration (f) . See 7.1. Create and customize Detached configuration .","title":"7.0. Overview"},{"location":"manual/07_Manage_Detached_configuration/7._Manage_Detached_configuration/#7-manage-detached-configuration","text":"\"Details\" view pane Controls Run schedule \"Gear\" icon Add Run Remove Save Detached configuration is a run configuration or a set of run configurations that allows running tools and pipelines. Note : In comparison with pipeline configurations, detached configurations do not require a pipeline.","title":"7. Manage Detached configuration"},{"location":"manual/07_Manage_Detached_configuration/7._Manage_Detached_configuration/#details-view-pane","text":"At the \" Details view \" pane you can find: Section Control Description Name Detach configuration (a) and its Run configuration (b) names Estimated price per hour Control shows machine hours prices. If you navigate mouse to \" info \" icon (c) , you'll see the maximum, minimum and average price for a particular pipeline version run as well as the price per hour. Exec environment This section lists execution environment parameters. Pipeline (d) A name of the pipeline to be executed (optional). Click on the field to select a pipeline in the pop-up. Execution environment (e) An environment platform for execution the pipeline. Click for select from the list. Docker image (f) A name of a Docker image to use for a pipeline execution (e.g. \"base-generic-centos7\"). Click on the field to select an image in the pop-up. Node type (g) An instance type in terms of Cloud Provider with specifying amounts of CPU, RAM and GPU (e.g. 4 CPU cores, 16 Gb RAM and 0 GPU cores). Disk (h) Size of a disk, that will be attached to the instance in Gb. Configure cluster button (i) By clicking on this button you can configure cluster or auto-scaled cluster. Cluster is a collection of instances which are connected so that they can be used together on a task. See here and here for more information. Cloud Region (j) A specific region for a compute node placement. Please note, if a non-default region is selected - certain CP features may be unavailable: FS mounts usage from the another region (e.g. \" EU West \" region cannot use FS mounts from the \" US East \"). Regular storages will be still available If a specific tool, used for a run, requires an on-premise license server (e.g. monolix, matlab, schrodinger, etc.) - such instances shall be run in a region, that hosts those license servers. Note : if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - in that control Cloud Provider auxiliary icons also will be displayed, e.g.: For a single-Provider deployments only Cloud Region icons are displayed. Total resources (q) Information about total resources that will be used for running pipeline with specified parameters (depends on node type and cluster configuration). See here for more details. Advanced Price type (k) Choose spot or on-demand type of instance. The \"Info\" icon can give you additional information, which helps you to make choice. Timeout (min) (l) After this time pipeline will shut down (optional). Before the shut down, all the contents of the $ANALYSIS_DIR directory will be copied to output storages. Limit mounts (m) Restricts available storages for the tools or pipelines. See here . Cmd template (n) A shell command that will be executed on the running node. \"Start idle\" The flag sets Cmd template to \"sleep infinity\". For more information about starting a job in this mode refer to 15. Interactive services . Parameters This section lists pipeline specific parameters that can be used during a pipeline run. Pipeline parameters can be assigned the following types (p) : String - generic scalar value (e.g. Sample name). Boolean - boolean value. Path - path in a data storage hierarchy. Input - path in a data storage hierarchy. During pipeline initialization, this path will be used to get data from a storage. Output - path in a data storage hierarchy. During pipeline finalization, this path will be used to upload resulting data to a storage. Common - path in a data storage hierarchy. Similar to \"Input\" type, but this data will not be erased from a calculation node, when a pipeline is finished (this is useful for reference data, as it can be reused by further pipeline runs that share the same reference). Note : You can use Project attribute values as parameters for the Run : Click an empty parameter value field. Enter \"project.\" In the drop-down list select the Project attribute value. Add parameter (o) This control helps to add an additional parameter to a configuration. Root entity type Note : This parameter is only available for configurations that are stored in Project type of the Folder and the Project has to store metadata object(s) within. See 7.1. Create and customize Detached configuration . It defines an entity which metadata will be used to process data. Default values: Participants, Samples, Pairs, Sets of Participants, Sets of Samples, Sets of Pairs.","title":"\"Details\" view pane"},{"location":"manual/07_Manage_Detached_configuration/7._Manage_Detached_configuration/#controls","text":"There are buttons at the top of the \"Details\" view:","title":"Controls"},{"location":"manual/07_Manage_Detached_configuration/7._Manage_Detached_configuration/#run-schedule","text":"Allows creating of schedule rules to launch runs from the default configuration in the scheduled day and time in automatic regimen (a) . See 7.2. Launch Detached Configuration .","title":"Run schedule"},{"location":"manual/07_Manage_Detached_configuration/7._Manage_Detached_configuration/#gear-icon","text":"Allows changing a name, description of the configuration and permissions for it (b) . See 7.1. Create and customize Detached configuration .","title":"\"Gear\" icon"},{"location":"manual/07_Manage_Detached_configuration/7._Manage_Detached_configuration/#add","text":"Allows adding machine configuration (c) . See 7.1. Create and customize Detached configuration .","title":"Add"},{"location":"manual/07_Manage_Detached_configuration/7._Manage_Detached_configuration/#run","text":"Allows launching one machine or all machines as a cluster (d) . See 7.2. Launch Detached Configuration .","title":"Run"},{"location":"manual/07_Manage_Detached_configuration/7._Manage_Detached_configuration/#remove","text":"Allows removing machine configuration (e) . See 7.4. Remove Detached configuration .","title":"Remove"},{"location":"manual/07_Manage_Detached_configuration/7._Manage_Detached_configuration/#save","text":"Allow saving machine configuration (f) . See 7.1. Create and customize Detached configuration .","title":"Save"},{"location":"manual/08_Manage_Data_Storage/8.1._Create_and_edit_storage/","text":"8.1. Create and edit storage Create object storage Edit storage To create a Storage in a Folder you need to have WRITE permission for that folder and the ROLE_STORAGE_MANAGER role. For more information see 13. Permissions . You also can create Storage via CLI . See 14.3. Manage Storage via CLI . Create object storage Navigate to the folder where you want to create data storage. Click + Create \u2192 Storage \u2192 Create new object storage . Note : choose Add existing object storage to use an already existing cloud storage for this data storage. Note : how to create FS mount see here . Fill in the \" Info \" form: Storage path - path to access the storage (cloud storage name). If on Data storage tab in Preferences section of system-level settings storage.object.prefix (see v.0.14 - 12.10. Manage system-level settings ) is set - all new storages will be created with this prefix (e.g. \" ds \"): Alias - object storage name (if not specified, it is set equal to the Storage path ). Cloud region - location region of a data storage. This select allows to decrease time of data movement for huge data volumes by choosing the nearest region. Please note, if a non-default region is selected - certain CP features may be unavailable: FS mounts usage from the another region (e.g. \" EU West \" region cannot use FS mounts from the \" US East \"). Regular storages will be still available. If a specific tool, used for a run, requires an on-premise license server (e.g. monolix, matlab, schrodinger, etc.) - such instances shall be run in a region, that hosts those license servers. Also note, if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - in that case auxiliary Cloud Provider icons are additionally displayed, e.g.: Description - description of the data storage and comments. STS duration - short-term storage support (in days). Allows to set duration of data store period in a \"usual\" object storage. After this period, the data will be transferred to the long-term storage. LTS duration - long-term storage support (in days). Allows to set duration of the data store period in a long-term storage (e.g. S3 Glacier for AWS ), where the data is transferred after the STS duration expiration. After LTS period expiration, the data will be removed permanently. Long-term storage is a secure, durable, and low-cost storage compared to a \"usual\" object storage, but using of such storage intends not so fast and frequently access to the data. So, long-term storages are ideal for archives where data is regularly retrieved and some of the data may be needed in minutes. Enable versioning box - allows to enable versioning for the storage files. Backup duration - how long backup is stored (days). After this period expiration all backup-versions \"older\" than specified period will be removed. This setting is available only if versioning is enabled. Note : If you want to store data permanently, leave fields empty. Mount-point - specific mount-point. Mount options - specific mount options. Enable sharing box - allows to share storage content with other users (see here ). Click \"Create\" button. Edit storage You also can edit Storage via CLI . See 14.3. Manage Storage via CLI . You may change Alias , Description , STS and LTS duration , Mount-point and Mount options . Example: Select storage. Click icon. Change number of days in STS and LTS duration fields. If you want to store data permanently, leave fields empty. Click \"Save\" button.","title":"8.1. Create and edit Data Storage"},{"location":"manual/08_Manage_Data_Storage/8.1._Create_and_edit_storage/#81-create-and-edit-storage","text":"Create object storage Edit storage To create a Storage in a Folder you need to have WRITE permission for that folder and the ROLE_STORAGE_MANAGER role. For more information see 13. Permissions . You also can create Storage via CLI . See 14.3. Manage Storage via CLI .","title":"8.1. Create and edit storage"},{"location":"manual/08_Manage_Data_Storage/8.1._Create_and_edit_storage/#create-object-storage","text":"Navigate to the folder where you want to create data storage. Click + Create \u2192 Storage \u2192 Create new object storage . Note : choose Add existing object storage to use an already existing cloud storage for this data storage. Note : how to create FS mount see here . Fill in the \" Info \" form: Storage path - path to access the storage (cloud storage name). If on Data storage tab in Preferences section of system-level settings storage.object.prefix (see v.0.14 - 12.10. Manage system-level settings ) is set - all new storages will be created with this prefix (e.g. \" ds \"): Alias - object storage name (if not specified, it is set equal to the Storage path ). Cloud region - location region of a data storage. This select allows to decrease time of data movement for huge data volumes by choosing the nearest region. Please note, if a non-default region is selected - certain CP features may be unavailable: FS mounts usage from the another region (e.g. \" EU West \" region cannot use FS mounts from the \" US East \"). Regular storages will be still available. If a specific tool, used for a run, requires an on-premise license server (e.g. monolix, matlab, schrodinger, etc.) - such instances shall be run in a region, that hosts those license servers. Also note, if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - in that case auxiliary Cloud Provider icons are additionally displayed, e.g.: Description - description of the data storage and comments. STS duration - short-term storage support (in days). Allows to set duration of data store period in a \"usual\" object storage. After this period, the data will be transferred to the long-term storage. LTS duration - long-term storage support (in days). Allows to set duration of the data store period in a long-term storage (e.g. S3 Glacier for AWS ), where the data is transferred after the STS duration expiration. After LTS period expiration, the data will be removed permanently. Long-term storage is a secure, durable, and low-cost storage compared to a \"usual\" object storage, but using of such storage intends not so fast and frequently access to the data. So, long-term storages are ideal for archives where data is regularly retrieved and some of the data may be needed in minutes. Enable versioning box - allows to enable versioning for the storage files. Backup duration - how long backup is stored (days). After this period expiration all backup-versions \"older\" than specified period will be removed. This setting is available only if versioning is enabled. Note : If you want to store data permanently, leave fields empty. Mount-point - specific mount-point. Mount options - specific mount options. Enable sharing box - allows to share storage content with other users (see here ). Click \"Create\" button.","title":"Create object storage"},{"location":"manual/08_Manage_Data_Storage/8.1._Create_and_edit_storage/#edit-storage","text":"You also can edit Storage via CLI . See 14.3. Manage Storage via CLI . You may change Alias , Description , STS and LTS duration , Mount-point and Mount options . Example: Select storage. Click icon. Change number of days in STS and LTS duration fields. If you want to store data permanently, leave fields empty. Click \"Save\" button.","title":"Edit storage"},{"location":"manual/08_Manage_Data_Storage/8.10._Storage_lifecycle/","text":"8.10. Storage lifecycle management Note : currently, described functionality may be supported not by all Cloud Providers Archiving Create transition rule Edit transition rule View rules and logs Rule logs View in attributes Archived files Listing via CLI View archive size Restoring Restore file Restore folder Users have the ability to manage the lifecycle of data in storages - configure the automatic data transition from standard storage to different types of archival storages by occurrence of a certain event and restore that data back as well if needed. That includes abilities: to create and edit multiple transition rules for a storage data based on file prefix and/or glob pattern to select an archive class as the data destination for each specific rule. Note : archive classes depend on the Cloud Provider to select the \"deletion\" operation as one of the possible data destinations - for the automated data removing to define the event by which the data shall be transferred to configure notifications: notifications on each lifecycle event ability to delay data transition using link from notification email to restore previously archived files and folders Archiving Note : only storage OWNER or users with the ROLE_STORAGE_ARCHIVE_MANAGER or ROLE_ADMIN role are able to manage the archival rules Data archiving in any storage is defined by the set of transition rules. For a storage, any count of transition rules can be created. Each rule includes: source path inside the storage where data for the transition will be searched glob that defines which specific data shall be transferred event that defines when data shall be transferred notification settings Once the rule is created and saved - it automatically becomes enabled for the storage. All files matching the rule conditions will be transferred to corresponding archives according to configured dates. Create transition rule To create a new transition rule: Open a storage Click icon to open storage settings Click the \" Transition rules \" tab: The tab with list of existing rules of the current storage will appear: Click the \" + Create \" button The pop-up will be opened: Here, the following fields shall be filled in: Root path - it is the path inside the storage, from which the search for files to transition (corresponding to the specified glob pattern) will be performed. Text and glob patterns are supported . Examples: / , /some_folder/sub_folder/ , /some_folder/* Glob - glob pattern for the file(s) to transfer. Pattern defines files that will be transferred by the current rule. Pattern will be applied in the storage path specified as root. Examples: *.csv - any CSV file in the current root path, /**/*.csv - any CSV file in any subfolder of the root path. Method - method that defines the transition process. There are 3 possible variants: One by one (default) - each file matches the glob pattern will be transferred separately. For each such file separate notification will be sent (if notifications are enabled) By the earliest file - all files match the glob pattern will be transferred simultaneously - when the earliest file from them corresponds to all conditions of the transition rule By the latest file - all files match the glob pattern will be transferred simultaneously - when the latest file from them corresponds to all conditions of the transition rule Condition - additional condition that shall be met to initiate the transition procedure. There are 2 possible variants: Default - no additional conditions are required Files matches - if this condition is selected, a new field appears - to specify additional glob pattern. In this case, transition of the files that match main glob pattern can be performed only after in the root path the files will appear that match this additional glob pattern. Example - rule for the transition of 123.csv file in case when 123.tsv file appears: Next, the Transitions section shall be filled in. Here, you can specify destination(s) where exactly files shall be transferred and in which dates: Destination - destination where files matches the glob pattern will be transferred. Possible variants are depend on the Cloud Provider. For example, for AWS there are: S3 Glacier Instant Retrieval , S3 Glacier Flexible Retrieval , S3 Glacier Deep Archive . Additionally, there is the Deletion destination - to remove files once the transition event has come: Date - date when files match the glob pattern will be transferred. There are two variants (only one can be selected for the rule): the count of days - days count from the file creation after which the transition will be performed. For example, if in this field the count 30 is specified - so files match the glob pattern in the root path will be transferred to S3 Glacier Instant Retrieval after 30 days after their appearance in the current storage: specific date - files match the glob pattern in the root path will be transferred exactly in that date: By default, at least one item shall be in the Transitions section. You can add several destinations here (up to total count of available destinations) - via the \" + Add \" button. For example: To remove the destination item from the list - click the Delete icon in the corresponding row Next, the Notify section shall be filled in. Note : notifications are defaultly disabled in case when One by one transition method is selected. To configure notifications you shall select any other method. Here you can configure: disable all notifications for the current rule by the corresponding checkbox, if needed Recipients - list of recipients (users and groups/roles) who shall receive notifications Notice period - period in days before the transition (according to specified date in the Transitions section) - when the notification about further transition will be sent. Note : if it is not specified - the default value will be used from the System Preference storage.lifecycle.notify.before.days during the rule performing Prolongation period - period in days for which the transition can be delayed - it may happen in case when user clicks the Prolongate button from the email notification. This prolongate period will be applied explicitly to files for which notification is received. After prolongation period is over, a new notification will be sent. Prolongations can be performed any number of times. Note : if Prolongation period is not specified - the default value will be used from the System Preference storage.lifecycle.prolong.days during the rule performing notification template. By default, the template from DATASTORAGE_LIFECYCLE_ACTION email notification is used (this behavior is defined by the corresponding checkbox). If you want to specify custom notification template for a rule - untick the checkbox Use default template - fields Subject and Notification will appear, where changes can be made: You have the ability to view/edit these fields and also preview - how the notification will be displayed in the email: Note : if the notification subject/body will be changed via this section - this will impact only to the ending view of the current rule notifications and will not change the default DATASTORAGE_LIFECYCLE_ACTION template After all fields of the rule are filled in - click the Save button to proceed: Just-created rule will appear in the list: Edit transition rule To edit an existing transition rule: Open a storage Click icon to open storage settings Click the \" Transition rules \" tab: The tab with list of existing rules of the current storage will appear: Click icon to open the desired rule, e.g.: The pop-up with rule details will be opened: Here, you can edit any details except Root path and Glob . Click the Save button to confirm changes. View rules and logs To view rules configured for the storage - open its settings and switch to the \" Transition rules \" tab: Note : if the rule has several transition destinations - they are shown above the special label within a tooltip, e.g.: Rule events logs To view all events performed according to the current rule - click the View events logs icon at the rule row. The form with list of events of the selected rule will appear: That form contains: info fields: Root - root path of the current rule Glob - glob of the current rule Action type dropdown list - to filter logs table by performed action ( transition / deletion / prolongation ). Empty value in this field means that logs table is not filtered logs table with columns: Date - datetime of the event Action - type of the performed action User - in case of system actions, there will be the stub System . In case of prolongation event, there will be a user name who has prolonged transition for a file(s) Path - path to the folder that contains files with which the action was performed Destination - transition destination Prolongation - period in days for which transition/deletion was delayed ( Note : applicable only for prolonged files) Renewed transition - a new date when transition/deletion will take place ( Note : applicable only for prolonged files) View rules in attributes If for a storage transition rules are configured - you can view that in the Attributes panel: Open the desired storage Click the \" Show attributes \" button in the right upper corner In the Attributes panel, total count of configured rules will be shown, e.g.: Note : you can check the count of rules configured for a specific folder - by similar way as described, you should open that folder and the Attributes panel for it. Archived files If file was transferred to any destination (except Deletion ) by some rule: this file becomes disabled for changing/renaming from the GUI/CLI content of this file can not be viewed from the GUI/CLI at the GUI, near such file a label appears that corresponds to the transition destination Note : regular files in storages has STANDARD label. Example for AWS Cloud Provider : Please note, archived files are not visible for general users: only storage OWNER or users with the ROLE_STORAGE_ARCHIVE_READER or ROLE_ADMIN role are able to view the archived files by default, archived files are hidden. To show them - the checkbox Show archived files shall be enabled: Listing via CLI For details, how to manage storages via the CLI see the corresponding section Manage Storages . As mentioned above, archived files are disabled for changing/renaming/viewing/copying/moving from the CLI. By default, archived files are invisible for users that perform any pipe storage commands (including mount abilities). For example, storage with archived files via the GUI: The same storage via the CLI (using pipe storage ls command): The attempt to copy one of the archived file (using pipe storage cp command): But if users want to browse archived files via the CLI, the special option --show-archive can be used. This flag allows to list archived files in storages. Note : flag allows only the listing of archived files, still not changing/renaming/copying/moving. The same storage as in the example above but with --show-archive flag via the CLI (using pipe storage ls command): View archive size User can separately view the common info about whole size of the archived files. To view that info, open the attributes panel of a storage, e.g.: Here, the following info is shown: the whole summary size of all archived files - it is based on the sum of sizes of current and previous versions of files (for all archive types) the whole summary size of only previous versions of archived files in the storage (for all archive types) - it is shown in the parentheses To view details, click the corresponding hyperlink: Details info pop-up will be opened, e.g.: Here, the following info about archived files size can be found: Storage class - archive type. The type is presented in the table only if there are/were any versions of files of this archive type Current ver. - the whole summary size of all current versions of archived files Previous ver. - the whole summary size of all previous versions of archived files Total - sum of the current and previous files versions Note : info about the archive size can be viewed via the CLI pipe storage du command as well, e.g.: In the CLI, only total summary size of current and previous files versions is shown. To view details about archive types, use -o f option ( --output-mode full ): For more details see in the pipe storage du section. Restoring Note : data restoring in the storage is available for admins and storage OWNER only If there are files in the storage folder that were previously transferred to any destination (except Deletion ) according to some transition rule - user has the ability to restore such files. That available: for separate file(s) for whole folder Restore file To restore a file: Open a storage Select the desired file(s) you want to restore Click the \" Restore transferred item \" button in the management menu: Pop-up to confirm restoring will appear: This pop-up contains fields for: Recovery period - to specify the period duration for which the file will be restored Recipients - to specify recipients who will be notified about file restoring process (by email notification DATASTORAGE_LIFECYCLE_RESTORE_ACTION ). If this field was left empty - file will be restored without notifications Restore mode - to select the restore mode ( Standard or Bulk ). This affects on duration of restoring processes and their cost. Additionally, you have the ability to define - restore only the latest file version (default behavior) or all file versions - via the corresponding checkbox Restore all versions ( Note : this checkbox is available only for versioning storages) Click the Restore button to confirm If for a file the restoring is requested - the restoring status will appear at the file icon: There are possible states: RESTORING - restore operation is in progress. This status is shown after the request for the restore was submitted and until the file is restored successfully or restoring is failed RESTORE FAILED - restore operation is failed. This status is not shown at the GUI RESTORED - restore operation is completed. This status is shown after the file is restored successfully and till the date when the recovery period is finished. You may hover over any state icon and view details of restoring process in a tooltip, e.g.: Note : if file was restored for some period: this file becomes enabled for changing/renaming from the GUI/CLI content of this file can be viewed from the GUI/CLI label of the file original destination doesn't disappear after restore period is over, file becomes regular archived file Restore folder When user restores the whole folder - all previously transferred files in that folder will be restored simultaneously. Also, all transferred files in sub-folders of the folder, will be restored too - recursively. To restore a folder: Open a storage Open the desired folder you want to restore Open the Attributes panel Click the \" Restore files \" hyperlink: Pop-up to confirm restoring will appear: Options are similar as for the file restore operation Click the Restore button to confirm All files in the folder that will be restored are shown with the corresponding state: Additionally, the state of the folder restore operation is shown in the Attributes panel You may navigate to the upper level and view the state for the restoring folder (similar to restoring files): Restored files in restored folder are shown fully the same as separately restored files :","title":"8.10. Storage lifecycle"},{"location":"manual/08_Manage_Data_Storage/8.10._Storage_lifecycle/#810-storage-lifecycle-management","text":"Note : currently, described functionality may be supported not by all Cloud Providers Archiving Create transition rule Edit transition rule View rules and logs Rule logs View in attributes Archived files Listing via CLI View archive size Restoring Restore file Restore folder Users have the ability to manage the lifecycle of data in storages - configure the automatic data transition from standard storage to different types of archival storages by occurrence of a certain event and restore that data back as well if needed. That includes abilities: to create and edit multiple transition rules for a storage data based on file prefix and/or glob pattern to select an archive class as the data destination for each specific rule. Note : archive classes depend on the Cloud Provider to select the \"deletion\" operation as one of the possible data destinations - for the automated data removing to define the event by which the data shall be transferred to configure notifications: notifications on each lifecycle event ability to delay data transition using link from notification email to restore previously archived files and folders","title":"8.10. Storage lifecycle management"},{"location":"manual/08_Manage_Data_Storage/8.10._Storage_lifecycle/#archiving","text":"Note : only storage OWNER or users with the ROLE_STORAGE_ARCHIVE_MANAGER or ROLE_ADMIN role are able to manage the archival rules Data archiving in any storage is defined by the set of transition rules. For a storage, any count of transition rules can be created. Each rule includes: source path inside the storage where data for the transition will be searched glob that defines which specific data shall be transferred event that defines when data shall be transferred notification settings Once the rule is created and saved - it automatically becomes enabled for the storage. All files matching the rule conditions will be transferred to corresponding archives according to configured dates.","title":"Archiving"},{"location":"manual/08_Manage_Data_Storage/8.10._Storage_lifecycle/#create-transition-rule","text":"To create a new transition rule: Open a storage Click icon to open storage settings Click the \" Transition rules \" tab: The tab with list of existing rules of the current storage will appear: Click the \" + Create \" button The pop-up will be opened: Here, the following fields shall be filled in: Root path - it is the path inside the storage, from which the search for files to transition (corresponding to the specified glob pattern) will be performed. Text and glob patterns are supported . Examples: / , /some_folder/sub_folder/ , /some_folder/* Glob - glob pattern for the file(s) to transfer. Pattern defines files that will be transferred by the current rule. Pattern will be applied in the storage path specified as root. Examples: *.csv - any CSV file in the current root path, /**/*.csv - any CSV file in any subfolder of the root path. Method - method that defines the transition process. There are 3 possible variants: One by one (default) - each file matches the glob pattern will be transferred separately. For each such file separate notification will be sent (if notifications are enabled) By the earliest file - all files match the glob pattern will be transferred simultaneously - when the earliest file from them corresponds to all conditions of the transition rule By the latest file - all files match the glob pattern will be transferred simultaneously - when the latest file from them corresponds to all conditions of the transition rule Condition - additional condition that shall be met to initiate the transition procedure. There are 2 possible variants: Default - no additional conditions are required Files matches - if this condition is selected, a new field appears - to specify additional glob pattern. In this case, transition of the files that match main glob pattern can be performed only after in the root path the files will appear that match this additional glob pattern. Example - rule for the transition of 123.csv file in case when 123.tsv file appears: Next, the Transitions section shall be filled in. Here, you can specify destination(s) where exactly files shall be transferred and in which dates: Destination - destination where files matches the glob pattern will be transferred. Possible variants are depend on the Cloud Provider. For example, for AWS there are: S3 Glacier Instant Retrieval , S3 Glacier Flexible Retrieval , S3 Glacier Deep Archive . Additionally, there is the Deletion destination - to remove files once the transition event has come: Date - date when files match the glob pattern will be transferred. There are two variants (only one can be selected for the rule): the count of days - days count from the file creation after which the transition will be performed. For example, if in this field the count 30 is specified - so files match the glob pattern in the root path will be transferred to S3 Glacier Instant Retrieval after 30 days after their appearance in the current storage: specific date - files match the glob pattern in the root path will be transferred exactly in that date: By default, at least one item shall be in the Transitions section. You can add several destinations here (up to total count of available destinations) - via the \" + Add \" button. For example: To remove the destination item from the list - click the Delete icon in the corresponding row Next, the Notify section shall be filled in. Note : notifications are defaultly disabled in case when One by one transition method is selected. To configure notifications you shall select any other method. Here you can configure: disable all notifications for the current rule by the corresponding checkbox, if needed Recipients - list of recipients (users and groups/roles) who shall receive notifications Notice period - period in days before the transition (according to specified date in the Transitions section) - when the notification about further transition will be sent. Note : if it is not specified - the default value will be used from the System Preference storage.lifecycle.notify.before.days during the rule performing Prolongation period - period in days for which the transition can be delayed - it may happen in case when user clicks the Prolongate button from the email notification. This prolongate period will be applied explicitly to files for which notification is received. After prolongation period is over, a new notification will be sent. Prolongations can be performed any number of times. Note : if Prolongation period is not specified - the default value will be used from the System Preference storage.lifecycle.prolong.days during the rule performing notification template. By default, the template from DATASTORAGE_LIFECYCLE_ACTION email notification is used (this behavior is defined by the corresponding checkbox). If you want to specify custom notification template for a rule - untick the checkbox Use default template - fields Subject and Notification will appear, where changes can be made: You have the ability to view/edit these fields and also preview - how the notification will be displayed in the email: Note : if the notification subject/body will be changed via this section - this will impact only to the ending view of the current rule notifications and will not change the default DATASTORAGE_LIFECYCLE_ACTION template After all fields of the rule are filled in - click the Save button to proceed: Just-created rule will appear in the list:","title":"Create transition rule"},{"location":"manual/08_Manage_Data_Storage/8.10._Storage_lifecycle/#edit-transition-rule","text":"To edit an existing transition rule: Open a storage Click icon to open storage settings Click the \" Transition rules \" tab: The tab with list of existing rules of the current storage will appear: Click icon to open the desired rule, e.g.: The pop-up with rule details will be opened: Here, you can edit any details except Root path and Glob . Click the Save button to confirm changes.","title":"Edit transition rule"},{"location":"manual/08_Manage_Data_Storage/8.10._Storage_lifecycle/#view-rules-and-logs","text":"To view rules configured for the storage - open its settings and switch to the \" Transition rules \" tab: Note : if the rule has several transition destinations - they are shown above the special label within a tooltip, e.g.:","title":"View rules and logs"},{"location":"manual/08_Manage_Data_Storage/8.10._Storage_lifecycle/#rule-events-logs","text":"To view all events performed according to the current rule - click the View events logs icon at the rule row. The form with list of events of the selected rule will appear: That form contains: info fields: Root - root path of the current rule Glob - glob of the current rule Action type dropdown list - to filter logs table by performed action ( transition / deletion / prolongation ). Empty value in this field means that logs table is not filtered logs table with columns: Date - datetime of the event Action - type of the performed action User - in case of system actions, there will be the stub System . In case of prolongation event, there will be a user name who has prolonged transition for a file(s) Path - path to the folder that contains files with which the action was performed Destination - transition destination Prolongation - period in days for which transition/deletion was delayed ( Note : applicable only for prolonged files) Renewed transition - a new date when transition/deletion will take place ( Note : applicable only for prolonged files)","title":"Rule events logs"},{"location":"manual/08_Manage_Data_Storage/8.10._Storage_lifecycle/#view-rules-in-attributes","text":"If for a storage transition rules are configured - you can view that in the Attributes panel: Open the desired storage Click the \" Show attributes \" button in the right upper corner In the Attributes panel, total count of configured rules will be shown, e.g.: Note : you can check the count of rules configured for a specific folder - by similar way as described, you should open that folder and the Attributes panel for it.","title":"View rules in attributes"},{"location":"manual/08_Manage_Data_Storage/8.10._Storage_lifecycle/#archived-files","text":"If file was transferred to any destination (except Deletion ) by some rule: this file becomes disabled for changing/renaming from the GUI/CLI content of this file can not be viewed from the GUI/CLI at the GUI, near such file a label appears that corresponds to the transition destination Note : regular files in storages has STANDARD label. Example for AWS Cloud Provider : Please note, archived files are not visible for general users: only storage OWNER or users with the ROLE_STORAGE_ARCHIVE_READER or ROLE_ADMIN role are able to view the archived files by default, archived files are hidden. To show them - the checkbox Show archived files shall be enabled:","title":"Archived files"},{"location":"manual/08_Manage_Data_Storage/8.10._Storage_lifecycle/#listing-via-cli","text":"For details, how to manage storages via the CLI see the corresponding section Manage Storages . As mentioned above, archived files are disabled for changing/renaming/viewing/copying/moving from the CLI. By default, archived files are invisible for users that perform any pipe storage commands (including mount abilities). For example, storage with archived files via the GUI: The same storage via the CLI (using pipe storage ls command): The attempt to copy one of the archived file (using pipe storage cp command): But if users want to browse archived files via the CLI, the special option --show-archive can be used. This flag allows to list archived files in storages. Note : flag allows only the listing of archived files, still not changing/renaming/copying/moving. The same storage as in the example above but with --show-archive flag via the CLI (using pipe storage ls command):","title":"Listing via CLI"},{"location":"manual/08_Manage_Data_Storage/8.10._Storage_lifecycle/#view-archive-size","text":"User can separately view the common info about whole size of the archived files. To view that info, open the attributes panel of a storage, e.g.: Here, the following info is shown: the whole summary size of all archived files - it is based on the sum of sizes of current and previous versions of files (for all archive types) the whole summary size of only previous versions of archived files in the storage (for all archive types) - it is shown in the parentheses To view details, click the corresponding hyperlink: Details info pop-up will be opened, e.g.: Here, the following info about archived files size can be found: Storage class - archive type. The type is presented in the table only if there are/were any versions of files of this archive type Current ver. - the whole summary size of all current versions of archived files Previous ver. - the whole summary size of all previous versions of archived files Total - sum of the current and previous files versions Note : info about the archive size can be viewed via the CLI pipe storage du command as well, e.g.: In the CLI, only total summary size of current and previous files versions is shown. To view details about archive types, use -o f option ( --output-mode full ): For more details see in the pipe storage du section.","title":"View archive size"},{"location":"manual/08_Manage_Data_Storage/8.10._Storage_lifecycle/#restoring","text":"Note : data restoring in the storage is available for admins and storage OWNER only If there are files in the storage folder that were previously transferred to any destination (except Deletion ) according to some transition rule - user has the ability to restore such files. That available: for separate file(s) for whole folder","title":"Restoring"},{"location":"manual/08_Manage_Data_Storage/8.10._Storage_lifecycle/#restore-file","text":"To restore a file: Open a storage Select the desired file(s) you want to restore Click the \" Restore transferred item \" button in the management menu: Pop-up to confirm restoring will appear: This pop-up contains fields for: Recovery period - to specify the period duration for which the file will be restored Recipients - to specify recipients who will be notified about file restoring process (by email notification DATASTORAGE_LIFECYCLE_RESTORE_ACTION ). If this field was left empty - file will be restored without notifications Restore mode - to select the restore mode ( Standard or Bulk ). This affects on duration of restoring processes and their cost. Additionally, you have the ability to define - restore only the latest file version (default behavior) or all file versions - via the corresponding checkbox Restore all versions ( Note : this checkbox is available only for versioning storages) Click the Restore button to confirm If for a file the restoring is requested - the restoring status will appear at the file icon: There are possible states: RESTORING - restore operation is in progress. This status is shown after the request for the restore was submitted and until the file is restored successfully or restoring is failed RESTORE FAILED - restore operation is failed. This status is not shown at the GUI RESTORED - restore operation is completed. This status is shown after the file is restored successfully and till the date when the recovery period is finished. You may hover over any state icon and view details of restoring process in a tooltip, e.g.: Note : if file was restored for some period: this file becomes enabled for changing/renaming from the GUI/CLI content of this file can be viewed from the GUI/CLI label of the file original destination doesn't disappear after restore period is over, file becomes regular archived file","title":"Restore file"},{"location":"manual/08_Manage_Data_Storage/8.10._Storage_lifecycle/#restore-folder","text":"When user restores the whole folder - all previously transferred files in that folder will be restored simultaneously. Also, all transferred files in sub-folders of the folder, will be restored too - recursively. To restore a folder: Open a storage Open the desired folder you want to restore Open the Attributes panel Click the \" Restore files \" hyperlink: Pop-up to confirm restoring will appear: Options are similar as for the file restore operation Click the Restore button to confirm All files in the folder that will be restored are shown with the corresponding state: Additionally, the state of the folder restore operation is shown in the Attributes panel You may navigate to the upper level and view the state for the restoring folder (similar to restoring files): Restored files in restored folder are shown fully the same as separately restored files :","title":"Restore folder"},{"location":"manual/08_Manage_Data_Storage/8.11._Sensitive_storages/","text":"8.11. Sensitive storages Create sensitive storage Mounting sensitive storages Main restrictions in sensitive runs By default, Cloud Pipeline platform allows performing upload/download operations for any authorized data storage. But certain storages may contain sensitive data, which shall not be copied anywhere outside that storage. For storing such data, special \"sensitive\" storages are suitable. Sensitive data from that storages can be used for calculations or different other jobs, but this data cannot be copy/download to another regular storage/local machine/via the Internet etc. Viewing of the sensitive data is also restricted. Note : at the moment, only the Object storages could be used as \"sensitive\". Create sensitive storage To create sensitive object storage: Start create a new object storage: In the appeared popup, specify desired storage path and set the \" Sensitive storage \" checkbox: Click the Create button to confirm The new sensitive storage will appear: Note : to distinguish sensitive storages from regular ones in the library, the sensitive storages are marked by red icon . Via the GUI, the sensitive storage looks similar to the regular object storage, but there are some differences (even for admin/storage OWNER ): Download buttons for the storage files are hidden Share / Generate URL button for the selected storage files is hidden Preview of any files aren't available Editing of any files isn't available So via the GUI, files/folders in the sensitive storage can be created/renamed/removed (by OWNER /admin) but can't be downloaded/viewed or edited by any user. Mounting sensitive storages User can configure the list of the storages that will be mounted to the container during the run initialization. This can be accomplished using the Limit mount field of the Launch form. By default, sensitive storages aren't available to mount - only non-sensitive storages could be mounted, e.g.: To change this restriction for a specific tool, admin or image owner shall: Open the Tool page At the Tool settings page, set the \" Allow sensitive storages \" checkbox: If this checkbox is set for a tool, sensitive storages also can be selected for the mounting for a job running - in the storages list, they are marked by the special label , e.g.: If at least one sensitive storage is selected for the mounting before a job launching - such job becomes \" sensitive run \". For sensitive runs, there are some restrictions - see the section below . Note : by default, even if sensitive storages are allowed for a tool, only the all non-sensitive storages will be mounted to a job. Sensitive storages will be mounted to a job only if they were enforced in the \" Limit mounts \" field. Before the launching a sensitive run, the additional warning is shown: For the launched run, the fact that it relates to \"sensitive runs\" is shown by additional labels: at the Runs page: at the Run logs page: Main restrictions in sensitive runs For sensitive runs (runs with at least one mounted sensitive storage) all selected storages for the mounting (not only sensitive) are being mounted in readonly mode to exclude any copy/move operations between storages. Files from the sensitive storages can be viewed inside the sensitive run and also copied into the inner instance disk, but not to any other storage. Files from the sensitive storages can't be viewed outside the sensitive run or copied/moved anywhere (for example, when using not the web-terminal version of pipe SSH). There are following additional restrictions while using sensitive storages: FSBrowser : FSBrowser isn't being installed for the sensitive runs FSBrowser button isn't being displayed in the GUI for such jobs SSH-session inside the sensitive run: additional warning about restrictions is shown pipe storage ls operation is available to list any mounted storage pipe storage cp operation is available to download data only to the run's filesystem, even from the sensitive storage pipe storage mv/rm operations aren't available for any mounted storage pipe storage mount -b operation allows to mount any object storage including sensitive, but in a readonly mode SSH-session outside the sensitive run: pipe storage ls operation is available to list any mounted storage pipe storage cp/mv/rm operations aren't available if the source or destination is a sensitive storage pipe storage mount -b operation doesn't allow to mount sensitive storages Interactive endpoint GUI - data transfer capabilities are also restricted for the following tools: RStudio , Jupyter (notebook/lab) . The behavior with the interactive endpoints GUI is similar to SSH-session inside the sensitive run - user can list/view files from any mounted storage and copy them into the inner run's filesystem, but move/remove operations for any mounted storage are forbidden.","title":"8.11. Sensitive storages"},{"location":"manual/08_Manage_Data_Storage/8.11._Sensitive_storages/#811-sensitive-storages","text":"Create sensitive storage Mounting sensitive storages Main restrictions in sensitive runs By default, Cloud Pipeline platform allows performing upload/download operations for any authorized data storage. But certain storages may contain sensitive data, which shall not be copied anywhere outside that storage. For storing such data, special \"sensitive\" storages are suitable. Sensitive data from that storages can be used for calculations or different other jobs, but this data cannot be copy/download to another regular storage/local machine/via the Internet etc. Viewing of the sensitive data is also restricted. Note : at the moment, only the Object storages could be used as \"sensitive\".","title":"8.11. Sensitive storages"},{"location":"manual/08_Manage_Data_Storage/8.11._Sensitive_storages/#create-sensitive-storage","text":"To create sensitive object storage: Start create a new object storage: In the appeared popup, specify desired storage path and set the \" Sensitive storage \" checkbox: Click the Create button to confirm The new sensitive storage will appear: Note : to distinguish sensitive storages from regular ones in the library, the sensitive storages are marked by red icon . Via the GUI, the sensitive storage looks similar to the regular object storage, but there are some differences (even for admin/storage OWNER ): Download buttons for the storage files are hidden Share / Generate URL button for the selected storage files is hidden Preview of any files aren't available Editing of any files isn't available So via the GUI, files/folders in the sensitive storage can be created/renamed/removed (by OWNER /admin) but can't be downloaded/viewed or edited by any user.","title":"Create sensitive storage"},{"location":"manual/08_Manage_Data_Storage/8.11._Sensitive_storages/#mounting-sensitive-storages","text":"User can configure the list of the storages that will be mounted to the container during the run initialization. This can be accomplished using the Limit mount field of the Launch form. By default, sensitive storages aren't available to mount - only non-sensitive storages could be mounted, e.g.: To change this restriction for a specific tool, admin or image owner shall: Open the Tool page At the Tool settings page, set the \" Allow sensitive storages \" checkbox: If this checkbox is set for a tool, sensitive storages also can be selected for the mounting for a job running - in the storages list, they are marked by the special label , e.g.: If at least one sensitive storage is selected for the mounting before a job launching - such job becomes \" sensitive run \". For sensitive runs, there are some restrictions - see the section below . Note : by default, even if sensitive storages are allowed for a tool, only the all non-sensitive storages will be mounted to a job. Sensitive storages will be mounted to a job only if they were enforced in the \" Limit mounts \" field. Before the launching a sensitive run, the additional warning is shown: For the launched run, the fact that it relates to \"sensitive runs\" is shown by additional labels: at the Runs page: at the Run logs page:","title":"Mounting sensitive storages"},{"location":"manual/08_Manage_Data_Storage/8.11._Sensitive_storages/#main-restrictions-in-sensitive-runs","text":"For sensitive runs (runs with at least one mounted sensitive storage) all selected storages for the mounting (not only sensitive) are being mounted in readonly mode to exclude any copy/move operations between storages. Files from the sensitive storages can be viewed inside the sensitive run and also copied into the inner instance disk, but not to any other storage. Files from the sensitive storages can't be viewed outside the sensitive run or copied/moved anywhere (for example, when using not the web-terminal version of pipe SSH). There are following additional restrictions while using sensitive storages: FSBrowser : FSBrowser isn't being installed for the sensitive runs FSBrowser button isn't being displayed in the GUI for such jobs SSH-session inside the sensitive run: additional warning about restrictions is shown pipe storage ls operation is available to list any mounted storage pipe storage cp operation is available to download data only to the run's filesystem, even from the sensitive storage pipe storage mv/rm operations aren't available for any mounted storage pipe storage mount -b operation allows to mount any object storage including sensitive, but in a readonly mode SSH-session outside the sensitive run: pipe storage ls operation is available to list any mounted storage pipe storage cp/mv/rm operations aren't available if the source or destination is a sensitive storage pipe storage mount -b operation doesn't allow to mount sensitive storages Interactive endpoint GUI - data transfer capabilities are also restricted for the following tools: RStudio , Jupyter (notebook/lab) . The behavior with the interactive endpoints GUI is similar to SSH-session inside the sensitive run - user can list/view files from any mounted storage and copy them into the inner run's filesystem, but move/remove operations for any mounted storage are forbidden.","title":"Main restrictions in sensitive runs"},{"location":"manual/08_Manage_Data_Storage/8.12._Cloud_Data_app/","text":"8.12. Cloud Data application Installation Overview Select panel source Allow an additional FS mount Allow an additional object storage Manage content Create a folder Copy file/folder Move file/folder Delete file/folder Configuration Configuration verification FTP/SFTP server config Add/remove FTP server connection Connect to FTP server Update application As you can see at 8.9. Storages mapping , Cloud Pipeline platform allows to mount NFS data storages to the local workstation as a network drive. This way has some restrictions (such as OS of the local workstation and ability to map only FS storages). There is another Platform capability that provides a simple and convenient way to manage files, copy/move them between Cloud data storage and local workstation or FTP-server. This is implemented via the separate application that can be downloaded from the Cloud Pipeline Platform and launched at the local machine - Cloud Data application. Installation To start use the Cloud Data application, you shall download it. URLs for downloading for different operation systems can be found via the System Preference base.cloud.data.distribution.url : Copy the desired URL and download the distribution archive in any way you prefer. In the current tutorial, the Windows distribution will be used: Unzip the downloaded archive, open the unzipped folder and launch cloud-data executable file: Note : in Windows, Microsoft Defender may prevent a launch of the Cloud Data application: In this case, click the More info button, and confirm the launch by click Run anyway button: The Cloud Data application will appear: Please note, that at first launch the configuration pop-up will appear over the main application. The following fields shall be autofilled (check this and if they are not filled or filled incorrectly - correct): Data Server . Shall contain the address of the data server in format <WebDAV authentication URL for <username>> . Where: <username> is the current user name <WebDAV authentication URL> is the corresponding URL value configured for your Cloud Pipeline environment - can be found, for example, in the System Preference base.dav.auth.url user name shall be inserted instead auth-sso part of URL, e.g. if base.dav.auth.url value is https://exampleserver.com:443/webdav/auth-sso/ and current user is USER123 then the Data Server address shall be https://exampleserver.com:443/webdav/USER123 API Server . Shall contain the address of the API server of your Cloud Pipeline environment. Can be found, for example, in the System Preference base.api.host User name . Shall contain the current user name To connect the application to the Cloud Pipeline, specify your password to the corresponding field - here, your Cloud Pipeline access token means. Note : to get your access token - in the Cloud Pipeline, open System Settings , then CLI tab, and in the sub-tab Pipe CLI , click the \" Generate access key \" button: After main settings are configured, click the Save button to confirm: Note : for more details about application configuration see below . Configuration pop-up will be closed and the main form of the Cloud Data application will become available: Overview Cloud Data application allows you manage files/folders as in a file commander. For example, here you can list files from different sources, create folders, copy/move files and folders between local workstation and Cloud datastorage, or between external FTP server and datastorage. Main application form contains two panels (left and right). In each panel, one of the following sources can be opened: local workstation / FTP server / Cloud data (datastorages). Panels have the identical structure: Where: 1 - content management buttons 2 - panel source selector 3 - path to the current opened content 4 - opened source content By default in panels, the following content is opened: in the left panel, the local content. This content shows files and folders of the local workstation (by default, home user's directory). Navigation between and inside folders is available. in the right panel, the Cloud data content. This content includes: all FS mounts from the Cloud Pipeline environment - to which current user has permissions. They are shown as simple folders those object storages from the Cloud Pipeline environment - to which current user has permissions and \"File system access\" is allowed. They are shown with storage icon Navigation between and inside folders/storages is available Select panel source To select the source for the panel content: Click the Source selector of the desired panel In the appeared list, select the desired source, e.g.: Selected source will be opened in the panel: Allow an additional FS mount To view the FS mount in the Cloud data list, user shall have corresponding persmissions on that mount. So, to get access to a specific FS mount in the Cloud Data application: Firstly, Admin or FS mount owner shall set permissions for the user on that FS mount, e.g.: Wait a couple of minutes after permissions are set Click the Reload button in the application panel with Cloud data: Additional FS mount appears in the Cloud data list: Allow an additional object storage To view the object storage in the Cloud data list: user shall have corresponding persmissions on that storage for that storage, \"file system access\" shall be requested File system access for the storage can be requested for a certain period of time. This period is defined by the System Preference storage.webdav.access.duration.seconds . So, to get access to a specific object storage in the Cloud Data application: Firstly, admin or object storage owner shall set permissions for the user on that object storage, e.g.: Click the storage icon in the right upper corner of the application: The list of all object storages available for the user appears: Click the \" Request access \" hyperlink near the object storage(s) that wish to get access When the access is requested, instead hyperlink the label appears with info - till which date and time the access is provided: Close the pop-up Wait a couple of minutes Click the Reload button in the application panel with Cloud data: Additional object storage appears in the Cloud data list: Note : \"file system access\" can be requested in the other way - using the Cloud Pipeline GUI. Instead steps 1-3, you may: Open the desired object storage via the Cloud Pipeline GUI Open the Attributes panel of the storage Click the \" Request file system access \" hyperlink in the Attributes panel: When the access is requested, instead hyperlink the label appears with info - till which date and time the access is provided: To cancel the requested file system access - click the Disable hyperlink. If the file system access was requested from the Cloud Data application - this will be shown in the Attributes panel of the storage as well, and vice versa. Manage content Below, the main abilities for data management via Cloud Data application are described. These abilities apply similarly independently of source/destination selected in the application panel. Create a folder User shall have WRITE permission to the object where the folder will be created. To create a folder: In any application panel, select a desired source and navigate to the path where you wish to create a folder, e.g.: Click the button in the upper side of the panel (or just press F7 key) In the appeared pop-up, specify the name for the creating folder, e.g.: Click the CREATE button to confirm. Just-created folder will appear in the selected panel: Copy file/folder User shall have WRITE permission to the object where the file/folder will be copied. To copy a file/folder: In one panel select a source, which you wish to copy - select it by click. In the second panel navigate to a destination path where you wish to copy, e.g.: Click the button in the upper side of the source panel (or just press F5 key) Object will be copied to the destination path: Note : in case of copying of a large object(s), you may observe the processing in the bottom side of the application, e.g.: If needed, you may cancel such copying process by click the cross-button. Move file/folder User shall have WRITE permission to the object from which the file/folder will be moved. User shall have WRITE permission to the object where the file/folder will be moved. To move a file/folder: In one panel select a source, which you wish to move - select it by click. In the second panel navigate to a destination path where you wish to move, e.g.: Click the button in the upper side of the source panel (or just press F6 key) Object will be moved to the destination path: Note : in case of moving of a large object(s), you may observe the processing in the bottom side of the application - identically as due the copying of large objects. Delete file/folder User shall have WRITE permission to the object where the file/folder will be removed. To move a file/folder: Select an object, which you wish to remove - by click it, e.g.: Click the Delete button in the upper side of the source panel (or just press F8 key). Confirm the deletion: Object will be removed from the source. Source panel will be refreshed automatically. Configuration To configure the Cloud Data application click the gear icon in the right upper corner of the main form. The Configuration pop-up will be opened: This pop-up contains the following configuration settings: application connection settings (see details above ): Data Server - address of the data server's WebDAV authentication URL API Server - address of the Cloud Pipeline environment's API server User name - user name who gets access the Cloud Pipeline environment Password - user's authentication token (access key) \" FTP/SFTP servers \" section - allows to specify credentials to connect the application to FTP/SFTP server(s) \" Ignore certificate errors \" checkbox - allows to ignore certificate errors of EDGE service and FTP servers (if they exist/occur) \" Enable logging \" checkbox - allows to store logs of all actions performed in the application. If this checkbox is enabled - a separate logs file will appear on the local disk (by default, path to the logs directory is <USER_HOME_DIR>/.pipe-webdav-client/ , logs file name has format <current_date>-logs.txt ) \" App component version \" label contains the current version of the Cloud Data application updates section ( optional ) - allows to install a newer version of the application if it is available. See details below . Note : below, the possible scenarios of the application configuration are described. Don't forget to save the configuration after updates - by click the Save button in the bottom side of the Configuration pop-up. Configuration verification When application connection settings are specified - you can verify if they are correct and application can connect to the Cloud Pipeline environment. To verify Data Server connection, click the TEST button near the Data Server field: Results will appear inside the Data Server field - it will be one of two icons: when specified settings are correct (i.e. connection can be initialized): when connection can not be initialized by specified settings. Hover over the icon - in a tooltip, the main error will be shown, e.g.: In both cases, testing connection log is created and placed to the local disk. Path to the current log file is shown in the WebDAV Network log file field under server settings (for each connection verification, a separate log file is created): By the similar way, you may verify the API Server connection. In this case, logs will be written to the API Network log file , e.g.: Note : do not confuse these logs files with the main application logs file that can be enabled by the corresponding checkbox in the Configuration menu (see section above for details). FTP/SFTP server config Cloud Data application allows to copy files/folders not only between local workstation and Cloud data storages, but from FTP-server as well. To use the FTP-server as source - it shall be previously added into the application configuration. Add/remove FTP server connection Click the Add FTP server button: The section will appear where you shall specify FTP connection settings: To the Server field specify FTP server address in the format <server>[:<port>] , e.g.: From the Protocol dropdown list, select the protocol of the adding FTP server. Possible options: FTP SFTP FTP-SSL (implicit) FTP-SSL (expilicit) If authentication (login-password) is required for the adding FTP server: untick \" Use default credentials \" checkbox specify user name and password If you wish to store logs of the connection process to this FTP server - tick the \" Enable protocol logging \" checkbox. If this checkbox is enabled - for this FTP server, a separate log file will appear on the local disk (by default, path to the logs directory is <USER_HOME_DIR>/.pipe-webdav-client/ , logs file name has format <current_date>-<ftp_server>.txt ). When all settings are specified - you can verify the connection settings are correct. Click the TEST button near the Server field: Results will appear inside the Server field - it will be one of two icons: when specified settings are correct (i.e. connection can be initialized): when connection can not be initialized by specified settings. Hover over the icon - in a tooltip, the main error will be shown, e.g.: In both cases, testing connection log is created and placed to the local disk. Path to the log file is shown in the Network log file field under server settings (for each connection verification, a separate log file is created): Note : do not confuse this log file with the connection logs file described at step 6. You may add any count of FTP servers - just repeat steps 1-7 for each desired server, e.g.: To remove the specified FTP connection - click the remove button near the Server field: Connect to FTP server If you have added FTP server(s) to the application configuration - you may use them as data source in any application panel. To connect to the FTP server: Click the Source selector of the desired panel In the appeared list, select the desired FTP server, e.g.: Selected FTP server will be opened in the panel: Then, you may use all abilities of data management described above . Update application When a new version of the Cloud Data application becomes available - you will receive the corresponding notification: Click the hyperlink Install updates to update the application. Don't close the Cloud Data application manually. Some auxiliary applications can be launched during the update. Once all updates are installed, the application will be automatically relaunched. Note : if updates for the application are available - they can be install from the configuration pop-up as well:","title":"8.12. Cloud Data application"},{"location":"manual/08_Manage_Data_Storage/8.12._Cloud_Data_app/#812-cloud-data-application","text":"Installation Overview Select panel source Allow an additional FS mount Allow an additional object storage Manage content Create a folder Copy file/folder Move file/folder Delete file/folder Configuration Configuration verification FTP/SFTP server config Add/remove FTP server connection Connect to FTP server Update application As you can see at 8.9. Storages mapping , Cloud Pipeline platform allows to mount NFS data storages to the local workstation as a network drive. This way has some restrictions (such as OS of the local workstation and ability to map only FS storages). There is another Platform capability that provides a simple and convenient way to manage files, copy/move them between Cloud data storage and local workstation or FTP-server. This is implemented via the separate application that can be downloaded from the Cloud Pipeline Platform and launched at the local machine - Cloud Data application.","title":"8.12. Cloud Data application"},{"location":"manual/08_Manage_Data_Storage/8.12._Cloud_Data_app/#installation","text":"To start use the Cloud Data application, you shall download it. URLs for downloading for different operation systems can be found via the System Preference base.cloud.data.distribution.url : Copy the desired URL and download the distribution archive in any way you prefer. In the current tutorial, the Windows distribution will be used: Unzip the downloaded archive, open the unzipped folder and launch cloud-data executable file: Note : in Windows, Microsoft Defender may prevent a launch of the Cloud Data application: In this case, click the More info button, and confirm the launch by click Run anyway button: The Cloud Data application will appear: Please note, that at first launch the configuration pop-up will appear over the main application. The following fields shall be autofilled (check this and if they are not filled or filled incorrectly - correct): Data Server . Shall contain the address of the data server in format <WebDAV authentication URL for <username>> . Where: <username> is the current user name <WebDAV authentication URL> is the corresponding URL value configured for your Cloud Pipeline environment - can be found, for example, in the System Preference base.dav.auth.url user name shall be inserted instead auth-sso part of URL, e.g. if base.dav.auth.url value is https://exampleserver.com:443/webdav/auth-sso/ and current user is USER123 then the Data Server address shall be https://exampleserver.com:443/webdav/USER123 API Server . Shall contain the address of the API server of your Cloud Pipeline environment. Can be found, for example, in the System Preference base.api.host User name . Shall contain the current user name To connect the application to the Cloud Pipeline, specify your password to the corresponding field - here, your Cloud Pipeline access token means. Note : to get your access token - in the Cloud Pipeline, open System Settings , then CLI tab, and in the sub-tab Pipe CLI , click the \" Generate access key \" button: After main settings are configured, click the Save button to confirm: Note : for more details about application configuration see below . Configuration pop-up will be closed and the main form of the Cloud Data application will become available:","title":"Installation"},{"location":"manual/08_Manage_Data_Storage/8.12._Cloud_Data_app/#overview","text":"Cloud Data application allows you manage files/folders as in a file commander. For example, here you can list files from different sources, create folders, copy/move files and folders between local workstation and Cloud datastorage, or between external FTP server and datastorage. Main application form contains two panels (left and right). In each panel, one of the following sources can be opened: local workstation / FTP server / Cloud data (datastorages). Panels have the identical structure: Where: 1 - content management buttons 2 - panel source selector 3 - path to the current opened content 4 - opened source content By default in panels, the following content is opened: in the left panel, the local content. This content shows files and folders of the local workstation (by default, home user's directory). Navigation between and inside folders is available. in the right panel, the Cloud data content. This content includes: all FS mounts from the Cloud Pipeline environment - to which current user has permissions. They are shown as simple folders those object storages from the Cloud Pipeline environment - to which current user has permissions and \"File system access\" is allowed. They are shown with storage icon Navigation between and inside folders/storages is available","title":"Overview"},{"location":"manual/08_Manage_Data_Storage/8.12._Cloud_Data_app/#select-panel-source","text":"To select the source for the panel content: Click the Source selector of the desired panel In the appeared list, select the desired source, e.g.: Selected source will be opened in the panel:","title":"Select panel source"},{"location":"manual/08_Manage_Data_Storage/8.12._Cloud_Data_app/#allow-an-additional-fs-mount","text":"To view the FS mount in the Cloud data list, user shall have corresponding persmissions on that mount. So, to get access to a specific FS mount in the Cloud Data application: Firstly, Admin or FS mount owner shall set permissions for the user on that FS mount, e.g.: Wait a couple of minutes after permissions are set Click the Reload button in the application panel with Cloud data: Additional FS mount appears in the Cloud data list:","title":"Allow an additional FS mount"},{"location":"manual/08_Manage_Data_Storage/8.12._Cloud_Data_app/#allow-an-additional-object-storage","text":"To view the object storage in the Cloud data list: user shall have corresponding persmissions on that storage for that storage, \"file system access\" shall be requested File system access for the storage can be requested for a certain period of time. This period is defined by the System Preference storage.webdav.access.duration.seconds . So, to get access to a specific object storage in the Cloud Data application: Firstly, admin or object storage owner shall set permissions for the user on that object storage, e.g.: Click the storage icon in the right upper corner of the application: The list of all object storages available for the user appears: Click the \" Request access \" hyperlink near the object storage(s) that wish to get access When the access is requested, instead hyperlink the label appears with info - till which date and time the access is provided: Close the pop-up Wait a couple of minutes Click the Reload button in the application panel with Cloud data: Additional object storage appears in the Cloud data list: Note : \"file system access\" can be requested in the other way - using the Cloud Pipeline GUI. Instead steps 1-3, you may: Open the desired object storage via the Cloud Pipeline GUI Open the Attributes panel of the storage Click the \" Request file system access \" hyperlink in the Attributes panel: When the access is requested, instead hyperlink the label appears with info - till which date and time the access is provided: To cancel the requested file system access - click the Disable hyperlink. If the file system access was requested from the Cloud Data application - this will be shown in the Attributes panel of the storage as well, and vice versa.","title":"Allow an additional object storage"},{"location":"manual/08_Manage_Data_Storage/8.12._Cloud_Data_app/#manage-content","text":"Below, the main abilities for data management via Cloud Data application are described. These abilities apply similarly independently of source/destination selected in the application panel.","title":"Manage content"},{"location":"manual/08_Manage_Data_Storage/8.12._Cloud_Data_app/#create-a-folder","text":"User shall have WRITE permission to the object where the folder will be created. To create a folder: In any application panel, select a desired source and navigate to the path where you wish to create a folder, e.g.: Click the button in the upper side of the panel (or just press F7 key) In the appeared pop-up, specify the name for the creating folder, e.g.: Click the CREATE button to confirm. Just-created folder will appear in the selected panel:","title":"Create a folder"},{"location":"manual/08_Manage_Data_Storage/8.12._Cloud_Data_app/#copy-filefolder","text":"User shall have WRITE permission to the object where the file/folder will be copied. To copy a file/folder: In one panel select a source, which you wish to copy - select it by click. In the second panel navigate to a destination path where you wish to copy, e.g.: Click the button in the upper side of the source panel (or just press F5 key) Object will be copied to the destination path: Note : in case of copying of a large object(s), you may observe the processing in the bottom side of the application, e.g.: If needed, you may cancel such copying process by click the cross-button.","title":"Copy file/folder"},{"location":"manual/08_Manage_Data_Storage/8.12._Cloud_Data_app/#move-filefolder","text":"User shall have WRITE permission to the object from which the file/folder will be moved. User shall have WRITE permission to the object where the file/folder will be moved. To move a file/folder: In one panel select a source, which you wish to move - select it by click. In the second panel navigate to a destination path where you wish to move, e.g.: Click the button in the upper side of the source panel (or just press F6 key) Object will be moved to the destination path: Note : in case of moving of a large object(s), you may observe the processing in the bottom side of the application - identically as due the copying of large objects.","title":"Move file/folder"},{"location":"manual/08_Manage_Data_Storage/8.12._Cloud_Data_app/#delete-filefolder","text":"User shall have WRITE permission to the object where the file/folder will be removed. To move a file/folder: Select an object, which you wish to remove - by click it, e.g.: Click the Delete button in the upper side of the source panel (or just press F8 key). Confirm the deletion: Object will be removed from the source. Source panel will be refreshed automatically.","title":"Delete file/folder"},{"location":"manual/08_Manage_Data_Storage/8.12._Cloud_Data_app/#configuration","text":"To configure the Cloud Data application click the gear icon in the right upper corner of the main form. The Configuration pop-up will be opened: This pop-up contains the following configuration settings: application connection settings (see details above ): Data Server - address of the data server's WebDAV authentication URL API Server - address of the Cloud Pipeline environment's API server User name - user name who gets access the Cloud Pipeline environment Password - user's authentication token (access key) \" FTP/SFTP servers \" section - allows to specify credentials to connect the application to FTP/SFTP server(s) \" Ignore certificate errors \" checkbox - allows to ignore certificate errors of EDGE service and FTP servers (if they exist/occur) \" Enable logging \" checkbox - allows to store logs of all actions performed in the application. If this checkbox is enabled - a separate logs file will appear on the local disk (by default, path to the logs directory is <USER_HOME_DIR>/.pipe-webdav-client/ , logs file name has format <current_date>-logs.txt ) \" App component version \" label contains the current version of the Cloud Data application updates section ( optional ) - allows to install a newer version of the application if it is available. See details below . Note : below, the possible scenarios of the application configuration are described. Don't forget to save the configuration after updates - by click the Save button in the bottom side of the Configuration pop-up.","title":"Configuration"},{"location":"manual/08_Manage_Data_Storage/8.12._Cloud_Data_app/#configuration-verification","text":"When application connection settings are specified - you can verify if they are correct and application can connect to the Cloud Pipeline environment. To verify Data Server connection, click the TEST button near the Data Server field: Results will appear inside the Data Server field - it will be one of two icons: when specified settings are correct (i.e. connection can be initialized): when connection can not be initialized by specified settings. Hover over the icon - in a tooltip, the main error will be shown, e.g.: In both cases, testing connection log is created and placed to the local disk. Path to the current log file is shown in the WebDAV Network log file field under server settings (for each connection verification, a separate log file is created): By the similar way, you may verify the API Server connection. In this case, logs will be written to the API Network log file , e.g.: Note : do not confuse these logs files with the main application logs file that can be enabled by the corresponding checkbox in the Configuration menu (see section above for details).","title":"Configuration verification"},{"location":"manual/08_Manage_Data_Storage/8.12._Cloud_Data_app/#ftpsftp-server-config","text":"Cloud Data application allows to copy files/folders not only between local workstation and Cloud data storages, but from FTP-server as well. To use the FTP-server as source - it shall be previously added into the application configuration.","title":"FTP/SFTP server config"},{"location":"manual/08_Manage_Data_Storage/8.12._Cloud_Data_app/#addremove-ftp-server-connection","text":"Click the Add FTP server button: The section will appear where you shall specify FTP connection settings: To the Server field specify FTP server address in the format <server>[:<port>] , e.g.: From the Protocol dropdown list, select the protocol of the adding FTP server. Possible options: FTP SFTP FTP-SSL (implicit) FTP-SSL (expilicit) If authentication (login-password) is required for the adding FTP server: untick \" Use default credentials \" checkbox specify user name and password If you wish to store logs of the connection process to this FTP server - tick the \" Enable protocol logging \" checkbox. If this checkbox is enabled - for this FTP server, a separate log file will appear on the local disk (by default, path to the logs directory is <USER_HOME_DIR>/.pipe-webdav-client/ , logs file name has format <current_date>-<ftp_server>.txt ). When all settings are specified - you can verify the connection settings are correct. Click the TEST button near the Server field: Results will appear inside the Server field - it will be one of two icons: when specified settings are correct (i.e. connection can be initialized): when connection can not be initialized by specified settings. Hover over the icon - in a tooltip, the main error will be shown, e.g.: In both cases, testing connection log is created and placed to the local disk. Path to the log file is shown in the Network log file field under server settings (for each connection verification, a separate log file is created): Note : do not confuse this log file with the connection logs file described at step 6. You may add any count of FTP servers - just repeat steps 1-7 for each desired server, e.g.: To remove the specified FTP connection - click the remove button near the Server field:","title":"Add/remove FTP server connection"},{"location":"manual/08_Manage_Data_Storage/8.12._Cloud_Data_app/#connect-to-ftp-server","text":"If you have added FTP server(s) to the application configuration - you may use them as data source in any application panel. To connect to the FTP server: Click the Source selector of the desired panel In the appeared list, select the desired FTP server, e.g.: Selected FTP server will be opened in the panel: Then, you may use all abilities of data management described above .","title":"Connect to FTP server"},{"location":"manual/08_Manage_Data_Storage/8.12._Cloud_Data_app/#update-application","text":"When a new version of the Cloud Data application becomes available - you will receive the corresponding notification: Click the hyperlink Install updates to update the application. Don't close the Cloud Data application manually. Some auxiliary applications can be launched during the update. Once all updates are installed, the application will be automatically relaunched. Note : if updates for the application are available - they can be install from the configuration pop-up as well:","title":"Update application"},{"location":"manual/08_Manage_Data_Storage/8.13._Versioned_storages/","text":"8.13. Versioned storages Manage versioned storages Create versioned storage View and edit content Create a folder/file Edit file content Rename a file/folder Remove a file/folder Version control Commit history of the file Download a specific version Revert to a specifid version View diffs Commit history of the folder Filter commit history Generate changes report GIT operations with versioned storages Load versioned storage content to the instance Launch a run with the content cloning Clone content to the already running instance View diffs of changed cloned content Save changed content from the run to the versioned storage Save changed content with the conflict resolving Update cloned content in the run Refresh cloned content Change the revision for the content In some cases, there is not enough to only store data in storages/place computation results there but also there is a request for the full value system of the revision control of stored data - to view revisions, history of changes, diffs between revisions. So far, for separate storages types (e.g. AWS s3 buckets), there is the ability to enable the versioning option. But it is not the same. Versioning allows to manage the versions of the certain file, not the revisions of the full storage, which revision can contain changes of several files or folders. For the needs of full version control of the storing data, there is a special storage type in the Cloud Pipeline - \"Versioned\" storage. These storages are GitLab repositories under the hood, all changes performed in their data are versioned. Users can view the history of changes, diffs, etc. Such storages can be also mounted during the runs, data can be used for the computations and results can be comitted back to such storages - with all the benefits of a version control system. Manage versioned storages Create versioned storage To create versioned storage: Navigate to the folder where you want to create data storage. Click + Create \u2192 VERSIONED STORAGE : The pop-up of the versioned Storage creation will appear: Specify the name of the creating storage ( mandatory ) and description ( optionally ), e.g.: By default, the creating versioned storage will be empty. But you may define a folders structure for it before the creation: set the \" Predefined folder structure \" checkbox in the appeared field, specify a structure you want the storage will have: structure shall include folder paths in format: <folder1>/<subfolder1>/... (each subfolder is is delimited by / ), each new folder on the same level from the root of the storage shall starts from the new line, e.g.: Click the Create button to confirm. Just-created versioned storage will appear in the folder: If you have predefined the structure - you may see it is created as well: Note : not to confuse with regular object storages, versioned storage icons (and all objects inside them) are colorized in blue color: View and edit content The view of the versioned storage is similar to regular data storage with some differences: for each file/folder in the storage, the following columns are displayed: Name - file/folder name Size - file size (empty for folders) Revision - latest revision (SHA-1 hash of the latest commit) touched that file/folder Date changed - date and time of the latest commit touched that file/folder Author - user name who performed the latest commit touched that file/folder Message - message of the latest commit touched that file/folder for each file in the storage, there are additional buttons similar to regular storage - to download file to the local workstation, to rename the file and to remove the file from the storage for each folder in the storage, there are additional buttons similar to regular storage - to rename the folder and to remove the folder from the storage RUN button - allows to run the tool with cloning of the current versioned storage into the instance - see details below Generate report button - allows to configure and then download the report of the storage usage (commit history, diffs, etc.) - see details below Show history button - allows to open the panel with commit history info of the current versioned storage or selected folder - see details below settings button (\" gear \" icon) - allows to open the seetings pop-up where: versioned storage can be edited (name and description can be changed) or removed: permissions on the storage can be configured: buttons to create file/folder and to upload data are similar to the corresponding ones in regular storages Create a folder/file To create a folder: Click \" + Create \" button in the versioned storage In the list, select the Folder item: The pop-up will appear to specify a new folder name and commit message: Specify a folder name ( mandatory ). The commit message may be omitted. If it is not specified - it will be set automatically. Click OK button to confirm: After the creation, a new revision of the storage will be created (contains only differences with the previous revision). Just-created folder will appear in the storage: To create a file: Click \" + Create \" button in the versioned storage In the list, select the File item: The pop-up will appear to specify a new file name and commit message. Specify a file name ( mandatory ). The commit message may be omitted. If it is not specified - it will be set automatically. Click OK button to confirm, e.g.: After the creation, a new revision of the storage will be created (contains only differences with the previous revision). Just-created file will appear in the storage: Note : during the file creation in the versioned storage, you are not able to specify the file content simultaneously (as it is possible for regular storages) - therefore file is created with the zero-size. To add a file content - edit it. Uploading of the data is similar to regular storages. After the upload, a new revision of the storage is being created. The commit message for uploaded files is being set automatically, e.g.: Edit file content To edit a text file: Click the file in the versioned storage File preview will be opened on the right. In case of the file is large you will see only its part Click the \" Expand \" icon in the upper right of the preview section, e.g.: Pop-up with the file content will appear: Click the Edit button to enable the edit-mode for the file. Change the file content and click the Save to confirm changes, e.g.: Specify the commit message in the appeared pop-up, e.g.: Click the Commit button to confirm. After saving, a new revision of the storage will be created: Note : binary file can not be edited by the described way. In this case, only download hyperlink is shown for such files instead of the preview, e.g.: Rename a file/folder To rename a file/folder: Click the Rename button in the row of the file/folder of the versioned storage In the appeared pop-up, specify a new name and commit message. The commit message may be omitted. If it is not specified - it will be set automatically. Click OK button to confirm: After saving, a new revision of the storage will be created. File/folder will be renamed: Remove a file/folder To remove a file/folder: Click the Remove button in the row of the file/folder of the versioned storage In the appeared pop-up, specify a commit message. The commit message may be omitted. If it is not specified - it will be set automatically. Click the Delete button to confirm: After saving, a new revision of the storage will be created. File/folder will be removed Note : removing of the folder also deletes all child files and subfolders. Version control Due to that versioned storage is a Git repository by the fact, so one of the important advantages of versioned storages in condition with regular object storages - ability to view commit history and all changes that were performed with the data in details. Commit history of the file You can view the commit history of the file in the versioned storage - i.e. history of all commits that touched this file. To open the commit history of the file, click it. The commit history will be displayed under the file preview section, e.g.: Commit history contains a list of records. Each record corresponds to one commit that touched the selected file. List of commits is sorted from the last commit to earlier ones. Each record includes: commit message commit SHA-1 hash user name - author of the commit date and time when commit (changes) was performed block of buttons: - Revert button - to revert the content of the current file to the selected commit - Download version button - to view/download revert version of the current file corresponding to the selected commit - Show diffs button - to view diffs between the content of the current file in the selected commit and in the previous commit Download a specific version To download a specific version of the file in the versioned storage: Click the file to open its commit history Find the commit Click the Download version button for the commit, e.g.: The file content in the selected commit will be displayed: Click the Download button in the upper side of the pop-up Revert to a specifid version To revert to a specific version of the file in the versioned storage: Click the file to open its commit history Find the commit Click the Revert button for the commit, e.g.: Pop-up will appear to specify a commit message - for the operation that will revert the current state of the file to the selected commit. The commit message may be omitted. If it is not specified - it will be set automatically. Click the Revert button to confirm: After the operation: content of the file will be reverted to the selected commit new commit will appear in the commit history Note : Revert button is available for any non-latest commit. View diffs To view differences of a specific file version conditionally to its previous version: Click the file to open its commit history Find the commit Click the Show diffs button for the commit, e.g.: The pop-up will be opened with diffs between file version in the selected commit and in the previous commit: Differences pop-up includes: selected commit's hash file name state of changes that were performed with the file in the commit. Possible variants: ADDED - new file was created CHANGED - existing file was edited DELETED - file was removed RENAMED - file was renamed section with details of changes: new added rows are shown in green color, e.g.: rows that were removed are shown in red color, e.g.: not changed rows or auxiliary info are shown without filling, e.g. changes in a binary file: or new empty text file creation: Commit history of the folder You can view the commit history of any folder in the versioned storage (including the root folder) - i.e. history of all commits that touched files in this folder and in its subfolders. To view the commit history of the folder in the versioned storage: Open the folder Click the \" Show history \" button in the right upper corner. The commit history will appear on the right side, \" Show history \" button will be renamed to \" Hide history \" e.g.: Commit history contains a list of records. Each record corresponds to one commit that touched the current folder's files. List of commits is sorted from the last commit to earlier ones. Each record includes: commit message commit SHA-1 hash user name - author of the commit date and time when commit (changes) was performed Show diffs button - - to view diffs between the content of the folder in the selected commit and in the previous commit. Displaying of diffs is similar to separate files, you can see examples above . Filter commit history You may filter the commit history (both for a separate file and for a folder) - for example, to find some specific commits. To open the Filter panel - click the corresponding button in the right upper corner of the commit history section - Filter panel has view: Here, the following filters for the commit history can be configured: by Author - you may select one or several users to filter revisions (commits) by their author(s). By default, commits of all authors are displayed. by Date - you may select period (date From and date To - note that any of the date may be omitted) to filter revisions (commits) by their dates. By default, commits for all periods of the object existence are displayed. by File type - you may specify file type(s) to filter the revisions (commits) list by only the ones that touched files of the specified type(s). File types shall be specified in a plain text format, comma-separated, simple masks are supported. Example: csv,ma*,md . By default, commits for all files are displayed. Once filters are configured, click the Apply button to confirm, e.g.: The commit history will be updated according to the configured filters: To reset (clear) configured filters: Click the Filter button. At the Filter panel, click the Reset button: Filters will be reset. The commit history will be updated. Generate changes report For the versioned storages, there is the ability to generate and download reports. Reports contain commit history of the current folder in the versioned storage and also can include diffs between commits. Such reports can be downloaded in Microsoft Word format ( docx file). To generate a report: Open the folder in the versioned storage. Click the \" Generate report \" button in the right upper corner of the page. \" Generate report \" form will appear: You may configure filters - only commits that match such filters will be included in the report (by default - if no filters are set, all commits will be included). These filters are fully the same as in the Filter panel : by Author - to restrict the list of reported commits by their author(s) by Date - to restrict the list of reported commits by date of their creation by File type - to restrict the list of reported commits according to included file types You may additionally include differences between files in each commit - by set the corresponding checkbox \" Include file diffs \": In this case, you may select how to split changes that will be include to the report: by revision - changes will be split by commits, i.e. from the recent commit to older ones, in each commit all files changes touched by this commit are included by files - changes will be split by all changed files, i.e. for each changed file all commits that touched this file are included additionally, you may set the checkbox \" Save changes separately \": in this case (no matter which split-mode is selected), changes will be saved in separate files - i.e. one report-file per each commit or one report-file per each changed file summary report will be downloaded as an archive Once settings are configured, click the \" Download report \" button. Report will be downloaded automatically: Example of the part of the report without included diffs: Example of the part of the report with included diffs (changes split by commits ): In the shown commit, one file was changed. Example of the part of the report with included diffs (changes split by files ): Shown file was changed in 3 commits. Report itself is being generated from the docx -format template. Platform has the \"base\" template but system admin has the ability to load own template and replace the \"base\" one. Template contains plain text and a set of predefined keywords that are replaced by actual values of the current versioned storage's data during the report generation according to the configured report filters. By this way, report view, its text blocks, tables, fonts, formats, etc. can be preconfigured for the customer needs. GIT operations with versioned storages Users have the ability to launch runs using versioned storages. Users have the access to the version storages during the run - similarly as to the folder of any mounted object storage. Users have the ability to read and write data (according to the permissions) to the version storages from inside the runs. But versioned storages have some differences to the regular object storages used in runs - especially in question of data saving and resolving conflicts. To manage versioning operations from active runs, there are additional controls: at the main Dashboard, in the \"Active runs\" panel, it is VCS (Version Control Storages) item - it is available when hovering any fully initialized run's tile, e.g.: at the \"Run logs\" page, it is VERSIONED STORAGE item - it is available for any fully initialized run, e.g.: Both these items are identical to each other. In our examples below we will use the VCS control from the main Dashboard, in generally. By click such items, additional menu appears - its content may vary in different cases. Let's view main scenarios of using versioned storages in running jobs. Load versioned storage content to the instance There are two possible ways to load versioned storage content to the running instance: launch the run from the versioned storage page with the automatic content cloning during the run initialization clone the versioned storage content to the already running instance Launch a run with the content cloning In this case, content of the selected versioned storage is being cloned during the run initialization. To launch a run from the versioned storage page: Open the versioned storage page. Click the RUN button in the right upper corner of the page: The pop-up will appear: Here, the list of tools is shown. By default, this list contains tools from the user personal group . Click the tool you wish to run with the versioned storage, e.g.: If needed, you may select the tool version from the appeared dropdown list in the tool tile. If you want to customize the tool settings (or you do not have the desired tool in the list) - you may configure the tool for the launch manually: click the Run custom button in the left bottom corner of the pop-up: the Launch form will appear - similar to the one that appears before the pipeline launch : you may specify options as you want Click the Launch button to confirm the run. Run of the selected tool will be launched: Selected versioned storage content will be automatically cloned into the running instance (during the run initialization) - by the path /versioned-data/{versioned_storage_name} . Once the run is fully initialized, you may open the SSH terminal for the launched run and check that the versioned storage content was cloned: Or via the filesystem browser : Click the VERSIONED STORAGE item at the \"Run logs\" page - cloned versioned storage will be displayed with its actions list: Clone content to the already running instance You may clone the versioned storage content to the already running instance: At the main Dashboard, hover over the active run's tile to which instance you want to clone the version storage content. Please note, that run shall be fully initialized. Click the VCS control. In the appeared menu, click the Clone item, e.g.: The pop-up will appear: Here, the full list of available versioned storages is shown. Click the storage you want to clone to the current run, e.g.: In the row of the selected storage, the field will appear where you may select the specific commit of the versioned storage you want to clone: By default, the recent commit is selected. Click the SELECT button to confirm. Selected versioned storage content will be cloned to the running instance: Click the VCS control for the run - cloned versioned storage will be displayed with its actions list: Also as shown in the section above, cloned versioned storage content is available in the running instance by the path /versioned-data/{versioned_storage_name} . You may clone any count of versioned storages to the running instance by the described way. For example: Click the VCS control for the run with the previously cloned storage: In the appeared pop-up, select a versioned storage to clone, e.g.: If there are versioned storages previously cloned to that run - they are also displayed in the list, but can not be selected repeatedly. After confirmation, all cloned versioned storages will be displayed in the list by click the VCS control: And all cloned versioned storages will be available by the path /versioned-data/{versioned_storage_name} : View diffs of changed cloned content At any point after the versioned storage was cloned to the running instance, you can request the diffs for that storage. These diffs will show all changes made by the current user in files of cloned versioned storage - i.e. differences between commit of the versioned storage that was cloned to the running instance and the current state of files in that cloned folder. To view diffs from the running instance with the cloned versioned storage: At the main Dashboard, hover over the active run's tile to which instance the versioned storage was cloned. Click the VCS control. In the appeared menu, select the versioned storage (if there are several ones) and click the Diffs item for it, e.g.: Please note, that if there are no changes in the cloned versioned storage were made - this item will be visible in the menu but disabled. The diffs pop-up will be opened, e.g.: Here, the full list of your changes in files in comparing with the cloned version of the selected versioned storage is shown. Changes are split by files. Displaying of diffs for each file is similar to the described in one of the section above . Save changed content from the run to the versioned storage At any point after the versioned storage was cloned to the running instance, you can \"save\" changes made in the cloned content to the origin versioned storage. Please note, if you will just stop the run with the cloned versioned storage without \"saving\" operation - all changes you performed in the cloned folder of the versioned storage will dissapear without any posibilities to restore the data. To perform the \"save\" operation from the running instance with the cloned versioned storage: At the main Dashboard, hover over the active run's tile to which instance the versioned storage was cloned. Click the VCS control. In the appeared menu, select the versioned storage (if there are several ones) and click the Save item for it, e.g.: Please note, that if there are no changes in the cloned versioned storage were made - this item will be visible in the menu but disabled. The pop-up will be opened, e.g.: Specify a commit message. View details of performed changes - here, the full list of your changes in files in comparing with the cloned version of the selected versioned storage is shown (similar to Diffs option). You may expand details by click the file name. Displaying of diffs for each file is similar to the described in one of the section above : By default, all changed files from the cloned versioned storage are included to the preparing commit (\"Save\" operation). But you can select only files you want to include to the commit - by the checkboxes near the file names, e.g.: Or via the buttons above the list: Select all - to include all changed files to the preparing commit Clear selection - to exclude all changed files from the preparing commit Please note, if you will exclude some file(s) from the preparing commit, you can include these files in another commit later. Once the commit message is specified and files to commit are defined - click the Commit button to confirm, e.g.: The system tries to perform corresponding Git actions to origin versioned storage. Changed files will be \"saved\" to the origin versioned storage: You can check it from the versioned storage page: Changed files that were not included in the commit are still shown in Diffs : And can be \"saved\" in another commit: Save changed content with the conflict resolving There may be cases, when any conflict is detected during the \"save\" operation attempt. Conflict may appears, for example, when you have cloned versioned storage to the run, made some changes and another user during this period of time made own changes to the same file(s) as in your current run and managed to \"save\" these changes (perform commit) to the origin versioned storage. Let's imagine, that there are two users (you and somebody else) that both cloned the same versioned storage to their runs and perform changes in the same files, touching the same parts of these files. Herewith, the second user managed to \"save\" these changes (performed a commit to the origin versioned storage). So, when you will try to \"save\" changes by steps described above : The pop-up before the saving will have view like: Once the Commit button is clicked, the conflict error will occur and \" Resolve conflicts \" pop-up will appear: This pop-up contains: list of conflicting files (left panel) - please note that changed non-conflicting files are not shown here, they will be saved without additional resolving 3 columns with the content of conflicting versions of the selected file: \" Your changes \" (left) - column shows changed state of the conflicting file (current state of the file in your run that you have tried to save) \" Changes from remote \" (right) - column shows the state of the conflicting file from the most recent revision of that file. In our example - it is changed file that second user has saved to the origin versioned storage \" Result \" (center) - column shows the merging state (result) of the conflict resolving. When you will resolve the conflict, the version that will be saved will be shown here all changes are highlighted: conflicting changes are in red color, i.e. in both versions the same part of the file is touched: non-conflicting changes (changes that are not touched the same parts): new added parts and places where they mya be inserted are in green color e.g.: changed existing parts are in yellow color e.g.: buttons to revert/repeat performed actions during the conflict resolving: buttons to apply only non-conflicting changes as a batch: - to apply all non-conflicting changes from your changed version of the file. They will be \"copied\" to the central column, i.e. to the version that will be saved after the conflict resolving - to apply all non-conflicting changes from the updated current version of the file in the origin versioned storage (i.e. in our example, it is the version saved by the second user). Changes will be \"copied\" to the central column, i.e. to the version that will be saved after the conflict resolving - to apply all non-conflicting changes from both file versions - yours, prepared for the saving, and theirs, from the updated origin versioned storage buttons to accept one of the full versions of the conflicting file as a conflict resolve: - to accept all changes ( conflicting and non-conflicting ) fully from your version of the file - to accept all changes ( conflicting and non-conflicting ) fully from the updated current version of the file in the origin versioned storage (i.e. in our example, it is the version saved by the second user) You shall resolve all changes in each conflicting file. You may resolve any change/conflict manually or use buttons for batch resolving that described in the previous item. For manually resolving, there are abilities for each change/conflict in the file: to accept it - by the arrows icon near the change, e.g. - by this action, the corresponding change will be \"copied\" to the central column, highlighting of the change will disappear to decline it - by the cross icon near the change, e.g. - by this action, the corresponding change will be declined, i.e. will be \"ignored\" in the central column, highlighting of the change will also disappear to change the text manually (e.g., if both versions are not satisfied) - for that, set the cursor to the change highlighting area in the central column and specify a new text Let's view different options: manually accept non-conflicting change from your version by click the arrows icon: Changed part from your version will appear in the central column, highlighting of the change will disappear: batch-apply of all non-conflicting changes from the origin versioned storage (saved by the second user): All non-conflicting changes from remote version will appear in the central column, their highlighting will disappear: manually decline conflicting change from your version: Change from your version will be declined, highlighting of the change will disappear: click the conflict area in the central column: Specify a new text as you want, e.g.: to finish resolving, decline conflicting change from the remote version: Once all changes and conflicts in the selected file are resolved, the corresponding info will appear in the upper side of the pop-up and in the files list: The version of the file displayed in the central column ( Result ) will be saved in the origin versioned storage. To proceed the saving process, all conflicts in all files shall be resolved. Therefore, open one-by-one each conflicting file in the list and resolve changes using instructions described above: In case of conflicting binary files, there is no ability to resolve each change: You shall only select which version to accept for the saving - your changed version of this file or updated version from the origin versioned storage (i.e. in our example, it is the version saved by the second user). Once all conflicts in all files are resolved, the button Resolve becomes available: Click it to finish the saving process. Corresponding messages will appear: You can check that changes were saved - from the versioned storage page: Update cloned content in the run At any point after the versioned storage was cloned to the running instance, you can update the cloned content in the running instance: \"refresh\" cloned content up to the latest revision (most recent) of the versioned storage. It may be useful as the origin content of the versioned storage could be changed by other users during your run. also, you can change the revision to any non-latest one - just to \"view\" the state of the versioned storage in a certain revision. Refresh cloned content To \"refresh\" cloned content up to the latest revision (most recent): At the main Dashboard, hover over the active run's tile to which instance the versioned storage was cloned. Click the VCS control. In the appeared menu, select the versioned storage (if there are several ones) and click the Refresh item for it, e.g.: The corresponding message will appear: Once the refresh is performed, confirmation will appear: Please note, that if you made changes in cloned files and did not save them, then 2 scenario are possible: if there are no conflicts between your changed unsaved files and files that were updated in the origin versioned storage - in such case: revision version for the cloned content will be updated to the latest without issues changed unsaved files in the cloned content will remain in their state, you may proceed work with them and then save, for example if there are conflicts between your changed unsaved files and files that were updated in the origin versioned storage - in such case: the error of the refresh unable will occur: the pop-up will appear to resolve the conflicts revision version for the cloned content will not be updated till you will not resolve the conflict as described in the section above . When you will resolve conflicts, new merge commit will be created and your cloned content will be update to this version (latest), previously conflicted changes will be saved how they were resolved. Change the revision for the content To change the revision of the cloned content to some non-latest one: At the main Dashboard, hover over the active run's tile to which instance the versioned storage was cloned. Click the VCS control. In the appeared menu, select the versioned storage (if there are several ones) and click the Checkout revision item for it, e.g.: Pop-up with the dropdown list of all storage's revisions will appear: Click the dropdown list and select the revision, e.g.: Note : current revision of the storage content is specified with the bold font. To confirm the checkout, click the \" Change Revision \" button: The corresponding message will appear: Please note, if you have changed the revision to any non-latest one and then change some files, these new changes can not be saved by the way described in the Save section, the corresponding item in the VSC menu will be disabled: In this case, to save made changes you may: refresh the content revision up to the latest version and then save changes or change the revision as described in the current section but to the latest one and then save changes Additionally note, if the revision of the cloned content is the latest, and you make any changes of the content and then change the revision to any non-latest - your unsaved changes will be lost. To avoid this, at the beginning, save changes and only then change the revision to non-latest.","title":"8.13. Versioned storages"},{"location":"manual/08_Manage_Data_Storage/8.13._Versioned_storages/#813-versioned-storages","text":"Manage versioned storages Create versioned storage View and edit content Create a folder/file Edit file content Rename a file/folder Remove a file/folder Version control Commit history of the file Download a specific version Revert to a specifid version View diffs Commit history of the folder Filter commit history Generate changes report GIT operations with versioned storages Load versioned storage content to the instance Launch a run with the content cloning Clone content to the already running instance View diffs of changed cloned content Save changed content from the run to the versioned storage Save changed content with the conflict resolving Update cloned content in the run Refresh cloned content Change the revision for the content In some cases, there is not enough to only store data in storages/place computation results there but also there is a request for the full value system of the revision control of stored data - to view revisions, history of changes, diffs between revisions. So far, for separate storages types (e.g. AWS s3 buckets), there is the ability to enable the versioning option. But it is not the same. Versioning allows to manage the versions of the certain file, not the revisions of the full storage, which revision can contain changes of several files or folders. For the needs of full version control of the storing data, there is a special storage type in the Cloud Pipeline - \"Versioned\" storage. These storages are GitLab repositories under the hood, all changes performed in their data are versioned. Users can view the history of changes, diffs, etc. Such storages can be also mounted during the runs, data can be used for the computations and results can be comitted back to such storages - with all the benefits of a version control system.","title":"8.13. Versioned storages"},{"location":"manual/08_Manage_Data_Storage/8.13._Versioned_storages/#manage-versioned-storages","text":"","title":"Manage versioned storages"},{"location":"manual/08_Manage_Data_Storage/8.13._Versioned_storages/#create-versioned-storage","text":"To create versioned storage: Navigate to the folder where you want to create data storage. Click + Create \u2192 VERSIONED STORAGE : The pop-up of the versioned Storage creation will appear: Specify the name of the creating storage ( mandatory ) and description ( optionally ), e.g.: By default, the creating versioned storage will be empty. But you may define a folders structure for it before the creation: set the \" Predefined folder structure \" checkbox in the appeared field, specify a structure you want the storage will have: structure shall include folder paths in format: <folder1>/<subfolder1>/... (each subfolder is is delimited by / ), each new folder on the same level from the root of the storage shall starts from the new line, e.g.: Click the Create button to confirm. Just-created versioned storage will appear in the folder: If you have predefined the structure - you may see it is created as well: Note : not to confuse with regular object storages, versioned storage icons (and all objects inside them) are colorized in blue color:","title":"Create versioned storage"},{"location":"manual/08_Manage_Data_Storage/8.13._Versioned_storages/#view-and-edit-content","text":"The view of the versioned storage is similar to regular data storage with some differences: for each file/folder in the storage, the following columns are displayed: Name - file/folder name Size - file size (empty for folders) Revision - latest revision (SHA-1 hash of the latest commit) touched that file/folder Date changed - date and time of the latest commit touched that file/folder Author - user name who performed the latest commit touched that file/folder Message - message of the latest commit touched that file/folder for each file in the storage, there are additional buttons similar to regular storage - to download file to the local workstation, to rename the file and to remove the file from the storage for each folder in the storage, there are additional buttons similar to regular storage - to rename the folder and to remove the folder from the storage RUN button - allows to run the tool with cloning of the current versioned storage into the instance - see details below Generate report button - allows to configure and then download the report of the storage usage (commit history, diffs, etc.) - see details below Show history button - allows to open the panel with commit history info of the current versioned storage or selected folder - see details below settings button (\" gear \" icon) - allows to open the seetings pop-up where: versioned storage can be edited (name and description can be changed) or removed: permissions on the storage can be configured: buttons to create file/folder and to upload data are similar to the corresponding ones in regular storages","title":"View and edit content"},{"location":"manual/08_Manage_Data_Storage/8.13._Versioned_storages/#create-a-folderfile","text":"To create a folder: Click \" + Create \" button in the versioned storage In the list, select the Folder item: The pop-up will appear to specify a new folder name and commit message: Specify a folder name ( mandatory ). The commit message may be omitted. If it is not specified - it will be set automatically. Click OK button to confirm: After the creation, a new revision of the storage will be created (contains only differences with the previous revision). Just-created folder will appear in the storage: To create a file: Click \" + Create \" button in the versioned storage In the list, select the File item: The pop-up will appear to specify a new file name and commit message. Specify a file name ( mandatory ). The commit message may be omitted. If it is not specified - it will be set automatically. Click OK button to confirm, e.g.: After the creation, a new revision of the storage will be created (contains only differences with the previous revision). Just-created file will appear in the storage: Note : during the file creation in the versioned storage, you are not able to specify the file content simultaneously (as it is possible for regular storages) - therefore file is created with the zero-size. To add a file content - edit it. Uploading of the data is similar to regular storages. After the upload, a new revision of the storage is being created. The commit message for uploaded files is being set automatically, e.g.:","title":"Create a folder/file"},{"location":"manual/08_Manage_Data_Storage/8.13._Versioned_storages/#edit-file-content","text":"To edit a text file: Click the file in the versioned storage File preview will be opened on the right. In case of the file is large you will see only its part Click the \" Expand \" icon in the upper right of the preview section, e.g.: Pop-up with the file content will appear: Click the Edit button to enable the edit-mode for the file. Change the file content and click the Save to confirm changes, e.g.: Specify the commit message in the appeared pop-up, e.g.: Click the Commit button to confirm. After saving, a new revision of the storage will be created: Note : binary file can not be edited by the described way. In this case, only download hyperlink is shown for such files instead of the preview, e.g.:","title":"Edit file content"},{"location":"manual/08_Manage_Data_Storage/8.13._Versioned_storages/#rename-a-filefolder","text":"To rename a file/folder: Click the Rename button in the row of the file/folder of the versioned storage In the appeared pop-up, specify a new name and commit message. The commit message may be omitted. If it is not specified - it will be set automatically. Click OK button to confirm: After saving, a new revision of the storage will be created. File/folder will be renamed:","title":"Rename a file/folder"},{"location":"manual/08_Manage_Data_Storage/8.13._Versioned_storages/#remove-a-filefolder","text":"To remove a file/folder: Click the Remove button in the row of the file/folder of the versioned storage In the appeared pop-up, specify a commit message. The commit message may be omitted. If it is not specified - it will be set automatically. Click the Delete button to confirm: After saving, a new revision of the storage will be created. File/folder will be removed Note : removing of the folder also deletes all child files and subfolders.","title":"Remove a file/folder"},{"location":"manual/08_Manage_Data_Storage/8.13._Versioned_storages/#version-control","text":"Due to that versioned storage is a Git repository by the fact, so one of the important advantages of versioned storages in condition with regular object storages - ability to view commit history and all changes that were performed with the data in details.","title":"Version control"},{"location":"manual/08_Manage_Data_Storage/8.13._Versioned_storages/#commit-history-of-the-file","text":"You can view the commit history of the file in the versioned storage - i.e. history of all commits that touched this file. To open the commit history of the file, click it. The commit history will be displayed under the file preview section, e.g.: Commit history contains a list of records. Each record corresponds to one commit that touched the selected file. List of commits is sorted from the last commit to earlier ones. Each record includes: commit message commit SHA-1 hash user name - author of the commit date and time when commit (changes) was performed block of buttons: - Revert button - to revert the content of the current file to the selected commit - Download version button - to view/download revert version of the current file corresponding to the selected commit - Show diffs button - to view diffs between the content of the current file in the selected commit and in the previous commit","title":"Commit history of the file"},{"location":"manual/08_Manage_Data_Storage/8.13._Versioned_storages/#download-a-specific-version","text":"To download a specific version of the file in the versioned storage: Click the file to open its commit history Find the commit Click the Download version button for the commit, e.g.: The file content in the selected commit will be displayed: Click the Download button in the upper side of the pop-up","title":"Download a specific version"},{"location":"manual/08_Manage_Data_Storage/8.13._Versioned_storages/#revert-to-a-specifid-version","text":"To revert to a specific version of the file in the versioned storage: Click the file to open its commit history Find the commit Click the Revert button for the commit, e.g.: Pop-up will appear to specify a commit message - for the operation that will revert the current state of the file to the selected commit. The commit message may be omitted. If it is not specified - it will be set automatically. Click the Revert button to confirm: After the operation: content of the file will be reverted to the selected commit new commit will appear in the commit history Note : Revert button is available for any non-latest commit.","title":"Revert to a specifid version"},{"location":"manual/08_Manage_Data_Storage/8.13._Versioned_storages/#view-diffs","text":"To view differences of a specific file version conditionally to its previous version: Click the file to open its commit history Find the commit Click the Show diffs button for the commit, e.g.: The pop-up will be opened with diffs between file version in the selected commit and in the previous commit: Differences pop-up includes: selected commit's hash file name state of changes that were performed with the file in the commit. Possible variants: ADDED - new file was created CHANGED - existing file was edited DELETED - file was removed RENAMED - file was renamed section with details of changes: new added rows are shown in green color, e.g.: rows that were removed are shown in red color, e.g.: not changed rows or auxiliary info are shown without filling, e.g. changes in a binary file: or new empty text file creation:","title":"View diffs"},{"location":"manual/08_Manage_Data_Storage/8.13._Versioned_storages/#commit-history-of-the-folder","text":"You can view the commit history of any folder in the versioned storage (including the root folder) - i.e. history of all commits that touched files in this folder and in its subfolders. To view the commit history of the folder in the versioned storage: Open the folder Click the \" Show history \" button in the right upper corner. The commit history will appear on the right side, \" Show history \" button will be renamed to \" Hide history \" e.g.: Commit history contains a list of records. Each record corresponds to one commit that touched the current folder's files. List of commits is sorted from the last commit to earlier ones. Each record includes: commit message commit SHA-1 hash user name - author of the commit date and time when commit (changes) was performed Show diffs button - - to view diffs between the content of the folder in the selected commit and in the previous commit. Displaying of diffs is similar to separate files, you can see examples above .","title":"Commit history of the folder"},{"location":"manual/08_Manage_Data_Storage/8.13._Versioned_storages/#filter-commit-history","text":"You may filter the commit history (both for a separate file and for a folder) - for example, to find some specific commits. To open the Filter panel - click the corresponding button in the right upper corner of the commit history section - Filter panel has view: Here, the following filters for the commit history can be configured: by Author - you may select one or several users to filter revisions (commits) by their author(s). By default, commits of all authors are displayed. by Date - you may select period (date From and date To - note that any of the date may be omitted) to filter revisions (commits) by their dates. By default, commits for all periods of the object existence are displayed. by File type - you may specify file type(s) to filter the revisions (commits) list by only the ones that touched files of the specified type(s). File types shall be specified in a plain text format, comma-separated, simple masks are supported. Example: csv,ma*,md . By default, commits for all files are displayed. Once filters are configured, click the Apply button to confirm, e.g.: The commit history will be updated according to the configured filters: To reset (clear) configured filters: Click the Filter button. At the Filter panel, click the Reset button: Filters will be reset. The commit history will be updated.","title":"Filter commit history"},{"location":"manual/08_Manage_Data_Storage/8.13._Versioned_storages/#generate-changes-report","text":"For the versioned storages, there is the ability to generate and download reports. Reports contain commit history of the current folder in the versioned storage and also can include diffs between commits. Such reports can be downloaded in Microsoft Word format ( docx file). To generate a report: Open the folder in the versioned storage. Click the \" Generate report \" button in the right upper corner of the page. \" Generate report \" form will appear: You may configure filters - only commits that match such filters will be included in the report (by default - if no filters are set, all commits will be included). These filters are fully the same as in the Filter panel : by Author - to restrict the list of reported commits by their author(s) by Date - to restrict the list of reported commits by date of their creation by File type - to restrict the list of reported commits according to included file types You may additionally include differences between files in each commit - by set the corresponding checkbox \" Include file diffs \": In this case, you may select how to split changes that will be include to the report: by revision - changes will be split by commits, i.e. from the recent commit to older ones, in each commit all files changes touched by this commit are included by files - changes will be split by all changed files, i.e. for each changed file all commits that touched this file are included additionally, you may set the checkbox \" Save changes separately \": in this case (no matter which split-mode is selected), changes will be saved in separate files - i.e. one report-file per each commit or one report-file per each changed file summary report will be downloaded as an archive Once settings are configured, click the \" Download report \" button. Report will be downloaded automatically: Example of the part of the report without included diffs: Example of the part of the report with included diffs (changes split by commits ): In the shown commit, one file was changed. Example of the part of the report with included diffs (changes split by files ): Shown file was changed in 3 commits. Report itself is being generated from the docx -format template. Platform has the \"base\" template but system admin has the ability to load own template and replace the \"base\" one. Template contains plain text and a set of predefined keywords that are replaced by actual values of the current versioned storage's data during the report generation according to the configured report filters. By this way, report view, its text blocks, tables, fonts, formats, etc. can be preconfigured for the customer needs.","title":"Generate changes report"},{"location":"manual/08_Manage_Data_Storage/8.13._Versioned_storages/#git-operations-with-versioned-storages","text":"Users have the ability to launch runs using versioned storages. Users have the access to the version storages during the run - similarly as to the folder of any mounted object storage. Users have the ability to read and write data (according to the permissions) to the version storages from inside the runs. But versioned storages have some differences to the regular object storages used in runs - especially in question of data saving and resolving conflicts. To manage versioning operations from active runs, there are additional controls: at the main Dashboard, in the \"Active runs\" panel, it is VCS (Version Control Storages) item - it is available when hovering any fully initialized run's tile, e.g.: at the \"Run logs\" page, it is VERSIONED STORAGE item - it is available for any fully initialized run, e.g.: Both these items are identical to each other. In our examples below we will use the VCS control from the main Dashboard, in generally. By click such items, additional menu appears - its content may vary in different cases. Let's view main scenarios of using versioned storages in running jobs.","title":"GIT operations with versioned storages"},{"location":"manual/08_Manage_Data_Storage/8.13._Versioned_storages/#load-versioned-storage-content-to-the-instance","text":"There are two possible ways to load versioned storage content to the running instance: launch the run from the versioned storage page with the automatic content cloning during the run initialization clone the versioned storage content to the already running instance","title":"Load versioned storage content to the instance"},{"location":"manual/08_Manage_Data_Storage/8.13._Versioned_storages/#launch-a-run-with-the-content-cloning","text":"In this case, content of the selected versioned storage is being cloned during the run initialization. To launch a run from the versioned storage page: Open the versioned storage page. Click the RUN button in the right upper corner of the page: The pop-up will appear: Here, the list of tools is shown. By default, this list contains tools from the user personal group . Click the tool you wish to run with the versioned storage, e.g.: If needed, you may select the tool version from the appeared dropdown list in the tool tile. If you want to customize the tool settings (or you do not have the desired tool in the list) - you may configure the tool for the launch manually: click the Run custom button in the left bottom corner of the pop-up: the Launch form will appear - similar to the one that appears before the pipeline launch : you may specify options as you want Click the Launch button to confirm the run. Run of the selected tool will be launched: Selected versioned storage content will be automatically cloned into the running instance (during the run initialization) - by the path /versioned-data/{versioned_storage_name} . Once the run is fully initialized, you may open the SSH terminal for the launched run and check that the versioned storage content was cloned: Or via the filesystem browser : Click the VERSIONED STORAGE item at the \"Run logs\" page - cloned versioned storage will be displayed with its actions list:","title":"Launch a run with the content cloning"},{"location":"manual/08_Manage_Data_Storage/8.13._Versioned_storages/#clone-content-to-the-already-running-instance","text":"You may clone the versioned storage content to the already running instance: At the main Dashboard, hover over the active run's tile to which instance you want to clone the version storage content. Please note, that run shall be fully initialized. Click the VCS control. In the appeared menu, click the Clone item, e.g.: The pop-up will appear: Here, the full list of available versioned storages is shown. Click the storage you want to clone to the current run, e.g.: In the row of the selected storage, the field will appear where you may select the specific commit of the versioned storage you want to clone: By default, the recent commit is selected. Click the SELECT button to confirm. Selected versioned storage content will be cloned to the running instance: Click the VCS control for the run - cloned versioned storage will be displayed with its actions list: Also as shown in the section above, cloned versioned storage content is available in the running instance by the path /versioned-data/{versioned_storage_name} . You may clone any count of versioned storages to the running instance by the described way. For example: Click the VCS control for the run with the previously cloned storage: In the appeared pop-up, select a versioned storage to clone, e.g.: If there are versioned storages previously cloned to that run - they are also displayed in the list, but can not be selected repeatedly. After confirmation, all cloned versioned storages will be displayed in the list by click the VCS control: And all cloned versioned storages will be available by the path /versioned-data/{versioned_storage_name} :","title":"Clone content to the already running instance"},{"location":"manual/08_Manage_Data_Storage/8.13._Versioned_storages/#view-diffs-of-changed-cloned-content","text":"At any point after the versioned storage was cloned to the running instance, you can request the diffs for that storage. These diffs will show all changes made by the current user in files of cloned versioned storage - i.e. differences between commit of the versioned storage that was cloned to the running instance and the current state of files in that cloned folder. To view diffs from the running instance with the cloned versioned storage: At the main Dashboard, hover over the active run's tile to which instance the versioned storage was cloned. Click the VCS control. In the appeared menu, select the versioned storage (if there are several ones) and click the Diffs item for it, e.g.: Please note, that if there are no changes in the cloned versioned storage were made - this item will be visible in the menu but disabled. The diffs pop-up will be opened, e.g.: Here, the full list of your changes in files in comparing with the cloned version of the selected versioned storage is shown. Changes are split by files. Displaying of diffs for each file is similar to the described in one of the section above .","title":"View diffs of changed cloned content"},{"location":"manual/08_Manage_Data_Storage/8.13._Versioned_storages/#save-changed-content-from-the-run-to-the-versioned-storage","text":"At any point after the versioned storage was cloned to the running instance, you can \"save\" changes made in the cloned content to the origin versioned storage. Please note, if you will just stop the run with the cloned versioned storage without \"saving\" operation - all changes you performed in the cloned folder of the versioned storage will dissapear without any posibilities to restore the data. To perform the \"save\" operation from the running instance with the cloned versioned storage: At the main Dashboard, hover over the active run's tile to which instance the versioned storage was cloned. Click the VCS control. In the appeared menu, select the versioned storage (if there are several ones) and click the Save item for it, e.g.: Please note, that if there are no changes in the cloned versioned storage were made - this item will be visible in the menu but disabled. The pop-up will be opened, e.g.: Specify a commit message. View details of performed changes - here, the full list of your changes in files in comparing with the cloned version of the selected versioned storage is shown (similar to Diffs option). You may expand details by click the file name. Displaying of diffs for each file is similar to the described in one of the section above : By default, all changed files from the cloned versioned storage are included to the preparing commit (\"Save\" operation). But you can select only files you want to include to the commit - by the checkboxes near the file names, e.g.: Or via the buttons above the list: Select all - to include all changed files to the preparing commit Clear selection - to exclude all changed files from the preparing commit Please note, if you will exclude some file(s) from the preparing commit, you can include these files in another commit later. Once the commit message is specified and files to commit are defined - click the Commit button to confirm, e.g.: The system tries to perform corresponding Git actions to origin versioned storage. Changed files will be \"saved\" to the origin versioned storage: You can check it from the versioned storage page: Changed files that were not included in the commit are still shown in Diffs : And can be \"saved\" in another commit:","title":"Save changed content from the run to the versioned storage"},{"location":"manual/08_Manage_Data_Storage/8.13._Versioned_storages/#save-changed-content-with-the-conflict-resolving","text":"There may be cases, when any conflict is detected during the \"save\" operation attempt. Conflict may appears, for example, when you have cloned versioned storage to the run, made some changes and another user during this period of time made own changes to the same file(s) as in your current run and managed to \"save\" these changes (perform commit) to the origin versioned storage. Let's imagine, that there are two users (you and somebody else) that both cloned the same versioned storage to their runs and perform changes in the same files, touching the same parts of these files. Herewith, the second user managed to \"save\" these changes (performed a commit to the origin versioned storage). So, when you will try to \"save\" changes by steps described above : The pop-up before the saving will have view like: Once the Commit button is clicked, the conflict error will occur and \" Resolve conflicts \" pop-up will appear: This pop-up contains: list of conflicting files (left panel) - please note that changed non-conflicting files are not shown here, they will be saved without additional resolving 3 columns with the content of conflicting versions of the selected file: \" Your changes \" (left) - column shows changed state of the conflicting file (current state of the file in your run that you have tried to save) \" Changes from remote \" (right) - column shows the state of the conflicting file from the most recent revision of that file. In our example - it is changed file that second user has saved to the origin versioned storage \" Result \" (center) - column shows the merging state (result) of the conflict resolving. When you will resolve the conflict, the version that will be saved will be shown here all changes are highlighted: conflicting changes are in red color, i.e. in both versions the same part of the file is touched: non-conflicting changes (changes that are not touched the same parts): new added parts and places where they mya be inserted are in green color e.g.: changed existing parts are in yellow color e.g.: buttons to revert/repeat performed actions during the conflict resolving: buttons to apply only non-conflicting changes as a batch: - to apply all non-conflicting changes from your changed version of the file. They will be \"copied\" to the central column, i.e. to the version that will be saved after the conflict resolving - to apply all non-conflicting changes from the updated current version of the file in the origin versioned storage (i.e. in our example, it is the version saved by the second user). Changes will be \"copied\" to the central column, i.e. to the version that will be saved after the conflict resolving - to apply all non-conflicting changes from both file versions - yours, prepared for the saving, and theirs, from the updated origin versioned storage buttons to accept one of the full versions of the conflicting file as a conflict resolve: - to accept all changes ( conflicting and non-conflicting ) fully from your version of the file - to accept all changes ( conflicting and non-conflicting ) fully from the updated current version of the file in the origin versioned storage (i.e. in our example, it is the version saved by the second user) You shall resolve all changes in each conflicting file. You may resolve any change/conflict manually or use buttons for batch resolving that described in the previous item. For manually resolving, there are abilities for each change/conflict in the file: to accept it - by the arrows icon near the change, e.g. - by this action, the corresponding change will be \"copied\" to the central column, highlighting of the change will disappear to decline it - by the cross icon near the change, e.g. - by this action, the corresponding change will be declined, i.e. will be \"ignored\" in the central column, highlighting of the change will also disappear to change the text manually (e.g., if both versions are not satisfied) - for that, set the cursor to the change highlighting area in the central column and specify a new text Let's view different options: manually accept non-conflicting change from your version by click the arrows icon: Changed part from your version will appear in the central column, highlighting of the change will disappear: batch-apply of all non-conflicting changes from the origin versioned storage (saved by the second user): All non-conflicting changes from remote version will appear in the central column, their highlighting will disappear: manually decline conflicting change from your version: Change from your version will be declined, highlighting of the change will disappear: click the conflict area in the central column: Specify a new text as you want, e.g.: to finish resolving, decline conflicting change from the remote version: Once all changes and conflicts in the selected file are resolved, the corresponding info will appear in the upper side of the pop-up and in the files list: The version of the file displayed in the central column ( Result ) will be saved in the origin versioned storage. To proceed the saving process, all conflicts in all files shall be resolved. Therefore, open one-by-one each conflicting file in the list and resolve changes using instructions described above: In case of conflicting binary files, there is no ability to resolve each change: You shall only select which version to accept for the saving - your changed version of this file or updated version from the origin versioned storage (i.e. in our example, it is the version saved by the second user). Once all conflicts in all files are resolved, the button Resolve becomes available: Click it to finish the saving process. Corresponding messages will appear: You can check that changes were saved - from the versioned storage page:","title":"Save changed content with the conflict resolving"},{"location":"manual/08_Manage_Data_Storage/8.13._Versioned_storages/#update-cloned-content-in-the-run","text":"At any point after the versioned storage was cloned to the running instance, you can update the cloned content in the running instance: \"refresh\" cloned content up to the latest revision (most recent) of the versioned storage. It may be useful as the origin content of the versioned storage could be changed by other users during your run. also, you can change the revision to any non-latest one - just to \"view\" the state of the versioned storage in a certain revision.","title":"Update cloned content in the run"},{"location":"manual/08_Manage_Data_Storage/8.13._Versioned_storages/#refresh-cloned-content","text":"To \"refresh\" cloned content up to the latest revision (most recent): At the main Dashboard, hover over the active run's tile to which instance the versioned storage was cloned. Click the VCS control. In the appeared menu, select the versioned storage (if there are several ones) and click the Refresh item for it, e.g.: The corresponding message will appear: Once the refresh is performed, confirmation will appear: Please note, that if you made changes in cloned files and did not save them, then 2 scenario are possible: if there are no conflicts between your changed unsaved files and files that were updated in the origin versioned storage - in such case: revision version for the cloned content will be updated to the latest without issues changed unsaved files in the cloned content will remain in their state, you may proceed work with them and then save, for example if there are conflicts between your changed unsaved files and files that were updated in the origin versioned storage - in such case: the error of the refresh unable will occur: the pop-up will appear to resolve the conflicts revision version for the cloned content will not be updated till you will not resolve the conflict as described in the section above . When you will resolve conflicts, new merge commit will be created and your cloned content will be update to this version (latest), previously conflicted changes will be saved how they were resolved.","title":"Refresh cloned content"},{"location":"manual/08_Manage_Data_Storage/8.13._Versioned_storages/#change-the-revision-for-the-content","text":"To change the revision of the cloned content to some non-latest one: At the main Dashboard, hover over the active run's tile to which instance the versioned storage was cloned. Click the VCS control. In the appeared menu, select the versioned storage (if there are several ones) and click the Checkout revision item for it, e.g.: Pop-up with the dropdown list of all storage's revisions will appear: Click the dropdown list and select the revision, e.g.: Note : current revision of the storage content is specified with the bold font. To confirm the checkout, click the \" Change Revision \" button: The corresponding message will appear: Please note, if you have changed the revision to any non-latest one and then change some files, these new changes can not be saved by the way described in the Save section, the corresponding item in the VSC menu will be disabled: In this case, to save made changes you may: refresh the content revision up to the latest version and then save changes or change the revision as described in the current section but to the latest one and then save changes Additionally note, if the revision of the cloned content is the latest, and you make any changes of the content and then change the revision to any non-latest - your unsaved changes will be lost. To avoid this, at the beginning, save changes and only then change the revision to non-latest.","title":"Change the revision for the content"},{"location":"manual/08_Manage_Data_Storage/8.14._Omics_storages/","text":"8.14. Omics storages Reference store Create reference store View and edit content of reference store Import reference Delete reference Sequence store Create sequence store View and edit content of sequence store Import sequence Upload sequence Download sequence Delete sequence Manage store Manage via the CLI Usage Cloud Pipeline supports AWS HealthOmics Storages . These are specialized storages that allow to store Omics data. There are two types of AWS HealthOmics Storages: reference store - for storing genome references sequence store - for storing genomics files (e.g., BAM, CRAM, FASTQ) Depending on the store type, management and abilities will vary. To create a Storage in a Folder you need to have WRITE permission for that folder and the ROLE_STORAGE_MANAGER role. For more information see 13. Permissions . Reference store Specialized storage for storing raw genome references. Create reference store Note : please note that for each Cloud Region, only one reference store can be created. To create reference store: Navigate to the folder where you want to create reference storage. Click + Create \u2192 Storages \u2192 Create AWS HealthOmics Store : The pop-up of the AWS HealthOmics Storage creation will appear: Specify a name of the creating store and select the service type as Reference store : Specify a description if necessary. Click the Create button to confirm. Reference store will appear in the folder: View and edit content of reference store Reference store has a flat structure: on the first level, there are only folders, where each folder presents a reference on the second level (inside the folder of each reference), there are only files of the reference itself and its index nested folders are not supported Inside the reference folder: In the row of the reference file/its folder, there is a set of labels, e.g.: reference name state of the file type of the content ( REFERENCE ) Possible actions in a reference store: Import new reference - load a reference from the data storage Download reference - download reference files to the local workstation Delete reference Import reference Note : to load a reference into a reference store, it shall be previously loaded to the s3 bucket available on the Cloud Pipeline Platform. To load a reference: Open the reference store. Click the Import button: Import form will appear: Specify mandatory fields: Name - reference genome name Subject id - source's subject ID Sample id - source's sample ID If necessary, specify optional fields: Description - reference description Generated from - reference details Select the reference source file: click the folder icon near the Source file label: the pop-up to select a file from a data storage will appear: select a reference file in one of the regular storages available in the Cloud Pipeline Platform, e.g.: click the OK button to confirm selection selected file will be shown near the Source file label: Once all fields are specified, click the Import button: Attributes panel with the section of import jobs will be opened automatically on the right side: At this panel, you can check the state of the file import jobs: set dates ( From and To ) and desired state (select from the list) click the Search button, results will be shown as the job IDs list, e.g. to find newly completed jobs: When the import is already completed, reference will appear in the storage: Click the reference, to display reference files: As you can see, reference folder contains the reference file itself ( source ) and automatically created index ( index ). Note : to show/hide import jobs section, you may use the special menu in the upper side of the store: Delete reference Note : you may remove only the reference entirely, separate reference files can not be removed. To remove a reference: Click the Delete button in a reference row, e.g.: Confirm the deletion in the appeared pop-up: Reference will be permanently removed. Sequence store Specialized storage for storing different types of genomics files - currently, these are BAM, CRAM, UBAM, FASTQ. Create sequence store To create sequence store: Navigate to the folder where you want to create sequence storage. Click + Create \u2192 Storages \u2192 Create AWS HealthOmics Store : The pop-up of the AWS HealthOmics Storage creation will appear: Specify a name of the creating store and select the service type as Sequence store : Specify a description if necessary, then click the Create button to confirm: Sequence store will appear in the folder: View and edit content of sequence store Sequence store has a flat structure: on the first level, there are only folders, where each folder presents a separate genomic sequence on the second level (inside the folder of each sequence), there are only sequence genomic files - depending on the format, these can be one (for example, a single UBAM file) or two files (for example, a BAM file and its index) nested folders are not supported Inside the sequence folder, there are sequence files: In the row of the sequence file/folder, there is a set of labels, e.g.: sequence name state of the file type of the content (e.g. FASTQ ) sample id subject id Possible actions in a sequence store: Import new sequence - load a sequence from the data storage Upload sequence - upload a sequence from the local workstation Download sequence - download a sequence to the local workstation Delete sequence Import sequence Note : to load a sequence into a sequence store, it shall be previously loaded to the s3 bucket available on the Cloud Pipeline Platform. To load a sequence: Open the sequence store. Click the Import button: Import form will appear: Specify mandatory fields: Name - sequence name Subject id - source's subject ID Sample id - source's sample ID If necessary, specify optional fields: Description - sequence description Generated from - sequence details From the corresponding dropdown list, select the type of a source file you want to load, e.g.: Select the sequence source file: click the folder icon near the Source file label: the pop-up to select a file from a data storage will appear: select a sequence file in one of the regular storages available in the Cloud Pipeline Platform, e.g.: click the OK button to confirm selection selected file will be shown near the Source file label: Additional field for a second source file will appear: You may add such additional file similarly as described at the previous step, e.g.: If necessary, you may link a sequence with a reference from the reference store: click the folder icon near the Reference path label: in the appeared pop-up, select a reference from the reference store, e.g.: click the OK button to confirm selection Once all fields are specified, click the Import button: Attributes panel with the section of import jobs will be opened automatically on the right side: At this panel, you can check the state of the file import jobs: set dates ( From and To ) and desired state (select from the list) click the Search button, results will be shown as the job IDs list, e.g. to find newly completed jobs: When the import is already completed, sequence will appear in the storage: Click the sequence, to display sequence files: As you can see, sequence folder contains files named by the format: source plus the index. Note : to show/hide import jobs section, you may use the special menu in the upper side of the store: Upload sequence To upload a sequence: Open the sequence store. Click the Upload button: Upload form will appear: Select the type of a source file you want to upload, e.g.: Select the sequence source file from your local workstation: click the Upload source file button: the OS pop-up to select a file will appear choose a sequence file and confirm, e.g.: selected file will be shown near the Upload source file button: Additional button for a second source file will appear: You may add such additional file similarly as described at the previous step, e.g.: If necessary, you may link a sequence with a reference from the reference store: click the folder icon near the Reference label: in the appeared pop-up, select a reference from the reference store, e.g.: click the OK button to confirm selection Specify mandatory fields: Name - sequence name Sample id - source's sample ID Subject id - source's subject ID If necessary, specify optional fields: Description - sequence description Generated from - sequence details Once all fields are specified, click the Upload button: The upload will take some time: Then, the pop-up will be automatically closed, sequence will appear in the storage: Please note, that the upload may take extra time - during this period state of the sequence will be shown as PROCESSING_UPLOAD , download button will not be shown: When the upload is already completed, sequence state will change to ACTIVE and download button near will appear: Download sequence To download at once all files of the sequence to the local workstation: Click the Download button in a sequence row. Download will be start automatically (please note that files will be loaded separately, not as an archive): To download specific sequence file to the local workstation: Open the sequence folder. Click the Download button in a row of the sequence file. Download of the selected file will be start automatically: Delete sequence Note : you may remove only the sequence entirely, separate sequence files can not be removed. To remove a sequence: Click the Delete button in a sequence row, e.g.: Confirm the deletion in the appeared pop-up: Sequence will be permanently removed. Manage store To edit reference or sequence store: Click the gear icon in the right upper corner of the store page. The settings pop-up will appear, e.g.: Here, you can: edit store alias and description - specify new value(s) and click the Save button grant store permissions - for more details see the section 13. Permissions delete a store - for more details the section 8.5. Delete Data Storage Manage via the CLI You can also manage AWS HealthOmics Storages and their data via CLI . Currently, the following CLI functionality is supported for AWS HealthOmics Storages: store creation - using pipe storage create command: Note : to specify the storage type during the creation (with -t | --type option) use the following values - AWS_OMICS_REF (for reference store ) and AWS_OMICS_SEQ (for sequence store ). store listing - using pipe storage ls command: Note : to specify the storage path, use omics Cloud prefix. moving store to another folder - using pipe storage mvtodir command: store deletion - using pipe storage delete command: genomic data uploading - using pipe storage cp <source> <destination> command, where: <source> - path to genomic data file(s) from the existing data storage registered in the Cloud Pipeline platform or from your local workstation <destination> - AWS HealthOmics Storage path Note : for loading genomic data, you shall use the option -a | --additional-options to specify arguments of the data file: name - genomic file name subject_id - source's subject ID sample_id - source's sample ID file_type - type of the loading file download genomic data - using pipe storage cp <source> <destination> command, where: <source> - path to genomic data in AWS HealthOmics Storage (folder or file) <destination> - path on your local workstation Usage AWS HealthOmics Storages can not be mounted to the running instances as regular storages. But these storages (and their data) can be used via pipeline's parameters - for more details see the corresponding section . You may use data from AWS HealthOmics Storages in 2 types of parameters: Path parameter : select the corresponding parameter type from the list: click the folder icon in the appeared parameter field: in the pop-up, select a reference or sequence store, e.g.: then, you may select specific genomic data or the whole AWS HealthOmics Store: an example of the whole selected bucket as the path parameter value: Input parameter : select the corresponding parameter type from the list: click the input icon in the appeared parameter field: in the pop-up, select a reference or sequence store, e.g.: then, you may select specific genomic data, e.g.: an example of the selected reference as the input path parameter's value:","title":"8.14. Omics storages"},{"location":"manual/08_Manage_Data_Storage/8.14._Omics_storages/#814-omics-storages","text":"Reference store Create reference store View and edit content of reference store Import reference Delete reference Sequence store Create sequence store View and edit content of sequence store Import sequence Upload sequence Download sequence Delete sequence Manage store Manage via the CLI Usage Cloud Pipeline supports AWS HealthOmics Storages . These are specialized storages that allow to store Omics data. There are two types of AWS HealthOmics Storages: reference store - for storing genome references sequence store - for storing genomics files (e.g., BAM, CRAM, FASTQ) Depending on the store type, management and abilities will vary. To create a Storage in a Folder you need to have WRITE permission for that folder and the ROLE_STORAGE_MANAGER role. For more information see 13. Permissions .","title":"8.14. Omics storages"},{"location":"manual/08_Manage_Data_Storage/8.14._Omics_storages/#reference-store","text":"Specialized storage for storing raw genome references.","title":"Reference store"},{"location":"manual/08_Manage_Data_Storage/8.14._Omics_storages/#create-reference-store","text":"Note : please note that for each Cloud Region, only one reference store can be created. To create reference store: Navigate to the folder where you want to create reference storage. Click + Create \u2192 Storages \u2192 Create AWS HealthOmics Store : The pop-up of the AWS HealthOmics Storage creation will appear: Specify a name of the creating store and select the service type as Reference store : Specify a description if necessary. Click the Create button to confirm. Reference store will appear in the folder:","title":"Create reference store"},{"location":"manual/08_Manage_Data_Storage/8.14._Omics_storages/#view-and-edit-content-of-reference-store","text":"Reference store has a flat structure: on the first level, there are only folders, where each folder presents a reference on the second level (inside the folder of each reference), there are only files of the reference itself and its index nested folders are not supported Inside the reference folder: In the row of the reference file/its folder, there is a set of labels, e.g.: reference name state of the file type of the content ( REFERENCE ) Possible actions in a reference store: Import new reference - load a reference from the data storage Download reference - download reference files to the local workstation Delete reference","title":"View and edit content of reference store"},{"location":"manual/08_Manage_Data_Storage/8.14._Omics_storages/#import-reference","text":"Note : to load a reference into a reference store, it shall be previously loaded to the s3 bucket available on the Cloud Pipeline Platform. To load a reference: Open the reference store. Click the Import button: Import form will appear: Specify mandatory fields: Name - reference genome name Subject id - source's subject ID Sample id - source's sample ID If necessary, specify optional fields: Description - reference description Generated from - reference details Select the reference source file: click the folder icon near the Source file label: the pop-up to select a file from a data storage will appear: select a reference file in one of the regular storages available in the Cloud Pipeline Platform, e.g.: click the OK button to confirm selection selected file will be shown near the Source file label: Once all fields are specified, click the Import button: Attributes panel with the section of import jobs will be opened automatically on the right side: At this panel, you can check the state of the file import jobs: set dates ( From and To ) and desired state (select from the list) click the Search button, results will be shown as the job IDs list, e.g. to find newly completed jobs: When the import is already completed, reference will appear in the storage: Click the reference, to display reference files: As you can see, reference folder contains the reference file itself ( source ) and automatically created index ( index ). Note : to show/hide import jobs section, you may use the special menu in the upper side of the store:","title":"Import reference"},{"location":"manual/08_Manage_Data_Storage/8.14._Omics_storages/#delete-reference","text":"Note : you may remove only the reference entirely, separate reference files can not be removed. To remove a reference: Click the Delete button in a reference row, e.g.: Confirm the deletion in the appeared pop-up: Reference will be permanently removed.","title":"Delete reference"},{"location":"manual/08_Manage_Data_Storage/8.14._Omics_storages/#sequence-store","text":"Specialized storage for storing different types of genomics files - currently, these are BAM, CRAM, UBAM, FASTQ.","title":"Sequence store"},{"location":"manual/08_Manage_Data_Storage/8.14._Omics_storages/#create-sequence-store","text":"To create sequence store: Navigate to the folder where you want to create sequence storage. Click + Create \u2192 Storages \u2192 Create AWS HealthOmics Store : The pop-up of the AWS HealthOmics Storage creation will appear: Specify a name of the creating store and select the service type as Sequence store : Specify a description if necessary, then click the Create button to confirm: Sequence store will appear in the folder:","title":"Create sequence store"},{"location":"manual/08_Manage_Data_Storage/8.14._Omics_storages/#view-and-edit-content-of-sequence-store","text":"Sequence store has a flat structure: on the first level, there are only folders, where each folder presents a separate genomic sequence on the second level (inside the folder of each sequence), there are only sequence genomic files - depending on the format, these can be one (for example, a single UBAM file) or two files (for example, a BAM file and its index) nested folders are not supported Inside the sequence folder, there are sequence files: In the row of the sequence file/folder, there is a set of labels, e.g.: sequence name state of the file type of the content (e.g. FASTQ ) sample id subject id Possible actions in a sequence store: Import new sequence - load a sequence from the data storage Upload sequence - upload a sequence from the local workstation Download sequence - download a sequence to the local workstation Delete sequence","title":"View and edit content of sequence store"},{"location":"manual/08_Manage_Data_Storage/8.14._Omics_storages/#import-sequence","text":"Note : to load a sequence into a sequence store, it shall be previously loaded to the s3 bucket available on the Cloud Pipeline Platform. To load a sequence: Open the sequence store. Click the Import button: Import form will appear: Specify mandatory fields: Name - sequence name Subject id - source's subject ID Sample id - source's sample ID If necessary, specify optional fields: Description - sequence description Generated from - sequence details From the corresponding dropdown list, select the type of a source file you want to load, e.g.: Select the sequence source file: click the folder icon near the Source file label: the pop-up to select a file from a data storage will appear: select a sequence file in one of the regular storages available in the Cloud Pipeline Platform, e.g.: click the OK button to confirm selection selected file will be shown near the Source file label: Additional field for a second source file will appear: You may add such additional file similarly as described at the previous step, e.g.: If necessary, you may link a sequence with a reference from the reference store: click the folder icon near the Reference path label: in the appeared pop-up, select a reference from the reference store, e.g.: click the OK button to confirm selection Once all fields are specified, click the Import button: Attributes panel with the section of import jobs will be opened automatically on the right side: At this panel, you can check the state of the file import jobs: set dates ( From and To ) and desired state (select from the list) click the Search button, results will be shown as the job IDs list, e.g. to find newly completed jobs: When the import is already completed, sequence will appear in the storage: Click the sequence, to display sequence files: As you can see, sequence folder contains files named by the format: source plus the index. Note : to show/hide import jobs section, you may use the special menu in the upper side of the store:","title":"Import sequence"},{"location":"manual/08_Manage_Data_Storage/8.14._Omics_storages/#upload-sequence","text":"To upload a sequence: Open the sequence store. Click the Upload button: Upload form will appear: Select the type of a source file you want to upload, e.g.: Select the sequence source file from your local workstation: click the Upload source file button: the OS pop-up to select a file will appear choose a sequence file and confirm, e.g.: selected file will be shown near the Upload source file button: Additional button for a second source file will appear: You may add such additional file similarly as described at the previous step, e.g.: If necessary, you may link a sequence with a reference from the reference store: click the folder icon near the Reference label: in the appeared pop-up, select a reference from the reference store, e.g.: click the OK button to confirm selection Specify mandatory fields: Name - sequence name Sample id - source's sample ID Subject id - source's subject ID If necessary, specify optional fields: Description - sequence description Generated from - sequence details Once all fields are specified, click the Upload button: The upload will take some time: Then, the pop-up will be automatically closed, sequence will appear in the storage: Please note, that the upload may take extra time - during this period state of the sequence will be shown as PROCESSING_UPLOAD , download button will not be shown: When the upload is already completed, sequence state will change to ACTIVE and download button near will appear:","title":"Upload sequence"},{"location":"manual/08_Manage_Data_Storage/8.14._Omics_storages/#download-sequence","text":"To download at once all files of the sequence to the local workstation: Click the Download button in a sequence row. Download will be start automatically (please note that files will be loaded separately, not as an archive): To download specific sequence file to the local workstation: Open the sequence folder. Click the Download button in a row of the sequence file. Download of the selected file will be start automatically:","title":"Download sequence"},{"location":"manual/08_Manage_Data_Storage/8.14._Omics_storages/#delete-sequence","text":"Note : you may remove only the sequence entirely, separate sequence files can not be removed. To remove a sequence: Click the Delete button in a sequence row, e.g.: Confirm the deletion in the appeared pop-up: Sequence will be permanently removed.","title":"Delete sequence"},{"location":"manual/08_Manage_Data_Storage/8.14._Omics_storages/#manage-store","text":"To edit reference or sequence store: Click the gear icon in the right upper corner of the store page. The settings pop-up will appear, e.g.: Here, you can: edit store alias and description - specify new value(s) and click the Save button grant store permissions - for more details see the section 13. Permissions delete a store - for more details the section 8.5. Delete Data Storage","title":"Manage store"},{"location":"manual/08_Manage_Data_Storage/8.14._Omics_storages/#manage-via-the-cli","text":"You can also manage AWS HealthOmics Storages and their data via CLI . Currently, the following CLI functionality is supported for AWS HealthOmics Storages: store creation - using pipe storage create command: Note : to specify the storage type during the creation (with -t | --type option) use the following values - AWS_OMICS_REF (for reference store ) and AWS_OMICS_SEQ (for sequence store ). store listing - using pipe storage ls command: Note : to specify the storage path, use omics Cloud prefix. moving store to another folder - using pipe storage mvtodir command: store deletion - using pipe storage delete command: genomic data uploading - using pipe storage cp <source> <destination> command, where: <source> - path to genomic data file(s) from the existing data storage registered in the Cloud Pipeline platform or from your local workstation <destination> - AWS HealthOmics Storage path Note : for loading genomic data, you shall use the option -a | --additional-options to specify arguments of the data file: name - genomic file name subject_id - source's subject ID sample_id - source's sample ID file_type - type of the loading file download genomic data - using pipe storage cp <source> <destination> command, where: <source> - path to genomic data in AWS HealthOmics Storage (folder or file) <destination> - path on your local workstation","title":"Manage via the CLI"},{"location":"manual/08_Manage_Data_Storage/8.14._Omics_storages/#usage","text":"AWS HealthOmics Storages can not be mounted to the running instances as regular storages. But these storages (and their data) can be used via pipeline's parameters - for more details see the corresponding section . You may use data from AWS HealthOmics Storages in 2 types of parameters: Path parameter : select the corresponding parameter type from the list: click the folder icon in the appeared parameter field: in the pop-up, select a reference or sequence store, e.g.: then, you may select specific genomic data or the whole AWS HealthOmics Store: an example of the whole selected bucket as the path parameter value: Input parameter : select the corresponding parameter type from the list: click the input icon in the appeared parameter field: in the pop-up, select a reference or sequence store, e.g.: then, you may select specific genomic data, e.g.: an example of the selected reference as the input path parameter's value:","title":"Usage"},{"location":"manual/08_Manage_Data_Storage/8.2._Upload_Download_data/","text":"8.2. Upload/Download data Upload data Download data Generate URL To edit a Storage you need to have WRITE permission for the Storage . For more information see 13. Permissions . You also can upload and download data via CLI. See 14.3. Manage Storage via CLI . Upload data Click Upload button in the storage and folder of your choice: Browse file(s) to upload. Note : make sure size doesn't exceed 5 Gb. Note : you can cancel upload process by clicking the \"Cancel\" button. As a result, the file will be uploaded to the CP system. Note: the uploaded file will be tagged with auto-created attribute: CP_OWNER . The value of the attribute will be set as a user ID. The exception is that the storage is based on FS mount. Files in such data storage don't have attributes at all. Download data Click the Download button next to a file name. Specify where to download in the pop-up window. As a result, the file will be downloaded via your browser to the specified location. Generate URL You can use this to generate URLs for a number of files and then download them manually one by one or via scripts. Select files using a checkbox. Click the Generate URL button. A list of URLs (one for each file) will be generated.","title":"8.2. Upload/Download data"},{"location":"manual/08_Manage_Data_Storage/8.2._Upload_Download_data/#82-uploaddownload-data","text":"Upload data Download data Generate URL To edit a Storage you need to have WRITE permission for the Storage . For more information see 13. Permissions . You also can upload and download data via CLI. See 14.3. Manage Storage via CLI .","title":"8.2. Upload/Download data"},{"location":"manual/08_Manage_Data_Storage/8.2._Upload_Download_data/#upload-data","text":"Click Upload button in the storage and folder of your choice: Browse file(s) to upload. Note : make sure size doesn't exceed 5 Gb. Note : you can cancel upload process by clicking the \"Cancel\" button. As a result, the file will be uploaded to the CP system. Note: the uploaded file will be tagged with auto-created attribute: CP_OWNER . The value of the attribute will be set as a user ID. The exception is that the storage is based on FS mount. Files in such data storage don't have attributes at all.","title":"Upload data"},{"location":"manual/08_Manage_Data_Storage/8.2._Upload_Download_data/#download-data","text":"Click the Download button next to a file name. Specify where to download in the pop-up window. As a result, the file will be downloaded via your browser to the specified location.","title":"Download data"},{"location":"manual/08_Manage_Data_Storage/8.2._Upload_Download_data/#generate-url","text":"You can use this to generate URLs for a number of files and then download them manually one by one or via scripts. Select files using a checkbox. Click the Generate URL button. A list of URLs (one for each file) will be generated.","title":"Generate URL"},{"location":"manual/08_Manage_Data_Storage/8.3._Create_and_Edit_text_files/","text":"8.3. Create and Edit text files You can create and edit files via GUI. It may be useful when you add some description or metadata about files in the storage. Create and rename a file View and edit text file's contents View and edit tabular file's contents To edit a Storage you need to have WRITE permission for the Storage . For more information see 13. Permissions . Create and rename a file To create a file: Click + Create \u2192 File . Enter file's name. Enter file's contents (optionally). Click OK . As a result, a new file will be created. Note: the new file will be tagged with auto-created attribute: CP_OWNER . The value of the attribute will be set as a user ID. The exception is that the storage is based on FS mount. Files in such data storage don't have attributes at all. To rename a file: Click the button in the desired file line. Rename it. Click OK . View and edit text file's contents Click on the file. You will see file preview on the right. In case of the file is large you will see only a part of it. Click \"Expand\" icon at the upper right of the preview window. A pop-up text editor window appears. Click Edit and change the file. Click Save to save the changes. View and edit tabular file's contents Click on the tabular file. You will see file preview on the right. In case of the file is large you will see only a part of it. Click \"Expand\" icon at the upper right of the preview window. A pop-up text editor window appears. Click Edit and change file in tabular view or in the text view. Click Save to save the changes.","title":"8.3. Create and edit text files"},{"location":"manual/08_Manage_Data_Storage/8.3._Create_and_Edit_text_files/#83-create-and-edit-text-files","text":"You can create and edit files via GUI. It may be useful when you add some description or metadata about files in the storage. Create and rename a file View and edit text file's contents View and edit tabular file's contents To edit a Storage you need to have WRITE permission for the Storage . For more information see 13. Permissions .","title":"8.3. Create and Edit text files"},{"location":"manual/08_Manage_Data_Storage/8.3._Create_and_Edit_text_files/#create-and-rename-a-file","text":"To create a file: Click + Create \u2192 File . Enter file's name. Enter file's contents (optionally). Click OK . As a result, a new file will be created. Note: the new file will be tagged with auto-created attribute: CP_OWNER . The value of the attribute will be set as a user ID. The exception is that the storage is based on FS mount. Files in such data storage don't have attributes at all. To rename a file: Click the button in the desired file line. Rename it. Click OK .","title":"Create and rename a file"},{"location":"manual/08_Manage_Data_Storage/8.3._Create_and_Edit_text_files/#view-and-edit-text-files-contents","text":"Click on the file. You will see file preview on the right. In case of the file is large you will see only a part of it. Click \"Expand\" icon at the upper right of the preview window. A pop-up text editor window appears. Click Edit and change the file. Click Save to save the changes.","title":"View and edit text file's contents"},{"location":"manual/08_Manage_Data_Storage/8.3._Create_and_Edit_text_files/#view-and-edit-tabular-files-contents","text":"Click on the tabular file. You will see file preview on the right. In case of the file is large you will see only a part of it. Click \"Expand\" icon at the upper right of the preview window. A pop-up text editor window appears. Click Edit and change file in tabular view or in the text view. Click Save to save the changes.","title":"View and edit tabular file's contents"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/","text":"8.4. Control File versions Note : This feature is available not for all Cloud Providers. Currently, it is supported by AWS and GCP . You can also control file versions via CLI. See 14.3. Manage Storage via CLI . Versioned files management Management of deleted files Management of existing files Object group level (not a version) Delete a file from a data storage Rename a file Download a file Latest version Delete the latest file version Rename the latest file version Download the latest file version Previous version(s) Set a file version as the latest Download one of the previous file versions Change backup duration File versioning management system prevents users from accident deletion of files and loss of data. It allows restoring specific versions of a file. Versioned files management To manage file versions in GUI: In the Library tab choose an appropriate data storage. Choose the Show files versions option. Note : an OWNER or a user with ROLE_ADMIN role are able to turn on files versioning view, other users will not be able to see/use that option. After that, you'll be able to see file versions and files marked as deleted (but they are not actually deleted from the data storage yet). Deleted files (i.e. objects where the latest file version is set to a \" Delete marker \") are highlighted in red (\" file2.txt \" in the example below). To expand a list of versions for each file press the \" Plus \" icon. Management of deleted files Deleted files are kept in a data storage for some time. See more details about backup duration later in this document. Available operations for deleted versioned files: on the object group level (not a version) : To delete a file completely from a data storage press the \" Delete \" icon. with the latest version: To delete a file completely press the \" Delete \" icon of its latest version (expand file version list to do it). Works the same way as for object group level. with the previous version(s): These operations are the same as listed in the next section. Management of existing files Available operations for existing versioned files: Object group level (not a version) Delete a file from a data storage To delete an existing file from a data storage press the \" Delete \" icon. If \" Show files versions \" is ON - you will have the following options: \" Set deletion marker \" - \" Delete marker \" will be set and the latest version of the file will NOT be really deleted from a data storage. Users (except \" ADMIN \" and \" OWNER \") will not be able to view this file after that. \" Delete from bucket \" - delete a file from a data storage. Note : if you don't have ROLE_ADMIN or OWNER rights, you won't have these options and you won't be able to delete a file from the data storage completely (\" Delete marker \" will be set). If \" Show files versions \" is OFF - \" Set deletion marker \" option will be used by default. Rename a file To rename an existing files press the \"Rename\" icon . In this example, we will rename \"1.txt\" to \"5.txt\". To confirm the action press OK . After that, the latest version of \"1.txt\" is set to \" Delete marker \". New file appears in the folder. Download a file To download the latest version of an existing file press the \" Download \" button . Latest version All operations below work in the same way as for object group level. Delete the latest file version To delete a file press the \" Delete \" button of its latest version. Rename the latest file version To rename the latest version of an object press the \" Rename \" button of its latest version. Download the latest file version To download the latest version of an object press the \" Download \" button of its latest version. Previous version(s) Set a file version as the latest To set a chosen version as the latest press the \" Restore \" button: Download one of the previous file versions To download an appropriate previous version of the file press the \" Download \" button. Change backup duration To change the backup time in GUI: Press the \" Edit \" button . Turn on \" Enable versioning \" option. Choose Backup duration in days. If not explicitly defined, the global default value will be applied (defined in service configuration). Note : setting such rule for a Data Storage (from GUI or CLI) will tell Cloud Provider to delete previous versions of an object that are older than a specified number of days. This configuration option will be available to users with ROLE_ADMIN or OWNER rights.","title":"8.4. Control file versions"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#84-control-file-versions","text":"Note : This feature is available not for all Cloud Providers. Currently, it is supported by AWS and GCP . You can also control file versions via CLI. See 14.3. Manage Storage via CLI . Versioned files management Management of deleted files Management of existing files Object group level (not a version) Delete a file from a data storage Rename a file Download a file Latest version Delete the latest file version Rename the latest file version Download the latest file version Previous version(s) Set a file version as the latest Download one of the previous file versions Change backup duration File versioning management system prevents users from accident deletion of files and loss of data. It allows restoring specific versions of a file.","title":"8.4. Control File versions"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#versioned-files-management","text":"To manage file versions in GUI: In the Library tab choose an appropriate data storage. Choose the Show files versions option. Note : an OWNER or a user with ROLE_ADMIN role are able to turn on files versioning view, other users will not be able to see/use that option. After that, you'll be able to see file versions and files marked as deleted (but they are not actually deleted from the data storage yet). Deleted files (i.e. objects where the latest file version is set to a \" Delete marker \") are highlighted in red (\" file2.txt \" in the example below). To expand a list of versions for each file press the \" Plus \" icon.","title":"Versioned files management"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#management-of-deleted-files","text":"Deleted files are kept in a data storage for some time. See more details about backup duration later in this document. Available operations for deleted versioned files: on the object group level (not a version) : To delete a file completely from a data storage press the \" Delete \" icon. with the latest version: To delete a file completely press the \" Delete \" icon of its latest version (expand file version list to do it). Works the same way as for object group level. with the previous version(s): These operations are the same as listed in the next section.","title":"Management of deleted files"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#management-of-existing-files","text":"Available operations for existing versioned files:","title":"Management of existing files"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#object-group-level-not-a-version","text":"","title":"Object group level (not a version)"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#delete-a-file-from-a-data-storage","text":"To delete an existing file from a data storage press the \" Delete \" icon. If \" Show files versions \" is ON - you will have the following options: \" Set deletion marker \" - \" Delete marker \" will be set and the latest version of the file will NOT be really deleted from a data storage. Users (except \" ADMIN \" and \" OWNER \") will not be able to view this file after that. \" Delete from bucket \" - delete a file from a data storage. Note : if you don't have ROLE_ADMIN or OWNER rights, you won't have these options and you won't be able to delete a file from the data storage completely (\" Delete marker \" will be set). If \" Show files versions \" is OFF - \" Set deletion marker \" option will be used by default.","title":"Delete a file from a data storage"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#rename-a-file","text":"To rename an existing files press the \"Rename\" icon . In this example, we will rename \"1.txt\" to \"5.txt\". To confirm the action press OK . After that, the latest version of \"1.txt\" is set to \" Delete marker \". New file appears in the folder.","title":"Rename a file"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#download-a-file","text":"To download the latest version of an existing file press the \" Download \" button .","title":"Download a file"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#latest-version","text":"All operations below work in the same way as for object group level.","title":"Latest version"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#delete-the-latest-file-version","text":"To delete a file press the \" Delete \" button of its latest version.","title":"Delete the latest file version"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#rename-the-latest-file-version","text":"To rename the latest version of an object press the \" Rename \" button of its latest version.","title":"Rename the latest file version"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#download-the-latest-file-version","text":"To download the latest version of an object press the \" Download \" button of its latest version.","title":"Download the latest file version"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#previous-versions","text":"","title":"Previous version(s)"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#set-a-file-version-as-the-latest","text":"To set a chosen version as the latest press the \" Restore \" button:","title":"Set a file version as the latest"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#download-one-of-the-previous-file-versions","text":"To download an appropriate previous version of the file press the \" Download \" button.","title":"Download one of the previous file versions"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#change-backup-duration","text":"To change the backup time in GUI: Press the \" Edit \" button . Turn on \" Enable versioning \" option. Choose Backup duration in days. If not explicitly defined, the global default value will be applied (defined in service configuration). Note : setting such rule for a Data Storage (from GUI or CLI) will tell Cloud Provider to delete previous versions of an object that are older than a specified number of days. This configuration option will be available to users with ROLE_ADMIN or OWNER rights.","title":"Change backup duration"},{"location":"manual/08_Manage_Data_Storage/8.5._Delete_and_unregister_Data_Storage/","text":"8.5. Delete and unregister Data Storage Delete storage Unregister storage To delete a Storage you need to have WRITE permission for that storage and the ROLE_STORAGE_MANAGER role. For more details see 13. Permissions . You can also delete and unregister Storage via CLI . See 14.3. Manage Storage via CLI . Delete storage Note : If storage contains only metadata, it will not prevent deletion. Select a Data storage. Click Edit . Choose Delete . You will be offered to unregister or delete a storage. Click Delete . Unregister storage A user can unregister storage. Cloud bucket with data neither will be deleted nor will be accessible in Cloud Pipeline. Do the same actions as in Delete storage but choose to Unregister in step 4 .","title":"8.5. Delete and unregister Data Storage"},{"location":"manual/08_Manage_Data_Storage/8.5._Delete_and_unregister_Data_Storage/#85-delete-and-unregister-data-storage","text":"Delete storage Unregister storage To delete a Storage you need to have WRITE permission for that storage and the ROLE_STORAGE_MANAGER role. For more details see 13. Permissions . You can also delete and unregister Storage via CLI . See 14.3. Manage Storage via CLI .","title":"8.5. Delete and unregister Data Storage"},{"location":"manual/08_Manage_Data_Storage/8.5._Delete_and_unregister_Data_Storage/#delete-storage","text":"Note : If storage contains only metadata, it will not prevent deletion. Select a Data storage. Click Edit . Choose Delete . You will be offered to unregister or delete a storage. Click Delete .","title":"Delete storage"},{"location":"manual/08_Manage_Data_Storage/8.5._Delete_and_unregister_Data_Storage/#unregister-storage","text":"A user can unregister storage. Cloud bucket with data neither will be deleted nor will be accessible in Cloud Pipeline. Do the same actions as in Delete storage but choose to Unregister in step 4 .","title":"Unregister storage"},{"location":"manual/08_Manage_Data_Storage/8.6._Delete_Files_and_Folders_from_Storage/","text":"8.6. Delete Files and Folders from Storage Note : when you delete a folder, all child folders and files will be removed as well. Click the Delete button in the right side of the row with file or folder you want to delete. Confirm the action.","title":"8.6. Delete Files and Folders from Data Storage"},{"location":"manual/08_Manage_Data_Storage/8.6._Delete_Files_and_Folders_from_Storage/#86-delete-files-and-folders-from-storage","text":"Note : when you delete a folder, all child folders and files will be removed as well. Click the Delete button in the right side of the row with file or folder you want to delete. Confirm the action.","title":"8.6. Delete Files and Folders from Storage"},{"location":"manual/08_Manage_Data_Storage/8.7._Create_shared_file_system/","text":"8.7. Create shared file system Create FS mount FS storage features FS storage size FS quotas Setup the quota Exclude directories from monitoring Usage examples Grace period for quotas User shall have ROLE_ADMIN to mount FS to the Cloud Pipeline. For more information see 13. Permissions . A shared file system is a data storage based on network file system. It has several advantages over regular data storages and local file system: While regular data storage is a great option for a long-term storage, it cannot be used as a shared file system for high-performance computing jobs as it does not support network-like interface. A local disk cannot be shared across several nodes. A user needs to specify local disk size when scheduling a run. If a user specifies a size that is not enough to finish a job - it will fail. Cloud-based shared file system could be used to workaround this issue. Create FS mount Navigate to a desired folder and click + Create \u2192 Storages \u2192 Create new FS mount . Note : For FS mounts - \"Add existing\" option is not available. Note : For the correct FS mount creation, at least one mount point shall be registered in the System Preferences for any Cloud Region. If no - the corresponding button of the FS mount creation becomes invisible: Specify Storage path and other optional parameters. Note : Storage path parameter contains FS mount path and the name of the storage to be created. Note : FS storages are just subdirectories of the mounted FS. One FS mount can have multiple FS storages. When deleted from GUI, FS storage is unmounted from the Cloud Pipeline. FS storage features For FS storages GUI doesn't display the following features typical for regular data storages: STS LTS Versioning and Backup duration. When a user selects Input/Common/Output path parameter for a pipeline run - it is impossible to set FS storage: FS storages aren't displayed in the \"Browse...\" dialog for Input/Common/Output path parameters; Value of Input/Common/Output path parameters is validated so that user is not able to specify a path to FS storage manually. The content of files stored in FS data storage could be previewed as well as in regular data storages: Since FS isn't an object data storage, it isn't possible to add metadata tags to files in the FS storage. Use FS storage between cluster nodes. If pipeline Tools contain FS client, FS storage(s) will be mounted automatically. FS storage size To know the summary size of all files in the FS storage - open its attributes: In the Attributes panel, you can view the whole size: Please note, in the Attributes panel here, the Effective size of all files in the FS storage is displayed. To know the Real size of all files in the storage - click the info icon. Details about Effective and Real sizes see in the section below . FS storage content can be changed during the time, and the displayed size value in the Attributes panel may be outdated. To know the actual FS storage size currently, you shall \"re-index\" the storage: For example, in the FS storage a new big file was added. But the displayed FS size is still previous: Click the Re-index button: Storage size will be refreshed in a few minutes. You will see the corresponding message. Couple minutes later refresh a page, and you can check that size is updated according to the added file: FS quotas There is a feature that allows to configure quota(s) to the FS storage volume that user can occupy. On exceeding such quota(s), different actions can be applied, e.g. read-only mode for that storage. This allows to minimize the shared filesystem costs by limiting the amount of data being stored in them and to notify the users/admins when FS storage is running out of the specific volume. Setup the quota To configure FS quotas, you need to have the ROLE_ADMIN role. To setup a quota and actions on quota's exceeding: Open a FS storage. Click the Show attributes button to open the Attributes panel of the storage: Click the Configure notifications hyperlink to open the notifications/quota settings for the storage: The pop-up with quota settings will appear: Type the username(s) or a groupname(s) in the \"Recipients\" input to choose who will get the FS quota notifications via emails and push notifications , e.g.: In the example above, one user and one group were selected. Click the Add notification to configure rules/thresholds: Put a threshold in Gb or % of the total volume and choose which action shall be performed when that threshold is reached. The following actions can be taken by the platform: Send email - just notify the recipients that a quota has been reached (notification will be resent each hour) Disable mount - used to let the users cleanup the data: GUI will still allow to perform the modification of this storage ( read-write mode ) In existing nodes (launched runs), FS storage mount will be switched to a read-only mode (if it was mounted previously) This FS storage will be mounted in a read-only mode to the new launched compute nodes Make read-only - used to stop any data activities from the users, only admins can cleanup the data per a request: GUI will show this FS storage in a read-only mode Existing nodes (launched runs) will turn this mounted FS storage in a read-only mode as well This FS storage will be mounted in a read-only mode to the new launched compute nodes You may add any count of quota rules by the described way. The notification/quota rules can be combined in any form. E.g. the following example sets three levels of the thresholds. Each level notifies the users about the threshold exceeding and also introduces a new restriction: Note : email notification type for FS quota-related processes is STORAGE_QUOTA_EXCEEDING . Exclude certain directories from monitoring Some of the tools (e.g. RStudio ) use a home directory to store the suspended sessions data. This may make the data storage to exceed all the quotas, if this home directory is the FS storage and some quotas are set for it. While the user will not be aware what has happened. To address such issues - admin can exclude certain \"well-know\" location(s) from the quota evaluation procedure. The system preference storage.quotas.skipped.paths is used to specify which storages/paths shall be excluded from the quota-related processing. Preference value shall be a JSON-formatted list: [ { \"storageName\": \"<storage_name>\", \"hiddenFilePathGlobs\": [ \"<path_to_dir>\" ] } ] Where: <storage_name> - can be one of: exact storage name (e.g. \"Storage-USER1\") - rules will be applied only to that storage wildcard (e.g. \"Storage-*\" or \"*\") - rules will be applied to any storage which match the mask <path_to_dir> - path to a directory or a file within a storage, which will be skipped: absolute path (e.g. \"path/to/dir\") will skip only exactly that directory/file wildcard path (e.g. \"path/to/dir/*\") will skip directory and it's content recursively E.g. to address the RStudio issue (described in the beginning of the current section), the preference shall look like: [ { \"storageName\": \"*\", \"hiddenFilePathGlobs\": [ \".local/share/rstudio/sessions/*\" ] } ] If the preference is set - you can view the following usage metrics of the data storage in its Attributes : Real size - a real size of the storage, i.e. summary size of all files in this storage. This value is used for the billing reports. Effective size - a real size minus the volume of the directories, specified in the storage.quotas.skipped.paths preference. This value is used to evaluate quotas. Usage examples Please note, examples below are shown for the \"general\" user. ROLE_ADMIN users will not be affected by the restrictions. Even if the storage is in read-only state - admins can perform READ and WRITE operations. Also note, transition between the FS storage states ( ACTIVE \u2192 MOUNT DISABLED \u2192 READ ONLY and vice versa) may take up to 5 min. For our examples: we have selected a data storage with a known effective volume (3.55 GB) we have setup two thresholds for this storage: 4 Gb - disable mounts, but keep GUI available for changes 5 Gb - make the datastorage fully read-only Action Results 1 Create 1 Gb file in the filesystem (FS exceeds 4 Gb threshold) Active jobs : filesystem mount becomes read-only and a user can not perform any modification New jobs : filesystem is mounted as read-only by default GUI : Permissions are not changed. Write operations can be performed, according to the permissions \"Warning\" icon is displayed. It shows MOUNT DISABLED state Storage size is increased by 1 Gb: 2 Create another 1 Gb file in the filesystem (FS exceeds 5 Gb threshold) Active jobs : filesystem mount remains read-only and a user can not perform any modification New jobs : filesystem is mounted as read-only by default GUI : Storage becomes read-only . User can not perform any modification to the filesystem \"Warning\" icon is still displayed. It shows READ ONLY state Storage size is increased by 1 Gb: 3 Delete one 1 Gb file ( Note : this action can be performed only by admin) Active jobs : filesystem is still read-only and a user can not perform any modification New jobs : filesystem is mounted as read-only by default GUI : Permissions are reverted to the previous state. Changes can be done. \"Warning\" icon is still displayed. Wording is reverted to MOUNT DISABLED state Storage size is decreased by 1 Gb 4 Delete second 1 Gb file Active jobs : filesystem is remounted according to the user's permissions New jobs : filesystem is mounted according to the user's permissions by default GUI : Permissions are not changed. WRITE operations can be performed, according to the permissions \"Warning\" icon is hidden Storage size is decreased by 1 Gb Grace period for quotas By default, FS quotas are being applied immediately as they had been reached. While this is totally correct - users may face the compute workloads to stop. To address this - there is a grace period for the quotas restrictions to become effective. The system preference storage.quotas.actions.grace.period is used to configure such grace period. Preference value shall be a JSON-formatted list: { \"READONLY\": <VALUE_1>, \"DISABLE_MOUNT\": <VALUE_2> } Preference allows to specify grace periods for both states: MOUNT DISABLED and READ ONLY . Value is specified in minutes. If that preference is specified - actions for FS quotas will be delayed. That means that restrictive actions will be applied only after specified \"grace\" period and the user has some time to free storage space up, i.e.: when the data storage volume exceeds a threshold - storage's state will not be changed to MOUNT DISABLED or READ ONLY until the grace period exceeds if a certain state does not have a grace period - for that state, the immediate state switching will be in place Example of using: Let's suppose that storage.quotas.actions.grace.period is configured as: I.e. for MOUNT DISABLED action grace period is 1 hour, for READ ONLY action grace period is 2 hours. Let's suppose we have a storage with 10 Gb volume and the following quota is configured: User uploads 15 Gb file and this information gets into the index. \"Effective\" storage size (25 Gb) exceeds the threshold configured for MOUNT DISABLED action. So, the quota monitor will perform the following actions: will send a notification to the user, that a quota has been reached, but the state is still ACTIVE will not change the real state of the storage once 1 hour is elapsed and the size of the storage will be still 25 Gb: storage state will be set to MOUNT DISABLED user will be notified about that event","title":"8.7. Create shared file system"},{"location":"manual/08_Manage_Data_Storage/8.7._Create_shared_file_system/#87-create-shared-file-system","text":"Create FS mount FS storage features FS storage size FS quotas Setup the quota Exclude directories from monitoring Usage examples Grace period for quotas User shall have ROLE_ADMIN to mount FS to the Cloud Pipeline. For more information see 13. Permissions . A shared file system is a data storage based on network file system. It has several advantages over regular data storages and local file system: While regular data storage is a great option for a long-term storage, it cannot be used as a shared file system for high-performance computing jobs as it does not support network-like interface. A local disk cannot be shared across several nodes. A user needs to specify local disk size when scheduling a run. If a user specifies a size that is not enough to finish a job - it will fail. Cloud-based shared file system could be used to workaround this issue.","title":"8.7. Create shared file system"},{"location":"manual/08_Manage_Data_Storage/8.7._Create_shared_file_system/#create-fs-mount","text":"Navigate to a desired folder and click + Create \u2192 Storages \u2192 Create new FS mount . Note : For FS mounts - \"Add existing\" option is not available. Note : For the correct FS mount creation, at least one mount point shall be registered in the System Preferences for any Cloud Region. If no - the corresponding button of the FS mount creation becomes invisible: Specify Storage path and other optional parameters. Note : Storage path parameter contains FS mount path and the name of the storage to be created. Note : FS storages are just subdirectories of the mounted FS. One FS mount can have multiple FS storages. When deleted from GUI, FS storage is unmounted from the Cloud Pipeline.","title":"Create FS mount"},{"location":"manual/08_Manage_Data_Storage/8.7._Create_shared_file_system/#fs-storage-features","text":"For FS storages GUI doesn't display the following features typical for regular data storages: STS LTS Versioning and Backup duration. When a user selects Input/Common/Output path parameter for a pipeline run - it is impossible to set FS storage: FS storages aren't displayed in the \"Browse...\" dialog for Input/Common/Output path parameters; Value of Input/Common/Output path parameters is validated so that user is not able to specify a path to FS storage manually. The content of files stored in FS data storage could be previewed as well as in regular data storages: Since FS isn't an object data storage, it isn't possible to add metadata tags to files in the FS storage. Use FS storage between cluster nodes. If pipeline Tools contain FS client, FS storage(s) will be mounted automatically.","title":"FS storage features"},{"location":"manual/08_Manage_Data_Storage/8.7._Create_shared_file_system/#fs-storage-size","text":"To know the summary size of all files in the FS storage - open its attributes: In the Attributes panel, you can view the whole size: Please note, in the Attributes panel here, the Effective size of all files in the FS storage is displayed. To know the Real size of all files in the storage - click the info icon. Details about Effective and Real sizes see in the section below . FS storage content can be changed during the time, and the displayed size value in the Attributes panel may be outdated. To know the actual FS storage size currently, you shall \"re-index\" the storage: For example, in the FS storage a new big file was added. But the displayed FS size is still previous: Click the Re-index button: Storage size will be refreshed in a few minutes. You will see the corresponding message. Couple minutes later refresh a page, and you can check that size is updated according to the added file:","title":"FS storage size"},{"location":"manual/08_Manage_Data_Storage/8.7._Create_shared_file_system/#fs-quotas","text":"There is a feature that allows to configure quota(s) to the FS storage volume that user can occupy. On exceeding such quota(s), different actions can be applied, e.g. read-only mode for that storage. This allows to minimize the shared filesystem costs by limiting the amount of data being stored in them and to notify the users/admins when FS storage is running out of the specific volume.","title":"FS quotas"},{"location":"manual/08_Manage_Data_Storage/8.7._Create_shared_file_system/#setup-the-quota","text":"To configure FS quotas, you need to have the ROLE_ADMIN role. To setup a quota and actions on quota's exceeding: Open a FS storage. Click the Show attributes button to open the Attributes panel of the storage: Click the Configure notifications hyperlink to open the notifications/quota settings for the storage: The pop-up with quota settings will appear: Type the username(s) or a groupname(s) in the \"Recipients\" input to choose who will get the FS quota notifications via emails and push notifications , e.g.: In the example above, one user and one group were selected. Click the Add notification to configure rules/thresholds: Put a threshold in Gb or % of the total volume and choose which action shall be performed when that threshold is reached. The following actions can be taken by the platform: Send email - just notify the recipients that a quota has been reached (notification will be resent each hour) Disable mount - used to let the users cleanup the data: GUI will still allow to perform the modification of this storage ( read-write mode ) In existing nodes (launched runs), FS storage mount will be switched to a read-only mode (if it was mounted previously) This FS storage will be mounted in a read-only mode to the new launched compute nodes Make read-only - used to stop any data activities from the users, only admins can cleanup the data per a request: GUI will show this FS storage in a read-only mode Existing nodes (launched runs) will turn this mounted FS storage in a read-only mode as well This FS storage will be mounted in a read-only mode to the new launched compute nodes You may add any count of quota rules by the described way. The notification/quota rules can be combined in any form. E.g. the following example sets three levels of the thresholds. Each level notifies the users about the threshold exceeding and also introduces a new restriction: Note : email notification type for FS quota-related processes is STORAGE_QUOTA_EXCEEDING .","title":"Setup the quota"},{"location":"manual/08_Manage_Data_Storage/8.7._Create_shared_file_system/#exclude-certain-directories-from-monitoring","text":"Some of the tools (e.g. RStudio ) use a home directory to store the suspended sessions data. This may make the data storage to exceed all the quotas, if this home directory is the FS storage and some quotas are set for it. While the user will not be aware what has happened. To address such issues - admin can exclude certain \"well-know\" location(s) from the quota evaluation procedure. The system preference storage.quotas.skipped.paths is used to specify which storages/paths shall be excluded from the quota-related processing. Preference value shall be a JSON-formatted list: [ { \"storageName\": \"<storage_name>\", \"hiddenFilePathGlobs\": [ \"<path_to_dir>\" ] } ] Where: <storage_name> - can be one of: exact storage name (e.g. \"Storage-USER1\") - rules will be applied only to that storage wildcard (e.g. \"Storage-*\" or \"*\") - rules will be applied to any storage which match the mask <path_to_dir> - path to a directory or a file within a storage, which will be skipped: absolute path (e.g. \"path/to/dir\") will skip only exactly that directory/file wildcard path (e.g. \"path/to/dir/*\") will skip directory and it's content recursively E.g. to address the RStudio issue (described in the beginning of the current section), the preference shall look like: [ { \"storageName\": \"*\", \"hiddenFilePathGlobs\": [ \".local/share/rstudio/sessions/*\" ] } ] If the preference is set - you can view the following usage metrics of the data storage in its Attributes : Real size - a real size of the storage, i.e. summary size of all files in this storage. This value is used for the billing reports. Effective size - a real size minus the volume of the directories, specified in the storage.quotas.skipped.paths preference. This value is used to evaluate quotas.","title":"Exclude certain directories from monitoring"},{"location":"manual/08_Manage_Data_Storage/8.7._Create_shared_file_system/#usage-examples","text":"Please note, examples below are shown for the \"general\" user. ROLE_ADMIN users will not be affected by the restrictions. Even if the storage is in read-only state - admins can perform READ and WRITE operations. Also note, transition between the FS storage states ( ACTIVE \u2192 MOUNT DISABLED \u2192 READ ONLY and vice versa) may take up to 5 min. For our examples: we have selected a data storage with a known effective volume (3.55 GB) we have setup two thresholds for this storage: 4 Gb - disable mounts, but keep GUI available for changes 5 Gb - make the datastorage fully read-only Action Results 1 Create 1 Gb file in the filesystem (FS exceeds 4 Gb threshold) Active jobs : filesystem mount becomes read-only and a user can not perform any modification New jobs : filesystem is mounted as read-only by default GUI : Permissions are not changed. Write operations can be performed, according to the permissions \"Warning\" icon is displayed. It shows MOUNT DISABLED state Storage size is increased by 1 Gb: 2 Create another 1 Gb file in the filesystem (FS exceeds 5 Gb threshold) Active jobs : filesystem mount remains read-only and a user can not perform any modification New jobs : filesystem is mounted as read-only by default GUI : Storage becomes read-only . User can not perform any modification to the filesystem \"Warning\" icon is still displayed. It shows READ ONLY state Storage size is increased by 1 Gb: 3 Delete one 1 Gb file ( Note : this action can be performed only by admin) Active jobs : filesystem is still read-only and a user can not perform any modification New jobs : filesystem is mounted as read-only by default GUI : Permissions are reverted to the previous state. Changes can be done. \"Warning\" icon is still displayed. Wording is reverted to MOUNT DISABLED state Storage size is decreased by 1 Gb 4 Delete second 1 Gb file Active jobs : filesystem is remounted according to the user's permissions New jobs : filesystem is mounted according to the user's permissions by default GUI : Permissions are not changed. WRITE operations can be performed, according to the permissions \"Warning\" icon is hidden Storage size is decreased by 1 Gb","title":"Usage examples"},{"location":"manual/08_Manage_Data_Storage/8.7._Create_shared_file_system/#grace-period-for-quotas","text":"By default, FS quotas are being applied immediately as they had been reached. While this is totally correct - users may face the compute workloads to stop. To address this - there is a grace period for the quotas restrictions to become effective. The system preference storage.quotas.actions.grace.period is used to configure such grace period. Preference value shall be a JSON-formatted list: { \"READONLY\": <VALUE_1>, \"DISABLE_MOUNT\": <VALUE_2> } Preference allows to specify grace periods for both states: MOUNT DISABLED and READ ONLY . Value is specified in minutes. If that preference is specified - actions for FS quotas will be delayed. That means that restrictive actions will be applied only after specified \"grace\" period and the user has some time to free storage space up, i.e.: when the data storage volume exceeds a threshold - storage's state will not be changed to MOUNT DISABLED or READ ONLY until the grace period exceeds if a certain state does not have a grace period - for that state, the immediate state switching will be in place Example of using: Let's suppose that storage.quotas.actions.grace.period is configured as: I.e. for MOUNT DISABLED action grace period is 1 hour, for READ ONLY action grace period is 2 hours. Let's suppose we have a storage with 10 Gb volume and the following quota is configured: User uploads 15 Gb file and this information gets into the index. \"Effective\" storage size (25 Gb) exceeds the threshold configured for MOUNT DISABLED action. So, the quota monitor will perform the following actions: will send a notification to the user, that a quota has been reached, but the state is still ACTIVE will not change the real state of the storage once 1 hour is elapsed and the size of the storage will be still 25 Gb: storage state will be set to MOUNT DISABLED user will be notified about that event","title":"Grace period for quotas"},{"location":"manual/08_Manage_Data_Storage/8.8._Data_sharing/","text":"8.8. Data sharing Create shared storage Upload data to shared storage Download data from shared storage To create a Shared storage in a Folder you need to have WRITE permission for that folder and the ROLE_STORAGE_MANAGER role. For more information see 13. Permissions . Users can share data storages within a Cloud Platform for enabling of getting data files by the external partners for processing. Create shared storage For the ability of getting data files by the external partners, users should be considered, that external partner has own CP account and R/W permissions for a storage. Start creating a new object storage (for more details see here ), fill Info items. Set Enable sharing . Click Create button: Open created storage by clicking on it in the folder tree ( 1 ). Click icon in upper right corner ( 2 ): Choose Permissions tab in opened pop-up window: Click on button, enter user, for whom you want to share created storage. Confirm by clicking \" Ok \" button: Click on user name. If you want your partner can only download data from creating shared space, set Allow checkbox for READ permission, set Deny checkbox for WRITE permission: If you want your partner can download data from creating shared spaced and upload on it, set Allow checkbox both for READ and WRITE permissions: Close pop-up window. Click button. In the pop-up window generated URL will be appeared. It can be shared with the external collaborator. Upload data to shared storage Shared storage's collaboration space can be used to exchange large data files (up to 5Tb per one file). For storage owner : Uploading data to shared storage is doing in the same way as on a regular. For more details see here . For external partner : Note : for uploading to shared storage, user account shall be registered within CP users catalog and granted READ & WRITE permissions for that storage. Open new browser window and insert URL, that you receive from the partner. In appeared login page enter user credentials and sign in. The following page will be opened: Click button. In opened pop-up window browse file(s) to upload. Confirm uploading. Note : make sure size doesn't exceed 5 Tb. Note : you can cancel upload process by clicking the \"Cancel\" button: As a result, the file(s) will be uploaded to the shared storage: Download data from shared storage For storage owner : Downloading data from shared storage is doing in the same way as from a regular. For more details see here . For external partner : Note : for downloading from shared storage, user account shall be registered within CP users catalog and granted READ permission for that storage. Open new browser window and insert URL, that you receive from the partner. In appeared login page enter user credentials and sign in. The following page will be opened: Click to download required file. Specify where to download in the pop-up window. As a result, the file will be downloaded via your browser to the specified location.","title":"8.8. Data sharing"},{"location":"manual/08_Manage_Data_Storage/8.8._Data_sharing/#88-data-sharing","text":"Create shared storage Upload data to shared storage Download data from shared storage To create a Shared storage in a Folder you need to have WRITE permission for that folder and the ROLE_STORAGE_MANAGER role. For more information see 13. Permissions . Users can share data storages within a Cloud Platform for enabling of getting data files by the external partners for processing.","title":"8.8. Data sharing"},{"location":"manual/08_Manage_Data_Storage/8.8._Data_sharing/#create-shared-storage","text":"For the ability of getting data files by the external partners, users should be considered, that external partner has own CP account and R/W permissions for a storage. Start creating a new object storage (for more details see here ), fill Info items. Set Enable sharing . Click Create button: Open created storage by clicking on it in the folder tree ( 1 ). Click icon in upper right corner ( 2 ): Choose Permissions tab in opened pop-up window: Click on button, enter user, for whom you want to share created storage. Confirm by clicking \" Ok \" button: Click on user name. If you want your partner can only download data from creating shared space, set Allow checkbox for READ permission, set Deny checkbox for WRITE permission: If you want your partner can download data from creating shared spaced and upload on it, set Allow checkbox both for READ and WRITE permissions: Close pop-up window. Click button. In the pop-up window generated URL will be appeared. It can be shared with the external collaborator.","title":"Create shared storage"},{"location":"manual/08_Manage_Data_Storage/8.8._Data_sharing/#upload-data-to-shared-storage","text":"Shared storage's collaboration space can be used to exchange large data files (up to 5Tb per one file). For storage owner : Uploading data to shared storage is doing in the same way as on a regular. For more details see here . For external partner : Note : for uploading to shared storage, user account shall be registered within CP users catalog and granted READ & WRITE permissions for that storage. Open new browser window and insert URL, that you receive from the partner. In appeared login page enter user credentials and sign in. The following page will be opened: Click button. In opened pop-up window browse file(s) to upload. Confirm uploading. Note : make sure size doesn't exceed 5 Tb. Note : you can cancel upload process by clicking the \"Cancel\" button: As a result, the file(s) will be uploaded to the shared storage:","title":"Upload data to shared storage"},{"location":"manual/08_Manage_Data_Storage/8.8._Data_sharing/#download-data-from-shared-storage","text":"For storage owner : Downloading data from shared storage is doing in the same way as from a regular. For more details see here . For external partner : Note : for downloading from shared storage, user account shall be registered within CP users catalog and granted READ permission for that storage. Open new browser window and insert URL, that you receive from the partner. In appeared login page enter user credentials and sign in. The following page will be opened: Click to download required file. Specify where to download in the pop-up window. As a result, the file will be downloaded via your browser to the specified location.","title":"Download data from shared storage"},{"location":"manual/08_Manage_Data_Storage/8.9._Mapping_storages/","text":"8.9. Mount Cloud Data Storage to the local workstation Introduction Cloud Pipeline platform allows to mount data storages, located in the Cloud, to the local workstation as a network drive. This provides an easy way to manage files, that shall be processed in the Cloud or downloaded locally. Authenticate within Cloud Pipeline platform and obtain mapping token Map a storage as the network drive to the Windows workstation using built-in tools Map a storage using CyberDuck tool Please consider the following limitations and prerequisites to map a network drive: 1. Network drive mapping is only supported for Windows workstations. 2. Internet Explorer 11 shall be used for Authentication section of this manual. Once authentication is done - Chrome or other web-browser can be used to further work. 3. Only NFS data storages (e.g. EFS/NFS/SMB) can be mapped to the local workstation. S3/AZ/GCP storages are not supported. 4. The following limits are applied to the data transfer: - Max 500 Mb per file - Max transfer duration: 30 min 5. The following requirements shall be set for the user account: - User shall be granted write permissions to any of the NFS data storages Authenticate within Cloud Pipeline platform and obtain mapping token Open Internet Explorer web-browser Navigate to the Cloud Pipeline GUI - https://<cloud pipeline adress>/pipeline/ Click the Settings button in the left menu Navigate to the CLI tab and select Drive mapping section in the left menu: Authentication prompt will be shown: Click the Authenticate button If authentication succeeds you will see a popup window with a link to map a network drive. Copy the URL shown in the popup, it will be used in the workstation configuration section: Map a storage as the network drive to the Windows workstation using built-in tools Open This PC ( My Computer ) view Click the Map network drive button: Map Network Drive dialog will be shown: Select a Drive letter (it will be used to address a drive, e.g. Z:\\ ) or keep the default value for the \"Drive\" field Paste a mapping URL (obtained from the Cloud Pipeline authentication dialog) into the \"Folder\" field Tick \" Reconnect at logon \" checkbox Click the Finish button: Drive will be mapped and opened. Further you can address it using a Drive letter set above (e.g. Z:\\ ): Map a storage using CyberDuck tool Prerequisites : get a mapping URL by the way described above . Launch CyberDuck tool. Click the Open Connection button: In the appeared popup, click the upper dropdown list with connection types: Select the item \" WebDAV (HTTPS) \": Paste a mapping URL (from the prerequisites) into the \"Server\" field. Fields \"URL\", \"Port\", \"Path\" will be filled in automatically: Specify your Cloud Pipeline credentials - username into the \"Username\" field, access key into the \"Password\" field: Note : to get your access token - open System Settings , then CLI tab, and in the sub-tab Pipe CLI , click the \" Generate access key \" button: Click the Connect button to confirm. After authentication will be done, all available to the user FS mounts will appear: They are displayed as folders. Double-click the folder you wish - FS storage content will appear, e.g.: In this window, you can manage files/folders as in any file commander, according to your permissions on that FS mount. More about Cyberduck functionality you may see here . After all, close the connection:","title":"8.9. Mapping storages"},{"location":"manual/08_Manage_Data_Storage/8.9._Mapping_storages/#89-mount-cloud-data-storage-to-the-local-workstation","text":"","title":"8.9. Mount Cloud Data Storage to the local workstation"},{"location":"manual/08_Manage_Data_Storage/8.9._Mapping_storages/#introduction","text":"Cloud Pipeline platform allows to mount data storages, located in the Cloud, to the local workstation as a network drive. This provides an easy way to manage files, that shall be processed in the Cloud or downloaded locally. Authenticate within Cloud Pipeline platform and obtain mapping token Map a storage as the network drive to the Windows workstation using built-in tools Map a storage using CyberDuck tool Please consider the following limitations and prerequisites to map a network drive: 1. Network drive mapping is only supported for Windows workstations. 2. Internet Explorer 11 shall be used for Authentication section of this manual. Once authentication is done - Chrome or other web-browser can be used to further work. 3. Only NFS data storages (e.g. EFS/NFS/SMB) can be mapped to the local workstation. S3/AZ/GCP storages are not supported. 4. The following limits are applied to the data transfer: - Max 500 Mb per file - Max transfer duration: 30 min 5. The following requirements shall be set for the user account: - User shall be granted write permissions to any of the NFS data storages","title":"Introduction"},{"location":"manual/08_Manage_Data_Storage/8.9._Mapping_storages/#authenticate-within-cloud-pipeline-platform-and-obtain-mapping-token","text":"Open Internet Explorer web-browser Navigate to the Cloud Pipeline GUI - https://<cloud pipeline adress>/pipeline/ Click the Settings button in the left menu Navigate to the CLI tab and select Drive mapping section in the left menu: Authentication prompt will be shown: Click the Authenticate button If authentication succeeds you will see a popup window with a link to map a network drive. Copy the URL shown in the popup, it will be used in the workstation configuration section:","title":"Authenticate within Cloud Pipeline platform and obtain mapping token"},{"location":"manual/08_Manage_Data_Storage/8.9._Mapping_storages/#map-a-storage-as-the-network-drive-to-the-windows-workstation-using-built-in-tools","text":"Open This PC ( My Computer ) view Click the Map network drive button: Map Network Drive dialog will be shown: Select a Drive letter (it will be used to address a drive, e.g. Z:\\ ) or keep the default value for the \"Drive\" field Paste a mapping URL (obtained from the Cloud Pipeline authentication dialog) into the \"Folder\" field Tick \" Reconnect at logon \" checkbox Click the Finish button: Drive will be mapped and opened. Further you can address it using a Drive letter set above (e.g. Z:\\ ):","title":"Map a storage as the network drive to the Windows workstation using built-in tools"},{"location":"manual/08_Manage_Data_Storage/8.9._Mapping_storages/#map-a-storage-using-cyberduck-tool","text":"Prerequisites : get a mapping URL by the way described above . Launch CyberDuck tool. Click the Open Connection button: In the appeared popup, click the upper dropdown list with connection types: Select the item \" WebDAV (HTTPS) \": Paste a mapping URL (from the prerequisites) into the \"Server\" field. Fields \"URL\", \"Port\", \"Path\" will be filled in automatically: Specify your Cloud Pipeline credentials - username into the \"Username\" field, access key into the \"Password\" field: Note : to get your access token - open System Settings , then CLI tab, and in the sub-tab Pipe CLI , click the \" Generate access key \" button: Click the Connect button to confirm. After authentication will be done, all available to the user FS mounts will appear: They are displayed as folders. Double-click the folder you wish - FS storage content will appear, e.g.: In this window, you can manage files/folders as in any file commander, according to your permissions on that FS mount. More about Cyberduck functionality you may see here . After all, close the connection:","title":"Map a storage using CyberDuck tool"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/","text":"8. Manage Data Storage Data storage is a Cloud Pipeline object that represents cloud storage and its content in a folder hierarchy. Controls Select page Show file versions Remove all selected Generate URL Show attributes/Hide attributes \"Gear\" icon Refresh + Create Upload Each-line controls Additional info and options View and edit a text file CLI Storage options Permissions management for a storage is described here . \"Details view\" lists content of the storage: files that may be organized into folders. Clicking on the inside folder will open its content in the \"Details view\" . Note : storage's folders hierarchy will not be represented in the \"Hierarchy view\" panel. Note : you can move Storage to a new parent folder using drag and drop approach. In \"Library tree view\" and \"Details view\" data storages are tagged with region flag to visually distinguish storage locations. Note : if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - in that case auxiliary Cloud Provider icons are additionally displayed, e.g.: Figure 1 Another option for navigation in the storage is to use \"breadcrumbs\" control at the top of the \"Details\" view (see picture above, 1 ): Clicking an item will navigate to that folder. Editing a path will allow to copy/paste a path and navigate to any custom location. Controls At the top of the \"Details\" view there are buttons: Select page Clicking this control (see picture Figure 1 above, item 2 ), the whole file and folders on the current page will be selected. It allows to perform bulk operations like deleting. Show file versions This feature is available not for all Cloud Providers. Currently, it is supported by AWS and GCP . Tick this checkbox (see picture Figure 1 above, item 3 ) and the view of a page will changed: the all file versions will be displayed. You can expand each version's list by clicking \" + \" in desired line. Note : the last version will be marked by \" (latest) \". Remove all selected This is a bulk operation control. It is visible, if at least one of the data storage item (folder or file) is selected. Generate URL This control helps to generate URLs for a number of files and then download them manually one by one or via scripts. See details here . Note : the control is available, if only files are selected. Show attributes/Hide attributes Allows see or edit a list of key=value attributes of the data storage (see picture Figure 1 above, item 4 ). Note : If selected storage has any defined attribute, Attributes pane is shown by default. See 17. CP objects tagging by additional attributes . Also, in the Attributes panel, there are additional options and info about the storage can be found - see details below . \"Gear\" icon Allows to edit the path, alias, description of the storage, manage its STS and LTS durations and enable versions control (see picture Figure 1 above, item 5 ). The delete option is also here (if storage contains only metadata, it will be deleted anyway). See 8.1. Create and edit storage . Refresh Allows updating representation of storage's contents (see picture Figure 1 above, item 6 ). + Create You can also create new folders and files via this button (see picture Figure 1 above, item 7 ). See 8.3. Create and Edit text files . Upload This control allows uploading files to the storage (see picture Figure 1 above, item 8 ). See 8.2. Upload/Download data . Each-line controls Control Description Download This control calls downloading of selected file. Edit Helps to rename a file or a folder. Delete Delete a file or a folder. Additional info and options You can view additional info about the storage and use the special abilities from the Attributes panel: Here, the following blocks are displayed: 1 - info block showing the number of transition rules , configured for the current folder. These rules allow to configure the automatic data transition from standard storage to different types of archival storages by occurrence of a certain event and restore that data back as well if needed. See section Storage lifecycle management for details about transition rules and files restoring. 2 - info block with the size of different storage tiers: Standard size - summary size of all files of standard type in the storage. First number shows sum of all current and previous files versions. A number in the parenthesis shows summary size of previous files versions only. Archive size - summary size of all files of archived types in the storage. First number shows sum of all current and previous files versions. A number in the parenthesis shows summary size of previous files versions only. See details here . Show details hyperlink - to view details about size of different storage tiers Re-index hyperlink - to recalculate sizes of storage tiers - as tiers info can not be updated immediately during the changes, user can use this hyperlink for forcible recalculation of the storage volume 3 - the hyperlink to request a file system access to the current storage Example of the storage details pop-up (appears by click the Show details hyperlink): Where: each row shows the volume of the specific storage tier columns: Storage class - type of a storage tier. It can be standard or one of archive types. The type is presented in the table only if there are/were any versions of files of this storage type Current ver. - the whole summary size in GB of all current versions of files of the specific type Previous ver. - the whole summary size in GB of all previous versions of files of the specific type Total - sum volume in GB of the current and previous files versions For more details about versioning in storages see the corresponding section . View and edit a text file You can view and edit text files. For more details see here . CLI Storage options There are also several options that are only implemented in CLI but not in GUI: To move files and folders from one storage to another or between local file system and storage. To copy files from one storage to another. See here for more details.","title":"8.0. Overview"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#8-manage-data-storage","text":"Data storage is a Cloud Pipeline object that represents cloud storage and its content in a folder hierarchy. Controls Select page Show file versions Remove all selected Generate URL Show attributes/Hide attributes \"Gear\" icon Refresh + Create Upload Each-line controls Additional info and options View and edit a text file CLI Storage options Permissions management for a storage is described here . \"Details view\" lists content of the storage: files that may be organized into folders. Clicking on the inside folder will open its content in the \"Details view\" . Note : storage's folders hierarchy will not be represented in the \"Hierarchy view\" panel. Note : you can move Storage to a new parent folder using drag and drop approach. In \"Library tree view\" and \"Details view\" data storages are tagged with region flag to visually distinguish storage locations. Note : if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - in that case auxiliary Cloud Provider icons are additionally displayed, e.g.: Figure 1 Another option for navigation in the storage is to use \"breadcrumbs\" control at the top of the \"Details\" view (see picture above, 1 ): Clicking an item will navigate to that folder. Editing a path will allow to copy/paste a path and navigate to any custom location.","title":"8. Manage Data Storage"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#controls","text":"At the top of the \"Details\" view there are buttons:","title":"Controls"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#select-page","text":"Clicking this control (see picture Figure 1 above, item 2 ), the whole file and folders on the current page will be selected. It allows to perform bulk operations like deleting.","title":"Select page"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#show-file-versions","text":"This feature is available not for all Cloud Providers. Currently, it is supported by AWS and GCP . Tick this checkbox (see picture Figure 1 above, item 3 ) and the view of a page will changed: the all file versions will be displayed. You can expand each version's list by clicking \" + \" in desired line. Note : the last version will be marked by \" (latest) \".","title":"Show file versions"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#remove-all-selected","text":"This is a bulk operation control. It is visible, if at least one of the data storage item (folder or file) is selected.","title":"Remove all selected"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#generate-url","text":"This control helps to generate URLs for a number of files and then download them manually one by one or via scripts. See details here . Note : the control is available, if only files are selected.","title":"Generate URL"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#show-attributeshide-attributes","text":"Allows see or edit a list of key=value attributes of the data storage (see picture Figure 1 above, item 4 ). Note : If selected storage has any defined attribute, Attributes pane is shown by default. See 17. CP objects tagging by additional attributes . Also, in the Attributes panel, there are additional options and info about the storage can be found - see details below .","title":"Show attributes/Hide attributes"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#gear-icon","text":"Allows to edit the path, alias, description of the storage, manage its STS and LTS durations and enable versions control (see picture Figure 1 above, item 5 ). The delete option is also here (if storage contains only metadata, it will be deleted anyway). See 8.1. Create and edit storage .","title":"\"Gear\" icon"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#refresh","text":"Allows updating representation of storage's contents (see picture Figure 1 above, item 6 ).","title":"Refresh"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#create","text":"You can also create new folders and files via this button (see picture Figure 1 above, item 7 ). See 8.3. Create and Edit text files .","title":"+ Create"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#upload","text":"This control allows uploading files to the storage (see picture Figure 1 above, item 8 ). See 8.2. Upload/Download data .","title":"Upload"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#each-line-controls","text":"Control Description Download This control calls downloading of selected file. Edit Helps to rename a file or a folder. Delete Delete a file or a folder.","title":"Each-line controls"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#additional-info-and-options","text":"You can view additional info about the storage and use the special abilities from the Attributes panel: Here, the following blocks are displayed: 1 - info block showing the number of transition rules , configured for the current folder. These rules allow to configure the automatic data transition from standard storage to different types of archival storages by occurrence of a certain event and restore that data back as well if needed. See section Storage lifecycle management for details about transition rules and files restoring. 2 - info block with the size of different storage tiers: Standard size - summary size of all files of standard type in the storage. First number shows sum of all current and previous files versions. A number in the parenthesis shows summary size of previous files versions only. Archive size - summary size of all files of archived types in the storage. First number shows sum of all current and previous files versions. A number in the parenthesis shows summary size of previous files versions only. See details here . Show details hyperlink - to view details about size of different storage tiers Re-index hyperlink - to recalculate sizes of storage tiers - as tiers info can not be updated immediately during the changes, user can use this hyperlink for forcible recalculation of the storage volume 3 - the hyperlink to request a file system access to the current storage Example of the storage details pop-up (appears by click the Show details hyperlink): Where: each row shows the volume of the specific storage tier columns: Storage class - type of a storage tier. It can be standard or one of archive types. The type is presented in the table only if there are/were any versions of files of this storage type Current ver. - the whole summary size in GB of all current versions of files of the specific type Previous ver. - the whole summary size in GB of all previous versions of files of the specific type Total - sum volume in GB of the current and previous files versions For more details about versioning in storages see the corresponding section .","title":"Additional info and options"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#view-and-edit-a-text-file","text":"You can view and edit text files. For more details see here .","title":"View and edit a text file"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#cli-storage-options","text":"There are also several options that are only implemented in CLI but not in GUI: To move files and folders from one storage to another or between local file system and storage. To copy files from one storage to another. See here for more details.","title":"CLI Storage options"},{"location":"manual/09_Manage_Cluster_nodes/9.1._Hot_node_pools/","text":"9.1. Hot node pools Overview Node pools management Pool creation View existing pools Edit existing pool Example Example of the \"autoscaled\" node pool For some jobs, a waiting for a node launch can be too long. It is convenient to have some scope of the running nodes in the background that will be always or on schedule be available. Cloud Pipeline allows controlling the number of persistent compute nodes in the cluster - i.e. a certain number ( cluster.min.size ) of the nodes of a specified size ( cluster.instance.type / cluster.instance.hdd ) will be always available in the cluster (even if there is no workload) - see details about System Preferences could be applied for the cluster here . This is useful to speed up the compute instances creation process (as the nodes are already up and running). But this mechanism can be expanded and be a bit more flexible by another platform ability - it is the ability to create \" hot node pools \". Overview Admin can create node pools: each pool contains one or several identical nodes - admin specifies the node configuration (instance type, disk, Cloud Region, etc.) and a corresponding number of such nodes. This count can be fixed or flexible (\"autoscaled\") each pool has the schedule of these nodes creation/termination . E.g. the majority of the new compute jobs are started during the workday, so no need to keep these persistent instances over the weekends. For the pool, several schedules can be specified for each pool can be configured additional filters - to restrict its usage by the specific users/groups or for the specific pipelines/tools etc. When the pool is created, corresponding nodes are being up ( according to pool's schedule(s) ) and waiting in background. If the user starts a job in this time and the instance requested for a job matches to the pool's node - such running node from the pool is automatically being assigned to the job. Note : pools management is available only for admins. Usage of pool nodes is available for any user. Node pools management For the node pools management a new tab is implemented in the Cluster State section - HOT NODE POOLS tab: Pool creation To create a new pool: At the HOT NODE POOLS tab, click the \" + Create \" button. The pop-up appears: Specify the pool name Specify the pool schedule - weekly time period during which the pool nodes will be up (running and waiting for the jobs): specify the day of the week and the time of the day for the beginning of the period, e.g.: specify the day of the week and the time of the day for the finish of the period, e.g.: if you wish, you can add several schedule periods for the pool. For that, click the \" + Add schedule \" button and repeats actions described above for a new period: remove the unnecessary schedule you can by the corresponding button next to the schedule Note : the pool shall have at least one schedule Specify the pool nodes count, e.g.: It is the number of nodes in the creating pool (nodes count that will be run according to the schedule and wait for jobs). Also you can set the checkbox Autoscaled - it allows to create instance pools with non-static size. In such case, the node pool size will be automatically adjust according to current cluster workload: increase pool size if most of its instances are assigned to active runs and decrease pool size if most of its instances are idle: In case of Autoscaled node pool, additional fields appear: Min Size - pool size cannot be decreased below this value. This count of nodes will be always run in cluster ( according to the schedule(s) ) Max Size - pool size cannot be increased above this value Scale Up Threshold - sets the threshold in percent. If percent of occupied instances of the pool is higher than this value, pool size shall be increased Scale Down Threshold - sets the threshold in percent. If percent of occupied instances of the pool is lower than this value, pool size shall be decreased Scale Step - pool size shall be decreased/increased with this step Specify the Cloud Provider / Region from where the pool nodes will be run: Specify the price type for the pool nodes ( Spot by default): Specify the instance type that will be used for the pool nodes: If you wish, you may specify the AMI that will be used for the pool nodes. It is non-required. Without specifying - the default deployment AMI will be used. Specify the disk size (in Gb ) for the pool nodes, e.g.: Note : the pool node will be assign to the job only if user request for a job a smaller disk than pool node has. Also note that the real disk size used for a job differs upwards from the user request - both these notes should be considered. E.g. if user requests 30 GB disk, real node disk may be 40 GB. Then if the pool node has disk setting 35 GB - it will not be assigned to the user job (as the real disk size 40 GB is more than the pool node disk size 35 GB). But if the pool node has disk setting 45 GB - it will be assigned normally to the user job. So, the pool disk should be set with a margin - of course, if the admin knows the disk size usually requested by the user. On the other hand, the pool node disk size can be purposely set as a large volume. In that case, it would be inefficient to use such pool nodes for user \"small\" requests. For resolve that issue, the certain system preference is implemented - cluster.reassign.disk.delta - this delta sets the max possible difference that could be between the pool node disk size and the real disk size requested by the user for a job. E.g. if the real disk size requested by the user is 100 GB, the pool node has disk setting 500 GB and cluster.reassign.disk.delta is set 300 GB - the pool node will not be assigned to the such user requested job (as the difference is more than delta). But if the real disk size requested by the user is 250 GB - the pool node will be assigned to the such requested job. Specify Docker image(s) that should be pre-uploaded to the pool nodes: click the \" + Add docker image \" button: select the image in the appeared dropdown list, e.g.: select the image version in the appeared additional dropdown list: you may add several images analogically as described above Note : at least one Docker image shall be added. Select of these images means that they will be pre-uploaded to the pool nodes. But such nodes also can be assigned to other jobs (with non-selected images) if other pool settings match the user request. In this case necessary requested image will be uploaded during the job initialization (so, it will just take longer). If you wish you can set additional filters for the pool usage. They can be customized at the Filters panel: Condition field allows to set which condition will use for the set filters: Filters section allows to add any count of additional filters for the created pool. Click the button to add a filter. In the appeared field select a property that will be used for the filtering: Possible properties: Run owner - allows to set user restrictions for the current pool usage Run owner group - allows to set user group restrictions for the current pool usage Pipeline - allows to set pipeline restrictions for the current pool usage Parameter - allows to set parameter (and its value) restrictions for the current pool usage Run configuration - allows to set detach configuration restrictions for the current pool usage Docker image - allows to set docker image restrictions for the current pool usage Set the desired property in the list, e.g.: For the selected property set the condition: For the selected property specify/select the value, e.g.: You can select any count of properties in the same way as described above, e.g.: In the example above, nodes from the pool will be assigned to the job only if it will be launched by any user (except USER1 and USER2 ) with the ROLE_USER role (of course, if other settings match). To remove the filter use the corresponding button in the filter row To confirm the pool creation, click the CREATE button Created pool will appear in the list: View existing pools At the HOT NODE POOLS tab, you can view all existing node pools, their schedules, settings, states. Where: a - pool name b - pool nodes settings (instance type, price type, disk size) c - pool nodes Cloud Provider / Region d - the summary count of nodes in the pool. In case of \"autoscaled\" pool, this count is displayed as an interval, e.g.: e - the list of pre-uploaded docker images on the pool nodes f - the list of pool schedules g - the count of pool nodes assigned to jobs in the current moment h - the summary count of pool nodes running in the current moment (in background or assigned to jobs) i - controls to edit/remove the pool To view the pool state, click it in the list, e.g.: The pool nodes list will appear: Nodes from the pool are marked by violet labels with the pool name. Pool nodes that are running in background but not assigned to jobs yet have Run ID with format P-<number> . Edit existing pool To edit the existing pool, click the corresponding button on its tile: The edit pop-up will be open: You can edit only pool name, pool schedules, pool nodes count (and \"autoscaled\" property/settings) and filters. Make changes and click the SAVE button to confirm. Example of using For example, there is the nodes pool that was created as described above : User launches the tool (in any time moment according to the schedule) with settings corresponding to pool nodes settings (same region, instance type, price type, smaller real disk size and in accordance with all configured filters ), e.g.: After the launch, the system checks if there are any empty nodes in the cluster. If not, then system checks are there any empty nodes in the \" hot node pools \" match the requested job - and uses such hot node pool if it matches: So, the running node from the \"hot node pool\" is assigned to the job after just several seconds. Please note, that the disk was changed according to the configured one for the pool node. Click the IP hyperlink to open node info: At the node monitor page, you can see that it is the node from the pool: Open the HOT NODE POOLS tab of the Cluster state section: See that the pool state is changed (one node from two is occupied). Example of using of the \"autoscaled\" node pool Open the static-size node pool, e.g.: Set the Autoscaled checkbox Specify the following autoscaled settings: Save changes: Launch at least 2 runs with settings corresponding to pool nodes settings - to occupy 2 of 3 nodes: As the occupied instances percent of the pool is higher than specified Scale Up Threshold (60%) - pool size will be automatically increased by the Scale Step size (2) in some time:","title":"9.1. Hot node pools"},{"location":"manual/09_Manage_Cluster_nodes/9.1._Hot_node_pools/#91-hot-node-pools","text":"Overview Node pools management Pool creation View existing pools Edit existing pool Example Example of the \"autoscaled\" node pool For some jobs, a waiting for a node launch can be too long. It is convenient to have some scope of the running nodes in the background that will be always or on schedule be available. Cloud Pipeline allows controlling the number of persistent compute nodes in the cluster - i.e. a certain number ( cluster.min.size ) of the nodes of a specified size ( cluster.instance.type / cluster.instance.hdd ) will be always available in the cluster (even if there is no workload) - see details about System Preferences could be applied for the cluster here . This is useful to speed up the compute instances creation process (as the nodes are already up and running). But this mechanism can be expanded and be a bit more flexible by another platform ability - it is the ability to create \" hot node pools \".","title":"9.1. Hot node pools"},{"location":"manual/09_Manage_Cluster_nodes/9.1._Hot_node_pools/#overview","text":"Admin can create node pools: each pool contains one or several identical nodes - admin specifies the node configuration (instance type, disk, Cloud Region, etc.) and a corresponding number of such nodes. This count can be fixed or flexible (\"autoscaled\") each pool has the schedule of these nodes creation/termination . E.g. the majority of the new compute jobs are started during the workday, so no need to keep these persistent instances over the weekends. For the pool, several schedules can be specified for each pool can be configured additional filters - to restrict its usage by the specific users/groups or for the specific pipelines/tools etc. When the pool is created, corresponding nodes are being up ( according to pool's schedule(s) ) and waiting in background. If the user starts a job in this time and the instance requested for a job matches to the pool's node - such running node from the pool is automatically being assigned to the job. Note : pools management is available only for admins. Usage of pool nodes is available for any user.","title":"Overview"},{"location":"manual/09_Manage_Cluster_nodes/9.1._Hot_node_pools/#node-pools-management","text":"For the node pools management a new tab is implemented in the Cluster State section - HOT NODE POOLS tab:","title":"Node pools management"},{"location":"manual/09_Manage_Cluster_nodes/9.1._Hot_node_pools/#pool-creation","text":"To create a new pool: At the HOT NODE POOLS tab, click the \" + Create \" button. The pop-up appears: Specify the pool name Specify the pool schedule - weekly time period during which the pool nodes will be up (running and waiting for the jobs): specify the day of the week and the time of the day for the beginning of the period, e.g.: specify the day of the week and the time of the day for the finish of the period, e.g.: if you wish, you can add several schedule periods for the pool. For that, click the \" + Add schedule \" button and repeats actions described above for a new period: remove the unnecessary schedule you can by the corresponding button next to the schedule Note : the pool shall have at least one schedule Specify the pool nodes count, e.g.: It is the number of nodes in the creating pool (nodes count that will be run according to the schedule and wait for jobs). Also you can set the checkbox Autoscaled - it allows to create instance pools with non-static size. In such case, the node pool size will be automatically adjust according to current cluster workload: increase pool size if most of its instances are assigned to active runs and decrease pool size if most of its instances are idle: In case of Autoscaled node pool, additional fields appear: Min Size - pool size cannot be decreased below this value. This count of nodes will be always run in cluster ( according to the schedule(s) ) Max Size - pool size cannot be increased above this value Scale Up Threshold - sets the threshold in percent. If percent of occupied instances of the pool is higher than this value, pool size shall be increased Scale Down Threshold - sets the threshold in percent. If percent of occupied instances of the pool is lower than this value, pool size shall be decreased Scale Step - pool size shall be decreased/increased with this step Specify the Cloud Provider / Region from where the pool nodes will be run: Specify the price type for the pool nodes ( Spot by default): Specify the instance type that will be used for the pool nodes: If you wish, you may specify the AMI that will be used for the pool nodes. It is non-required. Without specifying - the default deployment AMI will be used. Specify the disk size (in Gb ) for the pool nodes, e.g.: Note : the pool node will be assign to the job only if user request for a job a smaller disk than pool node has. Also note that the real disk size used for a job differs upwards from the user request - both these notes should be considered. E.g. if user requests 30 GB disk, real node disk may be 40 GB. Then if the pool node has disk setting 35 GB - it will not be assigned to the user job (as the real disk size 40 GB is more than the pool node disk size 35 GB). But if the pool node has disk setting 45 GB - it will be assigned normally to the user job. So, the pool disk should be set with a margin - of course, if the admin knows the disk size usually requested by the user. On the other hand, the pool node disk size can be purposely set as a large volume. In that case, it would be inefficient to use such pool nodes for user \"small\" requests. For resolve that issue, the certain system preference is implemented - cluster.reassign.disk.delta - this delta sets the max possible difference that could be between the pool node disk size and the real disk size requested by the user for a job. E.g. if the real disk size requested by the user is 100 GB, the pool node has disk setting 500 GB and cluster.reassign.disk.delta is set 300 GB - the pool node will not be assigned to the such user requested job (as the difference is more than delta). But if the real disk size requested by the user is 250 GB - the pool node will be assigned to the such requested job. Specify Docker image(s) that should be pre-uploaded to the pool nodes: click the \" + Add docker image \" button: select the image in the appeared dropdown list, e.g.: select the image version in the appeared additional dropdown list: you may add several images analogically as described above Note : at least one Docker image shall be added. Select of these images means that they will be pre-uploaded to the pool nodes. But such nodes also can be assigned to other jobs (with non-selected images) if other pool settings match the user request. In this case necessary requested image will be uploaded during the job initialization (so, it will just take longer). If you wish you can set additional filters for the pool usage. They can be customized at the Filters panel: Condition field allows to set which condition will use for the set filters: Filters section allows to add any count of additional filters for the created pool. Click the button to add a filter. In the appeared field select a property that will be used for the filtering: Possible properties: Run owner - allows to set user restrictions for the current pool usage Run owner group - allows to set user group restrictions for the current pool usage Pipeline - allows to set pipeline restrictions for the current pool usage Parameter - allows to set parameter (and its value) restrictions for the current pool usage Run configuration - allows to set detach configuration restrictions for the current pool usage Docker image - allows to set docker image restrictions for the current pool usage Set the desired property in the list, e.g.: For the selected property set the condition: For the selected property specify/select the value, e.g.: You can select any count of properties in the same way as described above, e.g.: In the example above, nodes from the pool will be assigned to the job only if it will be launched by any user (except USER1 and USER2 ) with the ROLE_USER role (of course, if other settings match). To remove the filter use the corresponding button in the filter row To confirm the pool creation, click the CREATE button Created pool will appear in the list:","title":"Pool creation"},{"location":"manual/09_Manage_Cluster_nodes/9.1._Hot_node_pools/#view-existing-pools","text":"At the HOT NODE POOLS tab, you can view all existing node pools, their schedules, settings, states. Where: a - pool name b - pool nodes settings (instance type, price type, disk size) c - pool nodes Cloud Provider / Region d - the summary count of nodes in the pool. In case of \"autoscaled\" pool, this count is displayed as an interval, e.g.: e - the list of pre-uploaded docker images on the pool nodes f - the list of pool schedules g - the count of pool nodes assigned to jobs in the current moment h - the summary count of pool nodes running in the current moment (in background or assigned to jobs) i - controls to edit/remove the pool To view the pool state, click it in the list, e.g.: The pool nodes list will appear: Nodes from the pool are marked by violet labels with the pool name. Pool nodes that are running in background but not assigned to jobs yet have Run ID with format P-<number> .","title":"View existing pools"},{"location":"manual/09_Manage_Cluster_nodes/9.1._Hot_node_pools/#edit-existing-pool","text":"To edit the existing pool, click the corresponding button on its tile: The edit pop-up will be open: You can edit only pool name, pool schedules, pool nodes count (and \"autoscaled\" property/settings) and filters. Make changes and click the SAVE button to confirm.","title":"Edit existing pool"},{"location":"manual/09_Manage_Cluster_nodes/9.1._Hot_node_pools/#example-of-using","text":"For example, there is the nodes pool that was created as described above : User launches the tool (in any time moment according to the schedule) with settings corresponding to pool nodes settings (same region, instance type, price type, smaller real disk size and in accordance with all configured filters ), e.g.: After the launch, the system checks if there are any empty nodes in the cluster. If not, then system checks are there any empty nodes in the \" hot node pools \" match the requested job - and uses such hot node pool if it matches: So, the running node from the \"hot node pool\" is assigned to the job after just several seconds. Please note, that the disk was changed according to the configured one for the pool node. Click the IP hyperlink to open node info: At the node monitor page, you can see that it is the node from the pool: Open the HOT NODE POOLS tab of the Cluster state section: See that the pool state is changed (one node from two is occupied).","title":"Example of using"},{"location":"manual/09_Manage_Cluster_nodes/9.1._Hot_node_pools/#example-of-using-of-the-autoscaled-node-pool","text":"Open the static-size node pool, e.g.: Set the Autoscaled checkbox Specify the following autoscaled settings: Save changes: Launch at least 2 runs with settings corresponding to pool nodes settings - to occupy 2 of 3 nodes: As the occupied instances percent of the pool is higher than specified Scale Up Threshold (60%) - pool size will be automatically increased by the Scale Step size (2) in some time:","title":"Example of using of the \"autoscaled\" node pool"},{"location":"manual/09_Manage_Cluster_nodes/9._Manage_Cluster_nodes/","text":"9. Cluster nodes \" Cluster nodes \" provides a list of working nodes. You can get information on nodes usage and terminate them in this tab. Overview Controls Node information page GENERAL INFO JOBS MONITOR Filters Zoom and scroll Export data Note : Nodes remain for the time that is already paid for, even if all runs at the node finished execution. So if you restart pipeline, new nodes will not be initialized saving time and money. Overview This tab shows Active nodes table that has information about: Name - a name of the node. Pipeline - a currently assigned run on the node. Labels of the node - characteristics extracted from the parameters of the node. There are common labels: RUN ID - ID of currently assigned run, MASTER/EDGE - service labels, nodes with this labels may be viewed only by ADMIN users. Addresses - node addresses. Created - a date of creation. Controls Control Description Terminate This control terminates node. Refresh To get currently active nodes list. Note : You can also terminate a node via CLI. For more details see here . Node information page Note : You can also view node information via CLI. See 14.6. View cluster nodes via CLI . Clicking on the row of the table will redirect you to detailed node information page. This page has three tabs. GENERAL INFO This tab allows seeing general info about the node, including: System information ; Addresses of internal network and domain name; Labels of the node automatically generated in accordance with system information; Node type - amounts of available and total memory, number of jobs and CPUs. JOBS \"JOBS\" tab lists jobs being processed at the moment. Name of the job; clicking \" + \" icon next to the name expands a list of containers needed for the job. Namespace for a job to be executed at; Status of the job; Requests and Limits of resources for the job. MONITOR \"MONITOR\" tab displays a dashboard with following diagrams: Diagram Description CPU usage A diagram represents CPU usage (cores) - time graph. The usage is displayed in fractions according to left vertical axis. There are two lines (data-series) - the first displays max value of the CPU usage in each moment and the second shows the average value of the usage during the time Memory usage A diagram represents memory usage - time graph. One type of graph represents usage in MB according to left vertical axis (includes two lines - first shows the max value in each moment and another shows the average values during the time). Another type of graph represents usage in % of available amounts of memory according to right vertical axis (analogically, includes two lines - max and average ). Network connection speed A diagram represents connection speed (bytes) - time graph. Blue graph ( TX ) represents \"transmit\" speed. Red graph ( RX ) represents \"receive\" speed. Drop-down at the top of the section allows changing connection protocol. File system load Represents all the disks of the machine and their loading. The current state of the resources utilization is available for all active runs. The maximum storage period for this data is set by the system preference system.resource.monitoring.stats.retention.period (in days, by default - 5). So, any utilization data older than that period is unavailable for users (no matter the run duration). The historical resources utilization is also available for completed runs (during the specified time storage period). It can be useful for debugging/optimization purposes. To view the monitor of resources utilization for the completed run: Open the COMPLETED RUNS page. Click the run you wish to view the resources utilization data, e.g.: At the opened Run logs page expand the \"Instance\" section: Click the node IP hyperlink. The monitor of the node resources utilization will appear: Please note, the resources utilization data for the completed run is available during system.resource.monitoring.stats.retention.period days. If you'll try to view monitor of the completed run after the specified period is over - the monitor will be empty. Filters User can manage plots date configurations. For this purpose the system has number of filters: Common range for all charts (1) User can synchronize the time period for all charts. To do so user should mark this filter. If this filter is unmarked, user can zoom or scroll any plot without any change for others. Live update (2) If this checkbox is marked the charts data will be updated every 5 seconds in a real-time manner. The fields with dates will be updated as well. This filter can be marked only in pair with the Common range for all charts filter. If both checkboxes were unmarked and user set the Live update filter active, the system would mark both checkboxes. This feature is available only for active runs. Set range (3) User can select the predefined time range for all plots from the list: Whole range Last week Last day Last hour This filter works in pair with the Common range for all charts filter. If user sets the date range, the system will mark the Common range for all charts checkbox, if it wasn't. So the data in all charts will be filtered by the selected range. Date filter (4) User can specify the Start and the End dates for plots using this filter. By default, the Start date (the left field of the filter) is the node creating datetime, the End date (the right field of the filter) is the current datetime. To change the Start \\/ End date the user should: click the corresponding date field, e.g.: the calendar will be displayed: The dates before the node creation and after today will be unavailable to select: click the specific available date in the calendar. Selected date will appear in the date field. Charts will be automatically redrawn according to the new set period. If the user focuses on the calendar icon or the whole field at any of date fields the \"Cross\" button will be displayed: If click this button in the Start date field - the node creation date will be substituted into that filter. If click this button in the End date field - the date in that field will be erased and the system will interpret it as the current datetime. Zooming and scrolling features In addition, user can scroll plots. To do so: focus on the plot, hold the left mouse button and move the mouse in the desired direction (left or right) Note : if Common range for all charts filter is on, all charts will be moving simultaneously Another feature is chart zooming . To zoom a chart: hold the Shift key and scroll the plot via mouse. The area will be highlighted: Then release the Shift key and the highlighted area will be automatically zoomed. another way of zooming plot - using the right panel. There are Plus and Minus buttons for such purpose on it: Click the desired button and the chart will be zoomed. Export utilization data Users have the ability to export the utilization information into a csv or xls file. This can be useful, if the user wants to keep locally the information for a longer period of time than defined by the preference system.resource.monitoring.stats.retention.period . csv reports contain only raw monitoring data. xls reports contain not only raw monitoring data but the graphical info (diagrams) too as users can see on the GUI. To export resources utilization data at the Monitor page of the node: Hover over the Export button in the right upper corner of the page: There are the following items in the list: Excel - to download the monitoring report in xls format with default settings CSV - to download the monitoring report in csv format with default settings Configure export - to customize intervals between resources utilization statistics, select format and download a report with that custom settings If you select Excel or CSV item in the list, the corresponding file will be downloaded automatically Example of csv report file: Report in xls format contains the following datasheets: CHARTS - datasheet with diagrams equal to the ones on the GUI (\" CPU usage \", \" Memory usage \", \" Network connection speed \", \" File system load \"), e.g.: DATA - datasheet with raw resources utilization statistics (similar to csv report), e.g.: SCALED_DATA and DISKS_SUMMARY - auxiliary datasheets with data selection from DATA datasheet that are needed for building charts To configure the export click the Configure export button in the list. The pop-up will be open: Here: Format dropdown list defines the format for the report ( xls / csv ) to be exported Ticks dropdown list defines the interval of resources utilization statistics that will be exported ( Note : the suggested intervals in the list depends on the node running duration), e.g.: once the settings are configured click the EXPORT button - the report will be prepared and downloaded You also can export node usage report via CLI . See here .","title":"9.0. Cluster nodes"},{"location":"manual/09_Manage_Cluster_nodes/9._Manage_Cluster_nodes/#9-cluster-nodes","text":"\" Cluster nodes \" provides a list of working nodes. You can get information on nodes usage and terminate them in this tab. Overview Controls Node information page GENERAL INFO JOBS MONITOR Filters Zoom and scroll Export data Note : Nodes remain for the time that is already paid for, even if all runs at the node finished execution. So if you restart pipeline, new nodes will not be initialized saving time and money.","title":"9. Cluster nodes"},{"location":"manual/09_Manage_Cluster_nodes/9._Manage_Cluster_nodes/#overview","text":"This tab shows Active nodes table that has information about: Name - a name of the node. Pipeline - a currently assigned run on the node. Labels of the node - characteristics extracted from the parameters of the node. There are common labels: RUN ID - ID of currently assigned run, MASTER/EDGE - service labels, nodes with this labels may be viewed only by ADMIN users. Addresses - node addresses. Created - a date of creation.","title":"Overview"},{"location":"manual/09_Manage_Cluster_nodes/9._Manage_Cluster_nodes/#controls","text":"Control Description Terminate This control terminates node. Refresh To get currently active nodes list. Note : You can also terminate a node via CLI. For more details see here .","title":"Controls"},{"location":"manual/09_Manage_Cluster_nodes/9._Manage_Cluster_nodes/#node-information-page","text":"Note : You can also view node information via CLI. See 14.6. View cluster nodes via CLI . Clicking on the row of the table will redirect you to detailed node information page. This page has three tabs.","title":"Node information page"},{"location":"manual/09_Manage_Cluster_nodes/9._Manage_Cluster_nodes/#general-info","text":"This tab allows seeing general info about the node, including: System information ; Addresses of internal network and domain name; Labels of the node automatically generated in accordance with system information; Node type - amounts of available and total memory, number of jobs and CPUs.","title":"GENERAL INFO"},{"location":"manual/09_Manage_Cluster_nodes/9._Manage_Cluster_nodes/#jobs","text":"\"JOBS\" tab lists jobs being processed at the moment. Name of the job; clicking \" + \" icon next to the name expands a list of containers needed for the job. Namespace for a job to be executed at; Status of the job; Requests and Limits of resources for the job.","title":"JOBS"},{"location":"manual/09_Manage_Cluster_nodes/9._Manage_Cluster_nodes/#monitor","text":"\"MONITOR\" tab displays a dashboard with following diagrams: Diagram Description CPU usage A diagram represents CPU usage (cores) - time graph. The usage is displayed in fractions according to left vertical axis. There are two lines (data-series) - the first displays max value of the CPU usage in each moment and the second shows the average value of the usage during the time Memory usage A diagram represents memory usage - time graph. One type of graph represents usage in MB according to left vertical axis (includes two lines - first shows the max value in each moment and another shows the average values during the time). Another type of graph represents usage in % of available amounts of memory according to right vertical axis (analogically, includes two lines - max and average ). Network connection speed A diagram represents connection speed (bytes) - time graph. Blue graph ( TX ) represents \"transmit\" speed. Red graph ( RX ) represents \"receive\" speed. Drop-down at the top of the section allows changing connection protocol. File system load Represents all the disks of the machine and their loading. The current state of the resources utilization is available for all active runs. The maximum storage period for this data is set by the system preference system.resource.monitoring.stats.retention.period (in days, by default - 5). So, any utilization data older than that period is unavailable for users (no matter the run duration). The historical resources utilization is also available for completed runs (during the specified time storage period). It can be useful for debugging/optimization purposes. To view the monitor of resources utilization for the completed run: Open the COMPLETED RUNS page. Click the run you wish to view the resources utilization data, e.g.: At the opened Run logs page expand the \"Instance\" section: Click the node IP hyperlink. The monitor of the node resources utilization will appear: Please note, the resources utilization data for the completed run is available during system.resource.monitoring.stats.retention.period days. If you'll try to view monitor of the completed run after the specified period is over - the monitor will be empty.","title":"MONITOR"},{"location":"manual/09_Manage_Cluster_nodes/9._Manage_Cluster_nodes/#filters","text":"User can manage plots date configurations. For this purpose the system has number of filters: Common range for all charts (1) User can synchronize the time period for all charts. To do so user should mark this filter. If this filter is unmarked, user can zoom or scroll any plot without any change for others. Live update (2) If this checkbox is marked the charts data will be updated every 5 seconds in a real-time manner. The fields with dates will be updated as well. This filter can be marked only in pair with the Common range for all charts filter. If both checkboxes were unmarked and user set the Live update filter active, the system would mark both checkboxes. This feature is available only for active runs. Set range (3) User can select the predefined time range for all plots from the list: Whole range Last week Last day Last hour This filter works in pair with the Common range for all charts filter. If user sets the date range, the system will mark the Common range for all charts checkbox, if it wasn't. So the data in all charts will be filtered by the selected range. Date filter (4) User can specify the Start and the End dates for plots using this filter. By default, the Start date (the left field of the filter) is the node creating datetime, the End date (the right field of the filter) is the current datetime. To change the Start \\/ End date the user should: click the corresponding date field, e.g.: the calendar will be displayed: The dates before the node creation and after today will be unavailable to select: click the specific available date in the calendar. Selected date will appear in the date field. Charts will be automatically redrawn according to the new set period. If the user focuses on the calendar icon or the whole field at any of date fields the \"Cross\" button will be displayed: If click this button in the Start date field - the node creation date will be substituted into that filter. If click this button in the End date field - the date in that field will be erased and the system will interpret it as the current datetime.","title":"Filters"},{"location":"manual/09_Manage_Cluster_nodes/9._Manage_Cluster_nodes/#zooming-and-scrolling-features","text":"In addition, user can scroll plots. To do so: focus on the plot, hold the left mouse button and move the mouse in the desired direction (left or right) Note : if Common range for all charts filter is on, all charts will be moving simultaneously Another feature is chart zooming . To zoom a chart: hold the Shift key and scroll the plot via mouse. The area will be highlighted: Then release the Shift key and the highlighted area will be automatically zoomed. another way of zooming plot - using the right panel. There are Plus and Minus buttons for such purpose on it: Click the desired button and the chart will be zoomed.","title":"Zooming and scrolling features"},{"location":"manual/09_Manage_Cluster_nodes/9._Manage_Cluster_nodes/#export-utilization-data","text":"Users have the ability to export the utilization information into a csv or xls file. This can be useful, if the user wants to keep locally the information for a longer period of time than defined by the preference system.resource.monitoring.stats.retention.period . csv reports contain only raw monitoring data. xls reports contain not only raw monitoring data but the graphical info (diagrams) too as users can see on the GUI. To export resources utilization data at the Monitor page of the node: Hover over the Export button in the right upper corner of the page: There are the following items in the list: Excel - to download the monitoring report in xls format with default settings CSV - to download the monitoring report in csv format with default settings Configure export - to customize intervals between resources utilization statistics, select format and download a report with that custom settings If you select Excel or CSV item in the list, the corresponding file will be downloaded automatically Example of csv report file: Report in xls format contains the following datasheets: CHARTS - datasheet with diagrams equal to the ones on the GUI (\" CPU usage \", \" Memory usage \", \" Network connection speed \", \" File system load \"), e.g.: DATA - datasheet with raw resources utilization statistics (similar to csv report), e.g.: SCALED_DATA and DISKS_SUMMARY - auxiliary datasheets with data selection from DATA datasheet that are needed for building charts To configure the export click the Configure export button in the list. The pop-up will be open: Here: Format dropdown list defines the format for the report ( xls / csv ) to be exported Ticks dropdown list defines the interval of resources utilization statistics that will be exported ( Note : the suggested intervals in the list depends on the node running duration), e.g.: once the settings are configured click the EXPORT button - the report will be prepared and downloaded You also can export node usage report via CLI . See here .","title":"Export utilization data"},{"location":"manual/10_Manage_Tools/10.1._Add_Edit_a_Docker_registry/","text":"10.1. Add/Edit a Docker registry Add registry Edit/delete registry Customize registry permissions A docker registry is a storage and content delivery system, holding named Docker images, available in different tagged versions. This page describes the process of adding and editing Docker registries to the Cloud Pipeline . Also here you will find information about permission management for Docker registries. Only administrators can create Docker registries. To edit its parameters you need to have WRITE permissions. For more information see 13. Permissions . Add registry Configured Docker registry can be added to the Cloud Pipeline with the following commands: In the Tools tab click the Gear icon \u2192 Registry \u2192 + Create . Then set the registry parameters: Path (mandatory field) - IP address/domain name of the machine with configured Docker registry. Description - registry description. Require security scanning - tick the box to allow scheduled security scanning procedure for the Tools in the registry. For more information see 10.6. Tool security check . User name * and Password * (optional field) - if the registry is closed, you have to set a username and a password. Certificate * - if Docker registry uses a self-signed certificate, upload it with the Choose file button to set HTTPS access to the registry. External URL * - URL that can be exploited to push/pull Docker images to/from a registry. Pipeline authentication * - tick to make registry use Cloud Pipeline authentication system. * click Edit credentials to get these fields. You'll see a new registry in the registry list. Edit/delete registry To edit/view registry attributes - see 17. CP objects tagging by additional attributes . Choose registry from the registry list and click the Gear icon \u2192 Registry \u2192 Edit . You'll be able to modify registry parameters in the Info tab ( a ). Note : you can modify all registry parameters except Path . If you wish to detach registry from the Cloud Pipeline - click the Delete button ( b ). Customize registry permissions Users with ROLE_ADMIN or OWNER rights can modify permissions for a registry. For detailed instruction refer to this document .","title":"10.1. Add and edit a Docker registry"},{"location":"manual/10_Manage_Tools/10.1._Add_Edit_a_Docker_registry/#101-addedit-a-docker-registry","text":"Add registry Edit/delete registry Customize registry permissions A docker registry is a storage and content delivery system, holding named Docker images, available in different tagged versions. This page describes the process of adding and editing Docker registries to the Cloud Pipeline . Also here you will find information about permission management for Docker registries. Only administrators can create Docker registries. To edit its parameters you need to have WRITE permissions. For more information see 13. Permissions .","title":"10.1. Add/Edit a Docker registry"},{"location":"manual/10_Manage_Tools/10.1._Add_Edit_a_Docker_registry/#add-registry","text":"Configured Docker registry can be added to the Cloud Pipeline with the following commands: In the Tools tab click the Gear icon \u2192 Registry \u2192 + Create . Then set the registry parameters: Path (mandatory field) - IP address/domain name of the machine with configured Docker registry. Description - registry description. Require security scanning - tick the box to allow scheduled security scanning procedure for the Tools in the registry. For more information see 10.6. Tool security check . User name * and Password * (optional field) - if the registry is closed, you have to set a username and a password. Certificate * - if Docker registry uses a self-signed certificate, upload it with the Choose file button to set HTTPS access to the registry. External URL * - URL that can be exploited to push/pull Docker images to/from a registry. Pipeline authentication * - tick to make registry use Cloud Pipeline authentication system. * click Edit credentials to get these fields. You'll see a new registry in the registry list.","title":"Add registry"},{"location":"manual/10_Manage_Tools/10.1._Add_Edit_a_Docker_registry/#editdelete-registry","text":"To edit/view registry attributes - see 17. CP objects tagging by additional attributes . Choose registry from the registry list and click the Gear icon \u2192 Registry \u2192 Edit . You'll be able to modify registry parameters in the Info tab ( a ). Note : you can modify all registry parameters except Path . If you wish to detach registry from the Cloud Pipeline - click the Delete button ( b ).","title":"Edit/delete registry"},{"location":"manual/10_Manage_Tools/10.1._Add_Edit_a_Docker_registry/#customize-registry-permissions","text":"Users with ROLE_ADMIN or OWNER rights can modify permissions for a registry. For detailed instruction refer to this document .","title":"Customize registry permissions"},{"location":"manual/10_Manage_Tools/10.2._Add_Edit_a_Tool_group/","text":"10.2. Add/Edit a Tool group Add a generic Tool group Add a personal Tool group Edit/delete a Tool group Customize Tool group permissions A Tool group is an object of the Cloud Pipeline that allows to organize Tools into groups. This page describes the process of adding and editing Tool groups. Also here you will find information about permission management for Tool groups. To create a Tool group, user need to have WRITE permission for a Docker registry and the ROLE_TOOL_GROUP_MANAGER role. To edit Tool group parameters you need to have WRITE permissions for it. For more information see 13. Permissions . Add a generic Tool group A Tool group can be added to a Docker registry in the following way: In the Tools tab click the Gear icon \u2192 Group \u2192 + Create . Give your Tool group a name and an optional description : Click the CREATE button to confirm. You'll be automatically redirected to the new Tool group in the current Docker registry. Add a personal Tool group In the Tools tab click the Gear icon \u2192 Group \u2192 + Create personal . Another way to do that is to select personal Tool group from the Tool group list And then press the Create personal tool group button. After that you'll be able to upload Tools to your personal Tool group. Edit/delete a Tool group Choose a Tool group and click the Gear icon \u2192 Group \u2192 Edit . You'll be able to modify Tool group description in the Info tab. If you wish to delete a Tool group: click the Gear icon \u2192 Group \u2192 Delete : set the \"Delete child tools\" checkbox if the group is not empty and click the Delete button to confirm: Customize Tool group permissions Users with ROLE_ADMIN or OWNER rights can modify permissions for this Tool group. It is convenient when you want to manage access for the whole Tool group and not for the individual Tools. For more details see here .","title":"10.2. Add/edit a Tool group"},{"location":"manual/10_Manage_Tools/10.2._Add_Edit_a_Tool_group/#102-addedit-a-tool-group","text":"Add a generic Tool group Add a personal Tool group Edit/delete a Tool group Customize Tool group permissions A Tool group is an object of the Cloud Pipeline that allows to organize Tools into groups. This page describes the process of adding and editing Tool groups. Also here you will find information about permission management for Tool groups. To create a Tool group, user need to have WRITE permission for a Docker registry and the ROLE_TOOL_GROUP_MANAGER role. To edit Tool group parameters you need to have WRITE permissions for it. For more information see 13. Permissions .","title":"10.2. Add/Edit a Tool group"},{"location":"manual/10_Manage_Tools/10.2._Add_Edit_a_Tool_group/#add-a-generic-tool-group","text":"A Tool group can be added to a Docker registry in the following way: In the Tools tab click the Gear icon \u2192 Group \u2192 + Create . Give your Tool group a name and an optional description : Click the CREATE button to confirm. You'll be automatically redirected to the new Tool group in the current Docker registry.","title":"Add a generic Tool group"},{"location":"manual/10_Manage_Tools/10.2._Add_Edit_a_Tool_group/#add-a-personal-tool-group","text":"In the Tools tab click the Gear icon \u2192 Group \u2192 + Create personal . Another way to do that is to select personal Tool group from the Tool group list And then press the Create personal tool group button. After that you'll be able to upload Tools to your personal Tool group.","title":"Add a personal Tool group"},{"location":"manual/10_Manage_Tools/10.2._Add_Edit_a_Tool_group/#editdelete-a-tool-group","text":"Choose a Tool group and click the Gear icon \u2192 Group \u2192 Edit . You'll be able to modify Tool group description in the Info tab. If you wish to delete a Tool group: click the Gear icon \u2192 Group \u2192 Delete : set the \"Delete child tools\" checkbox if the group is not empty and click the Delete button to confirm:","title":"Edit/delete a Tool group"},{"location":"manual/10_Manage_Tools/10.2._Add_Edit_a_Tool_group/#customize-tool-group-permissions","text":"Users with ROLE_ADMIN or OWNER rights can modify permissions for this Tool group. It is convenient when you want to manage access for the whole Tool group and not for the individual Tools. For more details see here .","title":"Customize Tool group permissions"},{"location":"manual/10_Manage_Tools/10.3._Add_a_Tool/","text":"10.3. Add a Tool This page describes the process of adding new Docker image to the Docker registry. Docker CLI has to be installed. A registry has to be configured to use Cloud Pipeline (CP) authentication system (see 10.1. Add/Edit a Docker registry ). In case of registry doesn't use CP authentication system - contact your system administrators for registry access details. Go to the Tools tab and choose a registry. Click the Gear icon \u2192 How to configure? Copy and paste a command from the Login into cloud registry section ( a ) into the Terminal, then run it. This command installs registry certificate and uses docker login command to access the registry. Note : this command requires \"root\" rights. Copy and run instructions from the Push a local docker image to the cloud registry section ( b ) consequentially to add the Tool to the registry: Create environment variable ( MY_LOCAL_DOCKER_IMAGE ) that holds the name of the Docker image; Tag the image you want to push with the domain name or IP address and the port of the Docker registry; Push the image to the registry. Example : here we push \" hello-world \" image to the registry \" 18.195.69.178:5000 \", \" library \" Tool group. Troubleshooting section ( c ) contains information about fixing common problems that may appear during the execution of docker login or docker pull/push commands. Make sure that image was pushed to the registry and enabled in the Tools tab. Note : Tool will be automatically enabled only if CP authentication is configured for the registry. Note : If registry doesn't use CP authentication system, after the image was pushed do the following: Navigate to the Tools tab. Choose a Registry and a Tool group . Click the Gear icon \u2192 + Enable Tool . Write the name of the pushed image. Registry and Tool group will be already written for you.","title":"10.3. Add a Tool"},{"location":"manual/10_Manage_Tools/10.3._Add_a_Tool/#103-add-a-tool","text":"This page describes the process of adding new Docker image to the Docker registry. Docker CLI has to be installed. A registry has to be configured to use Cloud Pipeline (CP) authentication system (see 10.1. Add/Edit a Docker registry ). In case of registry doesn't use CP authentication system - contact your system administrators for registry access details. Go to the Tools tab and choose a registry. Click the Gear icon \u2192 How to configure? Copy and paste a command from the Login into cloud registry section ( a ) into the Terminal, then run it. This command installs registry certificate and uses docker login command to access the registry. Note : this command requires \"root\" rights. Copy and run instructions from the Push a local docker image to the cloud registry section ( b ) consequentially to add the Tool to the registry: Create environment variable ( MY_LOCAL_DOCKER_IMAGE ) that holds the name of the Docker image; Tag the image you want to push with the domain name or IP address and the port of the Docker registry; Push the image to the registry. Example : here we push \" hello-world \" image to the registry \" 18.195.69.178:5000 \", \" library \" Tool group. Troubleshooting section ( c ) contains information about fixing common problems that may appear during the execution of docker login or docker pull/push commands. Make sure that image was pushed to the registry and enabled in the Tools tab. Note : Tool will be automatically enabled only if CP authentication is configured for the registry. Note : If registry doesn't use CP authentication system, after the image was pushed do the following: Navigate to the Tools tab. Choose a Registry and a Tool group . Click the Gear icon \u2192 + Enable Tool . Write the name of the pushed image. Registry and Tool group will be already written for you.","title":"10.3. Add a Tool"},{"location":"manual/10_Manage_Tools/10.4._Edit_a_Tool/","text":"10.4. Edit a Tool Edit a Tool description Edit a Tool version Run/Delete a Tool version Commit a Tool Edit a Tool settings Delete a Tool To view and edit tool's attributes see 17. CP objects tagging by additional attributes . Edit a Tool description Select a Tool and click its name. Navigate to the Description tab of a Tool. Click the Edit buttons on the right side of the screen to modify the Short description and Full description of the Tool. Edit a Tool version In this tab, you can run a Tool version with custom settings or delete a Tool version. To see an example of a launching a Tool with custom settings see here . Run/Delete a Tool version Navigate to the Versions tab of a Tool. Click on the arrow near the Run button. Select Custom settings option to configure run parameters. If you want to delete a Tool version, click the Delete button. Commit a Tool Commit function allows modifying existing Tools. Launch a Tool in the sleep infinity mode. See an example here . SSH into it via the SSH button at the \" Run logs \" page. Change something. Example : here we install a biopython package into this Docker image. Wait for the installation to complete! Go back to the Logs tab and click the COMMIT button. Choose a Docker registry and a Tool group. Change a name for the modified image or add a new version to the current Tool by typing the version name in a separate box. Note : to add a new version to the existing Tool don't change the original name of the Tool! Note : image name and a version name should be written according to the following rules: May contain lowercase letters, digits, and separators. A separator is defined as a period, one or two underscores, or one or more dashes. A name component may not start or end with a separator. Tick boxes if needed: Delete runtime files box - to delete all files from /runs/[pipeline name] folder before committing. Stop pipeline box - to stop the current run after committing. In this example, we will change \"base-generic-centos7\" to \"base-generic-centos7-biopython\". Committing may take some time: When it is complete COMMITING status on the right side of the screen will change to COMMIT SUCCEEDED . In round brackets the date/time of the latest commit is shown: Find a modified Tool in the registry. Committing features In certain use-cases, extra steps shall be executed before/after running the commit command in the container. For example, to avoid warning messages about terminating the previous session (which was committed) of the tool application in a non-graceful manner. Some applications may require extra cleanup to be performed before the termination. To workaround such issues in the Cloud Pipeline an approach of \" pre/post-commit hooks \" is implemented. That allows to perform some graceful cleanup/restore before/after performing the commit itself. Note : Those hooks are valid only for the specific images and therefore shall be contained within those images. Cloud Pipeline itself only performs the calls to the hooks if they exist. There are two preferences from system-level settings that determine a behavior of the \" pre/post-commit hooks \" approach: commit.pre.command.path : specified a path to a script within a docker image, that will be executed in a currently running container, before docker commit occurs (default value: /root/pre_commit.sh ). This option is useful if any operations shall be performed with the running processes (e.g. send a signal), because in the subsequent post operation - only filesystem operations will be available. Note : any changes done at this stage will affect the running container. commit.post.command.path : specified a path to a script within a docker image, that will be executed in a committed image, after docker commit occurs (default value: /root/post_commit.sh ). This hook can be used to perform any filesystem cleanup or other operations, that shall not affect the currently running processes. Note : User shall have ROLE_ADMIN to read and update system-level settings. So, when you try to commit some tool, the Cloud Pipeline will check preferences described above and execute scripts with the specified names, if that scripts files exist in a docker image. If a corresponding pre/post script is not found in the docker image - it will not be executed. Consider an example with RStudio tool, that Cloud Pipeline provides \"out of the box\". RStudio Docker image contains a post-commit script that cleans the session after the commit: Open the Tools page, launch RStudio . SSH to the launched run via the SSH control in the upper-right corner of the run logs page. Check that post_commit.sh script exists in the Docker image - use the command ls /root/ : View the contents of that script. It looks like that: Note : /etc/cp_env.sh is a special Cloud Pipeline script, that sets all environment variables of the current docker container. So, as you can see from the code, as the result of that script launch will be a cleanup (removing) of the RStudio active session from the user home directory. Close SSH tab and try to do a commit - click the COMMIT control in the upper-right corner of the run logs page. Specify a new name for the committed tool, e.g. rstudio-test : While image is being committed, you can see the follow-up log for the CommitPipelineRun task: Here: ( 1 ) pre-commit script is not found in the docker image - nothing will be executed before commit ( 2 ) post-commit script is found at the specified path in the docker image - and it is being executed ( 3 ) post-commit script was performed successfully Edit a Tool settings Settings in this tab are applied to all Tool versions (i.e. these settings will be a default for all Tool version). Navigate to the Settings tab. Specify Endpoints for a Tool by click \" Add endpoint \" button: In the example below: the port is 8788 and the endpoint name is rstudio-s3fs : Let's look at the endpoint closer: \"nginx\" - type of the endpoints (only nginx is currently supported) \"port\": XXXX - an application will be deployed on this port on the pipeline node. You can specify additional nginx configuration for that endpoint in the text field bottom in JSON format. Note : optional path parameter may be required in case your application starts on <host:port:>/ path . Note : optional additional parameter may be required in case you need to specify nginx location settings. See more information here . \"name\" - this value will be visible as a hyperlink in the UI. It is especially convenient when user sets more than one endpoint configuration for an interactive tool (learn more about interactive services - 15. Interactive services ). In the example below, we name one endpoint as \"rstudio-s3fs\" and another one as \"shiny\" . This is how everything will look in the Run log window: Click the + New Label button and add a label to the Tool (e.g. \"Awesome Tool\"). Specify \" Execution defaults \": Instance type (e.g. \"m4.large\") Price type (e.g. \"Spot\") Disk size in Gb (e.g. \"20\") select available storages configure cluster, if it's necessary for your Tool write the Default command for the Tool execution (e.g. echo \"Hello world!\" ) If it's necessary for your Tool - add system or custom parameters. For more details see 6.1. Create and configure pipeline . Click Save button to save these settings. Delete a Tool To detach a Tool from the Cloud Pipeline: Open the Tool you wish to detach Hover over the gear icon in the right upper corner and click the \"Delete tool\" item in the appeared list: Click the Delete button to confirm: Note : if you try to delete a tool/version, but there is/are active job(s) using it - the corresponding warning notification will be shown, e.g.:","title":"10.4. Edit a Tool"},{"location":"manual/10_Manage_Tools/10.4._Edit_a_Tool/#104-edit-a-tool","text":"Edit a Tool description Edit a Tool version Run/Delete a Tool version Commit a Tool Edit a Tool settings Delete a Tool To view and edit tool's attributes see 17. CP objects tagging by additional attributes .","title":"10.4. Edit a Tool"},{"location":"manual/10_Manage_Tools/10.4._Edit_a_Tool/#edit-a-tool-description","text":"Select a Tool and click its name. Navigate to the Description tab of a Tool. Click the Edit buttons on the right side of the screen to modify the Short description and Full description of the Tool.","title":"Edit a Tool description"},{"location":"manual/10_Manage_Tools/10.4._Edit_a_Tool/#edit-a-tool-version","text":"In this tab, you can run a Tool version with custom settings or delete a Tool version. To see an example of a launching a Tool with custom settings see here .","title":"Edit a Tool version"},{"location":"manual/10_Manage_Tools/10.4._Edit_a_Tool/#rundelete-a-tool-version","text":"Navigate to the Versions tab of a Tool. Click on the arrow near the Run button. Select Custom settings option to configure run parameters. If you want to delete a Tool version, click the Delete button.","title":"Run/Delete a Tool version"},{"location":"manual/10_Manage_Tools/10.4._Edit_a_Tool/#commit-a-tool","text":"Commit function allows modifying existing Tools. Launch a Tool in the sleep infinity mode. See an example here . SSH into it via the SSH button at the \" Run logs \" page. Change something. Example : here we install a biopython package into this Docker image. Wait for the installation to complete! Go back to the Logs tab and click the COMMIT button. Choose a Docker registry and a Tool group. Change a name for the modified image or add a new version to the current Tool by typing the version name in a separate box. Note : to add a new version to the existing Tool don't change the original name of the Tool! Note : image name and a version name should be written according to the following rules: May contain lowercase letters, digits, and separators. A separator is defined as a period, one or two underscores, or one or more dashes. A name component may not start or end with a separator. Tick boxes if needed: Delete runtime files box - to delete all files from /runs/[pipeline name] folder before committing. Stop pipeline box - to stop the current run after committing. In this example, we will change \"base-generic-centos7\" to \"base-generic-centos7-biopython\". Committing may take some time: When it is complete COMMITING status on the right side of the screen will change to COMMIT SUCCEEDED . In round brackets the date/time of the latest commit is shown: Find a modified Tool in the registry.","title":"Commit a Tool"},{"location":"manual/10_Manage_Tools/10.4._Edit_a_Tool/#committing-features","text":"In certain use-cases, extra steps shall be executed before/after running the commit command in the container. For example, to avoid warning messages about terminating the previous session (which was committed) of the tool application in a non-graceful manner. Some applications may require extra cleanup to be performed before the termination. To workaround such issues in the Cloud Pipeline an approach of \" pre/post-commit hooks \" is implemented. That allows to perform some graceful cleanup/restore before/after performing the commit itself. Note : Those hooks are valid only for the specific images and therefore shall be contained within those images. Cloud Pipeline itself only performs the calls to the hooks if they exist. There are two preferences from system-level settings that determine a behavior of the \" pre/post-commit hooks \" approach: commit.pre.command.path : specified a path to a script within a docker image, that will be executed in a currently running container, before docker commit occurs (default value: /root/pre_commit.sh ). This option is useful if any operations shall be performed with the running processes (e.g. send a signal), because in the subsequent post operation - only filesystem operations will be available. Note : any changes done at this stage will affect the running container. commit.post.command.path : specified a path to a script within a docker image, that will be executed in a committed image, after docker commit occurs (default value: /root/post_commit.sh ). This hook can be used to perform any filesystem cleanup or other operations, that shall not affect the currently running processes. Note : User shall have ROLE_ADMIN to read and update system-level settings. So, when you try to commit some tool, the Cloud Pipeline will check preferences described above and execute scripts with the specified names, if that scripts files exist in a docker image. If a corresponding pre/post script is not found in the docker image - it will not be executed. Consider an example with RStudio tool, that Cloud Pipeline provides \"out of the box\". RStudio Docker image contains a post-commit script that cleans the session after the commit: Open the Tools page, launch RStudio . SSH to the launched run via the SSH control in the upper-right corner of the run logs page. Check that post_commit.sh script exists in the Docker image - use the command ls /root/ : View the contents of that script. It looks like that: Note : /etc/cp_env.sh is a special Cloud Pipeline script, that sets all environment variables of the current docker container. So, as you can see from the code, as the result of that script launch will be a cleanup (removing) of the RStudio active session from the user home directory. Close SSH tab and try to do a commit - click the COMMIT control in the upper-right corner of the run logs page. Specify a new name for the committed tool, e.g. rstudio-test : While image is being committed, you can see the follow-up log for the CommitPipelineRun task: Here: ( 1 ) pre-commit script is not found in the docker image - nothing will be executed before commit ( 2 ) post-commit script is found at the specified path in the docker image - and it is being executed ( 3 ) post-commit script was performed successfully","title":"Committing features"},{"location":"manual/10_Manage_Tools/10.4._Edit_a_Tool/#edit-a-tool-settings","text":"Settings in this tab are applied to all Tool versions (i.e. these settings will be a default for all Tool version). Navigate to the Settings tab. Specify Endpoints for a Tool by click \" Add endpoint \" button: In the example below: the port is 8788 and the endpoint name is rstudio-s3fs : Let's look at the endpoint closer: \"nginx\" - type of the endpoints (only nginx is currently supported) \"port\": XXXX - an application will be deployed on this port on the pipeline node. You can specify additional nginx configuration for that endpoint in the text field bottom in JSON format. Note : optional path parameter may be required in case your application starts on <host:port:>/ path . Note : optional additional parameter may be required in case you need to specify nginx location settings. See more information here . \"name\" - this value will be visible as a hyperlink in the UI. It is especially convenient when user sets more than one endpoint configuration for an interactive tool (learn more about interactive services - 15. Interactive services ). In the example below, we name one endpoint as \"rstudio-s3fs\" and another one as \"shiny\" . This is how everything will look in the Run log window: Click the + New Label button and add a label to the Tool (e.g. \"Awesome Tool\"). Specify \" Execution defaults \": Instance type (e.g. \"m4.large\") Price type (e.g. \"Spot\") Disk size in Gb (e.g. \"20\") select available storages configure cluster, if it's necessary for your Tool write the Default command for the Tool execution (e.g. echo \"Hello world!\" ) If it's necessary for your Tool - add system or custom parameters. For more details see 6.1. Create and configure pipeline . Click Save button to save these settings.","title":"Edit a Tool settings"},{"location":"manual/10_Manage_Tools/10.4._Edit_a_Tool/#delete-a-tool","text":"To detach a Tool from the Cloud Pipeline: Open the Tool you wish to detach Hover over the gear icon in the right upper corner and click the \"Delete tool\" item in the appeared list: Click the Delete button to confirm: Note : if you try to delete a tool/version, but there is/are active job(s) using it - the corresponding warning notification will be shown, e.g.:","title":"Delete a Tool"},{"location":"manual/10_Manage_Tools/10.5._Launch_a_Tool/","text":"10.5. Launch a Tool Launch the latest version Launch particular Tool version Launch a Tool with \"friendly\" URL Launch a tool with \"hosted\" applications Instance management To launch a Tool you need to have EXECUTE permissions for it. For more information see 13. Permissions . You also can launch a tool via CLI. See here . Launch the latest version To run an instance with a selected Tool navigate to the Tools tab and click the Tool name . Click the Run button in the top-right corner of the screen and the latest version with default settings will be launched (these are defined for Cloud Pipeline globally). If you want to change settings, you shall click the arrow near the Run button \u2192 Custom settings . Launch tool page will be opened. If the Price type is set as \" On-demand \" - at the Launch page , an additional checkbox Auto pause appears: This checkbox allows to enable automatic pausing on-demand instance if it is not used. Such behavior could be controlled by Administrators using a set of parameters at System Preferences (see here ). Please note, this checkbox will not be displayed if any cluster is configured (\"Static\" or \"Autoscaled\"). If the Price type is set as \" On-demand \" - at the Launch page , an additional control Maintenance appears. It allows to configure schedule for automatical pause/resume a tool run: It could be useful when the tool is launched for a long time (several days/weeks) but it shall not stand idle, just increasing costs, in weekends and holidays, for example. For more details, how to configure the automatically schedule for a run see 6.2. Launch a pipeline (item 5). Please note, the Maintenance control will not be displayed if any cluster is configured (\"Static\" or \"Autoscaled\"). Users (who have permissions to pause/resume a run) can create/view/modify/delete schedule rules anytime launched run is active via the Run logs page - for more details see 11. Manage runs . Define the parameters in the Exec environment , Advanced and Parameters sections. Click the Launch button in the top-right corner of the screen. Please note, that the current user can launch a tool only if he/his group has corresponding permissions on that tool (for more information see 13. Permissions ), but the Launch button may be disabled also for one of the following reasons: execution isn't allowed for specified docker image; read operations aren't allowed for specified input or common path parameters; write operations aren't allowed for specified output path parameters. In such cases, hover over the Launch button to view warning notification with a reason of a run forbiddance, e.g.: Note : you can also launch a tool with the same settings via the CLI command or API request. To generate the corresponding command/request click the button near the \"Launch\" button. For more details see here . Launch particular Tool version To run a particular version click the Versions section. Select a version and click the Run button. The selected version with default settings will be launched (these are defined for Cloud Pipeline globally). If you want to change settings, you shall click the arrow near the Run button \u2192 Custom settings . Launch a tool page will be opened. Define the parameters. Click the \" Launch \" button. Example 1 In this example, we will run the \" Ubuntu \" Tool with custom settings: 30 Gb hard drive, 4 CPU cores, and 16 Gb RAM. Note : \"Start idle\" box is ticked to allow SSH access to the running Tool. To learn more about interactive services see 15. Interactive services . Click the Launch button in the top-right corner of the screen when all parameters are set. After the Tool is launched you will be redirected to the Runs tab: Click the Log button to see run details after instance finishes initialization. Wait until the SSH button will appear at the Run logs page, click it You will be redirected to the page with interactive shell session inside the Docker container. For example, we can list \"/\" directory content inside the container. Launch a Tool with \"friendly\" URL User can specify a \"friendly\"-format endpoint URL for persistent services. This produces the endpoint URL for the selected interactive service in a more friendly/descriptive format instead of the default general <host>/pipeline-<RunID>-<port> . \"Friendly\" format can be configured before the specific service launch in the \" Advanced \" section of the Launch page. This format has the following structure - <custom-host>/<friendly_url> , where: <custom-host> - valid existing domain name. If it's not specified, the default host name will be used <friendly_url> - valid endpoint name. If it's not specified but the <custom-host> is specified - the final endpoint URL will be equal the <custom-host> only Note : specified endpoint URL shall be unique among all active runs. Example 2 In this example we will configure a pretty URL for RStudio Tool. Note : for do that, user account shall be registered within CP users catalog and granted READ & EXECUTE permissions for the rstudio Tool. Navigate to the Tools tab. Select/find RStudio Tool in the list: At the opened page hover the \" Run \" button and click the appeared \" Custom settings \" point: Click the \" Advanced \" control ( a ), input desired \" Friendly URL \" ( b ) (name shall be unique) and then click the \" Launch \" button ( c ), e.g.: Open the Run logs page of RStudio Tool, wait until the tool successfully starts. Click the hyperlink opposite \" Endpoint \" label: In a new tab RStudio will be opened. Check that the URL is in the \"pretty\" format you specified at step 4: Launch a tool with \"hosted\" applications \"Long-running\" Cloud Pipeline applications may occasionally failed. And one of the main task caused this situation - saving the internal access to the services (e.g. if a database was hosted) as the IP and name (which match the pod) are being changed during the default run restarts. For that, there is a special option to assign an internal DNS name to the run. Name of the service and a list of ports can be supplied by the user in the GUI, at the Launch form before the run: Pop-up for DNS configuration for the upcoming run will appear: To configure the internal DNS: specify the valid service name, e.g.: specify the origin port of the application that should be hosted, e.g.: Please note, value of the Target port is being specified firstly the same as the origin Port value (automatically) change the value of the Target port if needs, e.g.: if the configuration of an additional origin port is required, click the button \" + Add ports configuration \": Specify values for the additional port and its target port. If the configuration of an additional origin port is required, click the button \" + Add ports configuration \" again. to delete \"extra\" added port in the list, click the delete icon for it, e.g.: after the service is configured, click the Save button to confirm: Configured DNS service is shown in the Advanced section of the Launch form: Example of \"hosted\" application running In the example below, we will run and \"host\" RStudio tool. Open the Tools page and find RStudio tool: At the tool's page, click the Settings tab: Save endpoints' port values. Select Run \u2192 Custom settings : At the Launch page, expand the Advanced section and click the Configure button near the \" Internal DNS name \" label: In the appeared pop-up, specify the service name, e.g.: Specify ports saved at step 2. To add the second port use \" + Add ports configuration \" button: Click the Save button. Check that configured service is displayed at the Launch page: Click the Launch button and launch the run. Open the Run logs page of the just-launched run. FQDN of both configured services are shown in the GUI - at the Run logs page: Currently, the format of the service FQDN is <service_name>.default.svc.cluster.local:<origin_port> Wait until SSH hyperlink appears. Click the SSH hyperlink to open the Web-terminal. In the opened terminal, perform the command: curl -I rstudio-example-service.default.svc.cluster.local:8788 to view info about RStudio hosted service: Perform the command: curl -I rstudio-example-service.default.svc.cluster.local:3838 to view info about Shiny hosted service: Instance management Instance management allows to set restrictions on instance types and price types for tool runs. User shall have ROLE_ADMIN or to be an OWNER of the Tool to launch Instance management panel. For more information see 13. Permissions . To open Instance management panel: Click button in the left upper corner of the main tool page. Click \"Instance management\": Such panel will be shown: On this panel you can specify some restrictions on allowed instance types and price types for launching tool. Here you can specify: Field Description Example Allowed tool instance types mask This mask restrict for a tool allowed instance types. If you want for that tool only some of \"large m5...\" instances types will be able, mask would be m5*.large* In that case, before launching tool, dropdown list of available node types will be look like this: Allowed price types In this field you may restrict, what price types will be allowed for a user. If you want \"On-demand\" runs only for that tool will be able, select it in the dropdown list: In that case, before launching tool, dropdown list of price types will be look like this: To apply set restrictions for a tool click button. Setting restrictions on allowed instance types/price types is a convenient way to minimize a number of invalid configurations runs. Such restrictions could be set not only for a tool, but on another levels too. In CP platform next hierarchy is set for applying of inputted allowed instance types (sorted by priority): User level (specified for a user on \"User management\" tab) (see v.0.14 - 12.4. Edit/delete a user ) User group level (specified for a group (role) on \"User management\" tab. If a user is a member of several groups - list of allowed instances will be summarized across all the groups) (see v.0.14 - 12.6. Edit a group/role ) Tool level (specified for a tool on \"Instance management\" panel) (see above ) (global) cluster.allowed.instance.types.docker (specified on \"Cluster\" tab in \"Preferences\" section of system-level settings) (see v.0.14 - 12.10. Manage system-level settings ) (global) cluster.allowed.instance.types (specified on \"Cluster\" tab in \"Preferences\" section of system-level settings) (see v.0.14 - 12.10. Manage system-level settings )","title":"10.5. Launch a Tool"},{"location":"manual/10_Manage_Tools/10.5._Launch_a_Tool/#105-launch-a-tool","text":"Launch the latest version Launch particular Tool version Launch a Tool with \"friendly\" URL Launch a tool with \"hosted\" applications Instance management To launch a Tool you need to have EXECUTE permissions for it. For more information see 13. Permissions . You also can launch a tool via CLI. See here .","title":"10.5. Launch a Tool"},{"location":"manual/10_Manage_Tools/10.5._Launch_a_Tool/#launch-the-latest-version","text":"To run an instance with a selected Tool navigate to the Tools tab and click the Tool name . Click the Run button in the top-right corner of the screen and the latest version with default settings will be launched (these are defined for Cloud Pipeline globally). If you want to change settings, you shall click the arrow near the Run button \u2192 Custom settings . Launch tool page will be opened. If the Price type is set as \" On-demand \" - at the Launch page , an additional checkbox Auto pause appears: This checkbox allows to enable automatic pausing on-demand instance if it is not used. Such behavior could be controlled by Administrators using a set of parameters at System Preferences (see here ). Please note, this checkbox will not be displayed if any cluster is configured (\"Static\" or \"Autoscaled\"). If the Price type is set as \" On-demand \" - at the Launch page , an additional control Maintenance appears. It allows to configure schedule for automatical pause/resume a tool run: It could be useful when the tool is launched for a long time (several days/weeks) but it shall not stand idle, just increasing costs, in weekends and holidays, for example. For more details, how to configure the automatically schedule for a run see 6.2. Launch a pipeline (item 5). Please note, the Maintenance control will not be displayed if any cluster is configured (\"Static\" or \"Autoscaled\"). Users (who have permissions to pause/resume a run) can create/view/modify/delete schedule rules anytime launched run is active via the Run logs page - for more details see 11. Manage runs . Define the parameters in the Exec environment , Advanced and Parameters sections. Click the Launch button in the top-right corner of the screen. Please note, that the current user can launch a tool only if he/his group has corresponding permissions on that tool (for more information see 13. Permissions ), but the Launch button may be disabled also for one of the following reasons: execution isn't allowed for specified docker image; read operations aren't allowed for specified input or common path parameters; write operations aren't allowed for specified output path parameters. In such cases, hover over the Launch button to view warning notification with a reason of a run forbiddance, e.g.: Note : you can also launch a tool with the same settings via the CLI command or API request. To generate the corresponding command/request click the button near the \"Launch\" button. For more details see here .","title":"Launch the latest version"},{"location":"manual/10_Manage_Tools/10.5._Launch_a_Tool/#launch-particular-tool-version","text":"To run a particular version click the Versions section. Select a version and click the Run button. The selected version with default settings will be launched (these are defined for Cloud Pipeline globally). If you want to change settings, you shall click the arrow near the Run button \u2192 Custom settings . Launch a tool page will be opened. Define the parameters. Click the \" Launch \" button.","title":"Launch\u00a0particular Tool version"},{"location":"manual/10_Manage_Tools/10.5._Launch_a_Tool/#example-1","text":"In this example, we will run the \" Ubuntu \" Tool with custom settings: 30 Gb hard drive, 4 CPU cores, and 16 Gb RAM. Note : \"Start idle\" box is ticked to allow SSH access to the running Tool. To learn more about interactive services see 15. Interactive services . Click the Launch button in the top-right corner of the screen when all parameters are set. After the Tool is launched you will be redirected to the Runs tab: Click the Log button to see run details after instance finishes initialization. Wait until the SSH button will appear at the Run logs page, click it You will be redirected to the page with interactive shell session inside the Docker container. For example, we can list \"/\" directory content inside the container.","title":"Example\u00a01"},{"location":"manual/10_Manage_Tools/10.5._Launch_a_Tool/#launch-a-tool-with-friendly-url","text":"User can specify a \"friendly\"-format endpoint URL for persistent services. This produces the endpoint URL for the selected interactive service in a more friendly/descriptive format instead of the default general <host>/pipeline-<RunID>-<port> . \"Friendly\" format can be configured before the specific service launch in the \" Advanced \" section of the Launch page. This format has the following structure - <custom-host>/<friendly_url> , where: <custom-host> - valid existing domain name. If it's not specified, the default host name will be used <friendly_url> - valid endpoint name. If it's not specified but the <custom-host> is specified - the final endpoint URL will be equal the <custom-host> only Note : specified endpoint URL shall be unique among all active runs.","title":"Launch a Tool with \"friendly\" URL"},{"location":"manual/10_Manage_Tools/10.5._Launch_a_Tool/#example-2","text":"In this example we will configure a pretty URL for RStudio Tool. Note : for do that, user account shall be registered within CP users catalog and granted READ & EXECUTE permissions for the rstudio Tool. Navigate to the Tools tab. Select/find RStudio Tool in the list: At the opened page hover the \" Run \" button and click the appeared \" Custom settings \" point: Click the \" Advanced \" control ( a ), input desired \" Friendly URL \" ( b ) (name shall be unique) and then click the \" Launch \" button ( c ), e.g.: Open the Run logs page of RStudio Tool, wait until the tool successfully starts. Click the hyperlink opposite \" Endpoint \" label: In a new tab RStudio will be opened. Check that the URL is in the \"pretty\" format you specified at step 4:","title":"Example 2"},{"location":"manual/10_Manage_Tools/10.5._Launch_a_Tool/#launch-a-tool-with-hosted-applications","text":"\"Long-running\" Cloud Pipeline applications may occasionally failed. And one of the main task caused this situation - saving the internal access to the services (e.g. if a database was hosted) as the IP and name (which match the pod) are being changed during the default run restarts. For that, there is a special option to assign an internal DNS name to the run. Name of the service and a list of ports can be supplied by the user in the GUI, at the Launch form before the run: Pop-up for DNS configuration for the upcoming run will appear: To configure the internal DNS: specify the valid service name, e.g.: specify the origin port of the application that should be hosted, e.g.: Please note, value of the Target port is being specified firstly the same as the origin Port value (automatically) change the value of the Target port if needs, e.g.: if the configuration of an additional origin port is required, click the button \" + Add ports configuration \": Specify values for the additional port and its target port. If the configuration of an additional origin port is required, click the button \" + Add ports configuration \" again. to delete \"extra\" added port in the list, click the delete icon for it, e.g.: after the service is configured, click the Save button to confirm: Configured DNS service is shown in the Advanced section of the Launch form:","title":"Launch a tool with \"hosted\" applications"},{"location":"manual/10_Manage_Tools/10.5._Launch_a_Tool/#example-of-hosted-application-running","text":"In the example below, we will run and \"host\" RStudio tool. Open the Tools page and find RStudio tool: At the tool's page, click the Settings tab: Save endpoints' port values. Select Run \u2192 Custom settings : At the Launch page, expand the Advanced section and click the Configure button near the \" Internal DNS name \" label: In the appeared pop-up, specify the service name, e.g.: Specify ports saved at step 2. To add the second port use \" + Add ports configuration \" button: Click the Save button. Check that configured service is displayed at the Launch page: Click the Launch button and launch the run. Open the Run logs page of the just-launched run. FQDN of both configured services are shown in the GUI - at the Run logs page: Currently, the format of the service FQDN is <service_name>.default.svc.cluster.local:<origin_port> Wait until SSH hyperlink appears. Click the SSH hyperlink to open the Web-terminal. In the opened terminal, perform the command: curl -I rstudio-example-service.default.svc.cluster.local:8788 to view info about RStudio hosted service: Perform the command: curl -I rstudio-example-service.default.svc.cluster.local:3838 to view info about Shiny hosted service:","title":"Example of \"hosted\" application running"},{"location":"manual/10_Manage_Tools/10.5._Launch_a_Tool/#instance-management","text":"Instance management allows to set restrictions on instance types and price types for tool runs. User shall have ROLE_ADMIN or to be an OWNER of the Tool to launch Instance management panel. For more information see 13. Permissions . To open Instance management panel: Click button in the left upper corner of the main tool page. Click \"Instance management\": Such panel will be shown: On this panel you can specify some restrictions on allowed instance types and price types for launching tool. Here you can specify: Field Description Example Allowed tool instance types mask This mask restrict for a tool allowed instance types. If you want for that tool only some of \"large m5...\" instances types will be able, mask would be m5*.large* In that case, before launching tool, dropdown list of available node types will be look like this: Allowed price types In this field you may restrict, what price types will be allowed for a user. If you want \"On-demand\" runs only for that tool will be able, select it in the dropdown list: In that case, before launching tool, dropdown list of price types will be look like this: To apply set restrictions for a tool click button. Setting restrictions on allowed instance types/price types is a convenient way to minimize a number of invalid configurations runs. Such restrictions could be set not only for a tool, but on another levels too. In CP platform next hierarchy is set for applying of inputted allowed instance types (sorted by priority): User level (specified for a user on \"User management\" tab) (see v.0.14 - 12.4. Edit/delete a user ) User group level (specified for a group (role) on \"User management\" tab. If a user is a member of several groups - list of allowed instances will be summarized across all the groups) (see v.0.14 - 12.6. Edit a group/role ) Tool level (specified for a tool on \"Instance management\" panel) (see above ) (global) cluster.allowed.instance.types.docker (specified on \"Cluster\" tab in \"Preferences\" section of system-level settings) (see v.0.14 - 12.10. Manage system-level settings ) (global) cluster.allowed.instance.types (specified on \"Cluster\" tab in \"Preferences\" section of system-level settings) (see v.0.14 - 12.10. Manage system-level settings )","title":"Instance management"},{"location":"manual/10_Manage_Tools/10.6._Tool_security_check/","text":"10.6. Tool security check Force a security scanning procedure Show/hide unscanned Tool versions \"White list\" for Tool versions A registry may potentially contain tools with vulnerable software which may cause damage. To prevent this issue, Cloud Pipeline performs Security scanning feature of the tools that are provided by users and restrict usage of not secure Tools. User shall have ROLE_ADMIN to force a security scanning procedure or to run unscanned Tools/Tools with critical number of vulnerabilities. For more information see 13. Permissions . Force a security scanning procedure In the Cloud Pipeline security scanning is performed on a scheduled basis - every N minutes (configurable parameter). However, a user with an administrator role can manually run a security scanning procedure: Select a Tool and navigate to the Versions tab. Click the SCAN button of a Tool version. Note : If scan results for version are against the security politics (i.e. vulnerabilities number exceeds the threshold) the Run button for version will be deactivated. Only users with ROLE_ADMIN role will be able to run this Tool. Generic users can run this version of Tool only if it is checked with a \"white list\" flag (see below ) or if its \"grace\" period is not elapsed yet (see security.tools.grace.hours setting here v.0.14 - 12.10. Manage system-level settings ). Note : If scan results for the latest version are against the security politics (i.e. vulnerabilities number exceeds the threshold) the Run button for Tool will be deactivated. Only users with ROLE_ADMIN role will be able to run this Tool. Generic users can run this version of Tool only if it is checked with a \"white list\" flag (see below ) or if its \"grace\" period is not elapsed yet (see security.tools.grace.hours setting here v.0.14 - 12.10. Manage system-level settings ). Show/hide unscanned Tool versions Select a Tool and navigate to the Versions tab. Press the View unscanned versions control. Unscanned version(s) will appear in the list of Tool versions. Note : for users with ADMIN role Run button of unscanned version contains \"warning\" sign. Hover over that button and you'll see a suggestion that the version shall be scanned for vulnerabilities. Generic users won't be able to run unscanned Tools (except when they're checked with a \"white list\" flag - see below or if \"grace\" period for this version of Tool is not elapsed yet (see security.tools.grace.hours setting here v.0.14 - 12.10. Manage system-level settings )). If you are user with ADMIN role and press the Run button anyway, you'll see another warning: Hide unscanned Tool versions by clicking Hide unscanned versions control. \"White list\" for Tool versions Generic users can't run tools with vulnerable software or unscanned tools because it may cause damage. But admin may allow users to run certain docker versions even if they are vulnerable/unscanned. For do that admin has to check a such docker version with a special \"white list\" flag. User shall have ROLE_ADMIN to set a \"white list\" flag for an unscanned Tools or Tools with critical number of vulnerabilities. To \"add\" a Tool version into the \"white list\": Open Versions tab on main tool's page. Click Add to white list button in the row of the version you want allow users to run in spite of possible damage: The version with a \"white list\" flag will be shown in green: Now, if user will try to launch this version of docker image, it will be run without any errors during launch time and viewing. If user will try to launch any other \"danger\" version of this tool without a \"white list\" flag, error message will be shown, tool won't be run: To \"delete\" a Tool version from the \"white list\": Open Versions tab on the main tool's page. Click Remove from white list button in the row of the version with a set \"white list\" flag:","title":"10.6. Tool security check"},{"location":"manual/10_Manage_Tools/10.6._Tool_security_check/#106-tool-security-check","text":"Force a security scanning procedure Show/hide unscanned Tool versions \"White list\" for Tool versions A registry may potentially contain tools with vulnerable software which may cause damage. To prevent this issue, Cloud Pipeline performs Security scanning feature of the tools that are provided by users and restrict usage of not secure Tools. User shall have ROLE_ADMIN to force a security scanning procedure or to run unscanned Tools/Tools with critical number of vulnerabilities. For more information see 13. Permissions .","title":"10.6. Tool security check"},{"location":"manual/10_Manage_Tools/10.6._Tool_security_check/#force-a-security-scanning-procedure","text":"In the Cloud Pipeline security scanning is performed on a scheduled basis - every N minutes (configurable parameter). However, a user with an administrator role can manually run a security scanning procedure: Select a Tool and navigate to the Versions tab. Click the SCAN button of a Tool version. Note : If scan results for version are against the security politics (i.e. vulnerabilities number exceeds the threshold) the Run button for version will be deactivated. Only users with ROLE_ADMIN role will be able to run this Tool. Generic users can run this version of Tool only if it is checked with a \"white list\" flag (see below ) or if its \"grace\" period is not elapsed yet (see security.tools.grace.hours setting here v.0.14 - 12.10. Manage system-level settings ). Note : If scan results for the latest version are against the security politics (i.e. vulnerabilities number exceeds the threshold) the Run button for Tool will be deactivated. Only users with ROLE_ADMIN role will be able to run this Tool. Generic users can run this version of Tool only if it is checked with a \"white list\" flag (see below ) or if its \"grace\" period is not elapsed yet (see security.tools.grace.hours setting here v.0.14 - 12.10. Manage system-level settings ).","title":"Force a security scanning procedure"},{"location":"manual/10_Manage_Tools/10.6._Tool_security_check/#showhide-unscanned-tool-versions","text":"Select a Tool and navigate to the Versions tab. Press the View unscanned versions control. Unscanned version(s) will appear in the list of Tool versions. Note : for users with ADMIN role Run button of unscanned version contains \"warning\" sign. Hover over that button and you'll see a suggestion that the version shall be scanned for vulnerabilities. Generic users won't be able to run unscanned Tools (except when they're checked with a \"white list\" flag - see below or if \"grace\" period for this version of Tool is not elapsed yet (see security.tools.grace.hours setting here v.0.14 - 12.10. Manage system-level settings )). If you are user with ADMIN role and press the Run button anyway, you'll see another warning: Hide unscanned Tool versions by clicking Hide unscanned versions control.","title":"Show/hide unscanned Tool versions"},{"location":"manual/10_Manage_Tools/10.6._Tool_security_check/#white-list-for-tool-versions","text":"Generic users can't run tools with vulnerable software or unscanned tools because it may cause damage. But admin may allow users to run certain docker versions even if they are vulnerable/unscanned. For do that admin has to check a such docker version with a special \"white list\" flag. User shall have ROLE_ADMIN to set a \"white list\" flag for an unscanned Tools or Tools with critical number of vulnerabilities. To \"add\" a Tool version into the \"white list\": Open Versions tab on main tool's page. Click Add to white list button in the row of the version you want allow users to run in spite of possible damage: The version with a \"white list\" flag will be shown in green: Now, if user will try to launch this version of docker image, it will be run without any errors during launch time and viewing. If user will try to launch any other \"danger\" version of this tool without a \"white list\" flag, error message will be shown, tool won't be run: To \"delete\" a Tool version from the \"white list\": Open Versions tab on the main tool's page. Click Remove from white list button in the row of the version with a set \"white list\" flag:","title":"\"White list\" for Tool versions"},{"location":"manual/10_Manage_Tools/10.7._Tool_version_menu/","text":"10.7. Tool version menu Vulnerabilities report Version settings Version packages Image history To see the detailed info of Docker image version: Click any Tool version : Tool version menu will be shown: Vulnerabilities report This tab contains the detailed info of the Docker image's scanning results. Here you can see vulnerable components ( 1 ). Each component has severity estimation ( 2 ): Expand each component details by clicking the \" Plus \" icon to see more information about it: a ) link to a page of the vulnerability description b ) the component version in which this vulnerability was fixed c ) severity level d ) short description of the vulnerability (it appears when hovering mouse pointer over the link a ) Sort components alphabetically, or by their severity: Version settings On this tab version-level settings are defined. If these settings are specified - they will be applied to each run of the docker image version. If version-level settings are not defined: docker-level settings will be applied for launch. If docker-level settings are not defined: global defaults will be applied. There are 3 groups of parameters that user can specify (they are analogical to the \"Execution environment\" of tool settings, for more details see here ): Execution defaults System parameters Custom parameters For change version-level settings, e.g.: Select an Instance type . Set the Price type . Input the Disk size. Click Save button: Click button to return into the tool menu. Click Run \u2192 Custom settings for the changed tool version. Check that version-level settings are applied: Note : via the Cloud Region field at the Version Settings tab, the admin/tool owner can select a specific Cloud Provider / Region to enforce users to run that tool version in it: By default, it has Not configured value. This means, that a tool version will be launched in a Default region (configured in the global settings) or a user can set any allowed Cloud Region / Provider manually. Version packages On this tab user can see the full list software packages installed into a specific Docker image. List of packages is generated from the docker version together with vulnerabilities scanning. This occurs nightly (all dockers are scanned) or if admin explicitly requests scanning by clicking SCAN button for a specific version. Currently the following types of software packages can be scanned: System package manager's database (i.e. yum , apt ) R packages Python packages Software packages are combined into groups named \" Ecosystems \". To view content of any ecosystem, select it in the dropdown list: Information about each package contains: Package name Package description ( if available ) For filter/search packages type some text into a search-query field. Search will be done automatically across all ecosystems : Image history Cloud Pipeline is able to display the exact commands, used to build the image. To view the list of the \"Docker layers\" and corresponding commands, which were used to generate those layers, use the Image history tab of the tool version menu. This tab contains a list of the Docker layers. Each layer corresponds to a separate command from the initial Dockerfile , used to create the docker image. Each layer entry shows: a - Short command text b - Size of the layer (in Mb) - this is a resulting filesystem volume, consumed by the layer. Overall docker image size is a sum of all the layers. Some of the layers might have zero size, typically these are commands which perform some environment configuration (like ENV and CMD ) c - the full command text of the selected layer in the right panel Notes : The list of the layers is a full stack of the docker image, which includes the layers from the parent images as well. \"Image history\" feature works fine for the images, which are built using Dockerfiles - then the commands and layers are listed correctly. But if the docker image was created using the \" COMMIT \" operation from the Cloud Pipeline capabilities - this will generate only one layer with the predefined command, e.g.: So if the docker layers list ends with the similar content - it means that an image was committed and it's not possible to determine the exact shell commands.","title":"10.7. Tool version menu"},{"location":"manual/10_Manage_Tools/10.7._Tool_version_menu/#107-tool-version-menu","text":"Vulnerabilities report Version settings Version packages Image history To see the detailed info of Docker image version: Click any Tool version : Tool version menu will be shown:","title":"10.7. Tool version menu"},{"location":"manual/10_Manage_Tools/10.7._Tool_version_menu/#vulnerabilities-report","text":"This tab contains the detailed info of the Docker image's scanning results. Here you can see vulnerable components ( 1 ). Each component has severity estimation ( 2 ): Expand each component details by clicking the \" Plus \" icon to see more information about it: a ) link to a page of the vulnerability description b ) the component version in which this vulnerability was fixed c ) severity level d ) short description of the vulnerability (it appears when hovering mouse pointer over the link a ) Sort components alphabetically, or by their severity:","title":"Vulnerabilities report"},{"location":"manual/10_Manage_Tools/10.7._Tool_version_menu/#version-settings","text":"On this tab version-level settings are defined. If these settings are specified - they will be applied to each run of the docker image version. If version-level settings are not defined: docker-level settings will be applied for launch. If docker-level settings are not defined: global defaults will be applied. There are 3 groups of parameters that user can specify (they are analogical to the \"Execution environment\" of tool settings, for more details see here ): Execution defaults System parameters Custom parameters For change version-level settings, e.g.: Select an Instance type . Set the Price type . Input the Disk size. Click Save button: Click button to return into the tool menu. Click Run \u2192 Custom settings for the changed tool version. Check that version-level settings are applied: Note : via the Cloud Region field at the Version Settings tab, the admin/tool owner can select a specific Cloud Provider / Region to enforce users to run that tool version in it: By default, it has Not configured value. This means, that a tool version will be launched in a Default region (configured in the global settings) or a user can set any allowed Cloud Region / Provider manually.","title":"Version settings"},{"location":"manual/10_Manage_Tools/10.7._Tool_version_menu/#version-packages","text":"On this tab user can see the full list software packages installed into a specific Docker image. List of packages is generated from the docker version together with vulnerabilities scanning. This occurs nightly (all dockers are scanned) or if admin explicitly requests scanning by clicking SCAN button for a specific version. Currently the following types of software packages can be scanned: System package manager's database (i.e. yum , apt ) R packages Python packages Software packages are combined into groups named \" Ecosystems \". To view content of any ecosystem, select it in the dropdown list: Information about each package contains: Package name Package description ( if available ) For filter/search packages type some text into a search-query field. Search will be done automatically across all ecosystems :","title":"Version packages"},{"location":"manual/10_Manage_Tools/10.7._Tool_version_menu/#image-history","text":"Cloud Pipeline is able to display the exact commands, used to build the image. To view the list of the \"Docker layers\" and corresponding commands, which were used to generate those layers, use the Image history tab of the tool version menu. This tab contains a list of the Docker layers. Each layer corresponds to a separate command from the initial Dockerfile , used to create the docker image. Each layer entry shows: a - Short command text b - Size of the layer (in Mb) - this is a resulting filesystem volume, consumed by the layer. Overall docker image size is a sum of all the layers. Some of the layers might have zero size, typically these are commands which perform some environment configuration (like ENV and CMD ) c - the full command text of the selected layer in the right panel Notes : The list of the layers is a full stack of the docker image, which includes the layers from the parent images as well. \"Image history\" feature works fine for the images, which are built using Dockerfiles - then the commands and layers are listed correctly. But if the docker image was created using the \" COMMIT \" operation from the Cloud Pipeline capabilities - this will generate only one layer with the predefined command, e.g.: So if the docker layers list ends with the similar content - it means that an image was committed and it's not possible to determine the exact shell commands.","title":"Image history"},{"location":"manual/10_Manage_Tools/10.8._Symlinks_between_tools/","text":"10.8. \"Symlinked\" tools The majority of the tools for general users are managed by the administrators and are available via the library tool group. But for some of the users it is convenient to have separate tool groups, which contain a mix of the custom tools (managed by the users themselves) and the library tools (managed by the admins). For the latter ones the ability to create \" symlinks \" into the other tool groups exists. \"Symlinked\" tools are displayed in that users' tool groups as the original tools but can't be edited/updated. When a run is started with \"symlinked\" tool as docker image it is being replaced with original image for Kubernetes pod spec. The following behavior is implemented: to create a \"symlink\" to the tool, the user shall have READ access to the source tool and WRITE access to the destination tool group for the \"symlinked\" tool all the same description, icon, settings as in the source image are displayed. It isn't possible to make any changes to the \"symlink\" data (description, icon, settings, attributes, issues, etc.), even for the admins image owners and admins are able to manage the permissions for the \"symlinked\" tools. Permissions on the \"symlinked\" tools are configured separately from the original tool two levels of \"symlinks\" is not possible (\"symlink\" to the \"symlinked\" tool can't be created) it isn't possible to \"push\" into the \"symlinked\" tool Creation and usage of a \"symlink\" to the tool A \"symlink\" to the tool can be created by any user with the ROLE_TOOL_GROUP_MANAGER role, that has read access to the original (source) tool and write access to the destination tool group. To create a \"symlink\" to the tool: Open the Tools page. Open the tool you wish to create the \"symlink\". Click the link icon in the upper-right corner: In the opened popup select the destination Registry and Tool group where you wish to create the \"symlink\", e.g.: Click the Create Link button to confirm. Just-created \"symlinked\" tool will be open automatically: The link icon before the tool name indicates a \"symlink\". In the tool group, \"symlinked\" tool is displayed with the same icon: Please note, that all \"edit\" functions are unavailable even for the \"symlink\" OWNER . All settings, descriptions, icon, attributes, issues are loaded from the original tool: Version management (update/delete versions/attributes/settings/scan/vulnerabilities) is unavailable. Necessary data is also loaded from the original tool, e.g.: Run of the \"symlinked\" tool/its version is being performed as for usual tool. Commit of the launched \"symlinked\" tool can be performed only to the non-\"symlinked\" tool (no matter - new or existing): Management of a \"symlinked\" tool \"Symlinked\" tool can't be edited. Possible operations for OWNERs/admins: set the permissions on the \"symlink\" delete the \"symlink\" Set permissions Permissions are being set by the way as for general tools: Click the gear icon in the right-upper corner of the \"symlinked\" tool page. Click the Permissions item: In the opened popup configure desired permissions: Note : configured permissions are valid only for the \"symlink\". Permissions for the original tool shall be configured separately at the original tool page Remove a \"symlinked\" tool To delete a \"symlinked\" tool: Click the gear icon in the right-upper corner of the \"symlinked\" tool page. Click the Delete tool link item: Confirm the deletion:","title":"10.8. \"Symlinked\" tools"},{"location":"manual/10_Manage_Tools/10.8._Symlinks_between_tools/#108-symlinked-tools","text":"The majority of the tools for general users are managed by the administrators and are available via the library tool group. But for some of the users it is convenient to have separate tool groups, which contain a mix of the custom tools (managed by the users themselves) and the library tools (managed by the admins). For the latter ones the ability to create \" symlinks \" into the other tool groups exists. \"Symlinked\" tools are displayed in that users' tool groups as the original tools but can't be edited/updated. When a run is started with \"symlinked\" tool as docker image it is being replaced with original image for Kubernetes pod spec. The following behavior is implemented: to create a \"symlink\" to the tool, the user shall have READ access to the source tool and WRITE access to the destination tool group for the \"symlinked\" tool all the same description, icon, settings as in the source image are displayed. It isn't possible to make any changes to the \"symlink\" data (description, icon, settings, attributes, issues, etc.), even for the admins image owners and admins are able to manage the permissions for the \"symlinked\" tools. Permissions on the \"symlinked\" tools are configured separately from the original tool two levels of \"symlinks\" is not possible (\"symlink\" to the \"symlinked\" tool can't be created) it isn't possible to \"push\" into the \"symlinked\" tool","title":"10.8. \"Symlinked\" tools"},{"location":"manual/10_Manage_Tools/10.8._Symlinks_between_tools/#creation-and-usage-of-a-symlink-to-the-tool","text":"A \"symlink\" to the tool can be created by any user with the ROLE_TOOL_GROUP_MANAGER role, that has read access to the original (source) tool and write access to the destination tool group. To create a \"symlink\" to the tool: Open the Tools page. Open the tool you wish to create the \"symlink\". Click the link icon in the upper-right corner: In the opened popup select the destination Registry and Tool group where you wish to create the \"symlink\", e.g.: Click the Create Link button to confirm. Just-created \"symlinked\" tool will be open automatically: The link icon before the tool name indicates a \"symlink\". In the tool group, \"symlinked\" tool is displayed with the same icon: Please note, that all \"edit\" functions are unavailable even for the \"symlink\" OWNER . All settings, descriptions, icon, attributes, issues are loaded from the original tool: Version management (update/delete versions/attributes/settings/scan/vulnerabilities) is unavailable. Necessary data is also loaded from the original tool, e.g.: Run of the \"symlinked\" tool/its version is being performed as for usual tool. Commit of the launched \"symlinked\" tool can be performed only to the non-\"symlinked\" tool (no matter - new or existing):","title":"Creation and usage of a \"symlink\" to the tool"},{"location":"manual/10_Manage_Tools/10.8._Symlinks_between_tools/#management-of-a-symlinked-tool","text":"\"Symlinked\" tool can't be edited. Possible operations for OWNERs/admins: set the permissions on the \"symlink\" delete the \"symlink\"","title":"Management of a \"symlinked\" tool"},{"location":"manual/10_Manage_Tools/10.8._Symlinks_between_tools/#set-permissions","text":"Permissions are being set by the way as for general tools: Click the gear icon in the right-upper corner of the \"symlinked\" tool page. Click the Permissions item: In the opened popup configure desired permissions: Note : configured permissions are valid only for the \"symlink\". Permissions for the original tool shall be configured separately at the original tool page","title":"Set permissions"},{"location":"manual/10_Manage_Tools/10.8._Symlinks_between_tools/#remove-a-symlinked-tool","text":"To delete a \"symlinked\" tool: Click the gear icon in the right-upper corner of the \"symlinked\" tool page. Click the Delete tool link item: Confirm the deletion:","title":"Remove a \"symlinked\" tool"},{"location":"manual/10_Manage_Tools/10.9._Run_capabilities/","text":"10.9. Run capabilities Disable Hyper-Threading Custom capabilities Create capability Example of usage Users always can launch tool runs with default settings (as they are configured in the tool/tool version's settings). On the other hand, there are cases when for the run specific custom settings shall be specified. In addition to the general frequently-used settings (like instance type and disk size), the platform allows you to customize special system behavior/capabilities. Some of these capabilities can be configured via System parameters, but more convenient way - to use the GUI control \" Run capabilities \" and select necessary items from the list. Ways to set such capabilities for a Tool run: at the Launch page in the \"Exec environment\" section, before the run, e.g.: In such case, selected capabilities will be applied only for the upcoming run. Note : for a run, several capabilities can be selected (multi-select for the dropdown list is supported) at the Tool settings page, e.g.: In such case, selected capabilities will be applied for all runs of that tool launched with default settings. Note : several capabilities can be selected (multi-select for the dropdown list is supported) at the Tool version's settings page, e.g.: In such case, selected capabilities will be applied for all runs of that tool version launched with default settings. Note : several capabilities can be selected (multi-select for the dropdown list is supported) Same capabilities can be also configured for pipeline runs and detach configuration runs. In all cases the behavior is configured in the similar way - by the selecting of necessary items in the \" Run capabilities \" dropdown list before the run - in configuration or at the Launch page. Below you can find descriptions and using examples of the separate capabilities. Disable Hyper-Threading Hyper-Threading technology makes a single physical processor appear as multiple logical processors. To do this, there is one copy of the architecture state for each logical processor, and the logical processors share a single set of physical execution resources. Hyper-Threading technology is enabled by default for Cloud instances launched in Cloud Pipeline deployment. But for some cases, users want to disable Hyper-Treading technology for specific runs as it may slow the computation. So, this technology can be turned off via \" Run capabilities \", as is best for a particular application at the user's discretion. Note : also the Hyper-Treading technology can be disabled manually by setting the parameter CP_DISABLE_HYPER_THREADING with true value before the run In Cloud Provider environment, each vCPU is a thread of a physical processor core. All cores of the instance has two threads. Disabling of Hyper-Threading disables the set of vCPUs that are relied to the second thread, set of first thread vCPUs stays enabled. For example, instance with enabled Hyper-Threading: The same instance with disabled Hyper-Threading: Example of disabling Hyper-Threading: Open any tool you wish to run with disabled Hyper-Threading. Select \" Run \" \u2192 \" Custom settings \". At the Launch page, expand the \" Exec environment \" section. Set the instance type with several CPUs (we will use the instance with 8 CPUs). Select the item \" Disable Hyper-Threading \" in the \" Run capabilities \" dropdown list: Launch the run. Open the Run logs page of the just-launched run, expand the \" Parameters \" section: Check that the parameter to disable Hyper-Threading was set. Wait until the SSH hyperlink appears. Click it. In the web-terminal perform the command lscpu : Here you can check that Hyper-Threading is disabled (only 1 thread per core is set) and virtual CPUs 4-7 are offline. So, only one thread is enabled (set of CPUs 0-3). Custom capabilities Except of the selection a predefined set of capabilities in the GUI for a job or a tool, Cloud Pipeline allows to create own non-complex scripts and use them via the Run capabilities menu. Note : only admins can create and edit custom run capabilities. But use them for a job/tool run all users can. Create/edit capability Managing of the custom capabilities is being performed via the launch.capabilities system preference. This preference contains an array of capability descriptions in JSON -format and has the following structure: { \"<capability_name_1>\": { \"description\": \"<Description of the capability>\", \"commands\": [ \"<command_1>\", \"<command_2>\", ... ], \"params\": { \"<parameter_1>\": \"<value_1>\", \"<parameter_2>\": \"<value_2>\", ... }, \"os\": \"all\" }, \"<capability_name_2\": { ... }, ... } Where: <capability_name> - defines the capability name. This name will be displayed on the GUI, in the Run capabilities dropdown list description - defines the description of the capability. This description will be displayed on the GUI, when hovering the capability in the list commands - block defines the array of the shell commands that will be performed during the job launch params - block defines the array of key-value pairs. Each of those pair will be set as a parameter for a job os - parameter that defines for which docker image(s) the capability is allowed. Possible values: all - capability is allowed for all docker images without restrictions list of comma-separated docker image names. Each item in the list can be specified in one of the following formats: <docker_image_name> - capability is allowed for any version of the docker image. Example: \"os\": \"ubuntu\" - capability with such parameter will be allowed only for the ubuntu images (for any version) <docker_image_name> <version_mask> - capability is allowed only for specific versions of a certain docker image. Versions are being defined by the mask, mask can include version name or its part, and * symbol for masking. Examples: \"os\": \"centos 7*, ubuntu 16.04\" - capability with such parameter will be allowed only for all centos 7 versions (e.g. centos 7.0-1406 and centos 7.2-1511 ) and for ubuntu 16.04 as well. if os is not specified at all (skipped) - capability is allowed for all docker images without restrictions (same behavior as \"os\": \"all\" ) Usage of the custom capability Let's create a custom capability that will set Docker-in-Docker mode for a job and perform a couple of simple commands. Open the System settings Navigate to the Preference tab Find launch.capabilities parameter: Add a new custom capability, e.g.: Here, we added the Docker-in-Docker setup via the system parameter CP_CAP_DIND_CONTAINER and the performing a couple of simple commands that write an example text file. After the capability specifying, click the Save button Open any tool (for our example, the Ubuntu will be used) Click v button near the Run button, select the Custom settings item At the Launch page, expand the Exec environment section: Click the Run capabilities dropdown list Check that in the list, there is a custom item with the name specified at step 4: Hover over this item - in a tooltip, the description specified at step 4 will appear: Select this item: Launch the run Open the just-launched run At the Run logs page, expand the Parameters section: All parameters specified at step 4 are displayed. Also, here you can see - each selected run capability is set as an environment variable in the format: CP_CAP_CUSTOM_{CAPABILITY_NAME} with true value Wait until all initialization tasks are done. Check that Docker-in-Docker was setup: Check in the Console that capability commands specified at step 4 were also performed: Click the SSH hyperlink. In the web-terminal, check the result of the commands performing:","title":"10.9. Run capabilities"},{"location":"manual/10_Manage_Tools/10.9._Run_capabilities/#109-run-capabilities","text":"Disable Hyper-Threading Custom capabilities Create capability Example of usage Users always can launch tool runs with default settings (as they are configured in the tool/tool version's settings). On the other hand, there are cases when for the run specific custom settings shall be specified. In addition to the general frequently-used settings (like instance type and disk size), the platform allows you to customize special system behavior/capabilities. Some of these capabilities can be configured via System parameters, but more convenient way - to use the GUI control \" Run capabilities \" and select necessary items from the list. Ways to set such capabilities for a Tool run: at the Launch page in the \"Exec environment\" section, before the run, e.g.: In such case, selected capabilities will be applied only for the upcoming run. Note : for a run, several capabilities can be selected (multi-select for the dropdown list is supported) at the Tool settings page, e.g.: In such case, selected capabilities will be applied for all runs of that tool launched with default settings. Note : several capabilities can be selected (multi-select for the dropdown list is supported) at the Tool version's settings page, e.g.: In such case, selected capabilities will be applied for all runs of that tool version launched with default settings. Note : several capabilities can be selected (multi-select for the dropdown list is supported) Same capabilities can be also configured for pipeline runs and detach configuration runs. In all cases the behavior is configured in the similar way - by the selecting of necessary items in the \" Run capabilities \" dropdown list before the run - in configuration or at the Launch page. Below you can find descriptions and using examples of the separate capabilities.","title":"10.9. Run capabilities"},{"location":"manual/10_Manage_Tools/10.9._Run_capabilities/#disable-hyper-threading","text":"Hyper-Threading technology makes a single physical processor appear as multiple logical processors. To do this, there is one copy of the architecture state for each logical processor, and the logical processors share a single set of physical execution resources. Hyper-Threading technology is enabled by default for Cloud instances launched in Cloud Pipeline deployment. But for some cases, users want to disable Hyper-Treading technology for specific runs as it may slow the computation. So, this technology can be turned off via \" Run capabilities \", as is best for a particular application at the user's discretion. Note : also the Hyper-Treading technology can be disabled manually by setting the parameter CP_DISABLE_HYPER_THREADING with true value before the run In Cloud Provider environment, each vCPU is a thread of a physical processor core. All cores of the instance has two threads. Disabling of Hyper-Threading disables the set of vCPUs that are relied to the second thread, set of first thread vCPUs stays enabled. For example, instance with enabled Hyper-Threading: The same instance with disabled Hyper-Threading: Example of disabling Hyper-Threading: Open any tool you wish to run with disabled Hyper-Threading. Select \" Run \" \u2192 \" Custom settings \". At the Launch page, expand the \" Exec environment \" section. Set the instance type with several CPUs (we will use the instance with 8 CPUs). Select the item \" Disable Hyper-Threading \" in the \" Run capabilities \" dropdown list: Launch the run. Open the Run logs page of the just-launched run, expand the \" Parameters \" section: Check that the parameter to disable Hyper-Threading was set. Wait until the SSH hyperlink appears. Click it. In the web-terminal perform the command lscpu : Here you can check that Hyper-Threading is disabled (only 1 thread per core is set) and virtual CPUs 4-7 are offline. So, only one thread is enabled (set of CPUs 0-3).","title":"Disable Hyper-Threading"},{"location":"manual/10_Manage_Tools/10.9._Run_capabilities/#custom-capabilities","text":"Except of the selection a predefined set of capabilities in the GUI for a job or a tool, Cloud Pipeline allows to create own non-complex scripts and use them via the Run capabilities menu. Note : only admins can create and edit custom run capabilities. But use them for a job/tool run all users can.","title":"Custom capabilities"},{"location":"manual/10_Manage_Tools/10.9._Run_capabilities/#createedit-capability","text":"Managing of the custom capabilities is being performed via the launch.capabilities system preference. This preference contains an array of capability descriptions in JSON -format and has the following structure: { \"<capability_name_1>\": { \"description\": \"<Description of the capability>\", \"commands\": [ \"<command_1>\", \"<command_2>\", ... ], \"params\": { \"<parameter_1>\": \"<value_1>\", \"<parameter_2>\": \"<value_2>\", ... }, \"os\": \"all\" }, \"<capability_name_2\": { ... }, ... } Where: <capability_name> - defines the capability name. This name will be displayed on the GUI, in the Run capabilities dropdown list description - defines the description of the capability. This description will be displayed on the GUI, when hovering the capability in the list commands - block defines the array of the shell commands that will be performed during the job launch params - block defines the array of key-value pairs. Each of those pair will be set as a parameter for a job os - parameter that defines for which docker image(s) the capability is allowed. Possible values: all - capability is allowed for all docker images without restrictions list of comma-separated docker image names. Each item in the list can be specified in one of the following formats: <docker_image_name> - capability is allowed for any version of the docker image. Example: \"os\": \"ubuntu\" - capability with such parameter will be allowed only for the ubuntu images (for any version) <docker_image_name> <version_mask> - capability is allowed only for specific versions of a certain docker image. Versions are being defined by the mask, mask can include version name or its part, and * symbol for masking. Examples: \"os\": \"centos 7*, ubuntu 16.04\" - capability with such parameter will be allowed only for all centos 7 versions (e.g. centos 7.0-1406 and centos 7.2-1511 ) and for ubuntu 16.04 as well. if os is not specified at all (skipped) - capability is allowed for all docker images without restrictions (same behavior as \"os\": \"all\" )","title":"Create/edit capability"},{"location":"manual/10_Manage_Tools/10.9._Run_capabilities/#usage-of-the-custom-capability","text":"Let's create a custom capability that will set Docker-in-Docker mode for a job and perform a couple of simple commands. Open the System settings Navigate to the Preference tab Find launch.capabilities parameter: Add a new custom capability, e.g.: Here, we added the Docker-in-Docker setup via the system parameter CP_CAP_DIND_CONTAINER and the performing a couple of simple commands that write an example text file. After the capability specifying, click the Save button Open any tool (for our example, the Ubuntu will be used) Click v button near the Run button, select the Custom settings item At the Launch page, expand the Exec environment section: Click the Run capabilities dropdown list Check that in the list, there is a custom item with the name specified at step 4: Hover over this item - in a tooltip, the description specified at step 4 will appear: Select this item: Launch the run Open the just-launched run At the Run logs page, expand the Parameters section: All parameters specified at step 4 are displayed. Also, here you can see - each selected run capability is set as an environment variable in the format: CP_CAP_CUSTOM_{CAPABILITY_NAME} with true value Wait until all initialization tasks are done. Check that Docker-in-Docker was setup: Check in the Console that capability commands specified at step 4 were also performed: Click the SSH hyperlink. In the web-terminal, check the result of the commands performing:","title":"Usage of the custom capability"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/","text":"10. Manage Tools Overview \"Details\" view pane Registry Tool group Search Show attributes/Hide attributes \"Gear\" icon Personal Docker repository (Tool group) List of Tools Tool information page Description tab Versions tab Settings tab For more information about Docker container lifecycle and instance lifecycle see at Appendix A. Instance and Docker container lifecycles . You also can view tools details via the CLI . See 14.8. View tools definitions . Overview The Tools tab represents a list of available docker registries and docker images that contain tools. Using Docker images allows you to configure the same processing environment on each node regardless of the node type. Every Tool object in the Cloud Pipeline is a representation of Docker image. \"Details\" view pane At the top of the page, you'll see the basic objects of the \"Tools\" space. Registry Click on the registry name to see a drop-down list of available Docker registries and choose one. Tool group Press under the arrow to see a list of available Tool groups or to search Tool group by the name. Only the lower-case alphanumeric string is allowed for a Tool group name. Note : when you navigate to the Docker registry, the Tool group shown by default will be chosen based on the following conditions: If a user is included into some group (e.g. \"cancer\") and a Tool group with the same name exists, it will be shown by default. See more about user groups here . If the first condition isn't met, \" library \" group will be shown by default. If \" library \" group doesn't exist, \" default \" group will be shown by default. If the \" default \" group doesn't exist either, \" personal \" group will be shown. If none of the above groups doesn't exist/user doesn't have access to them, the first Tool group from the list will be shown by default. Search field This field helps to find a Tool by name in particular Tool group in a registry. Show attributes/Hide attributes Show attributes/Hide attributes opens the Attributes pane, where you can see and edit a list of key=value attributes of the tool group. See 17. CP objects tagging by additional attributes . \"Gear\" icon The following options are available: Option Description Registry This button allows to create a new registry or Edit/Delete current. Group Allows creating new Tool group or Edit/Delete current. + Enable Tool Enables a Tool in a registry. How to configure Configures the Docker client to push/pull images to/from a registry. Note : Docker client needs to be installed. For installation instructions refer to https://docs.docker.com/install/ . Personal Docker repository (Tool group) All tools within such repository are named as <user_name>/<tool_name> . Note : If a user loads a registry and there is no \"personal\" group in it, it shall be checked whether he has WRITE access to the registry. If No - do not display \"personal\" section (1) in the registry. If Yes - a user will see the message \"Personal tool group was not found in registry\" (2) . On top of that, you'll see a suggestion to explore library Tool group. See how to create personal Tool group here . List of Tools Tools list will be shown after you select group and registry. Tool information page Click on a Tool's name to open Tool information page. In the top right corner you can find the following buttons: Control Description \"Displays\" icon This icon includes: Show attributes/Hide attributes This button is used to show attributes of a Tool. Note : If selected Tool has any defined attribute, attributes pane is shown by default. For more details see 17. CP objects tagging by additional attributes . \"Show issues/Hide issues\" shows/hides the issues of the current Tool to discuss. To learn more see here . \"Gear\" icon Manage Tool permissions or delete a Tool. Run Run an instance with this Tool. Tool information page is divided into 3 tabs. Description tab This tab shows Tool description. For registries with Pipeline authentication option, you'll also see the Docker pull command on this tab if you have READ access to a Tool. Versions tab Choose this tab to see a list of Tool versions. About internal Tool version menu see 10.7. Tool version menu . Each version has the following icons and controls: Control/Label Description Name Name of version Scanning status If the security scanning is forced, you'll see the status \" in progress \" of security scanning. If the security scanning is failed, you'll see the status \" failed \". Last successful scan: The label shows if a version is successfully scanned at any time. The label contains date and time of the last successful attempt. Last scan date The label shows if a version scanning is failed. The label contains date and time of the last scanning attempt. Colored bars Hover over the colored bars to see scan status - a number of vulnerabilities grouped by severity (e.g. Critical, High, Medium, ...). Digest The label shows unique identifier of docker image. Corresponding aliases The label shows aliases of docker image (e.g. if some digest has more than one alias). Image size The label shows the size of docker image. Note : this value is provided for the \"gzipped\" docker image. When pulled to the local workstation or the cloud instance - the size of the image will be greater. Modified date The label shows modified date of docker image. SCAN Control forces the security scanning process. Available only for users with ROLE_ADMIN role. Run Allow to run the particular Tool version with default settings or customize it. Delete Delete the particular Tool version. In addition, the Version tab contains View unscanned version control. The control is visible if unscanned versions exist. More about Security scan feature you could learn here . Settings tab Navigate to this tab to see Tool attributes and execution defaults: Tool endpoints - specify an endpoint for the service launched in a Tool. Tool attributes - tool labels to briefly describe the Tool. Execution defaults - execution environment settings with which tool will be run by default: Instance type - an instance type in terms of Cloud Provider with specifying amounts of CPU, RAM and GPU. Price type - spot or on-demand type of instance. Disk - instance disk size in Gb. Limit mounts - available storages for Tool execution. Configure cluster - by clicking on this button cluster can be configured (for more details see here ). Cloud Region - the field where the admin/tool owner could select a specific Cloud Provider / Region to enforce users to run that tool in it. By default, has Not configured value. This means, that a tool will be launched in a Default region (configured in the global settings) or a user can set any allowed Cloud Region / Provider manually. Cmd template - default command for Tool execution. System parameters - system parameters that can be used during Tool execution. Custom parameters - specific parameters that can be used during Tool execution. Navigate back to the Tools group page from the Tool description with the arrow button on the top-left corner.","title":"10.0. Overview"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#10-manage-tools","text":"Overview \"Details\" view pane Registry Tool group Search Show attributes/Hide attributes \"Gear\" icon Personal Docker repository (Tool group) List of Tools Tool information page Description tab Versions tab Settings tab For more information about Docker container lifecycle and instance lifecycle see at Appendix A. Instance and Docker container lifecycles . You also can view tools details via the CLI . See 14.8. View tools definitions .","title":"10. Manage Tools"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#overview","text":"The Tools tab represents a list of available docker registries and docker images that contain tools. Using Docker images allows you to configure the same processing environment on each node regardless of the node type. Every Tool object in the Cloud Pipeline is a representation of Docker image.","title":"Overview"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#details-view-pane","text":"At the top of the page, you'll see the basic objects of the \"Tools\" space.","title":"\"Details\" view pane"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#registry","text":"Click on the registry name to see a drop-down list of available Docker registries and choose one.","title":"Registry"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#tool-group","text":"Press under the arrow to see a list of available Tool groups or to search Tool group by the name. Only the lower-case alphanumeric string is allowed for a Tool group name. Note : when you navigate to the Docker registry, the Tool group shown by default will be chosen based on the following conditions: If a user is included into some group (e.g. \"cancer\") and a Tool group with the same name exists, it will be shown by default. See more about user groups here . If the first condition isn't met, \" library \" group will be shown by default. If \" library \" group doesn't exist, \" default \" group will be shown by default. If the \" default \" group doesn't exist either, \" personal \" group will be shown. If none of the above groups doesn't exist/user doesn't have access to them, the first Tool group from the list will be shown by default.","title":"Tool group"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#search-field","text":"This field helps to find a Tool by name in particular Tool group in a registry.","title":"Search field"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#show-attributeshide-attributes","text":"Show attributes/Hide attributes opens the Attributes pane, where you can see and edit a list of key=value attributes of the tool group. See 17. CP objects tagging by additional attributes .","title":"Show attributes/Hide attributes"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#gear-icon","text":"The following options are available: Option Description Registry This button allows to create a new registry or Edit/Delete current. Group Allows creating new Tool group or Edit/Delete current. + Enable Tool Enables a Tool in a registry. How to configure Configures the Docker client to push/pull images to/from a registry. Note : Docker client needs to be installed. For installation instructions refer to https://docs.docker.com/install/ .","title":"\"Gear\" icon"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#personal-docker-repository-tool-group","text":"All tools within such repository are named as <user_name>/<tool_name> . Note : If a user loads a registry and there is no \"personal\" group in it, it shall be checked whether he has WRITE access to the registry. If No - do not display \"personal\" section (1) in the registry. If Yes - a user will see the message \"Personal tool group was not found in registry\" (2) . On top of that, you'll see a suggestion to explore library Tool group. See how to create personal Tool group here .","title":"Personal Docker repository (Tool group)"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#list-of-tools","text":"Tools list will be shown after you select group and registry.","title":"List of Tools"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#tool-information-page","text":"Click on a Tool's name to open Tool information page. In the top right corner you can find the following buttons: Control Description \"Displays\" icon This icon includes: Show attributes/Hide attributes This button is used to show attributes of a Tool. Note : If selected Tool has any defined attribute, attributes pane is shown by default. For more details see 17. CP objects tagging by additional attributes . \"Show issues/Hide issues\" shows/hides the issues of the current Tool to discuss. To learn more see here . \"Gear\" icon Manage Tool permissions or delete a Tool. Run Run an instance with this Tool. Tool information page is divided into 3 tabs.","title":"Tool information page"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#description-tab","text":"This tab shows Tool description. For registries with Pipeline authentication option, you'll also see the Docker pull command on this tab if you have READ access to a Tool.","title":"Description\u00a0tab"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#versions-tab","text":"Choose this tab to see a list of Tool versions. About internal Tool version menu see 10.7. Tool version menu . Each version has the following icons and controls: Control/Label Description Name Name of version Scanning status If the security scanning is forced, you'll see the status \" in progress \" of security scanning. If the security scanning is failed, you'll see the status \" failed \". Last successful scan: The label shows if a version is successfully scanned at any time. The label contains date and time of the last successful attempt. Last scan date The label shows if a version scanning is failed. The label contains date and time of the last scanning attempt. Colored bars Hover over the colored bars to see scan status - a number of vulnerabilities grouped by severity (e.g. Critical, High, Medium, ...). Digest The label shows unique identifier of docker image. Corresponding aliases The label shows aliases of docker image (e.g. if some digest has more than one alias). Image size The label shows the size of docker image. Note : this value is provided for the \"gzipped\" docker image. When pulled to the local workstation or the cloud instance - the size of the image will be greater. Modified date The label shows modified date of docker image. SCAN Control forces the security scanning process. Available only for users with ROLE_ADMIN role. Run Allow to run the particular Tool version with default settings or customize it. Delete Delete the particular Tool version. In addition, the Version tab contains View unscanned version control. The control is visible if unscanned versions exist. More about Security scan feature you could learn here .","title":"Versions\u00a0tab"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#settings-tab","text":"Navigate to this tab to see Tool attributes and execution defaults: Tool endpoints - specify an endpoint for the service launched in a Tool. Tool attributes - tool labels to briefly describe the Tool. Execution defaults - execution environment settings with which tool will be run by default: Instance type - an instance type in terms of Cloud Provider with specifying amounts of CPU, RAM and GPU. Price type - spot or on-demand type of instance. Disk - instance disk size in Gb. Limit mounts - available storages for Tool execution. Configure cluster - by clicking on this button cluster can be configured (for more details see here ). Cloud Region - the field where the admin/tool owner could select a specific Cloud Provider / Region to enforce users to run that tool in it. By default, has Not configured value. This means, that a tool will be launched in a Default region (configured in the global settings) or a user can set any allowed Cloud Region / Provider manually. Cmd template - default command for Tool execution. System parameters - system parameters that can be used during Tool execution. Custom parameters - specific parameters that can be used during Tool execution. Navigate back to the Tools group page from the Tool description with the arrow button on the top-left corner.","title":"Settings\u00a0tab"},{"location":"manual/11_Manage_Runs/11.1._Manage_runs_lifecycles/","text":"11.1. Manage runs lifecycles Pause/resume run Stop/terminate run Rerun Resolve variables for a rerun Preselect metadata instances for a rerun Only users with ROLE_ADMIN or OWNERS can manage runs lifecycles (pause/resume/stop/terminate). Cloud Platform currently provides functionality to launch and access services on Cloud hosted calculation nodes. Launching a service takes up to several minutes depending on multiple factors. When work with service is done, instance is terminated and all the local data and environment (installed tools, settings) are completely lost. In order to store the data it should be uploaded to Cloud data storage before service termination, to save service environment user may user COMMIT option to update a service or create a new one, but for some use cases, e.g. script development in RStudio , these options may be inconvenient. PAUSE and RESUME options allow to reduce time to start a service, have an option to store service state and to reduce expenses for idle services. Stopped instances cost less than running instances. Note : pause/resume options are available only for on-demand instances. Price type can be set during Run configuration in the Advanced tab. Note : you can't pause/resume cluster runs even with On-demand price type. Pause/resume run Find a run you want to pause in the Active runs tab and press Pause . Confirm pausing. A Run will have status PAUSING for a short period of time. Then RESUME option will appear. To resume the Run press the Resume button and confirm this action. A Run will have status RESUMING for a short period of time. Then a Run will continue working again. Note : Not always paused run could be resumed. E.g. user may hit a situation of resource limits - instance type was available when run was initially launched, but at the moment of resume operation provider has no sufficient capacity for this type. In such cases, run will be returned to the Paused state. User will be notified about that - by the hint message near the RESUME button: Or at the Run information page : Or at the ACTIVE RUNS panel of the main Dashboard: Stop/terminate run STOP option allows to stop a run execution forcibly. Once a run is stopped - all its local data will be deleted, this action couldn't be undone. Note : This option is available only for initializing/executing runs, not for paused ones. Note : This action only stops run and doesn't terminate cluster node. Find a run you want to stop in the Active runs tab and press STOP . Confirm action by click \" STOP \" button. After that, run execution will be stopped and run with \"Stopped\" state will appear at \"COMPLETED RUNS\" tab. Note : also you can stop a run via CLI. For more details see here . Some of the jobs, that were paused (either manually, or by the automated service), may be not needed anymore. In that case, user is more convenient to use TERMINATE option. TERMINATE option allows to terminate compute node of a paused run without its resuming. Note : This option is shown only for paused runs. Note : This action terminates cluster node and marks run as \"Stopped\". Find a paused run you want to terminate in the Active runs tab and press TERMINATE . Confirm terminating. After that, cluster node of that run will be terminated and run with \"Stopped\" state will appear at \"COMPLETED RUNS\" tab. Rerun Often, users may want to launch a run with all the same settings as another run that already have been launched before. It can be useful, for example, if the job was failed by any outside reasons and user need to launch all the same job again. For such cases, the RERUN option is available in the Cloud Pipeline . It allows to re-launch a completed run, i.e. to launch a new run with all the same settings (execution environments, advanced settings, parameters) that this completed run had. Rerun can be performed from the Run logs page of the completed run, e.g.: Or from the COMPLETED RUNS page, e.g.: Example: Open the completed run you wish to rerun, pay attention to the instance settings and parameters, e.g.: Click the Rerun button in the right-upper corner. The Launch page will be opened: All settings (execution environments, advanced settings, parameters) are automatically set as they were in the completed run. If you need, change the desired launch parameters. To continue - click the Launch button and confirm the launch in the pop-up that will appear. Re-launched run will appear at the Runs page: Click it. At the Run logs page, you can see that all settings are the same as were set for the run opened at step 1: Resolve variables for a rerun If the initial run configuration contains environment variables in its parameters - these variables will have new values after resolving during the rerun (default behavior). But also users can manually make a choice for a rerun: to resolve such variables or not and use their initial values. For example: Launch any run that uses an environment variable in parameters, e.g. $RUN_ID in output path for the results storing: Open the launched run: Expand the Parameters section, check that the environment variable in parameters was resolved (instead $RUN_ID the ID of the current run is substituted): Wait until the run will be completed or manually stop it. The RERUN button will appear in the right-upper corner. Click it: The Launch form will appear similar to the shown at step 1. All settings for the coming run are set as they were for the initial run. The only difference - the checkbox \" Use resolved values \" appeared in the Parameters section: By default, this checkbox is disabled. In this case, all environment variables are shown as is and will be resolved only during the new (re-launched) run - with the values corresponding to this new run. Tick the checkbox \" Use resolved values \": In this case, all environment variables will be resolved with the values of the initial run (that was launched at steps 1-2). Correspondingly, parameters that use environment variables will not be changed during the new launch. So, by this checkbox you can choice to resolve environment variable or not and then perform a launch. Preselect metadata instances for a rerun If the initial run configuration contains expansion expressions in its parameters for using metadata instances - in case of rerun, these expressions will be resolved with the same values as in the initial run (default behavior). But also users can manually make a choice - to rerun with the same resolved expressions values from the initial run or preselect another metadata instance(s) for a coming rerun. For example: Create a Project . Upload a metadata to this Project, e.g.: Create a Detach configuration in this Project, e.g.: In this configuration, add a parameter that uses expansion expression - as the parameter SName on the picture above that will be resolved as SampleName attribute value of the metadata instance for which the run will be launched. Click the Run button. The pop-up will appear where you should select a metadata instance for which the run will be launched: Expand your metadata object and select an instance, e.g.: Click the OK button and confirm the launch. Open the launched run: Expand the Parameters section, check that the expansion expression in parameters was resolved (instead this.SampleName for the SName parameter, the name of the sample selected for the run is substituted): Wait until the run will be completed or manually stop it. The RERUN button will appear in the right-upper corner. Click it: The Launch form will appear. All settings for the coming run are set as they were for the initial run. Parameters are substituted fully the same as they were in the initial run: If click the Launch button in this case - during the rerun, all parameter(s) that use expansion expression(s) will use their resolved values from the initial run. But if you wish, you can preselect another metadata instance for the re-launch. For that, click \" v \" button near the launch button and in the appeared list click \" Select metadata entries and launch \" item: The pop-up will appear where you can select a metadata instance for which the rerun will be launched: In this case - during the rerun, all parameter(s) that use expansion expression(s) will be resolved according to a new selected metadata instance(s). So, by this additional control at the re-launch form, you can choice - to use resolved expansion expression value(s) from the initial run or to select another metadata instance(s) for the rerun.","title":"11.1. Manage runs lifecycles"},{"location":"manual/11_Manage_Runs/11.1._Manage_runs_lifecycles/#111-manage-runs-lifecycles","text":"Pause/resume run Stop/terminate run Rerun Resolve variables for a rerun Preselect metadata instances for a rerun Only users with ROLE_ADMIN or OWNERS can manage runs lifecycles (pause/resume/stop/terminate). Cloud Platform currently provides functionality to launch and access services on Cloud hosted calculation nodes. Launching a service takes up to several minutes depending on multiple factors. When work with service is done, instance is terminated and all the local data and environment (installed tools, settings) are completely lost. In order to store the data it should be uploaded to Cloud data storage before service termination, to save service environment user may user COMMIT option to update a service or create a new one, but for some use cases, e.g. script development in RStudio , these options may be inconvenient. PAUSE and RESUME options allow to reduce time to start a service, have an option to store service state and to reduce expenses for idle services. Stopped instances cost less than running instances. Note : pause/resume options are available only for on-demand instances. Price type can be set during Run configuration in the Advanced tab. Note : you can't pause/resume cluster runs even with On-demand price type.","title":"11.1. Manage runs lifecycles"},{"location":"manual/11_Manage_Runs/11.1._Manage_runs_lifecycles/#pauseresume-run","text":"Find a run you want to pause in the Active runs tab and press Pause . Confirm pausing. A Run will have status PAUSING for a short period of time. Then RESUME option will appear. To resume the Run press the Resume button and confirm this action. A Run will have status RESUMING for a short period of time. Then a Run will continue working again. Note : Not always paused run could be resumed. E.g. user may hit a situation of resource limits - instance type was available when run was initially launched, but at the moment of resume operation provider has no sufficient capacity for this type. In such cases, run will be returned to the Paused state. User will be notified about that - by the hint message near the RESUME button: Or at the Run information page : Or at the ACTIVE RUNS panel of the main Dashboard:","title":"Pause/resume run"},{"location":"manual/11_Manage_Runs/11.1._Manage_runs_lifecycles/#stopterminate-run","text":"STOP option allows to stop a run execution forcibly. Once a run is stopped - all its local data will be deleted, this action couldn't be undone. Note : This option is available only for initializing/executing runs, not for paused ones. Note : This action only stops run and doesn't terminate cluster node. Find a run you want to stop in the Active runs tab and press STOP . Confirm action by click \" STOP \" button. After that, run execution will be stopped and run with \"Stopped\" state will appear at \"COMPLETED RUNS\" tab. Note : also you can stop a run via CLI. For more details see here . Some of the jobs, that were paused (either manually, or by the automated service), may be not needed anymore. In that case, user is more convenient to use TERMINATE option. TERMINATE option allows to terminate compute node of a paused run without its resuming. Note : This option is shown only for paused runs. Note : This action terminates cluster node and marks run as \"Stopped\". Find a paused run you want to terminate in the Active runs tab and press TERMINATE . Confirm terminating. After that, cluster node of that run will be terminated and run with \"Stopped\" state will appear at \"COMPLETED RUNS\" tab.","title":"Stop/terminate run"},{"location":"manual/11_Manage_Runs/11.1._Manage_runs_lifecycles/#rerun","text":"Often, users may want to launch a run with all the same settings as another run that already have been launched before. It can be useful, for example, if the job was failed by any outside reasons and user need to launch all the same job again. For such cases, the RERUN option is available in the Cloud Pipeline . It allows to re-launch a completed run, i.e. to launch a new run with all the same settings (execution environments, advanced settings, parameters) that this completed run had. Rerun can be performed from the Run logs page of the completed run, e.g.: Or from the COMPLETED RUNS page, e.g.: Example: Open the completed run you wish to rerun, pay attention to the instance settings and parameters, e.g.: Click the Rerun button in the right-upper corner. The Launch page will be opened: All settings (execution environments, advanced settings, parameters) are automatically set as they were in the completed run. If you need, change the desired launch parameters. To continue - click the Launch button and confirm the launch in the pop-up that will appear. Re-launched run will appear at the Runs page: Click it. At the Run logs page, you can see that all settings are the same as were set for the run opened at step 1:","title":"Rerun"},{"location":"manual/11_Manage_Runs/11.1._Manage_runs_lifecycles/#resolve-variables-for-a-rerun","text":"If the initial run configuration contains environment variables in its parameters - these variables will have new values after resolving during the rerun (default behavior). But also users can manually make a choice for a rerun: to resolve such variables or not and use their initial values. For example: Launch any run that uses an environment variable in parameters, e.g. $RUN_ID in output path for the results storing: Open the launched run: Expand the Parameters section, check that the environment variable in parameters was resolved (instead $RUN_ID the ID of the current run is substituted): Wait until the run will be completed or manually stop it. The RERUN button will appear in the right-upper corner. Click it: The Launch form will appear similar to the shown at step 1. All settings for the coming run are set as they were for the initial run. The only difference - the checkbox \" Use resolved values \" appeared in the Parameters section: By default, this checkbox is disabled. In this case, all environment variables are shown as is and will be resolved only during the new (re-launched) run - with the values corresponding to this new run. Tick the checkbox \" Use resolved values \": In this case, all environment variables will be resolved with the values of the initial run (that was launched at steps 1-2). Correspondingly, parameters that use environment variables will not be changed during the new launch. So, by this checkbox you can choice to resolve environment variable or not and then perform a launch.","title":"Resolve variables for a rerun"},{"location":"manual/11_Manage_Runs/11.1._Manage_runs_lifecycles/#preselect-metadata-instances-for-a-rerun","text":"If the initial run configuration contains expansion expressions in its parameters for using metadata instances - in case of rerun, these expressions will be resolved with the same values as in the initial run (default behavior). But also users can manually make a choice - to rerun with the same resolved expressions values from the initial run or preselect another metadata instance(s) for a coming rerun. For example: Create a Project . Upload a metadata to this Project, e.g.: Create a Detach configuration in this Project, e.g.: In this configuration, add a parameter that uses expansion expression - as the parameter SName on the picture above that will be resolved as SampleName attribute value of the metadata instance for which the run will be launched. Click the Run button. The pop-up will appear where you should select a metadata instance for which the run will be launched: Expand your metadata object and select an instance, e.g.: Click the OK button and confirm the launch. Open the launched run: Expand the Parameters section, check that the expansion expression in parameters was resolved (instead this.SampleName for the SName parameter, the name of the sample selected for the run is substituted): Wait until the run will be completed or manually stop it. The RERUN button will appear in the right-upper corner. Click it: The Launch form will appear. All settings for the coming run are set as they were for the initial run. Parameters are substituted fully the same as they were in the initial run: If click the Launch button in this case - during the rerun, all parameter(s) that use expansion expression(s) will use their resolved values from the initial run. But if you wish, you can preselect another metadata instance for the re-launch. For that, click \" v \" button near the launch button and in the appeared list click \" Select metadata entries and launch \" item: The pop-up will appear where you can select a metadata instance for which the rerun will be launched: In this case - during the rerun, all parameter(s) that use expansion expression(s) will be resolved according to a new selected metadata instance(s). So, by this additional control at the re-launch form, you can choice - to use resolved expansion expression value(s) from the initial run or to select another metadata instance(s) for the rerun.","title":"Preselect metadata instances for a rerun"},{"location":"manual/11_Manage_Runs/11.2._Auto-commit_Docker_image/","text":"11.2. Auto-commit Docker image User shall have ROLE_ADMIN role or be an OWNER of the Run to stop it and auto-commit Docker image. Auto-committing is a useful Cloud Pipeline option that allows to save current Docker image state before stopping a Run. In the Active runs tab select a Run and press STOP Select the checkbox Persist current docker image state , give that Docker image a name and a version (optional), e.g. auto-committed-version ). Press STOP . After that a Run will have a COMMITTING status for a short period of time. And then it will be stopped.","title":"11.2. Auto-commit Docker images"},{"location":"manual/11_Manage_Runs/11.2._Auto-commit_Docker_image/#112-auto-commit-docker-image","text":"User shall have ROLE_ADMIN role or be an OWNER of the Run to stop it and auto-commit Docker image. Auto-committing is a useful Cloud Pipeline option that allows to save current Docker image state before stopping a Run. In the Active runs tab select a Run and press STOP Select the checkbox Persist current docker image state , give that Docker image a name and a version (optional), e.g. auto-committed-version ). Press STOP . After that a Run will have a COMMITTING status for a short period of time. And then it will be stopped.","title":"11.2. Auto-commit Docker image"},{"location":"manual/11_Manage_Runs/11.3._Sharing_with_other_users_or_groups_of_users/","text":"11.3. Sharing with other users or groups of users Overview Sharing a run with user(s) Sharing a run with users group(s) Work with a sharing running instance (for not owners/admins) Sharing runs with the anonymous users Overview For certain use cases it is beneficial to be able to share applications with other users/groups. Cloud platform allows ability when runs environments will be accessed for several users, not only for the user, who launched the run ( OWNER ). Sharing of a run - allows to share as the interactive tools endpoints (e.g. rstudio , jupyter , nomachine , etc.) and SSH sessions too. Sharing a run with user(s) In this example we will share a run with other user(s). Note : for do that, user account shall be registered within CP users catalog and granted READ & EXECUTE for the pipeline/tool. User must be an OWNER of the running instance. Note : in the example we will share a run of the RStudio tool (for more information about launching Tools see 10.5. Launch a Tool ). Open run logs of the instance: Click the link near the label \" Share with \": In the opened pop-up window click button. In the appeared window enter user name, for whom you want to share running instance. Confirm selected user by clicking the OK button: If necessary, add more users. If you want to give a SSH-access to the instance - set the Enable SSH connection checkbox. If this checkbox is not set, only the interactive tools endpoints will be shared. When finished, click the Ok button: In the run logs, users names for whom you shared running instance will appear near the label Share with : Copy the link near the label Endpoint , send it to users, for whom you shared the instance: To share the SSH-access to a non-interactive run for the user - use the similar steps. For a non-interactive run the \"share\" pop-up looks slightly different (\" Enable SSH connection \" checkboxes are missed): Sharing a run with users group(s) In this example we will share a run with other users group(s). Note : for do that, user account shall be registered within CP users catalog and granted READ & EXECUTE for the pipeline/tool. User must be an OWNER of the running instance. Note : in the example we will share a run of the RStudio tool (for more information about launching Tools see 10.5. Launch a Tool ). Open run logs of the instance: Click the link near the label Share with : In the opened pop-up window click button. In the appeared window enter user group's name, for which you want to share running instance. Confirm selected user group by click the OK button: If necessary, add more groups. If you want to give a SSH-access to the instance - set the Enable SSH connection checkbox. If this checkbox is not set, only the interactive tools endpoints will be shared. When finished, click the OK button: In the run logs, user group names, for which you shared running instance, will appear near the label Share with : Copy the link near the label Endpoint , send it to users, for which group(s) you shared the instance: To share the SSH-access to a non-interactive run for the user group - use the similar steps. For a non-interactive run the \"share\" pop-up looks slightly different (\" Enable SSH connection \" checkboxes are missed): Work with a sharing running instance (for not owners/admins) A current user can be accessed to a service, without running own jobs, if that service was shared for a current user or his group(s). Note : for do that, user account shall be registered within CP users catalog and granted \"sharing\" permission for the instance. Way 1 Log in at the Cloud Pipeline. Open a new tab in a browser and input the link of the sharing running instance, that you received. The GUI of the Tool, of that running instance was shared, will be displayed. For example described above, it will be the RStudio GUI: Way 2 On the Home dashboard click button. In the opened popup enable the checkbox SERVICES and click the OK button: On the appeared SERVICES widget at the Home dashboard page accessible \"shared\" services will be displayed: Click it. The GUI of the Tool of the running shared instance will be displayed. Way 3. For the runs with the shared SSH-access If Enable SSH connection checkbox was set at the sharing configure form (for the interactive run) or the non-interactive run was shared with a current user or his group(s), the SSH-access to the running instance can be obtained. For that - hover over the service \"card\" in the SERVICES widget at the Home dashboard page and click the SSH hyperlink: A new page with the Terminal access to the shared instance will appear: Sharing runs with the anonymous users For certain use-cases, it is required to allow such type of access for any user, who has successfully passed the IdP authentication but is not registered in the Cloud Pipeline and also such users shall not be automatically registered at all and remain Anonymous . To enable such behavior, the following application properties have to be specified before the deployment: saml.user.auto.create=EXPLICIT_GROUP saml.user.allow.anonymous=true Once anonymous access is enabled system-wide each run that requires to be accessible by Anonymous has to be configured to share endpoints with the following user group - ROLE_ANONYMOUS_USER . It could be performed in the following way: Cloud Pipeline user launches the run whose interactive endpoint he wishes to share. The user opens the Run logs page of the launched run Then clicks the link near the Share with label: In the opened popup clicks the corresponding button to share with a group/role: In the appeared window, the user should select the ROLE_ANONYMOUS_USER role: Sharing with the Anonymous will be displayed at the Run logs page: The user should copy the Endpoint-link of the run and send it to the Anonymous user he wants to share. Anonymous user should open a new tab in a browser and just input the link of the sharing running instance, that he has received. If that user passes SAML authentication, he will get access to the endpoint. Attempts to open any other Platform pages will fail. Note : a user is treated as Anonymous if he is logging in and the SAML response is valid, but the user is not registered in the Platform and the auto-registration (of any kind) is not enabled.","title":"11.3. Sharing with other users"},{"location":"manual/11_Manage_Runs/11.3._Sharing_with_other_users_or_groups_of_users/#113-sharing-with-other-users-or-groups-of-users","text":"Overview Sharing a run with user(s) Sharing a run with users group(s) Work with a sharing running instance (for not owners/admins) Sharing runs with the anonymous users","title":"11.3. Sharing with other users or groups of users"},{"location":"manual/11_Manage_Runs/11.3._Sharing_with_other_users_or_groups_of_users/#overview","text":"For certain use cases it is beneficial to be able to share applications with other users/groups. Cloud platform allows ability when runs environments will be accessed for several users, not only for the user, who launched the run ( OWNER ). Sharing of a run - allows to share as the interactive tools endpoints (e.g. rstudio , jupyter , nomachine , etc.) and SSH sessions too.","title":"Overview"},{"location":"manual/11_Manage_Runs/11.3._Sharing_with_other_users_or_groups_of_users/#sharing-a-run-with-users","text":"In this example we will share a run with other user(s). Note : for do that, user account shall be registered within CP users catalog and granted READ & EXECUTE for the pipeline/tool. User must be an OWNER of the running instance. Note : in the example we will share a run of the RStudio tool (for more information about launching Tools see 10.5. Launch a Tool ). Open run logs of the instance: Click the link near the label \" Share with \": In the opened pop-up window click button. In the appeared window enter user name, for whom you want to share running instance. Confirm selected user by clicking the OK button: If necessary, add more users. If you want to give a SSH-access to the instance - set the Enable SSH connection checkbox. If this checkbox is not set, only the interactive tools endpoints will be shared. When finished, click the Ok button: In the run logs, users names for whom you shared running instance will appear near the label Share with : Copy the link near the label Endpoint , send it to users, for whom you shared the instance: To share the SSH-access to a non-interactive run for the user - use the similar steps. For a non-interactive run the \"share\" pop-up looks slightly different (\" Enable SSH connection \" checkboxes are missed):","title":"Sharing a run with user(s)"},{"location":"manual/11_Manage_Runs/11.3._Sharing_with_other_users_or_groups_of_users/#sharing-a-run-with-users-groups","text":"In this example we will share a run with other users group(s). Note : for do that, user account shall be registered within CP users catalog and granted READ & EXECUTE for the pipeline/tool. User must be an OWNER of the running instance. Note : in the example we will share a run of the RStudio tool (for more information about launching Tools see 10.5. Launch a Tool ). Open run logs of the instance: Click the link near the label Share with : In the opened pop-up window click button. In the appeared window enter user group's name, for which you want to share running instance. Confirm selected user group by click the OK button: If necessary, add more groups. If you want to give a SSH-access to the instance - set the Enable SSH connection checkbox. If this checkbox is not set, only the interactive tools endpoints will be shared. When finished, click the OK button: In the run logs, user group names, for which you shared running instance, will appear near the label Share with : Copy the link near the label Endpoint , send it to users, for which group(s) you shared the instance: To share the SSH-access to a non-interactive run for the user group - use the similar steps. For a non-interactive run the \"share\" pop-up looks slightly different (\" Enable SSH connection \" checkboxes are missed):","title":"Sharing a run with users group(s)"},{"location":"manual/11_Manage_Runs/11.3._Sharing_with_other_users_or_groups_of_users/#work-with-a-sharing-running-instance-for-not-ownersadmins","text":"A current user can be accessed to a service, without running own jobs, if that service was shared for a current user or his group(s). Note : for do that, user account shall be registered within CP users catalog and granted \"sharing\" permission for the instance.","title":"Work with a sharing running instance (for not owners/admins)"},{"location":"manual/11_Manage_Runs/11.3._Sharing_with_other_users_or_groups_of_users/#way-1","text":"Log in at the Cloud Pipeline. Open a new tab in a browser and input the link of the sharing running instance, that you received. The GUI of the Tool, of that running instance was shared, will be displayed. For example described above, it will be the RStudio GUI:","title":"Way 1"},{"location":"manual/11_Manage_Runs/11.3._Sharing_with_other_users_or_groups_of_users/#way-2","text":"On the Home dashboard click button. In the opened popup enable the checkbox SERVICES and click the OK button: On the appeared SERVICES widget at the Home dashboard page accessible \"shared\" services will be displayed: Click it. The GUI of the Tool of the running shared instance will be displayed.","title":"Way 2"},{"location":"manual/11_Manage_Runs/11.3._Sharing_with_other_users_or_groups_of_users/#way-3-for-the-runs-with-the-shared-ssh-access","text":"If Enable SSH connection checkbox was set at the sharing configure form (for the interactive run) or the non-interactive run was shared with a current user or his group(s), the SSH-access to the running instance can be obtained. For that - hover over the service \"card\" in the SERVICES widget at the Home dashboard page and click the SSH hyperlink: A new page with the Terminal access to the shared instance will appear:","title":"Way 3. For the runs with the shared SSH-access"},{"location":"manual/11_Manage_Runs/11.3._Sharing_with_other_users_or_groups_of_users/#sharing-runs-with-the-anonymous-users","text":"For certain use-cases, it is required to allow such type of access for any user, who has successfully passed the IdP authentication but is not registered in the Cloud Pipeline and also such users shall not be automatically registered at all and remain Anonymous . To enable such behavior, the following application properties have to be specified before the deployment: saml.user.auto.create=EXPLICIT_GROUP saml.user.allow.anonymous=true Once anonymous access is enabled system-wide each run that requires to be accessible by Anonymous has to be configured to share endpoints with the following user group - ROLE_ANONYMOUS_USER . It could be performed in the following way: Cloud Pipeline user launches the run whose interactive endpoint he wishes to share. The user opens the Run logs page of the launched run Then clicks the link near the Share with label: In the opened popup clicks the corresponding button to share with a group/role: In the appeared window, the user should select the ROLE_ANONYMOUS_USER role: Sharing with the Anonymous will be displayed at the Run logs page: The user should copy the Endpoint-link of the run and send it to the Anonymous user he wants to share. Anonymous user should open a new tab in a browser and just input the link of the sharing running instance, that he has received. If that user passes SAML authentication, he will get access to the endpoint. Attempts to open any other Platform pages will fail. Note : a user is treated as Anonymous if he is logging in and the SAML response is valid, but the user is not registered in the Platform and the auto-registration (of any kind) is not enabled.","title":"Sharing runs with the anonymous users"},{"location":"manual/11_Manage_Runs/11.4._Automatic_actions_after_notifications/","text":"Automatic actions with runs In this section, let's consider the configurable behavior of automatic actions/notifications for the launched runs that are being in \"idle\" or \"under pressure\" state for a long time. The following view of high-level metrics information for the Active Runs is implemented: - this auxiliary label is shown when node's CPU consumption is lower than a certain level, defined by the admin. This label should attract the users attention cause such run may produce extra costs. - this auxiliary label is shown when node's Memory/Disk consumption is higher than a certain level, defined by the admin. This label should attract the users attention cause such run may accidentally fail. These labels are displayed: at the Runs page at the Run logs page at the main dashboard (the ACTIVE RUNS panel) If a user clicks that label from the Runs or the Run logs page the Cluster node Monitor will be opened to view the current node consumption. Admins can configure the emergence of these labels and system actions for each run state (\"Idle\"/\"Pressure\") by the system-level parameters. \"Idle\" runs The system behavior for the \"idle\" runs is defined by the set of the following System-level parameters ( Preferences ): Preference name Description system.max.idle.timeout.minutes Specifies the duration in minutes after that the system should check node's activity. If after this duration node's CPU utilization will be below system.idle.cpu.threshold - email notification IDLE_RUN will be sent to the user and the run itself will be marked by the label system.idle.action.timeout.minutes Specifies the duration in minutes. If node's CPU utilization is below system.idle.cpu.threshold for this duration after the system.max.idle.timeout.minutes is over - an action, specified in system.idle.action will be performed system.idle.cpu.threshold Specifies percentage of the node's CPU utilization, below which an action shall be taken system.idle.action Sets which action to perform with the node, that has the CPU utilization below than system.idle.cpu.threshold : NOTIFY - only send email notification IDLE_RUN . This action will be repeated every system.idle.action.timeout.minutes if the node's CPU utilization will be still below than system.idle.cpu.threshold PAUSE - pause an instance if possible (only if the instance is On-Demand , Spot instances are skipped) and send single email notification IDLE_RUN_PAUSED PAUSE_OR_STOP - pause an instance if it is On-Demand or stop an instance if it is Spot and send the corresponding single email notification IDLE_RUN_PAUSED / IDLE_RUN_STOPPED STOP - Stop an instance, disregarding price-type, and send single email notification IDLE_RUN_STOPPED system.resource.monitoring.period Specifies period (in milliseconds) between the scannings of running instances to collect the monitoring metrics. After each such period, it's defined to display label for the specific instance or not Example of these settings: In general, the behavior will be the following: User launches a run After system.max.idle.timeout.minutes period, the system starts to check the node's activity. If the node's CPU utilization becomes below system.idle.cpu.threshold : email notification IDLE_RUN is being sent, the run itself is being marked by the label After system.idle.action.timeout.minutes , if the node's CPU utilization is still below system.idle.cpu.threshold : email notification IDLE_RUN is being sent (in case when system.idle.action is set as NOTIFY ) run is being paused/stopped and the corresponding email notification IDLE_RUN_PAUSED or IDLE_RUN_STOPPED is being sent (in case when system.idle.action is set as PAUSE / PAUSE_OR_STOP / STOP ) In case when system.idle.action is set as NOTIFY , email notifications IDLE_RUN continue to be sent every system.idle.action.timeout.minutes , if the node's CPU utilization remains below the system.idle.cpu.threshold The state of the label (to display or not) is checked every system.resource.monitoring.period The settings of the email notifications (message, the list of informed users, etc.) the admin can configure via the corresponding tab Email notifications of the system-level settings: Note : users can manually disable the automatic pausing of on-demand instances if they aren't used. For that the \" Auto pause \" checkbox at the Launch page shall be unchecked before the run: This action cancels only the auto pause, but the RUN_IDLE email notifications will be being sent (if the corresponding conditions will be met). \"Pressure\" runs The system behavior for the runs \"under pressure\" (high-consumed) is defined by the set of the following System-level parameters ( Preferences ): Preference name Description system.disk.consume.threshold Specifies the node's disk threshold (in %) above which the email notification HIGH_CONSUMED_RESOURCES will be sent and the corresponding run will be marked by the label system.memory.consume.threshold Specifies the node's memory threshold (in %) above which the email notification HIGH_CONSUMED_RESOURCES will be sent and the corresponding run will be marked by the label Example of these settings: So, when memory or disk consuming will be higher than a threshold value for a specified period of time (in average) - a notification will be sent (and resent after a delay, if the problem is still in place. The default repeat delay is 30 minutes, it could be configured before the stand deployment). Preferences of the notification could be configured at the HIGH_CONSUMED_RESOURCES section of the Email notifications of the system-level settings:","title":"11.4. Automatic labels and actions for the runs"},{"location":"manual/11_Manage_Runs/11.4._Automatic_actions_after_notifications/#automatic-actions-with-runs","text":"In this section, let's consider the configurable behavior of automatic actions/notifications for the launched runs that are being in \"idle\" or \"under pressure\" state for a long time. The following view of high-level metrics information for the Active Runs is implemented: - this auxiliary label is shown when node's CPU consumption is lower than a certain level, defined by the admin. This label should attract the users attention cause such run may produce extra costs. - this auxiliary label is shown when node's Memory/Disk consumption is higher than a certain level, defined by the admin. This label should attract the users attention cause such run may accidentally fail. These labels are displayed: at the Runs page at the Run logs page at the main dashboard (the ACTIVE RUNS panel) If a user clicks that label from the Runs or the Run logs page the Cluster node Monitor will be opened to view the current node consumption. Admins can configure the emergence of these labels and system actions for each run state (\"Idle\"/\"Pressure\") by the system-level parameters.","title":"Automatic actions with runs"},{"location":"manual/11_Manage_Runs/11.4._Automatic_actions_after_notifications/#idle-runs","text":"The system behavior for the \"idle\" runs is defined by the set of the following System-level parameters ( Preferences ): Preference name Description system.max.idle.timeout.minutes Specifies the duration in minutes after that the system should check node's activity. If after this duration node's CPU utilization will be below system.idle.cpu.threshold - email notification IDLE_RUN will be sent to the user and the run itself will be marked by the label system.idle.action.timeout.minutes Specifies the duration in minutes. If node's CPU utilization is below system.idle.cpu.threshold for this duration after the system.max.idle.timeout.minutes is over - an action, specified in system.idle.action will be performed system.idle.cpu.threshold Specifies percentage of the node's CPU utilization, below which an action shall be taken system.idle.action Sets which action to perform with the node, that has the CPU utilization below than system.idle.cpu.threshold : NOTIFY - only send email notification IDLE_RUN . This action will be repeated every system.idle.action.timeout.minutes if the node's CPU utilization will be still below than system.idle.cpu.threshold PAUSE - pause an instance if possible (only if the instance is On-Demand , Spot instances are skipped) and send single email notification IDLE_RUN_PAUSED PAUSE_OR_STOP - pause an instance if it is On-Demand or stop an instance if it is Spot and send the corresponding single email notification IDLE_RUN_PAUSED / IDLE_RUN_STOPPED STOP - Stop an instance, disregarding price-type, and send single email notification IDLE_RUN_STOPPED system.resource.monitoring.period Specifies period (in milliseconds) between the scannings of running instances to collect the monitoring metrics. After each such period, it's defined to display label for the specific instance or not Example of these settings: In general, the behavior will be the following: User launches a run After system.max.idle.timeout.minutes period, the system starts to check the node's activity. If the node's CPU utilization becomes below system.idle.cpu.threshold : email notification IDLE_RUN is being sent, the run itself is being marked by the label After system.idle.action.timeout.minutes , if the node's CPU utilization is still below system.idle.cpu.threshold : email notification IDLE_RUN is being sent (in case when system.idle.action is set as NOTIFY ) run is being paused/stopped and the corresponding email notification IDLE_RUN_PAUSED or IDLE_RUN_STOPPED is being sent (in case when system.idle.action is set as PAUSE / PAUSE_OR_STOP / STOP ) In case when system.idle.action is set as NOTIFY , email notifications IDLE_RUN continue to be sent every system.idle.action.timeout.minutes , if the node's CPU utilization remains below the system.idle.cpu.threshold The state of the label (to display or not) is checked every system.resource.monitoring.period The settings of the email notifications (message, the list of informed users, etc.) the admin can configure via the corresponding tab Email notifications of the system-level settings: Note : users can manually disable the automatic pausing of on-demand instances if they aren't used. For that the \" Auto pause \" checkbox at the Launch page shall be unchecked before the run: This action cancels only the auto pause, but the RUN_IDLE email notifications will be being sent (if the corresponding conditions will be met).","title":"\"Idle\" runs"},{"location":"manual/11_Manage_Runs/11.4._Automatic_actions_after_notifications/#pressure-runs","text":"The system behavior for the runs \"under pressure\" (high-consumed) is defined by the set of the following System-level parameters ( Preferences ): Preference name Description system.disk.consume.threshold Specifies the node's disk threshold (in %) above which the email notification HIGH_CONSUMED_RESOURCES will be sent and the corresponding run will be marked by the label system.memory.consume.threshold Specifies the node's memory threshold (in %) above which the email notification HIGH_CONSUMED_RESOURCES will be sent and the corresponding run will be marked by the label Example of these settings: So, when memory or disk consuming will be higher than a threshold value for a specified period of time (in average) - a notification will be sent (and resent after a delay, if the problem is still in place. The default repeat delay is 30 minutes, it could be configured before the stand deployment). Preferences of the notification could be configured at the HIGH_CONSUMED_RESOURCES section of the Email notifications of the system-level settings:","title":"\"Pressure\" runs"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/","text":"11. Manage Runs Overview ACTIVE RUNS Active run states Active run controls Active cluster runs Displaying additional node metrics COMPLETED RUNS Completed run states Completed run controls Completed cluster runs Run information page General information Nested runs Cluster run usage Maintenance Instance Parameters Tasks Console output Controls Automatically rerun if a spot instance is terminated Overview \" Runs \" provides a list of active and completed pipeline runs. You can get parameters and logs of specific run and stop run here. \" Runs \" space has two tabs: Active runs view Completed runs view. Runs are organized in a table which is the same for both tabs: \"State\" icon - state of the run. Run - include: run name (upper row) - pipeline name and run id Cloud Region (bottom row) Note : if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - corresponding text information also has a Provider name, e.g.: Parent run - parent run ID, if a run was launched by another run. Pipeline - include: pipeline name (upper row) - a name of a pipeline version name (bottom row) - a name of a pipeline version Docker image - a name of docker image. Started - time when a run was started. Completed - time when a run was finished. Elapsed - include: elapsed time (upper row) - a duration of a run estimated price (bottom row) - estimated price of run, which is calculated based on the run duration and selected instance type. This field is updated interactively (i.e. each 5 - 10 seconds). Owner - a user, which launched a run. Note : also you can view information about runs via CLI. See here . ACTIVE RUNS This tab displays a list of all pipelines that are currently running. Active run states - Queued state (\"sandglass\" icon) - a run is waiting in the queue for the available compute node. - Initializing state (\"rotating\" icon) - a run is being initialized, at this stage a new compute node will be created or an existing node will be reused. - Pulling state (\"download\" icon) - now pipeline Docker image is downloaded to the node. - Running state (stable \"play\" icon) - a pipeline is running. The node is appearing and pipeline input data is being downloaded to the node before the \" InitializeEnvironment \" service task appears. Pausing state (blinking \"pause\" icon) - a run is being paused. At this moment compute node will be stopped (but persisted) and the docker image state will be kept as well. - Paused state (stable \"pause\" icon) - a run is paused. At this moment compute node is already stopped but keeps it's state. Such run may be resumed. Resuming state (blinking \"play\" icon) - a paused run is being resumed. At this moment compute node is starting back from the stopped state. Also, help tooltips are provided when hovering a run state icon, e.g.: Tooltips contain a state name in bold (e.g. Queued ) and a short description of the state and info on the next stage. Active run controls Control Description PAUSE/RESUME Pauses/resumes a run. Available for On-demand non-cluster instances only. Learn more about feature here . TERMINATE Terminates compute node of a paused run without its resuming. Available for On-demand non-cluster instances only. Learn more about feature here . STOP This control stops a run execution. LOG To open a Run information page, press LOG button. Active cluster runs Cluster is a collection of instances which are connected so that they can be used together on a task. If launched run uses a cluster or an auto-scaled cluster (see sections here ), it has a certain designation: By default, only master-run is displaying at the table. To view nested runs (child-runs) click the Expand control in front of the muster-run ID: So, you can view an information about each child-run and its state, also you can stop specific nested run without stopping a parent run. You can open Run logs page (see below ) for any of the cluster runs by click it or LOG button next to run ID. Note : you can't pause cluster runs even with On-demand price type. Note : stopping a parent run will stop execution of all nested runs too. For runs with the auto-scaled cluster not all of the child-runs appear in the list immediately after parent run was launched, \"scale-up\" runs will appear only of necessity. Displaying additional node metrics According to the run states and system-level settings, additional metrics (labels) could be displayed for active runs: - Idle label - displays only for runs in Running state, for which node's CPU consumption level is below than a certain threshold for a certain period of time or longer. - Pressure label - displays only for runs in Running state, for which node's Memory/Disk consumption level is higher than a certain threshold. Values of these thresholds and time period are specified by admins via system-level settings (see here for more details and an example of using - here ). COMPLETED RUNS This tab displays a list of all pipelines runs that are already finished. Completed run states - Success state (\"OK\" icon) - successful pipeline execution. - Failed state (\"caution\" icon) - unsuccessful pipeline execution. - Stopped state (\"clock\" icon) - a pipeline manually stopped. Help tooltips are also provided when hovering a completed run state icon, e.g.: Completed run controls Control Description LINKS This control show input/output links of the pipeline RERUN This control allow rerunning of a completed run. The Launch a pipeline page will be open. LOG To open a Run information page, press LOG button. Completed cluster runs If completed run used a cluster or an auto-scaled cluster (see sections here ), it has a certain designation. Displaying of such runs on the COMPLETED RUNS tab is similar to the active cluster runs. You can view an information about each child-run and its state, also you can rerun specific nested run without a parent run. You can open Run logs page (see below ) for any of the cluster runs by click it or LOG button next to run ID: Run information page Click a row within a run list, \"Run information\" page will appear. It consists of several sections: General information This section displays general information about a run: Field Description State icon state of the run. Help tooltips are provided when hovering a run state icon, e.g.: Run ID unique ID of the run. Endpoint ( available only for tools runs ) endpoint hyperlink for the service launched in an interactive tool. For more details see 15. Interactive services . Share with ( available only for tools runs ) list of users/groups with whom an interactive tool application is shared. For more details see 11.3 Sharing with other users or groups of users . Owner a name of the user who started pipeline. Scheduled time when a pipeline was launched. Waiting for/Running for time a pipeline has been running. Started time when the node is initialized and a pipeline has started execution. Finished time when a pipeline finished execution. Estimated price price of a run according to a run duration and selected instance type. Nested runs the child-runs list in cases when a run has a number of children (e.g. a cluster run or any other case with the parent-id specified) Maintenance the list of rules that define automatical pausing/restarting schedule for on-demand non-clusters runs Nested runs Nested runs list is displaying only for master runs. It is the list with short informations about cluster child-runs: Each child-run record contains: State icons with help tooltips when hovering over them Pipeline name and version or docker image and version Run time duration Similar as a parent-run state, states for nested runs are automatically updated without page refreshing. To open any child-run logs page - click its name in the list. If there are several nested runs, only the first couple are displayed at the parent-run logs page. To view all nested runs - click the corresponding hyperlink: The full list of nested runs for the selected parent-run will be opened, e.g.: Cluster run usage User can view the cluster usage at the parent-run logs page - near the Nested runs label, number of nested runs active at the moment is displayed: If the cluster run was completed - here, the summary number of nested runs launched during the cluster run is displayed, e.g.: Also, user can view how the cluster usage has been changing during the run of this cluster. This is especially useful information for auto-scaled clusters, as the number of worker nodes in such clusters can vary greatly over time. To view the cluster usage - click the corresponding hyperlink near the number of active nested runs: The chart pop-up will be opened, e.g.: The chart shows a cluster usage - number of all active instances (including the master node) of the current cluster over time. To view details - hover over the chart point, e.g.: In this case, summary info about the number of active cluster instances in a specific moment will be displayed. To view which runs exactly were active in the cluster (including the master node) in a specific moment - click the chart point, e.g.: You can click any run ID in such a tooltip - the corresponding run's logs page will be opened. Please note, the cluster usage chart is available for completed cluster runs as well. Maintenance Here user can create/view/edit/remove schedule rules for the specific active launched run. That set rules allow to pause/resume the run automatically in the scheduled day and time. The Maintenance control is available only for active \"On-demand\" non-cluster runs. E.g.: To edit an existing schedule for the active launched run click the Configure button. The popup with created rules will appear: You can, for example, remove an existing rule and add new ones: Changes will be displayed at the Run logs page and will be applied to the active run: In general, this behavior is configured identically to the Maintenance control at the Launch page - for more details see 6.2. Launch a pipeline (item 5). Instance The \" Instance \" section lists calculation node and execution environment details that were assigned to the run when it was launched. Note : node IP is presented as a hyperlink. Clicking it will navigate to the node details, where technical information and resources utilization is available - for more details see here . Note : Docker image name link leads to a specific Tool's detail page (see an example ). Note : if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - corresponding auxiliary Cloud Provider icon is additionally displayed, e.g.: Note : if specific run CPU/Memory/Disk consumption is lower or higher specified in the configurations values, the IDLE or PRESSURE labels will be displayed respectively: Parameters The parameters that were assigned to the run when it was launched are contained in this section. Note : parameters with types input/output/common/path are presented as hyperlinks, and will navigate to appropriate location in a Data Storage hierarchy. Note : if a user specifies system environment variables in parameter (e.g. RUN_ID ), GUI will substitute these variables with their values automatically in the \" Run information \" page. Tasks Here you can find a list of tasks of pipeline that are being executed or already finished. Clicking a task and its console output will be loaded in the right panel. Console output Here you can view console output from a whole pipeline or a selected task. It also shows a run failure cause if a run failed. Note : the Follow log control enables auto scrolling of the console output. It is useful for logs monitoring. Follow log is enabled by default, tick the box to turn it off. Also, during a pipeline run an extended node-level logging is maintained: kubelet logs (from all compute nodes) are written to the files Log files are streamed to the storage, identified by the storage.system.storage.name preference Users with the ROLE_ADMIN role can find the corresponding node logs (e.g. by the hostname or ip that are attached to the run information) in that storage by the path logs/nodes/{hostname} : Open the Run logs page of the run you want to see kubelet logs Select the InitializeNode task, find a node hostname in the console output: Copy the found hostname's value. Check the storage path specified at the storage.system.storage.name preference: Open in the Library that storage. Navigate in the opened storage to the path logs/nodes/ : Click the \"breadcrumbs\" control at the upper side of the page, enter / into the end of the path and after it paste the hostname value, copied at step 2: Press the Enter key. The folder with kubelet logs for the specified node will be opened: You can open it and see the list of logs files, divided by the messages type: You can view any of these files using Cloud Pipeline facilities or download them to your local machine: Controls Note : Completed and active runs have different controls. Example : controls of running Spot pipeline: Here's the list of all existing buttons Control Description BROWSE Allows to open the run instance filesystem in a Storage browser Web GUI - so the user can view, download, upload, delete files and directories for the current active run. COMMIT Allows modifying an existing tool that has been changed via ssh. See 10.4. Edit a Tool . EXPORT LOGS Allows to export logs. GRAPH VIEW For Luigi and WDL pipelines GRAPH VIEW is available along with a usual plain view of tasks. See 6.1.1. Building WDL pipeline with graphical PipelineBuilder . LAUNCH COMMAND Allows to generate the CLI pipe run command/API POST request for a job launch. PAUSE Allows to pause a run ( only for On-demand non-cluster runs ). RERUN Allows to rerun completed runs. RESUME Allows to resume a paused run ( only for On-demand non-cluster runs ). SHOW TIMINGS / HIDE TIMINGS Allows to show/hide duration of each task. SSH Allows to shh to the instance running \"sleep infinity\" mode. See 6.1. Create and configure pipeline . STOP Allows to stop a run. TERMINATE Allows to terminate compute node of a paused run without resuming ( only for On-demand non-cluster runs ). Automatically rerun if a spot instance is terminated In certain cases - Cloud Provider may terminate a node, that is used to run a job or an interactive tool. It may be in cases: Spot prices changed Cloud Provider experienced a hardware issue These cases aren't a Cloud Platform bug. In these cases: If a job fails due to server-related issue, special message is displayed, describing a reason for the hardware failure: If a batch job fails due to server-related issue and Cloud Provider reports one of the following instance status codes: Server.SpotInstanceShutdown - a spot instance was stopped due to price changes, Server.SpotInstanceTermination - a spot instance was terminated due to price changes, Server.InternalError - Cloud Provider hardware issue, batch job will be restarted from scratch automatically. Note : this behavior will occur, only if administrator applied and configured it (for more information see 12.10. Manage system-level settings ).","title":"11.0. Overview"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#11-manage-runs","text":"Overview ACTIVE RUNS Active run states Active run controls Active cluster runs Displaying additional node metrics COMPLETED RUNS Completed run states Completed run controls Completed cluster runs Run information page General information Nested runs Cluster run usage Maintenance Instance Parameters Tasks Console output Controls Automatically rerun if a spot instance is terminated","title":"11. Manage Runs"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#overview","text":"\" Runs \" provides a list of active and completed pipeline runs. You can get parameters and logs of specific run and stop run here. \" Runs \" space has two tabs: Active runs view Completed runs view. Runs are organized in a table which is the same for both tabs: \"State\" icon - state of the run. Run - include: run name (upper row) - pipeline name and run id Cloud Region (bottom row) Note : if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - corresponding text information also has a Provider name, e.g.: Parent run - parent run ID, if a run was launched by another run. Pipeline - include: pipeline name (upper row) - a name of a pipeline version name (bottom row) - a name of a pipeline version Docker image - a name of docker image. Started - time when a run was started. Completed - time when a run was finished. Elapsed - include: elapsed time (upper row) - a duration of a run estimated price (bottom row) - estimated price of run, which is calculated based on the run duration and selected instance type. This field is updated interactively (i.e. each 5 - 10 seconds). Owner - a user, which launched a run. Note : also you can view information about runs via CLI. See here .","title":"Overview"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#active-runs","text":"This tab displays a list of all pipelines that are currently running.","title":"ACTIVE RUNS"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#active-run-states","text":"- Queued state (\"sandglass\" icon) - a run is waiting in the queue for the available compute node. - Initializing state (\"rotating\" icon) - a run is being initialized, at this stage a new compute node will be created or an existing node will be reused. - Pulling state (\"download\" icon) - now pipeline Docker image is downloaded to the node. - Running state (stable \"play\" icon) - a pipeline is running. The node is appearing and pipeline input data is being downloaded to the node before the \" InitializeEnvironment \" service task appears. Pausing state (blinking \"pause\" icon) - a run is being paused. At this moment compute node will be stopped (but persisted) and the docker image state will be kept as well. - Paused state (stable \"pause\" icon) - a run is paused. At this moment compute node is already stopped but keeps it's state. Such run may be resumed. Resuming state (blinking \"play\" icon) - a paused run is being resumed. At this moment compute node is starting back from the stopped state. Also, help tooltips are provided when hovering a run state icon, e.g.: Tooltips contain a state name in bold (e.g. Queued ) and a short description of the state and info on the next stage.","title":"Active run states"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#active-run-controls","text":"Control Description PAUSE/RESUME Pauses/resumes a run. Available for On-demand non-cluster instances only. Learn more about feature here . TERMINATE Terminates compute node of a paused run without its resuming. Available for On-demand non-cluster instances only. Learn more about feature here . STOP This control stops a run execution. LOG To open a Run information page, press LOG button.","title":"Active run controls"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#active-cluster-runs","text":"Cluster is a collection of instances which are connected so that they can be used together on a task. If launched run uses a cluster or an auto-scaled cluster (see sections here ), it has a certain designation: By default, only master-run is displaying at the table. To view nested runs (child-runs) click the Expand control in front of the muster-run ID: So, you can view an information about each child-run and its state, also you can stop specific nested run without stopping a parent run. You can open Run logs page (see below ) for any of the cluster runs by click it or LOG button next to run ID. Note : you can't pause cluster runs even with On-demand price type. Note : stopping a parent run will stop execution of all nested runs too. For runs with the auto-scaled cluster not all of the child-runs appear in the list immediately after parent run was launched, \"scale-up\" runs will appear only of necessity.","title":"Active cluster runs"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#displaying-additional-node-metrics","text":"According to the run states and system-level settings, additional metrics (labels) could be displayed for active runs: - Idle label - displays only for runs in Running state, for which node's CPU consumption level is below than a certain threshold for a certain period of time or longer. - Pressure label - displays only for runs in Running state, for which node's Memory/Disk consumption level is higher than a certain threshold. Values of these thresholds and time period are specified by admins via system-level settings (see here for more details and an example of using - here ).","title":"Displaying additional node metrics"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#completed-runs","text":"This tab displays a list of all pipelines runs that are already finished.","title":"COMPLETED RUNS"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#completed-run-states","text":"- Success state (\"OK\" icon) - successful pipeline execution. - Failed state (\"caution\" icon) - unsuccessful pipeline execution. - Stopped state (\"clock\" icon) - a pipeline manually stopped. Help tooltips are also provided when hovering a completed run state icon, e.g.:","title":"Completed run states"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#completed-run-controls","text":"Control Description LINKS This control show input/output links of the pipeline RERUN This control allow rerunning of a completed run. The Launch a pipeline page will be open. LOG To open a Run information page, press LOG button.","title":"Completed run controls"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#completed-cluster-runs","text":"If completed run used a cluster or an auto-scaled cluster (see sections here ), it has a certain designation. Displaying of such runs on the COMPLETED RUNS tab is similar to the active cluster runs. You can view an information about each child-run and its state, also you can rerun specific nested run without a parent run. You can open Run logs page (see below ) for any of the cluster runs by click it or LOG button next to run ID:","title":"Completed cluster runs"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#run-information-page","text":"Click a row within a run list, \"Run information\" page will appear. It consists of several sections:","title":"Run information page"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#general-information","text":"This section displays general information about a run: Field Description State icon state of the run. Help tooltips are provided when hovering a run state icon, e.g.: Run ID unique ID of the run. Endpoint ( available only for tools runs ) endpoint hyperlink for the service launched in an interactive tool. For more details see 15. Interactive services . Share with ( available only for tools runs ) list of users/groups with whom an interactive tool application is shared. For more details see 11.3 Sharing with other users or groups of users . Owner a name of the user who started pipeline. Scheduled time when a pipeline was launched. Waiting for/Running for time a pipeline has been running. Started time when the node is initialized and a pipeline has started execution. Finished time when a pipeline finished execution. Estimated price price of a run according to a run duration and selected instance type. Nested runs the child-runs list in cases when a run has a number of children (e.g. a cluster run or any other case with the parent-id specified) Maintenance the list of rules that define automatical pausing/restarting schedule for on-demand non-clusters runs","title":"General information"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#nested-runs","text":"Nested runs list is displaying only for master runs. It is the list with short informations about cluster child-runs: Each child-run record contains: State icons with help tooltips when hovering over them Pipeline name and version or docker image and version Run time duration Similar as a parent-run state, states for nested runs are automatically updated without page refreshing. To open any child-run logs page - click its name in the list. If there are several nested runs, only the first couple are displayed at the parent-run logs page. To view all nested runs - click the corresponding hyperlink: The full list of nested runs for the selected parent-run will be opened, e.g.:","title":"Nested runs"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#cluster-run-usage","text":"User can view the cluster usage at the parent-run logs page - near the Nested runs label, number of nested runs active at the moment is displayed: If the cluster run was completed - here, the summary number of nested runs launched during the cluster run is displayed, e.g.: Also, user can view how the cluster usage has been changing during the run of this cluster. This is especially useful information for auto-scaled clusters, as the number of worker nodes in such clusters can vary greatly over time. To view the cluster usage - click the corresponding hyperlink near the number of active nested runs: The chart pop-up will be opened, e.g.: The chart shows a cluster usage - number of all active instances (including the master node) of the current cluster over time. To view details - hover over the chart point, e.g.: In this case, summary info about the number of active cluster instances in a specific moment will be displayed. To view which runs exactly were active in the cluster (including the master node) in a specific moment - click the chart point, e.g.: You can click any run ID in such a tooltip - the corresponding run's logs page will be opened. Please note, the cluster usage chart is available for completed cluster runs as well.","title":"Cluster run usage"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#maintenance","text":"Here user can create/view/edit/remove schedule rules for the specific active launched run. That set rules allow to pause/resume the run automatically in the scheduled day and time. The Maintenance control is available only for active \"On-demand\" non-cluster runs. E.g.: To edit an existing schedule for the active launched run click the Configure button. The popup with created rules will appear: You can, for example, remove an existing rule and add new ones: Changes will be displayed at the Run logs page and will be applied to the active run: In general, this behavior is configured identically to the Maintenance control at the Launch page - for more details see 6.2. Launch a pipeline (item 5).","title":"Maintenance"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#instance","text":"The \" Instance \" section lists calculation node and execution environment details that were assigned to the run when it was launched. Note : node IP is presented as a hyperlink. Clicking it will navigate to the node details, where technical information and resources utilization is available - for more details see here . Note : Docker image name link leads to a specific Tool's detail page (see an example ). Note : if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - corresponding auxiliary Cloud Provider icon is additionally displayed, e.g.: Note : if specific run CPU/Memory/Disk consumption is lower or higher specified in the configurations values, the IDLE or PRESSURE labels will be displayed respectively:","title":"Instance"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#parameters","text":"The parameters that were assigned to the run when it was launched are contained in this section. Note : parameters with types input/output/common/path are presented as hyperlinks, and will navigate to appropriate location in a Data Storage hierarchy. Note : if a user specifies system environment variables in parameter (e.g. RUN_ID ), GUI will substitute these variables with their values automatically in the \" Run information \" page.","title":"Parameters"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#tasks","text":"Here you can find a list of tasks of pipeline that are being executed or already finished. Clicking a task and its console output will be loaded in the right panel.","title":"Tasks"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#console-output","text":"Here you can view console output from a whole pipeline or a selected task. It also shows a run failure cause if a run failed. Note : the Follow log control enables auto scrolling of the console output. It is useful for logs monitoring. Follow log is enabled by default, tick the box to turn it off. Also, during a pipeline run an extended node-level logging is maintained: kubelet logs (from all compute nodes) are written to the files Log files are streamed to the storage, identified by the storage.system.storage.name preference Users with the ROLE_ADMIN role can find the corresponding node logs (e.g. by the hostname or ip that are attached to the run information) in that storage by the path logs/nodes/{hostname} : Open the Run logs page of the run you want to see kubelet logs Select the InitializeNode task, find a node hostname in the console output: Copy the found hostname's value. Check the storage path specified at the storage.system.storage.name preference: Open in the Library that storage. Navigate in the opened storage to the path logs/nodes/ : Click the \"breadcrumbs\" control at the upper side of the page, enter / into the end of the path and after it paste the hostname value, copied at step 2: Press the Enter key. The folder with kubelet logs for the specified node will be opened: You can open it and see the list of logs files, divided by the messages type: You can view any of these files using Cloud Pipeline facilities or download them to your local machine:","title":"Console output"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#controls","text":"Note : Completed and active runs have different controls. Example : controls of running Spot pipeline: Here's the list of all existing buttons Control Description BROWSE Allows to open the run instance filesystem in a Storage browser Web GUI - so the user can view, download, upload, delete files and directories for the current active run. COMMIT Allows modifying an existing tool that has been changed via ssh. See 10.4. Edit a Tool . EXPORT LOGS Allows to export logs. GRAPH VIEW For Luigi and WDL pipelines GRAPH VIEW is available along with a usual plain view of tasks. See 6.1.1. Building WDL pipeline with graphical PipelineBuilder . LAUNCH COMMAND Allows to generate the CLI pipe run command/API POST request for a job launch. PAUSE Allows to pause a run ( only for On-demand non-cluster runs ). RERUN Allows to rerun completed runs. RESUME Allows to resume a paused run ( only for On-demand non-cluster runs ). SHOW TIMINGS / HIDE TIMINGS Allows to show/hide duration of each task. SSH Allows to shh to the instance running \"sleep infinity\" mode. See 6.1. Create and configure pipeline . STOP Allows to stop a run. TERMINATE Allows to terminate compute node of a paused run without resuming ( only for On-demand non-cluster runs ).","title":"Controls"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#automatically-rerun-if-a-spot-instance-is-terminated","text":"In certain cases - Cloud Provider may terminate a node, that is used to run a job or an interactive tool. It may be in cases: Spot prices changed Cloud Provider experienced a hardware issue These cases aren't a Cloud Platform bug. In these cases: If a job fails due to server-related issue, special message is displayed, describing a reason for the hardware failure: If a batch job fails due to server-related issue and Cloud Provider reports one of the following instance status codes: Server.SpotInstanceShutdown - a spot instance was stopped due to price changes, Server.SpotInstanceTermination - a spot instance was terminated due to price changes, Server.InternalError - Cloud Provider hardware issue, batch job will be restarted from scratch automatically. Note : this behavior will occur, only if administrator applied and configured it (for more information see 12.10. Manage system-level settings ).","title":"Automatically rerun if a spot instance is terminated"},{"location":"manual/12_Manage_Settings/12.1._Add_a_new_system_event/","text":"12.1. Add a new system event An administrator can add System event notification only. Navigate to System events tab. Click +ADD . Enter a Title of notification. Enter a Body of the notification (optional). Markdown is supported for the body text. You can preview result at the Preview tab: Rank notification Severity (\" info \", \" warning \" or \" critical \"). Mark as blocking ( optional ). Blocking event emerges in the middle of the window and requires confirmation from the user to disappear. Mark as active ( optional ). Active notifications will be shown for all users of the Cloud Pipeline until admin sets them inactive. Click Create .","title":"12.1. Add a new system event"},{"location":"manual/12_Manage_Settings/12.1._Add_a_new_system_event/#121-add-a-new-system-event","text":"An administrator can add System event notification only. Navigate to System events tab. Click +ADD . Enter a Title of notification. Enter a Body of the notification (optional). Markdown is supported for the body text. You can preview result at the Preview tab: Rank notification Severity (\" info \", \" warning \" or \" critical \"). Mark as blocking ( optional ). Blocking event emerges in the middle of the window and requires confirmation from the user to disappear. Mark as active ( optional ). Active notifications will be shown for all users of the Cloud Pipeline until admin sets them inactive. Click Create .","title":"12.1. Add a new system event"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/","text":"12.10. Manage system-level settings User shall have ROLE_ADMIN to read and update system-level settings. Read system-level settings Base URLs Billing Reports Cluster Commit DTS Data sharing Data Storage Docker security Faceted Filter FireCloud GCP Git Grid engine autoscaling Launch Lustre FS Miscellaneous Search System User Interface Make system-level settings visible to all users Update system-level settings Read system-level settings Hover to the Settings tab. Select the Preferences section. All system-level parameters are categorized into the several groups. Base URLs These settings define pipeline URLs: Setting name Description base.invalidate.edge.auth.path base.api.host REST API endpoint base.pipe.distributions.url URL that is used to download pipeline scripts base.cloud.data.distribution.url URLs that are used to download Cloud Data app distribution base.dav.auth.url base.api.host.external REST API external endpoint Billing Reports These settings define parameters of Billing reports pages displaying: Setting name Description billing.reports.enabled Defines whether \"general\" users can access personal billing information - runs/storages where the user is an owner billing.reports.enabled.admins Defines whether admins and users with the ROLE_BILLING_MANAGER role can have full access to the Billing reports billing.reports.user.name.attribute Defines which user's attribute is used to display the users in the Billing reports (charts, tables, export reports) Cluster Settings in this tab define different cluster options: Setting name Description instance.compute.family.names instance.dns.hosted.zone.base instance.dns.hosted.zone.id instance.limit.state.reasons instance.offer.update.rate How often instance cost is calculated (in milliseconds) instance.restart.state.reasons Instance status codes, upon receipt of which an instance tries automatically to restart cluster.instance.device.prefix cluster.instance.device.suffixes cluster.instance.type Default instance type cluster.instance.hdd Default hard drive size for instance (in gigabytes) cluster.instance.hdd_extra_multi Disk extra multiplier. Allows to add extra space, during a launch, to the disk size selected through the GUI. The size of that extra space is calculated as (Unarchived docker image size) * (Disk extra multiplier) cluster.instance.hdd.scale.enabled cluster.instance.hdd.scale.delta.ratio cluster.instance.hdd.scale.monitoring.delay cluster.instance.hdd.scale.max.devices cluster.instance.hdd.scale.max.size cluster.instance.hdd.scale.disk.max.size cluster.instance.hdd.scale.disk.min.size cluster.instance.hdd.scale.threshold.ratio cluster.keep.alive.minutes If node doesn't have a running pipeline on it for that amount of minutes, it will be shut down cluster.keep.alive.policy cluster.random.scheduling If this property is true, pipeline scheduler will rely on Kubernetes order of pods, otherwise pipelines will be ordered according to their parent (batch) ID cluster.reassign.disk.delta This delta sets the max possible difference that could be between the disk size of the node from the hot pool and the real disk size requested by the user for a job cluster.max.size Maximal number of nodes to be launched simultaneously cluster.min.size Minimal number of nodes to be launched at a time cluster.allowed.instance.types Allowed instance types. Can restrict available instance types for launching tools, pipelines, configurations cluster.allowed.instance.types.docker Allowed instance types for docker images (tools). Can restrict available instance types for launching tools. Has a higher priority for a tool than cluster.allowed.instance.types cluster.allowed.price.types Allowed price types. Can restrict available price types for launching tools, pipelines, configurations cluster.allowed.price.types.master Allowed price types for the cluster master node. Can restrict available price types for the master node of the cluster while launching tools, pipelines, detach configurations cluster.enable.autoscaling Enables/disables Kubernetes autoscaler service cluster.networks.config Config that contains information to start new nodes in Cloud Provider cluster.batch.retry.count Count of automatically retries to relaunch a job, if one of the instance status codes from instance.restart.state.reasons returns, when a batch job fails cluster.autoscale.rate How often autoscaler checks what tasks are executed on each node (in milliseconds) cluster.monitoring.elastic.minimal.interval cluster.monitoring.elastic.intervals.number cloud.provider.default Sets a default CLoud Provider cluster.docker.extra_multi Docker image extra multiplier. Allows to get an approximate size of the unarchived docker image. That size is calculated as (Archived docker image size) * (Docker image extra multiplier) cluster.kill.not.matching.nodes If this property is true, any free node that doesn't match configuration of a pending pod will be scaled down immediately, otherwise it will be left until it will be reused or expired. If most of the time we use nodes with the same configuration set this to true cluster.spot If this is true, spot instances will be launched by default cluster.spot.alloc.strategy Parameter that sets the strategy of calculating the price limit for instance: on-demand - maximal instance price equals the price of the on-demand instance of the same type; manual - uses value from the cluster.spot.bid.price parameter cluster.spot.bid.price The maximum price per hour that you are willing to pay for a Spot Instance. The default is the On-Demand price cluster.spot.max.attempts cluster.nodeup.max.threads Maximal number of nodes that can be started simultaneously cluster.nodeup.retry.count Maximal number of tries to start the node cluster.high.non.batch.priority If this property is true, pipelines without parent (batch ID) will have the highest priority, otherwise - the lowest Commit This tab contains various commit settings: Setting name Description commit.pre.command.path Specifies a path to a script within a docker image, that will be executed in a currently running container, before docker commit occurs commit.username Git username commit.deploy.key Used to SSH for COMMIT. Key is stored in a DB commit.timeout Commit will fail if exceeded (in seconds) commit.post.command.path Specifies a path to a script within a docker image, that will be executed in a committed image, after docker commit occurs pause.timeout DTS These settings define DTS parameters: Setting name Description dts.launch.cmd dts.launch.script.url dts.dist.url Data Sharing These settings define data sharing parameters: Setting name Description data.sharing.base.api Specifies a format of the generating URL to the data storage with enabled sharing data.sharing.disclaimer Allows to set a warning text for the \"Share storage link\" pop-up Data Storage These settings define storage parameters: Setting name Description storage.mounts.per.gb.ratio This preference allows to specify the \"safe\" number of storages per Gb of RAM . When launching a job - the user's available object storages count is being calculated and checked that it does not exceed selected instance type RAM value multiplied by this preference value. If it's exceeded - the user is being warned storage.mounts.nfs.sensitive.policy storage.fsbrowser.enabled Allows to enable FSBrowser storage.fsbrowser.black.list List of directories/files which shall not be visible via FSBrowser storage.fsbrowser.wd Allows to set a work directory for FSBrowser (this directory will be opened by default and set as root ) storage.fsbrowser.transfer Allows to specify intermediate object storage for data transfer operations in FSBrowser (this is actual for \"heavy\" transfer operations to not reduce performance) storage.fsbrowser.port Allows to set a port for FSBrowser storage.fsbrowser.tmp A path to the directory where the temporary archive shall be created (when the folder is downloading). Archive is being removed when download is finished storage.incomplete.upload.clean.days storage.user.home.auto Controls whether the home storages will be created automatically for new users storage.webdav.access.duration.seconds Period duration for which user can request the file system access to the storage storage.user.home.template Describes a template that shall be used for the automatic home storage creation for new users storage.temp.credentials.duration Temporary credentials lifetime for Cloud Provider's operations with data storages (in seconds) storage.transfer.pipeline.id Pipeline ID that is used to automated data transfers from the external sites storage.system.run.shared.storage.name storage.system.run.shared.folder.pattern storage.system.storage.name Configures a system data storage for storing attachments from e.g. issues storage.mount.black.list List of directories where Data Storages couldn't be mounted storage.transfer.pipeline.version Pipeline version that is used to automated data transfers from the external sites storage.max.download.size Chunk size to download (bytes) storage.object.prefix A mandatory prefix for the new creating data storages storage.operations.bulk.size storage.listing.time.limit Sets the timeout (in milliseconds) for the processing of the size getting for all input/common files before the pipeline launch. Default: 3000 milliseconds (3 sec). If computation of the files size doesn't end in this timeout, accumulated size will return as is profile.temp.credentials.duration storage.quotas.actions.grace.period Allows to specify grace periods for both states: MOUNT DISABLED and READ ONLY FS quota-related processes. If that preference is specified - corresponding actions for FS quotas will be delayed. Values are specified in minutes. storage.quotas.skipped.paths Allows to specify which storages/paths shall be excluded from the FS quota-related processing. Docker security This tab contains settings related to Docker security checks: Setting name Description security.tools.scan.all.registries If this is true, all registries will be scanned for Tools vulnerability security.tools.scan.clair.root.url Clair root URL security.tools.docker.comp.scan.root.url security.tools.jwt.token.expiration security.tools.scan.clair.connect.timeout Sets timeout for connection with Clair (in seconds) security.tools.os security.tools.policy.max.high.vulnerabilities Denies running a Tool if the number of high vulnerabilities exceeds the threshold. To disable the policy, set to -1 security.tools.grace.hours Allows users to run a new docker image (if it is not scanned yet) or an image with a lot of vulnerabilities during a specified period. During this period user will be able to run a tool, but an appropriate message will be displayed. Period lasts from date/time since the docker version became vulnerable or since the docker image's push time (if this version was not scanned yet) security.tools.scan.clair.read.timeout Sets timeout for Clair response (in seconds) security.tools.policy.deny.not.scanned Allow/deny execution of unscanned Tools security.tools.scan.enabled Enables/disables security scan security.tools.policy.max.medium.vulnerabilities Denies running a Tool if the number of medium vulnerabilities exceeds the threshold. To disable the policy, set to -1 security.tools.policy.max.critical.vulnerabilities Denies running a Tool if the number of critical vulnerabilities exceeds the threshold. To disable the policy, set to -1 security.tools.scan.schedule.cron Security scan schedule Faceted Filter These settings define parameters of the faceted filters at the \"Advanced search\" form: Setting name Description faceted.filter.dictionaries Allows to specify system dictionaries that should be used as faceted filters. Additionally can be specified: filter (dictionary) order in the \"Faceted filters\" panel, default count of filters entries to display FireCloud These settings define FireCloud parameters: Setting name Description google.client.settings firecloud.base.url firecloud.billing.project google.client.id firecloud.launcher.cmd firecloud.instance.disk firecloud.launcher.tool google.redirect.url firecloud.api.scopes firecloud.instance.type firecloud.enable.user.auth GCP These settings define specific Google Cloud Platform parameters: Setting name Description gcp.sku.mapping gcp.regions.list Git These settings define Git parameters: Setting name Description ui.git.cli.configure.template Template for the message of the Git config that would be displayed for user in the \" Git CLI \" section of the System Settings git.user.name User name to access Git with pipelines git.external.url git.fork.retry.count git.fork.wait.timeout git.repository.hook.url git.token Token to access Git with pipelines git.repository.indexing.enabled Allows to enable the indexing of Git repository with pipelines git.reader.service.host git.user.id User id to access Git with pipelines git.host IP address where Git service is deployed Grid engine autoscaling See Appendix C. Working with autoscaled cluster runs for details. These settings define auto-scaled cluster parameters: Setting name Description ge.autoscaling.scale.down.timeout If jobs queue is empty or all jobs are running and there are some idle nodes longer than that timeout in seconds - auto-scaled cluster will start to drop idle auto-scaled nodes (\"scale-down\") ge.autoscaling.scale.up.to.max ge.autoscaling.scale.up.timeout If some jobs are in waiting state longer than that timeout in seconds - auto-scaled cluster will start to attach new computation nodes to the cluster (\"scale-up\") ge.autoscaling.scale.up.polling.timeout Defines how many seconds GE autoscaler should wait for pod initialization and run initialization Launch Settings in this tab contains default Launch parameters: Setting name Description launch.kube.pod.search.path launch.kube.pod.domains.enabled launch.kube.pod.service launch.kube.pod.subdomain launch.kube.service.suffix launch.jwt.token.expiration Lifetime of a pipeline token (in seconds) launch.max.scheduled.number Controls maximum number of scheduled at once runs summary by all platform users launch.max.runs.user.global Controls maximum number of scheduled at once runs for a single user. I.e. if it set to 5, each Platform user can launch 5 jobs at a maximum launch.env.properties Sets of environment variables that will be passed to each running Tool launch.docker.image Default Docker image launch.cmd.template Default cmd template launch.container.cpu.resource launch.container.memory.resource.policy launch.container.memory.resource.request launch.insufficient.capacity.message Defines the text displayed in the run logs in case when an instance requested by the user is missing due to InsufficientInstanceCapacity error (means insufficient capacity in the selected Cloud Region) launch.run.visibility Allow to view foreign runs based on pipeline permissions (value INHERIT ) or restrict visibility of all non-owner runs (value OWNER ) launch.dind.enable Enables Docker in Docker functionality launch.dind.container.vars Allows to specify the variables, which will be passed to the DIND container (if they are set for the host environment) launch.dind.mounts List of mounts that shall be added to k8s pod for Docker in Docker launch.task.status.update.rate Sets task status update rate, on which application will query kubernetes cluster for running task status, ms. Pod Monitor launch.pods.release.rate launch.serverless.endpoint.wait.count launch.serverless.endpoint.wait.time launch.serverless.stop.timeout launch.serverless.wait.count launch.system.parameters System parameters, that are used when launching pipelines Lustre FS These settings define Lustre FS parameters: Setting name Description lustre.fs.default.throughput lustre.fs.mount.options lustre.fs.deployment.type lustre.fs.default.size.gb lustre.fs.backup.retention.days Miscellaneous Setting name Description misc.metadata.sensitive.keys Allows to specify the list of the metadata that will not be exported during the users export operation misc.max.tool.icon.size.kb Sets maximum size (in Kb) of the uploaded icon for the tool system.events.confirmation.metadata.key Sets the KEY for the user's attribute displaying information about \"blocking\" notifications confirmation Search Settings in this tab contains Elasticsearch parameters: Setting name Description search.aggs.max.count search.elastic.scheme search.elastic.allowed.users.field search.elastic.allowed.groups.field search.elastic.denied.users.field search.elastic.denied.groups.field search.elastic.type.field search.elastic.host search.elastic.index.type.prefix search.elastic.port search.elastic.search.fields search.elastic.index.common.prefix search.elastic.index.metadata.fields Allows to specify attributes keys which values will be shown in corresponding additional columns in table view of the search results of the \"Advanced search\" System The settings in this tab contain parameters and actions that are performed depending on the system monitoring metrics: Setting name Description system.ssh.default.root.user.enabled Enables launch of the SSH terminal from the run under root -user access system.max.idle.timeout.minutes Specifies a duration in minutes. If CPU utilization is below system.idle.cpu.threshold for this duration - notification will be sent to the user and the corresponding run will be marked by the \"IDLE\" label system.idle.action.timeout.minutes Specifies a duration in minutes. If CPU utilization is below system.idle.cpu.threshold for this duration - an action, specified in system.idle.action will be performed system.resource.monitoring.period Specifies period (in seconds) between the users' instances scanning to collect the monitoring metrics system.monitoring.time.range Specifies time delay (in sec) after which the notification \"HIGH_CONSUMED_RESOURCES\" would be sent again, if the problem is still in place system.disk.consume.threshold Specifies disk threshold (in %) above which the notification \"HIGH_CONSUMED_RESOURCES\" would be sent and the corresponding run will be marked by the \"PRESSURE\" label system.idle.cpu.threshold Specifies percentage of the CPU utilization, below which action shall be taken system.resource.monitoring.stats.retention.period Specifies the time period (in days) during which resources utilization data is stored system.memory.consume.threshold Specifies memory threshold (in %) above which the notification \"HIGH_CONSUMED_RESOURCES\" would be sent and the corresponding run will be marked by the \"PRESSURE\" label system.idle.action Sets which action to perform on the instance, that showed low CPU utilization (that is below system.idle.cpu.threshold ): NOTIFY - only send notification PAUSE - pause an instance if possible (e.g. instance is On-Demand, Spot instances are skipped) PAUSE_OR_STOP - pause an instance if it is On-Demand, stop an instance if it is Spot STOP - Stop an instance, disregarding price-type system.long.paused.action Sets which action to perform on the instance, that is in the \"paused\" state for a long time: NOTIFY - only send LONG_PAUSED notification(s) STOP - send LONG_PAUSED_STOPPED notification and terminate the run system.notifications.exclude.instance.types Defines a list of node types. If a job runs on any node from that list - IDLE_RUN , LONG_PAUSED , LONG_RUNNING email notifications will not be submitted for that job system.external.services.endpoints system.log.line.limit System Jobs Here settings for System jobs can be found: Setting name Description system.jobs.pipeline.id The ID of the prepared system pipeline that contains system jobs scripts system.jobs.scripts.location The path to the system scripts directory inside the pipeline code. Default value is src/system-jobs system.jobs.output.pipeline.task The name of the task at the Run logs page of the system pipeline that is launched for the system job. Task contains system job output results. Default value is SystemJob User Interface Here different user interface settings can be found: Setting name Description ui.pipeline.deployment.name UI deployment name ui.launch.command.template ui.hidden.objects ui.library.drag ui.pipe.cli.install.template CLI install templates for different operating systems ui.pipe.file.browser.app ui.pipe.drive.mapping ui.project.indicator These attributes define a Project folder ui.pipe.cli.configure.template CLI configure templates for different operating systems ui.support.template Markdown-formatted text that will be displayed in tooltip of the \"support\" info in the main menu. If nothing is specified (empty), support icon in the main menu will not be displayed ui.controls.settings JSON file that contains control settings Make system-level settings visible to all users Hover to the Settings tab. Select the Preferences section. Choose one of the tabs with system level settings (e.g. Grid engine autoscaling ). Press the \" Eye \" button near any setting. Now it will be visible to all users by using the API. Note : press \" Eye \" button again to hide it from all users. Update system-level settings Choose any system-level setting and change its value (e.g. change cluster.keep.alive.minutes value from 10 to 15). Press the Save button. Note : before saving you can press the Revert button to return setting's value to the previous state.","title":"12.10. Manage system-level settings"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#1210-manage-system-level-settings","text":"User shall have ROLE_ADMIN to read and update system-level settings. Read system-level settings Base URLs Billing Reports Cluster Commit DTS Data sharing Data Storage Docker security Faceted Filter FireCloud GCP Git Grid engine autoscaling Launch Lustre FS Miscellaneous Search System User Interface Make system-level settings visible to all users Update system-level settings","title":"12.10. Manage system-level settings"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#read-system-level-settings","text":"Hover to the Settings tab. Select the Preferences section. All system-level parameters are categorized into the several groups.","title":"Read system-level settings"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#base-urls","text":"These settings define pipeline URLs: Setting name Description base.invalidate.edge.auth.path base.api.host REST API endpoint base.pipe.distributions.url URL that is used to download pipeline scripts base.cloud.data.distribution.url URLs that are used to download Cloud Data app distribution base.dav.auth.url base.api.host.external REST API external endpoint","title":"Base URLs"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#billing-reports","text":"These settings define parameters of Billing reports pages displaying: Setting name Description billing.reports.enabled Defines whether \"general\" users can access personal billing information - runs/storages where the user is an owner billing.reports.enabled.admins Defines whether admins and users with the ROLE_BILLING_MANAGER role can have full access to the Billing reports billing.reports.user.name.attribute Defines which user's attribute is used to display the users in the Billing reports (charts, tables, export reports)","title":"Billing Reports"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#cluster","text":"Settings in this tab define different cluster options: Setting name Description instance.compute.family.names instance.dns.hosted.zone.base instance.dns.hosted.zone.id instance.limit.state.reasons instance.offer.update.rate How often instance cost is calculated (in milliseconds) instance.restart.state.reasons Instance status codes, upon receipt of which an instance tries automatically to restart cluster.instance.device.prefix cluster.instance.device.suffixes cluster.instance.type Default instance type cluster.instance.hdd Default hard drive size for instance (in gigabytes) cluster.instance.hdd_extra_multi Disk extra multiplier. Allows to add extra space, during a launch, to the disk size selected through the GUI. The size of that extra space is calculated as (Unarchived docker image size) * (Disk extra multiplier) cluster.instance.hdd.scale.enabled cluster.instance.hdd.scale.delta.ratio cluster.instance.hdd.scale.monitoring.delay cluster.instance.hdd.scale.max.devices cluster.instance.hdd.scale.max.size cluster.instance.hdd.scale.disk.max.size cluster.instance.hdd.scale.disk.min.size cluster.instance.hdd.scale.threshold.ratio cluster.keep.alive.minutes If node doesn't have a running pipeline on it for that amount of minutes, it will be shut down cluster.keep.alive.policy cluster.random.scheduling If this property is true, pipeline scheduler will rely on Kubernetes order of pods, otherwise pipelines will be ordered according to their parent (batch) ID cluster.reassign.disk.delta This delta sets the max possible difference that could be between the disk size of the node from the hot pool and the real disk size requested by the user for a job cluster.max.size Maximal number of nodes to be launched simultaneously cluster.min.size Minimal number of nodes to be launched at a time cluster.allowed.instance.types Allowed instance types. Can restrict available instance types for launching tools, pipelines, configurations cluster.allowed.instance.types.docker Allowed instance types for docker images (tools). Can restrict available instance types for launching tools. Has a higher priority for a tool than cluster.allowed.instance.types cluster.allowed.price.types Allowed price types. Can restrict available price types for launching tools, pipelines, configurations cluster.allowed.price.types.master Allowed price types for the cluster master node. Can restrict available price types for the master node of the cluster while launching tools, pipelines, detach configurations cluster.enable.autoscaling Enables/disables Kubernetes autoscaler service cluster.networks.config Config that contains information to start new nodes in Cloud Provider cluster.batch.retry.count Count of automatically retries to relaunch a job, if one of the instance status codes from instance.restart.state.reasons returns, when a batch job fails cluster.autoscale.rate How often autoscaler checks what tasks are executed on each node (in milliseconds) cluster.monitoring.elastic.minimal.interval cluster.monitoring.elastic.intervals.number cloud.provider.default Sets a default CLoud Provider cluster.docker.extra_multi Docker image extra multiplier. Allows to get an approximate size of the unarchived docker image. That size is calculated as (Archived docker image size) * (Docker image extra multiplier) cluster.kill.not.matching.nodes If this property is true, any free node that doesn't match configuration of a pending pod will be scaled down immediately, otherwise it will be left until it will be reused or expired. If most of the time we use nodes with the same configuration set this to true cluster.spot If this is true, spot instances will be launched by default cluster.spot.alloc.strategy Parameter that sets the strategy of calculating the price limit for instance: on-demand - maximal instance price equals the price of the on-demand instance of the same type; manual - uses value from the cluster.spot.bid.price parameter cluster.spot.bid.price The maximum price per hour that you are willing to pay for a Spot Instance. The default is the On-Demand price cluster.spot.max.attempts cluster.nodeup.max.threads Maximal number of nodes that can be started simultaneously cluster.nodeup.retry.count Maximal number of tries to start the node cluster.high.non.batch.priority If this property is true, pipelines without parent (batch ID) will have the highest priority, otherwise - the lowest","title":"Cluster"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#commit","text":"This tab contains various commit settings: Setting name Description commit.pre.command.path Specifies a path to a script within a docker image, that will be executed in a currently running container, before docker commit occurs commit.username Git username commit.deploy.key Used to SSH for COMMIT. Key is stored in a DB commit.timeout Commit will fail if exceeded (in seconds) commit.post.command.path Specifies a path to a script within a docker image, that will be executed in a committed image, after docker commit occurs pause.timeout","title":"Commit"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#dts","text":"These settings define DTS parameters: Setting name Description dts.launch.cmd dts.launch.script.url dts.dist.url","title":"DTS"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#data-sharing","text":"These settings define data sharing parameters: Setting name Description data.sharing.base.api Specifies a format of the generating URL to the data storage with enabled sharing data.sharing.disclaimer Allows to set a warning text for the \"Share storage link\" pop-up","title":"Data Sharing"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#data-storage","text":"These settings define storage parameters: Setting name Description storage.mounts.per.gb.ratio This preference allows to specify the \"safe\" number of storages per Gb of RAM . When launching a job - the user's available object storages count is being calculated and checked that it does not exceed selected instance type RAM value multiplied by this preference value. If it's exceeded - the user is being warned storage.mounts.nfs.sensitive.policy storage.fsbrowser.enabled Allows to enable FSBrowser storage.fsbrowser.black.list List of directories/files which shall not be visible via FSBrowser storage.fsbrowser.wd Allows to set a work directory for FSBrowser (this directory will be opened by default and set as root ) storage.fsbrowser.transfer Allows to specify intermediate object storage for data transfer operations in FSBrowser (this is actual for \"heavy\" transfer operations to not reduce performance) storage.fsbrowser.port Allows to set a port for FSBrowser storage.fsbrowser.tmp A path to the directory where the temporary archive shall be created (when the folder is downloading). Archive is being removed when download is finished storage.incomplete.upload.clean.days storage.user.home.auto Controls whether the home storages will be created automatically for new users storage.webdav.access.duration.seconds Period duration for which user can request the file system access to the storage storage.user.home.template Describes a template that shall be used for the automatic home storage creation for new users storage.temp.credentials.duration Temporary credentials lifetime for Cloud Provider's operations with data storages (in seconds) storage.transfer.pipeline.id Pipeline ID that is used to automated data transfers from the external sites storage.system.run.shared.storage.name storage.system.run.shared.folder.pattern storage.system.storage.name Configures a system data storage for storing attachments from e.g. issues storage.mount.black.list List of directories where Data Storages couldn't be mounted storage.transfer.pipeline.version Pipeline version that is used to automated data transfers from the external sites storage.max.download.size Chunk size to download (bytes) storage.object.prefix A mandatory prefix for the new creating data storages storage.operations.bulk.size storage.listing.time.limit Sets the timeout (in milliseconds) for the processing of the size getting for all input/common files before the pipeline launch. Default: 3000 milliseconds (3 sec). If computation of the files size doesn't end in this timeout, accumulated size will return as is profile.temp.credentials.duration storage.quotas.actions.grace.period Allows to specify grace periods for both states: MOUNT DISABLED and READ ONLY FS quota-related processes. If that preference is specified - corresponding actions for FS quotas will be delayed. Values are specified in minutes. storage.quotas.skipped.paths Allows to specify which storages/paths shall be excluded from the FS quota-related processing.","title":"Data Storage"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#docker-security","text":"This tab contains settings related to Docker security checks: Setting name Description security.tools.scan.all.registries If this is true, all registries will be scanned for Tools vulnerability security.tools.scan.clair.root.url Clair root URL security.tools.docker.comp.scan.root.url security.tools.jwt.token.expiration security.tools.scan.clair.connect.timeout Sets timeout for connection with Clair (in seconds) security.tools.os security.tools.policy.max.high.vulnerabilities Denies running a Tool if the number of high vulnerabilities exceeds the threshold. To disable the policy, set to -1 security.tools.grace.hours Allows users to run a new docker image (if it is not scanned yet) or an image with a lot of vulnerabilities during a specified period. During this period user will be able to run a tool, but an appropriate message will be displayed. Period lasts from date/time since the docker version became vulnerable or since the docker image's push time (if this version was not scanned yet) security.tools.scan.clair.read.timeout Sets timeout for Clair response (in seconds) security.tools.policy.deny.not.scanned Allow/deny execution of unscanned Tools security.tools.scan.enabled Enables/disables security scan security.tools.policy.max.medium.vulnerabilities Denies running a Tool if the number of medium vulnerabilities exceeds the threshold. To disable the policy, set to -1 security.tools.policy.max.critical.vulnerabilities Denies running a Tool if the number of critical vulnerabilities exceeds the threshold. To disable the policy, set to -1 security.tools.scan.schedule.cron Security scan schedule","title":"Docker security"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#faceted-filter","text":"These settings define parameters of the faceted filters at the \"Advanced search\" form: Setting name Description faceted.filter.dictionaries Allows to specify system dictionaries that should be used as faceted filters. Additionally can be specified: filter (dictionary) order in the \"Faceted filters\" panel, default count of filters entries to display","title":"Faceted Filter"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#firecloud","text":"These settings define FireCloud parameters: Setting name Description google.client.settings firecloud.base.url firecloud.billing.project google.client.id firecloud.launcher.cmd firecloud.instance.disk firecloud.launcher.tool google.redirect.url firecloud.api.scopes firecloud.instance.type firecloud.enable.user.auth","title":"FireCloud"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#gcp","text":"These settings define specific Google Cloud Platform parameters: Setting name Description gcp.sku.mapping gcp.regions.list","title":"GCP"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#git","text":"These settings define Git parameters: Setting name Description ui.git.cli.configure.template Template for the message of the Git config that would be displayed for user in the \" Git CLI \" section of the System Settings git.user.name User name to access Git with pipelines git.external.url git.fork.retry.count git.fork.wait.timeout git.repository.hook.url git.token Token to access Git with pipelines git.repository.indexing.enabled Allows to enable the indexing of Git repository with pipelines git.reader.service.host git.user.id User id to access Git with pipelines git.host IP address where Git service is deployed","title":"Git"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#grid-engine-autoscaling","text":"See Appendix C. Working with autoscaled cluster runs for details. These settings define auto-scaled cluster parameters: Setting name Description ge.autoscaling.scale.down.timeout If jobs queue is empty or all jobs are running and there are some idle nodes longer than that timeout in seconds - auto-scaled cluster will start to drop idle auto-scaled nodes (\"scale-down\") ge.autoscaling.scale.up.to.max ge.autoscaling.scale.up.timeout If some jobs are in waiting state longer than that timeout in seconds - auto-scaled cluster will start to attach new computation nodes to the cluster (\"scale-up\") ge.autoscaling.scale.up.polling.timeout Defines how many seconds GE autoscaler should wait for pod initialization and run initialization","title":"Grid engine autoscaling"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#launch","text":"Settings in this tab contains default Launch parameters: Setting name Description launch.kube.pod.search.path launch.kube.pod.domains.enabled launch.kube.pod.service launch.kube.pod.subdomain launch.kube.service.suffix launch.jwt.token.expiration Lifetime of a pipeline token (in seconds) launch.max.scheduled.number Controls maximum number of scheduled at once runs summary by all platform users launch.max.runs.user.global Controls maximum number of scheduled at once runs for a single user. I.e. if it set to 5, each Platform user can launch 5 jobs at a maximum launch.env.properties Sets of environment variables that will be passed to each running Tool launch.docker.image Default Docker image launch.cmd.template Default cmd template launch.container.cpu.resource launch.container.memory.resource.policy launch.container.memory.resource.request launch.insufficient.capacity.message Defines the text displayed in the run logs in case when an instance requested by the user is missing due to InsufficientInstanceCapacity error (means insufficient capacity in the selected Cloud Region) launch.run.visibility Allow to view foreign runs based on pipeline permissions (value INHERIT ) or restrict visibility of all non-owner runs (value OWNER ) launch.dind.enable Enables Docker in Docker functionality launch.dind.container.vars Allows to specify the variables, which will be passed to the DIND container (if they are set for the host environment) launch.dind.mounts List of mounts that shall be added to k8s pod for Docker in Docker launch.task.status.update.rate Sets task status update rate, on which application will query kubernetes cluster for running task status, ms. Pod Monitor launch.pods.release.rate launch.serverless.endpoint.wait.count launch.serverless.endpoint.wait.time launch.serverless.stop.timeout launch.serverless.wait.count launch.system.parameters System parameters, that are used when launching pipelines","title":"Launch"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#lustre-fs","text":"These settings define Lustre FS parameters: Setting name Description lustre.fs.default.throughput lustre.fs.mount.options lustre.fs.deployment.type lustre.fs.default.size.gb lustre.fs.backup.retention.days","title":"Lustre FS"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#miscellaneous","text":"Setting name Description misc.metadata.sensitive.keys Allows to specify the list of the metadata that will not be exported during the users export operation misc.max.tool.icon.size.kb Sets maximum size (in Kb) of the uploaded icon for the tool system.events.confirmation.metadata.key Sets the KEY for the user's attribute displaying information about \"blocking\" notifications confirmation","title":"Miscellaneous"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#search","text":"Settings in this tab contains Elasticsearch parameters: Setting name Description search.aggs.max.count search.elastic.scheme search.elastic.allowed.users.field search.elastic.allowed.groups.field search.elastic.denied.users.field search.elastic.denied.groups.field search.elastic.type.field search.elastic.host search.elastic.index.type.prefix search.elastic.port search.elastic.search.fields search.elastic.index.common.prefix search.elastic.index.metadata.fields Allows to specify attributes keys which values will be shown in corresponding additional columns in table view of the search results of the \"Advanced search\"","title":"Search"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#system","text":"The settings in this tab contain parameters and actions that are performed depending on the system monitoring metrics: Setting name Description system.ssh.default.root.user.enabled Enables launch of the SSH terminal from the run under root -user access system.max.idle.timeout.minutes Specifies a duration in minutes. If CPU utilization is below system.idle.cpu.threshold for this duration - notification will be sent to the user and the corresponding run will be marked by the \"IDLE\" label system.idle.action.timeout.minutes Specifies a duration in minutes. If CPU utilization is below system.idle.cpu.threshold for this duration - an action, specified in system.idle.action will be performed system.resource.monitoring.period Specifies period (in seconds) between the users' instances scanning to collect the monitoring metrics system.monitoring.time.range Specifies time delay (in sec) after which the notification \"HIGH_CONSUMED_RESOURCES\" would be sent again, if the problem is still in place system.disk.consume.threshold Specifies disk threshold (in %) above which the notification \"HIGH_CONSUMED_RESOURCES\" would be sent and the corresponding run will be marked by the \"PRESSURE\" label system.idle.cpu.threshold Specifies percentage of the CPU utilization, below which action shall be taken system.resource.monitoring.stats.retention.period Specifies the time period (in days) during which resources utilization data is stored system.memory.consume.threshold Specifies memory threshold (in %) above which the notification \"HIGH_CONSUMED_RESOURCES\" would be sent and the corresponding run will be marked by the \"PRESSURE\" label system.idle.action Sets which action to perform on the instance, that showed low CPU utilization (that is below system.idle.cpu.threshold ): NOTIFY - only send notification PAUSE - pause an instance if possible (e.g. instance is On-Demand, Spot instances are skipped) PAUSE_OR_STOP - pause an instance if it is On-Demand, stop an instance if it is Spot STOP - Stop an instance, disregarding price-type system.long.paused.action Sets which action to perform on the instance, that is in the \"paused\" state for a long time: NOTIFY - only send LONG_PAUSED notification(s) STOP - send LONG_PAUSED_STOPPED notification and terminate the run system.notifications.exclude.instance.types Defines a list of node types. If a job runs on any node from that list - IDLE_RUN , LONG_PAUSED , LONG_RUNNING email notifications will not be submitted for that job system.external.services.endpoints system.log.line.limit","title":"System"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#system-jobs","text":"Here settings for System jobs can be found: Setting name Description system.jobs.pipeline.id The ID of the prepared system pipeline that contains system jobs scripts system.jobs.scripts.location The path to the system scripts directory inside the pipeline code. Default value is src/system-jobs system.jobs.output.pipeline.task The name of the task at the Run logs page of the system pipeline that is launched for the system job. Task contains system job output results. Default value is SystemJob","title":"System Jobs"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#user-interface","text":"Here different user interface settings can be found: Setting name Description ui.pipeline.deployment.name UI deployment name ui.launch.command.template ui.hidden.objects ui.library.drag ui.pipe.cli.install.template CLI install templates for different operating systems ui.pipe.file.browser.app ui.pipe.drive.mapping ui.project.indicator These attributes define a Project folder ui.pipe.cli.configure.template CLI configure templates for different operating systems ui.support.template Markdown-formatted text that will be displayed in tooltip of the \"support\" info in the main menu. If nothing is specified (empty), support icon in the main menu will not be displayed ui.controls.settings JSON file that contains control settings","title":"User Interface"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#make-system-level-settings-visible-to-all-users","text":"Hover to the Settings tab. Select the Preferences section. Choose one of the tabs with system level settings (e.g. Grid engine autoscaling ). Press the \" Eye \" button near any setting. Now it will be visible to all users by using the API. Note : press \" Eye \" button again to hide it from all users.","title":"Make system-level settings visible to all users"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#update-system-level-settings","text":"Choose any system-level setting and change its value (e.g. change cluster.keep.alive.minutes value from 10 to 15). Press the Save button. Note : before saving you can press the Revert button to return setting's value to the previous state.","title":"Update system-level settings"},{"location":"manual/12_Manage_Settings/12.11._Advanced_features/","text":"12.11. Advanced features via System Preferences Setup swap files for the Cloud VMs Home storage for each user Seamless authentication in the Cloud Provider Switching of regions for jobs in case of insufficient capacity User shall have ROLE_ADMIN to configure system-level settings. Setup swap files for the Cloud VMs In certain cases jobs may fail with unexpected errors if the compute node runs Out of memory . Admin users can configure a default swap file to the compute node being created. This allow to avoid runs failures due to memory limits. To configure the size and the location of the swap : Open the Settings pop-up Click the Preference tab Select the Cluster section Click in the field under the cluster.networks.config label Insert the similar json block into a region/cloud specific configuration: Where: swap_ratio - defines a swap file size. It is equal the node RAM multiplied by that ratio. If ratio is 0, a swap file will not be created (default value: 0) swap_location - defines a location of the swap file. If that option is not set - default location will be used (default: AWS will use SSD/gp2 EBS, Azure will use Temporary Storage ) Click the Save button Click the OK button Now, while launch any pipeline, you can see specified swap settings in the run logs: To check that settings were applied, open SSH session and input the swapon command: Home storage for each user Cloud Pipeline allows creating home storages for the newly created users in automatic mode. It could be conveniently when multiple users are being created (otherwise this task can be really tedious - create each user/create storage for the user/grant permissions on the storage). This behavior is controlled by the system preference storage.user.home.auto ( Boolean , default value is false ). It controls whether the home storages shall be created automatically. If it is set to true - new storage will be created for the user automatically simultaneously with the user creation. Also the just-created user is being granted OWNER permissions for the new storage. And the newly created storage is being set as a \"default\" storage in the user's profile. The \"home\" storage automatic creation is being driven by a template. The template is being described as JSON element in the other system preference - storage.user.home.template . In this preference for the template, being described: settings for the storage permissions on the storage To different storage types there are different templates. Object storage template The template for object storages has the following view: { \"datastorage\": { \"name\": \"<storage_alias>\", \"description\": \"<storage_description>\", \"path\": \"<storage_path>\", \"serviceType\": \"OBJECT_STORAGE\", \"regionId\": <region_ID>, \"parentFolderId\": <parent_folder_ID> }, \"metadata\": { \"owner\": { \"value\": \"@@\" } } } Where: \" datastorage \" sub-block defines settings of the creating storage: <storage_alias> - storage name <storage_description> - storage description <storage_path> - full path to the storage <region_ID> - Cloud Region ID in which the storage will be created <parent_folder_ID> - the ID of the folder in the Library in which the storage will be located once the creation \" serviceType \" - defines the type of the storage: \" metadata \" sub-block defines permissions on the created storage (the OWNER permissions): @@ - designation that will be automatic replaced with the username during the storage creation Note : designation @@ should be used also in \" datastorage \" sub-block - to create storages with unique names and paths. Example of the template for the object storage: { \"datastorage\": { \"name\": \"cp-home-@@\", \"description\": \"Home storage of @@\", \"path\": \"cp-home-storage-@@\", \"serviceType\": \"OBJECT_STORAGE\", \"regionId\": 1, \"parentFolderId\": 123 }, \"metadata\": { \"owner\": { \"value\": \"@@\" } } } FS storage template The template for FS mounts has the following view: { \"datastorage\": { \"name\": \"<storage_alias>\", \"description\": \"<storage_description>\", \"path\": \"<storage_path>\", \"serviceType\": \"FILE_SHARE\", \"regionId\": <region_ID>, \"parentFolderId\": <parent_folder_ID>, \"fileShareMountId\": <file_share_mount_ID> }, \"metadata\": { \"owner\": { \"value\": \"@@\" } } } Where: \" datastorage \" sub-block defines settings of the creating FS mount: <storage_alias> - FS mount's alias name <storage_description> - FS mount description <storage_path> - full path of the FS mount <region_ID> - Cloud Region ID in which the FS mount will be created <parent_folder_ID> - the ID of the folder in the Library in which the FS mount will be located once the creation \" serviceType \" - defines the type of the storage <file_share_mount_ID> - the ID of the share mount which will be used for the FS mount creation \" metadata \" sub-block defines permissions on the created FS mount (the OWNER permissions): @@ - designation that will be automatic replaced with username during the FS mount creation Designation @@ should be used also in \" datastorage \" sub-block - to create FS mounts with unique names and paths. Example of the template for the FS mount: { \"datastorage\": { \"name\": \"cp-home-@@\", \"description\": \"Home storage of @@\", \"path\": \"nfs://10.10.10.10:/home/@@\", \"serviceType\": \"FILE_SHARE\", \"regionId\": 1, \"parentFolderId\": 123, \"fileShareMountId\": 10 }, \"metadata\": { \"owner\": { \"value\": \"@@\" } } } Usage example Open the System Preference . Find the preference storage.user.home.auto and set the checkbox to \" enabled \". Find the preference storage.user.home.template and fill in it suchlike as described above , e.g.: Click the Save button. Open the \" USER MANAGEMENT \" tab. Click the \" + Create user \" button: In the appeared pop-up, specify the user name and click the Create button, e.g.: After the creation, find and open the card of the just-created user: See that the default storage for the user is already set and has name and path according to the template (from step 3) Close the user card. Open the library and navigate to the folder which ID was also specified in the template (from step 3): See that the storage is already created and has name, path, description and the OWNER according to the template (from step 3) Open the storage: Additional notes If there were issues with the storage creation, the user creation doesn't fail too (but the system send a corresponding error message to the client). If the corresponding \"home\" storage already exists - the storage creation step will be skipped during the user creation. Default storage for the user isn't being set in that case. It can be set manually after the user creation. If the default storage is forcibly specified for the user before the creation - separate \"home\" storage will not be created independent to the value of storage.user.home.auto preference. Seamless authentication in Cloud Provider Please note this functionality is currently available only for AWS Cloud Pipeline supports the seamless Cloud Provider authetication mechanism. It allows users to execute any request to the Cloud Provider \u2019s API, from inside the Cloud Pipeline environment, without an authentication request. The following mechanism automates the Cloud Provider authentication for the user\u2019s scripts: Administrator is able to configure the user\u2019s access permissions in the Cloud Pipeline account of the Cloud Provider or provide credentials for the external Cloud Provider account All the requests to the Cloud Provider authentication are handled by the certain Cloud Pipeline service, which authenticates the user with the configured credentials Users are able to use the Cloud Provider API without the authentication request How to configure Administrator can create specific interfaces - Cloud Credentials Profiles . Each profile includes the following fields: Provider - to specify the Cloud Provider Name - to specify the profile name Assumed Role - to specify the role of the Cloud Provider that will be used for the authentication to the Cloud Provider API Policy - to specify the Cloud Provider policy of the objects access Profiles management To manage the Cloud Credentials Profiles , administrator should: Open the System Settings of the application Open the CLOUD REGIONS tab of the Settings Click the Cloud Provider label which Profiles shall be configured: The form of the Cloud Credentials Profiles management will be opened: To create/edit a profile: Click the \" + Create profile \" button in the right-upper corner In the appeared pop-up, fill in corresponding fields: After all fields are filled in, click the Create button to confirm creation: Just-created profile will appear in the list: To edit/remove a profile - click the button next to the profile name. Then perform desired actions Profiles assigning To leverage created profiles, administrator assigns them to User/Role/Group entity. For each entity many profiles can be assigned. Also, from the profiles assigned to the certain User/Role/Group the one can be selected as default - in such case, this profile will be used by default during the authentication operation in Cloud Provider . If the default profile for the certain User/Role/Group isn't selected - during the authentication operation there shall be selected which profile from the available ones should be used. Profiles assigning can be performed via the USER MANAGEMENT (at the section \" Launch options \" of the certain User or Group/Role card). For example, to assign profiles to a user administrator shall: Open the System Settings of the application Open the USER MANAGEMENT tab of the Settings Find the desired user and click it to open the user card, e.g.: Profiles assigning can be performed in the \" Launch options \" section: Click the \" Cloud Credential Profiles \" dropdown list and select one or several profiles to assign, e.g.: In this list, all profiles added as described above are available. To select the profile which should be used as default - click the \" Default Credential Profile \" dropdown list and select the desired profile, e.g.: Note : this action is optional Click the OK button to save changes For assigning profile to the Group/Role, similar actions should be performed in the corresponding cards Profiles usage during the run Now, we have Cloud Credentials Profiles (with their own Providers' roles and policies), we assigned them to the certain User/Role/Group. How to allow their usage during the run? It is being configured at the CLOUD REGIONS tab (for each Cloud Region separately). There is a special field - \" Mount Credentials Rule \" with the following allowed values: NONE - for runs in this region, do not configure credentials SAME CLOUD - for runs in this region, configure credentials allowed for user of the same Cloud Provider only ALL - for runs in this region, configure all credentials allowed for user Example: Example: seamless authentication in AWS In the example below, we will create the Profile with Read/Write access to AWS data storage ( S3 bucket) and will access it from the run via AWS CLI directly. Open the System Settings Open the CLOUD REGIONS tab of the Settings Click the AWS label in the Cloud Provider list Click the \" + Create profile \" button: In the pop-up, specify the profile name, e.g.: In the pop-up, specify the AWS role you wish to use for the current profile (role shall be received from the AWS support), e.g.: The role, used in our example, allows all operations with AWS buckets (read/write) In the pop-up, specify the policy you wish to use with the current profile, e.g.: Please note although the role allows all operations with any bucket - we will restrict access to only one bucket - via the specified policy Save the profile - click the Create button. The profile will appear in the list: Open the \" USER MANAGEMENT \" tab, find the user for whom you wish to use the created profile. Click the user tile Click the \" Cloud Credentials Profiles \" dropdown list and select the profile created at step 8: Click the \" Default Credentials Profile \" dropdown list and select the profile selected at step 10: Save changes: Open the CLOUD REGIONS tab of the Settings Click the AWS Cloud Region from which the run will be launched Click the \" Mount credentials rule \" dropdown list and select the item \" Same cloud \": Save changes Launch the tool (in our example, we will use library/ubuntu ) from the user you assigned the profile at steps 10-11, e.g.: Open the Run logs page of the just launched tool Wait until the SSH hyperlink appears. Click it: In the opened web-terminal, perform the command pip install awscli to install AWS CLI. Then check that installation is successful: Now, you can use AWS CLI directly to the bucket specified at the Profile policy (at step 7) without extra-authentication. For example, list the content: Upload a file to the bucket: Download a file from the bucket: But if you try to get access to the existing object on which the policy (specified at step 7) doesn't allow access - you will be rejected, e.g.: Switching of Cloud Regions for launched jobs in case of insufficient capacity Please note this functionality is currently available only for AWS If there are not enough instances of specified type to launch a run in one region - Cloud Pipeline can automatically try to launch identical instance in other region(s) of the same Cloud Provider. This behaviour is defined by the special Cloud Region setting - \" Run shift policy \": The switching region procedure looks like: User launches a job. If during the run initialization, an instance requested by user is missing due to InsufficientInstanceCapacity error (that means run failed with insufficient capacity in the selected region) - next steps below will be performed: Note : the displayed text for this error can be configured by admin via the System preference launch.insufficient.capacity.message Possibility to switch the current region is checking - option \" Run shift policy \" shall be previously enabled: Possibility to switch to any vacant region from the same Cloud Provider is checking - option \" Run shift policy \" shall be previously enabled for the vacant region, e.g.: Current run is being automatically stopped. InsufficientInstanceCapacity error is displayed at the Run logs page as the failure reason: A new run is being automatically launched - in the vacant Cloud Region. You can view info about that new run in the tile at the Run logs page of the original run. Also, the task RestartPipelineRun appears for the original run - in its logs, the information about shifting run is displayed as well: At the Run logs page of the switched (new) run, there is also a link to the original run: If a new instance is not available with a new region - steps 1-5 will be performed in one more region as long as there are regions of the same Cloud Provider with the enabled option \" Run shift policy \". Restrictions of this feature: available only for on-demand runs available only for runs that have not any Cloud dependent parameters (parameter is Cloud dependent if it, for example, contains some storage path) not supported for worker or cluster runs For a run that does not meet these restrictions - in case of InsufficientInstanceCapacity error, original run will be just terminated during the region's shifting process initialization. New run in any other region will not be launched. The reason of the failure will be shown in the RestartPipelineRun task logs, e.g. for a cluster run shifting attempt:","title":"12.11. Advanced features"},{"location":"manual/12_Manage_Settings/12.11._Advanced_features/#1211-advanced-features-via-system-preferences","text":"Setup swap files for the Cloud VMs Home storage for each user Seamless authentication in the Cloud Provider Switching of regions for jobs in case of insufficient capacity User shall have ROLE_ADMIN to configure system-level settings.","title":"12.11. Advanced features via System Preferences"},{"location":"manual/12_Manage_Settings/12.11._Advanced_features/#setup-swap-files-for-the-cloud-vms","text":"In certain cases jobs may fail with unexpected errors if the compute node runs Out of memory . Admin users can configure a default swap file to the compute node being created. This allow to avoid runs failures due to memory limits. To configure the size and the location of the swap : Open the Settings pop-up Click the Preference tab Select the Cluster section Click in the field under the cluster.networks.config label Insert the similar json block into a region/cloud specific configuration: Where: swap_ratio - defines a swap file size. It is equal the node RAM multiplied by that ratio. If ratio is 0, a swap file will not be created (default value: 0) swap_location - defines a location of the swap file. If that option is not set - default location will be used (default: AWS will use SSD/gp2 EBS, Azure will use Temporary Storage ) Click the Save button Click the OK button Now, while launch any pipeline, you can see specified swap settings in the run logs: To check that settings were applied, open SSH session and input the swapon command:","title":"Setup swap files for the Cloud VMs"},{"location":"manual/12_Manage_Settings/12.11._Advanced_features/#home-storage-for-each-user","text":"Cloud Pipeline allows creating home storages for the newly created users in automatic mode. It could be conveniently when multiple users are being created (otherwise this task can be really tedious - create each user/create storage for the user/grant permissions on the storage). This behavior is controlled by the system preference storage.user.home.auto ( Boolean , default value is false ). It controls whether the home storages shall be created automatically. If it is set to true - new storage will be created for the user automatically simultaneously with the user creation. Also the just-created user is being granted OWNER permissions for the new storage. And the newly created storage is being set as a \"default\" storage in the user's profile. The \"home\" storage automatic creation is being driven by a template. The template is being described as JSON element in the other system preference - storage.user.home.template . In this preference for the template, being described: settings for the storage permissions on the storage To different storage types there are different templates.","title":"Home storage for each user"},{"location":"manual/12_Manage_Settings/12.11._Advanced_features/#object-storage-template","text":"The template for object storages has the following view: { \"datastorage\": { \"name\": \"<storage_alias>\", \"description\": \"<storage_description>\", \"path\": \"<storage_path>\", \"serviceType\": \"OBJECT_STORAGE\", \"regionId\": <region_ID>, \"parentFolderId\": <parent_folder_ID> }, \"metadata\": { \"owner\": { \"value\": \"@@\" } } } Where: \" datastorage \" sub-block defines settings of the creating storage: <storage_alias> - storage name <storage_description> - storage description <storage_path> - full path to the storage <region_ID> - Cloud Region ID in which the storage will be created <parent_folder_ID> - the ID of the folder in the Library in which the storage will be located once the creation \" serviceType \" - defines the type of the storage: \" metadata \" sub-block defines permissions on the created storage (the OWNER permissions): @@ - designation that will be automatic replaced with the username during the storage creation Note : designation @@ should be used also in \" datastorage \" sub-block - to create storages with unique names and paths. Example of the template for the object storage: { \"datastorage\": { \"name\": \"cp-home-@@\", \"description\": \"Home storage of @@\", \"path\": \"cp-home-storage-@@\", \"serviceType\": \"OBJECT_STORAGE\", \"regionId\": 1, \"parentFolderId\": 123 }, \"metadata\": { \"owner\": { \"value\": \"@@\" } } }","title":"Object storage template"},{"location":"manual/12_Manage_Settings/12.11._Advanced_features/#fs-storage-template","text":"The template for FS mounts has the following view: { \"datastorage\": { \"name\": \"<storage_alias>\", \"description\": \"<storage_description>\", \"path\": \"<storage_path>\", \"serviceType\": \"FILE_SHARE\", \"regionId\": <region_ID>, \"parentFolderId\": <parent_folder_ID>, \"fileShareMountId\": <file_share_mount_ID> }, \"metadata\": { \"owner\": { \"value\": \"@@\" } } } Where: \" datastorage \" sub-block defines settings of the creating FS mount: <storage_alias> - FS mount's alias name <storage_description> - FS mount description <storage_path> - full path of the FS mount <region_ID> - Cloud Region ID in which the FS mount will be created <parent_folder_ID> - the ID of the folder in the Library in which the FS mount will be located once the creation \" serviceType \" - defines the type of the storage <file_share_mount_ID> - the ID of the share mount which will be used for the FS mount creation \" metadata \" sub-block defines permissions on the created FS mount (the OWNER permissions): @@ - designation that will be automatic replaced with username during the FS mount creation Designation @@ should be used also in \" datastorage \" sub-block - to create FS mounts with unique names and paths. Example of the template for the FS mount: { \"datastorage\": { \"name\": \"cp-home-@@\", \"description\": \"Home storage of @@\", \"path\": \"nfs://10.10.10.10:/home/@@\", \"serviceType\": \"FILE_SHARE\", \"regionId\": 1, \"parentFolderId\": 123, \"fileShareMountId\": 10 }, \"metadata\": { \"owner\": { \"value\": \"@@\" } } }","title":"FS storage template"},{"location":"manual/12_Manage_Settings/12.11._Advanced_features/#usage-example","text":"Open the System Preference . Find the preference storage.user.home.auto and set the checkbox to \" enabled \". Find the preference storage.user.home.template and fill in it suchlike as described above , e.g.: Click the Save button. Open the \" USER MANAGEMENT \" tab. Click the \" + Create user \" button: In the appeared pop-up, specify the user name and click the Create button, e.g.: After the creation, find and open the card of the just-created user: See that the default storage for the user is already set and has name and path according to the template (from step 3) Close the user card. Open the library and navigate to the folder which ID was also specified in the template (from step 3): See that the storage is already created and has name, path, description and the OWNER according to the template (from step 3) Open the storage:","title":"Usage example"},{"location":"manual/12_Manage_Settings/12.11._Advanced_features/#additional-notes","text":"If there were issues with the storage creation, the user creation doesn't fail too (but the system send a corresponding error message to the client). If the corresponding \"home\" storage already exists - the storage creation step will be skipped during the user creation. Default storage for the user isn't being set in that case. It can be set manually after the user creation. If the default storage is forcibly specified for the user before the creation - separate \"home\" storage will not be created independent to the value of storage.user.home.auto preference.","title":"Additional notes"},{"location":"manual/12_Manage_Settings/12.11._Advanced_features/#seamless-authentication-in-cloud-provider","text":"Please note this functionality is currently available only for AWS Cloud Pipeline supports the seamless Cloud Provider authetication mechanism. It allows users to execute any request to the Cloud Provider \u2019s API, from inside the Cloud Pipeline environment, without an authentication request. The following mechanism automates the Cloud Provider authentication for the user\u2019s scripts: Administrator is able to configure the user\u2019s access permissions in the Cloud Pipeline account of the Cloud Provider or provide credentials for the external Cloud Provider account All the requests to the Cloud Provider authentication are handled by the certain Cloud Pipeline service, which authenticates the user with the configured credentials Users are able to use the Cloud Provider API without the authentication request","title":"Seamless authentication in Cloud Provider"},{"location":"manual/12_Manage_Settings/12.11._Advanced_features/#how-to-configure","text":"Administrator can create specific interfaces - Cloud Credentials Profiles . Each profile includes the following fields: Provider - to specify the Cloud Provider Name - to specify the profile name Assumed Role - to specify the role of the Cloud Provider that will be used for the authentication to the Cloud Provider API Policy - to specify the Cloud Provider policy of the objects access","title":"How to configure"},{"location":"manual/12_Manage_Settings/12.11._Advanced_features/#profiles-management","text":"To manage the Cloud Credentials Profiles , administrator should: Open the System Settings of the application Open the CLOUD REGIONS tab of the Settings Click the Cloud Provider label which Profiles shall be configured: The form of the Cloud Credentials Profiles management will be opened: To create/edit a profile: Click the \" + Create profile \" button in the right-upper corner In the appeared pop-up, fill in corresponding fields: After all fields are filled in, click the Create button to confirm creation: Just-created profile will appear in the list: To edit/remove a profile - click the button next to the profile name. Then perform desired actions","title":"Profiles management"},{"location":"manual/12_Manage_Settings/12.11._Advanced_features/#profiles-assigning","text":"To leverage created profiles, administrator assigns them to User/Role/Group entity. For each entity many profiles can be assigned. Also, from the profiles assigned to the certain User/Role/Group the one can be selected as default - in such case, this profile will be used by default during the authentication operation in Cloud Provider . If the default profile for the certain User/Role/Group isn't selected - during the authentication operation there shall be selected which profile from the available ones should be used. Profiles assigning can be performed via the USER MANAGEMENT (at the section \" Launch options \" of the certain User or Group/Role card). For example, to assign profiles to a user administrator shall: Open the System Settings of the application Open the USER MANAGEMENT tab of the Settings Find the desired user and click it to open the user card, e.g.: Profiles assigning can be performed in the \" Launch options \" section: Click the \" Cloud Credential Profiles \" dropdown list and select one or several profiles to assign, e.g.: In this list, all profiles added as described above are available. To select the profile which should be used as default - click the \" Default Credential Profile \" dropdown list and select the desired profile, e.g.: Note : this action is optional Click the OK button to save changes For assigning profile to the Group/Role, similar actions should be performed in the corresponding cards","title":"Profiles assigning"},{"location":"manual/12_Manage_Settings/12.11._Advanced_features/#profiles-usage-during-the-run","text":"Now, we have Cloud Credentials Profiles (with their own Providers' roles and policies), we assigned them to the certain User/Role/Group. How to allow their usage during the run? It is being configured at the CLOUD REGIONS tab (for each Cloud Region separately). There is a special field - \" Mount Credentials Rule \" with the following allowed values: NONE - for runs in this region, do not configure credentials SAME CLOUD - for runs in this region, configure credentials allowed for user of the same Cloud Provider only ALL - for runs in this region, configure all credentials allowed for user Example:","title":"Profiles usage during the run"},{"location":"manual/12_Manage_Settings/12.11._Advanced_features/#example-seamless-authentication-in-aws","text":"In the example below, we will create the Profile with Read/Write access to AWS data storage ( S3 bucket) and will access it from the run via AWS CLI directly. Open the System Settings Open the CLOUD REGIONS tab of the Settings Click the AWS label in the Cloud Provider list Click the \" + Create profile \" button: In the pop-up, specify the profile name, e.g.: In the pop-up, specify the AWS role you wish to use for the current profile (role shall be received from the AWS support), e.g.: The role, used in our example, allows all operations with AWS buckets (read/write) In the pop-up, specify the policy you wish to use with the current profile, e.g.: Please note although the role allows all operations with any bucket - we will restrict access to only one bucket - via the specified policy Save the profile - click the Create button. The profile will appear in the list: Open the \" USER MANAGEMENT \" tab, find the user for whom you wish to use the created profile. Click the user tile Click the \" Cloud Credentials Profiles \" dropdown list and select the profile created at step 8: Click the \" Default Credentials Profile \" dropdown list and select the profile selected at step 10: Save changes: Open the CLOUD REGIONS tab of the Settings Click the AWS Cloud Region from which the run will be launched Click the \" Mount credentials rule \" dropdown list and select the item \" Same cloud \": Save changes Launch the tool (in our example, we will use library/ubuntu ) from the user you assigned the profile at steps 10-11, e.g.: Open the Run logs page of the just launched tool Wait until the SSH hyperlink appears. Click it: In the opened web-terminal, perform the command pip install awscli to install AWS CLI. Then check that installation is successful: Now, you can use AWS CLI directly to the bucket specified at the Profile policy (at step 7) without extra-authentication. For example, list the content: Upload a file to the bucket: Download a file from the bucket: But if you try to get access to the existing object on which the policy (specified at step 7) doesn't allow access - you will be rejected, e.g.:","title":"Example: seamless authentication in AWS"},{"location":"manual/12_Manage_Settings/12.11._Advanced_features/#switching-of-cloud-regions-for-launched-jobs-in-case-of-insufficient-capacity","text":"Please note this functionality is currently available only for AWS If there are not enough instances of specified type to launch a run in one region - Cloud Pipeline can automatically try to launch identical instance in other region(s) of the same Cloud Provider. This behaviour is defined by the special Cloud Region setting - \" Run shift policy \": The switching region procedure looks like: User launches a job. If during the run initialization, an instance requested by user is missing due to InsufficientInstanceCapacity error (that means run failed with insufficient capacity in the selected region) - next steps below will be performed: Note : the displayed text for this error can be configured by admin via the System preference launch.insufficient.capacity.message Possibility to switch the current region is checking - option \" Run shift policy \" shall be previously enabled: Possibility to switch to any vacant region from the same Cloud Provider is checking - option \" Run shift policy \" shall be previously enabled for the vacant region, e.g.: Current run is being automatically stopped. InsufficientInstanceCapacity error is displayed at the Run logs page as the failure reason: A new run is being automatically launched - in the vacant Cloud Region. You can view info about that new run in the tile at the Run logs page of the original run. Also, the task RestartPipelineRun appears for the original run - in its logs, the information about shifting run is displayed as well: At the Run logs page of the switched (new) run, there is also a link to the original run: If a new instance is not available with a new region - steps 1-5 will be performed in one more region as long as there are regions of the same Cloud Provider with the enabled option \" Run shift policy \". Restrictions of this feature: available only for on-demand runs available only for runs that have not any Cloud dependent parameters (parameter is Cloud dependent if it, for example, contains some storage path) not supported for worker or cluster runs For a run that does not meet these restrictions - in case of InsufficientInstanceCapacity error, original run will be just terminated during the region's shifting process initialization. New run in any other region will not be launched. The reason of the failure will be shown in the RestartPipelineRun task logs, e.g. for a cluster run shifting attempt:","title":"Switching of Cloud Regions for launched jobs in case of insufficient capacity"},{"location":"manual/12_Manage_Settings/12.12._System_logs/","text":"12.12. System logs User shall have ROLE_ADMIN to view system security logs. Filters Date filter Service filter User filter Message filter Advanced filters Hostname filter Type filter Show service account events The System Logs tab contains the full list of security trail events. Each record in the logs list contains: Field Description Date The date and time of the log event Log status The status of the log message ( INFO , ERROR , etc.) Log message Description of the log event User User name who performed the event Service Service name that registered the event ( api-srv , edge ) Type Log message type (currently, only security type is available) Filters By default, in the list all logs are displayed from new to old. For the more convenient search of the desired logs, there are filters over the logs list: You may combine them in any order for your needs. Date filter To restrict the list of logs for a specific date/time interval - use the From and To controls. For example, to view logs for all events that were today after 21:30 : Click the From control: Click the Select time button Select the desired time (the left column for hours, middle - for minutes, right - for secs): Click the Ok button The logs list will be filtered: Service filter To restrict the list of logs for a specific service - use the Service control. You may select the desired service from the dropdown list, e.g.: The logs list will be filtered automatically: Multi-select is supported. User filter To restrict the list of logs for a specific user(s) - use the User control. You may select the desired user from the dropdown list. Multi-select is supported. Message filter To find the log by its event message (or its part) - use the Message field. Just click this field, specify the desired text and press the Enter key, e.g.: Advanced filters To open advanced filters click the Show advanced button. Additional filters will appear: Hostname filter To restrict the list of logs for a certain service host(s) - use the Hostname control. You may select the desired host from the dropdown list. Multi-select is supported. Type filter To restrict the list of logs for a certain log message type(s) - use the Type control. You may select the desired type from the dropdown list. Multi-select is supported. The following types can be here: security - logs related to security events - e.g. authentication in the Platform/services or access to objects (granting permissions) audit - logs related to any access to the data stored in the object storages. All operations ( READ / WRITE / DELETE ) excluding only listing are being logged. The following sources are being logged: data access operations from the Platform GUI. These logs are accumulated from services api-srv or gui . Operations may be like these - via the GUI in an Object storage, user opened a file-preview, user created a new file, user deleted a file, etc.: data access operations from the pipe CLI. These logs are accumulated from the pipe-cli service. Operations may be like these - via the pipe CLI in a console or web SSH-terminal, user opened a file content from an Object storage, user copied/moved a file to an Object storage, etc.: data access operations performed in mounted Object storages - all Object storages that mounted via the Platform for using in runs ( ~/cloud-data folder in each run) or storages that were mounted manually by user via pipe storage mount command. These logs are accumulated from the pipe-mount service. Operations may be like these - user mounted an Object storage as a folder and uploaded a file into the mounted folder, user opened the web SSH-terminal and read a file content from one of the Object storages mounted into ~/cloud-data folder, etc.: storage lifecycle - logs related to management events with storage lifecycle rules ( CREATE / EDIT / DELETE operations for rules), e.g.: Show service account events The Include Service Account Events checkbox allows to show/hide log message from the service account (main admin user). Since from this account many messages are received, much more than from other users, by default these messages are hidden.","title":"12.12. System logs"},{"location":"manual/12_Manage_Settings/12.12._System_logs/#1212-system-logs","text":"User shall have ROLE_ADMIN to view system security logs. Filters Date filter Service filter User filter Message filter Advanced filters Hostname filter Type filter Show service account events The System Logs tab contains the full list of security trail events. Each record in the logs list contains: Field Description Date The date and time of the log event Log status The status of the log message ( INFO , ERROR , etc.) Log message Description of the log event User User name who performed the event Service Service name that registered the event ( api-srv , edge ) Type Log message type (currently, only security type is available)","title":"12.12. System logs"},{"location":"manual/12_Manage_Settings/12.12._System_logs/#filters","text":"By default, in the list all logs are displayed from new to old. For the more convenient search of the desired logs, there are filters over the logs list: You may combine them in any order for your needs.","title":"Filters"},{"location":"manual/12_Manage_Settings/12.12._System_logs/#date-filter","text":"To restrict the list of logs for a specific date/time interval - use the From and To controls. For example, to view logs for all events that were today after 21:30 : Click the From control: Click the Select time button Select the desired time (the left column for hours, middle - for minutes, right - for secs): Click the Ok button The logs list will be filtered:","title":"Date filter"},{"location":"manual/12_Manage_Settings/12.12._System_logs/#service-filter","text":"To restrict the list of logs for a specific service - use the Service control. You may select the desired service from the dropdown list, e.g.: The logs list will be filtered automatically: Multi-select is supported.","title":"Service filter"},{"location":"manual/12_Manage_Settings/12.12._System_logs/#user-filter","text":"To restrict the list of logs for a specific user(s) - use the User control. You may select the desired user from the dropdown list. Multi-select is supported.","title":"User filter"},{"location":"manual/12_Manage_Settings/12.12._System_logs/#message-filter","text":"To find the log by its event message (or its part) - use the Message field. Just click this field, specify the desired text and press the Enter key, e.g.:","title":"Message filter"},{"location":"manual/12_Manage_Settings/12.12._System_logs/#advanced-filters","text":"To open advanced filters click the Show advanced button. Additional filters will appear:","title":"Advanced filters"},{"location":"manual/12_Manage_Settings/12.12._System_logs/#hostname-filter","text":"To restrict the list of logs for a certain service host(s) - use the Hostname control. You may select the desired host from the dropdown list. Multi-select is supported.","title":"Hostname filter"},{"location":"manual/12_Manage_Settings/12.12._System_logs/#type-filter","text":"To restrict the list of logs for a certain log message type(s) - use the Type control. You may select the desired type from the dropdown list. Multi-select is supported. The following types can be here: security - logs related to security events - e.g. authentication in the Platform/services or access to objects (granting permissions) audit - logs related to any access to the data stored in the object storages. All operations ( READ / WRITE / DELETE ) excluding only listing are being logged. The following sources are being logged: data access operations from the Platform GUI. These logs are accumulated from services api-srv or gui . Operations may be like these - via the GUI in an Object storage, user opened a file-preview, user created a new file, user deleted a file, etc.: data access operations from the pipe CLI. These logs are accumulated from the pipe-cli service. Operations may be like these - via the pipe CLI in a console or web SSH-terminal, user opened a file content from an Object storage, user copied/moved a file to an Object storage, etc.: data access operations performed in mounted Object storages - all Object storages that mounted via the Platform for using in runs ( ~/cloud-data folder in each run) or storages that were mounted manually by user via pipe storage mount command. These logs are accumulated from the pipe-mount service. Operations may be like these - user mounted an Object storage as a folder and uploaded a file into the mounted folder, user opened the web SSH-terminal and read a file content from one of the Object storages mounted into ~/cloud-data folder, etc.: storage lifecycle - logs related to management events with storage lifecycle rules ( CREATE / EDIT / DELETE operations for rules), e.g.:","title":"Type filter"},{"location":"manual/12_Manage_Settings/12.12._System_logs/#show-service-account-events","text":"The Include Service Account Events checkbox allows to show/hide log message from the service account (main admin user). Since from this account many messages are received, much more than from other users, by default these messages are hidden.","title":"Show service account events"},{"location":"manual/12_Manage_Settings/12.13._System_dictionaries/","text":"12.13. System dictionaries User shall have the ROLE_ADMIN to view/use system dictionaries. Dictionary management Create dictionary Edit dictionary Remove dictionary Manage links between dictionaries Using dictionaries Dictionary management Create dictionary Open the System dictionaries tab of the System-level Settings . Click the \" + Add dictionary \" button to create a new dictionary. Specify the dictionary name, e.g.: Add values for the dictionary. For that, click the \" + Add value \" button and specify the value in the appeared field, e.g.: Repeat previous step to add more values. After all values are specified, click the \" Save \" button: Just-created dictionary will appear in the dictionary list: Edit dictionary To edit the dictionary values: Find the appropriate dictionary in the list, click it. To rename a dictionary - click the field with the dictionary name, then change the name, e.g.: To rename any dictionary value - click it, then change the value, e.g.: To delete any dictionary value - click the corresponding icon near it, e.g.: To add a new dictionary value - click the \" + Add value \" button and specify a new value, e.g.: After all changes were done, click the \" Save \" button to confirm them. Remove dictionary To remove the dictionary: Find the appropriate dictionary in the list, click it. Click the \" Delete \" button: Click the \" OK \" button to confirm the deletion: Manage links between dictionaries To create a link from one value to another dictionary, click the corresponding link under the appropriate value: In the appeared pop-up, select the existing dictionary and its value that shall be auto filled, then click the \"Add\" button to confirm, e.g.: By the same way, you can add several links to one or more dictionaries. Link can be deleted by the corresponding button near (\" Remove \" button): After all links are added, close the pop-up. Links will be displayed near the source value, for which they are linked: Click the \" Save \" button to confirm changes Using dictionaries We will use the following dictionary: Open the attributes section for the entity you wish to add an attribute (in our example, we will add the attribute for the user). Click the \" + Add \" button in the \" Attributes \" section: Into the \" Key \" field specify the dictionary name that you wish to use as categorical attribute: Note : as you start typing into the \" Key \" field, possible dictionaries names will appear in the dropdown list When the existing dictionary name will be specified into the \" Key \" field, the \" Value \" field \"transforms\" to the dropdown list with the all values of the corresponding dictionary: Any value from the list can be selected. Click the \"Add\" button to confirm the selection: Added attribute will appear: Later, the attribute value can be edit - in the same way (via the dropdown list with possible dictionary values). Also, the attribute name (\" Key \" field) can be renamed - in the same way as \"regular\" attributes. If the \" Key \" will be changed to any value that doesn't matches with any existing dictionary - the \" Value \" field will be automatically transform to the plain text field. If at step 5, the value with \"links\" will be selected, the linked dictionaries and their predefined values, will appear automatically: Please note, all functionality described above is available only for admins. \"General\" users can't view/edit System Dictionaries. Also, if non-admin user tries to specify any existing dictionary as the attribute \" Key \" - it will be displayed as plain text attribute. No dictionary values will be loaded/displayed.","title":"12.13. System dictionaries"},{"location":"manual/12_Manage_Settings/12.13._System_dictionaries/#1213-system-dictionaries","text":"User shall have the ROLE_ADMIN to view/use system dictionaries. Dictionary management Create dictionary Edit dictionary Remove dictionary Manage links between dictionaries Using dictionaries","title":"12.13. System dictionaries"},{"location":"manual/12_Manage_Settings/12.13._System_dictionaries/#dictionary-management","text":"","title":"Dictionary management"},{"location":"manual/12_Manage_Settings/12.13._System_dictionaries/#create-dictionary","text":"Open the System dictionaries tab of the System-level Settings . Click the \" + Add dictionary \" button to create a new dictionary. Specify the dictionary name, e.g.: Add values for the dictionary. For that, click the \" + Add value \" button and specify the value in the appeared field, e.g.: Repeat previous step to add more values. After all values are specified, click the \" Save \" button: Just-created dictionary will appear in the dictionary list:","title":"Create dictionary"},{"location":"manual/12_Manage_Settings/12.13._System_dictionaries/#edit-dictionary","text":"To edit the dictionary values: Find the appropriate dictionary in the list, click it. To rename a dictionary - click the field with the dictionary name, then change the name, e.g.: To rename any dictionary value - click it, then change the value, e.g.: To delete any dictionary value - click the corresponding icon near it, e.g.: To add a new dictionary value - click the \" + Add value \" button and specify a new value, e.g.: After all changes were done, click the \" Save \" button to confirm them.","title":"Edit dictionary"},{"location":"manual/12_Manage_Settings/12.13._System_dictionaries/#remove-dictionary","text":"To remove the dictionary: Find the appropriate dictionary in the list, click it. Click the \" Delete \" button: Click the \" OK \" button to confirm the deletion:","title":"Remove dictionary"},{"location":"manual/12_Manage_Settings/12.13._System_dictionaries/#manage-links-between-dictionaries","text":"To create a link from one value to another dictionary, click the corresponding link under the appropriate value: In the appeared pop-up, select the existing dictionary and its value that shall be auto filled, then click the \"Add\" button to confirm, e.g.: By the same way, you can add several links to one or more dictionaries. Link can be deleted by the corresponding button near (\" Remove \" button): After all links are added, close the pop-up. Links will be displayed near the source value, for which they are linked: Click the \" Save \" button to confirm changes","title":"Manage links between dictionaries"},{"location":"manual/12_Manage_Settings/12.13._System_dictionaries/#using-dictionaries","text":"We will use the following dictionary: Open the attributes section for the entity you wish to add an attribute (in our example, we will add the attribute for the user). Click the \" + Add \" button in the \" Attributes \" section: Into the \" Key \" field specify the dictionary name that you wish to use as categorical attribute: Note : as you start typing into the \" Key \" field, possible dictionaries names will appear in the dropdown list When the existing dictionary name will be specified into the \" Key \" field, the \" Value \" field \"transforms\" to the dropdown list with the all values of the corresponding dictionary: Any value from the list can be selected. Click the \"Add\" button to confirm the selection: Added attribute will appear: Later, the attribute value can be edit - in the same way (via the dropdown list with possible dictionary values). Also, the attribute name (\" Key \" field) can be renamed - in the same way as \"regular\" attributes. If the \" Key \" will be changed to any value that doesn't matches with any existing dictionary - the \" Value \" field will be automatically transform to the plain text field. If at step 5, the value with \"links\" will be selected, the linked dictionaries and their predefined values, will appear automatically: Please note, all functionality described above is available only for admins. \"General\" users can't view/edit System Dictionaries. Also, if non-admin user tries to specify any existing dictionary as the attribute \" Key \" - it will be displayed as plain text attribute. No dictionary values will be loaded/displayed.","title":"Using dictionaries"},{"location":"manual/12_Manage_Settings/12.14._NAT_gateway/","text":"12.14. NAT gateway User shall have ROLE_ADMIN to manage network routes. Add route Remove route Via the NAT gateway subtab, admin can configure network routing: This form displays the list of previously created routes (port forwarding map). Each route record contains: info about external resource: icon with the route status server name IP port info about corresponding internal config (mapping details): service name service IP port comment field button to remove the route Add new route To add a new route, administrator shall: Click the ADD ROUTE button: The pop-up to specify external server details will appear: Specify the external server name, e.g.: If you need to specify IP address of the external server - set the Specify IP address checkbox: The system will try to resolve the IP automatically. You can click the Resolve button to auto-detect the external server IP. Note : if the IP is not resolved or resolved incorrectly - you can specify it manually. Specify the port from which you wish to route, e.g.: Several ports for the same server can be specified simultaneously. To add a port - click the \" +Add port \" button and specify another port, e.g.: Repeat if necessary. To remove extra-added port from the list, click the button near the port. Optionally, you can add a comment/description for the created route, e.g.: Once all details are specified, click the ADD button: Just-added external server will appear in the list: We've added a temporary record in external resources list. Unsaved routes are displayed in blue color. Also: if necessary, any other server(s) or new ports for the existing servers can be added in the same way as described. To add a new record, click the button to cancel all made changes, click the button Click the SAVE button to confirm made changes: Until the route creation procedures will not be finished, the scheduling icon will be displayed near the corresponding record: The route table is updated automatically every few seconds. But if you need to refresh the table manually, use the corresponding button above the table. Once the route creation is done, the route details will appear in the INTERNAL CONFIG fields and the route status will be changed to ACTIVE : Remove existing route To remove an existing route, administrator shall: Click the remove button for the desired route, e.g.: The route to remove will be strikethroughed and be highlighted in gray: To permanently remove the route, click the SAVE button. Notes : to cancel removing of the route, click the button in the route row to cancel all made changes, click the button Once the SAVE button is clicked, removing procedures will be started. Until the route removing procedures will not be finished, the waiting icon will be displayed near the corresponding record: The route table is updated automatically every few seconds. But if you need to refresh the table manually, use the corresponding button above the table. Once the removing is done, the route will disappear from the table:","title":"12.14. NAT gateway"},{"location":"manual/12_Manage_Settings/12.14._NAT_gateway/#1214-nat-gateway","text":"User shall have ROLE_ADMIN to manage network routes. Add route Remove route Via the NAT gateway subtab, admin can configure network routing: This form displays the list of previously created routes (port forwarding map). Each route record contains: info about external resource: icon with the route status server name IP port info about corresponding internal config (mapping details): service name service IP port comment field button to remove the route","title":"12.14. NAT gateway"},{"location":"manual/12_Manage_Settings/12.14._NAT_gateway/#add-new-route","text":"To add a new route, administrator shall: Click the ADD ROUTE button: The pop-up to specify external server details will appear: Specify the external server name, e.g.: If you need to specify IP address of the external server - set the Specify IP address checkbox: The system will try to resolve the IP automatically. You can click the Resolve button to auto-detect the external server IP. Note : if the IP is not resolved or resolved incorrectly - you can specify it manually. Specify the port from which you wish to route, e.g.: Several ports for the same server can be specified simultaneously. To add a port - click the \" +Add port \" button and specify another port, e.g.: Repeat if necessary. To remove extra-added port from the list, click the button near the port. Optionally, you can add a comment/description for the created route, e.g.: Once all details are specified, click the ADD button: Just-added external server will appear in the list: We've added a temporary record in external resources list. Unsaved routes are displayed in blue color. Also: if necessary, any other server(s) or new ports for the existing servers can be added in the same way as described. To add a new record, click the button to cancel all made changes, click the button Click the SAVE button to confirm made changes: Until the route creation procedures will not be finished, the scheduling icon will be displayed near the corresponding record: The route table is updated automatically every few seconds. But if you need to refresh the table manually, use the corresponding button above the table. Once the route creation is done, the route details will appear in the INTERNAL CONFIG fields and the route status will be changed to ACTIVE :","title":"Add new route"},{"location":"manual/12_Manage_Settings/12.14._NAT_gateway/#remove-existing-route","text":"To remove an existing route, administrator shall: Click the remove button for the desired route, e.g.: The route to remove will be strikethroughed and be highlighted in gray: To permanently remove the route, click the SAVE button. Notes : to cancel removing of the route, click the button in the route row to cancel all made changes, click the button Once the SAVE button is clicked, removing procedures will be started. Until the route removing procedures will not be finished, the waiting icon will be displayed near the corresponding record: The route table is updated automatically every few seconds. But if you need to refresh the table manually, use the corresponding button above the table. Once the removing is done, the route will disappear from the table:","title":"Remove existing route"},{"location":"manual/12_Manage_Settings/12.15._System_jobs/","text":"12.15. System jobs User shall have ROLE_ADMIN to any access to the System jobs. Overview Configuration Create system job System jobs panel Run system job and view results System jobs allow admins to create and easily launch system scripts for different needs: to get some system statistics or system information about the current Platform state (for example, collect information about all storages that have specific size, list all unattached EBS volumes, set some s3 bucket policy to all storages, etc.) to create some sort of automation scripts - with help of Kubectl , pipe CLI, Cloud Pipeline API, Cloud CLI Overview System jobs solution uses the existing Cloud-Pipeline infrastructure, to reduce number of preparation steps to be done to get desire output. In a nutshell, the approach to perform the System jobs is the following: There are: prepared system pipeline that contains system jobs scripts. Admin can add new scripts or edit/delete existing ones. Also, pipeline config contains: Kubernetes service account to perform kubectl commands from such pipeline during the system job run special assign policy that allows to assign the pipeline to one of the running system node ( MASTER node, for example). It is convenient as no additional instances (waiting or initializing ones) are required to perform a job prepared special docker image that includes pre-installed packages such as system packages ( curl , nano , git , etc.), kubectl , pipe CLI, Cloud CLI ( AWS / Azure / GCP ), LustreFS client When admin launches a system job - the system instance according to specified assign policy ( MASTER instance, by default) is found for the system job pipeline performing At the selected system instance, the docker-container is launched from the special docker-image for system jobs In the launched docker-container, the system job script is being performed Configuration The following System Preferences are currently used to configure the System jobs behaviour: system.jobs.pipeline.id - ID of the prepared system pipeline that contains system jobs scripts system.jobs.scripts.location - path to the system scripts directory inside the pipeline repo. Default value is src/system-jobs system.jobs.output.pipeline.task - name of the task at the Run logs page of the system pipeline that is launched for the system job. Task contains system job output results. Default value is SystemJob For example: Create a new System job To create a new System job, admin shall: Open the pipeline defined in the system.jobs.pipeline.id System Preference, e.g.: Open the CODE tab inside the pipeline: Navigate to the folder defined as the folder for system jobs scripts (specified via system.jobs.scripts.location System Preference) In the opened folder, you can view previously created scripts: Add a new system script - you can create it manually or upload the existing one from the local workstation. We will create a new script manually. Click the \" + NEW FILE \" button: In the appeared pop-up, specify a new script name and commit message for the pipeline changes ( optionally ). For our example, we will create a simple bash script that lists s3 object storages using AWS Cloud CLI or outputs storage content if the storage name was specified as a parameter. So, the script will be called storages_listing : Click the OK button to confirm. Click the just-created file to edit it: In the appeared pop-up, specify the script itself and save changes: Specify a commit message, e.g.: New system script is created: System jobs panel Entry point for the usage of existing System script jobs is the System jobs subtab of the SYSTEM MANAGEMENT tab in the System settings: Here, admin can view the whole list of stored system scripts, select any script and launch it or observe script's runs history. This panel contains: a - list of all stored scripts. Click any to select it b - selected script name c - list of selected script's runs history d - button to refresh runs history e - button to launch the script: click the button itself to launch the script as is click v button near to launch the script with specifying parameters f - view output logs of the script's specific launch g - view run's details that used for the script launch Run a script and view results For our example, we will use a simple bash script created in the section above . To run a script from the System jobs panel: Click the script in the list. Click the LAUNCH button to perform a script as is (without parameters), e.g.: Just-launched script run will appear in the runs history: Jobs states are similar to pipelines states . Once the script is performed, the state will be changed to Success : Click the script run's row or the button LOG to view the script performing output: Script logs will appear: If needed, you may download these logs as a text file by click the corresponding button - DOWNLOAD : To run a script with parameters: Click the script in the list. Click the v button near the LAUNCH and select the item Launch with parameters , e.g.: In the appeared form, specify parameters for the script separated by spaces (in the format <parameter_1> <parameter_2> ... ) and click the LAUNCH button: Just-launched script will appear in the runs history: Script performing logs can be viewed in the same way as was described in the example above. To view run's details that was used for the script launch: Click the DETAILS button in the script run's row: The Run logs page will be opened: Here you can also view system job results - click the task SystemJob (default name, it can be changed via the System Preference system.jobs.output.pipeline.task ):","title":"12.15. System jobs"},{"location":"manual/12_Manage_Settings/12.15._System_jobs/#1215-system-jobs","text":"User shall have ROLE_ADMIN to any access to the System jobs. Overview Configuration Create system job System jobs panel Run system job and view results System jobs allow admins to create and easily launch system scripts for different needs: to get some system statistics or system information about the current Platform state (for example, collect information about all storages that have specific size, list all unattached EBS volumes, set some s3 bucket policy to all storages, etc.) to create some sort of automation scripts - with help of Kubectl , pipe CLI, Cloud Pipeline API, Cloud CLI","title":"12.15. System jobs"},{"location":"manual/12_Manage_Settings/12.15._System_jobs/#overview","text":"System jobs solution uses the existing Cloud-Pipeline infrastructure, to reduce number of preparation steps to be done to get desire output. In a nutshell, the approach to perform the System jobs is the following: There are: prepared system pipeline that contains system jobs scripts. Admin can add new scripts or edit/delete existing ones. Also, pipeline config contains: Kubernetes service account to perform kubectl commands from such pipeline during the system job run special assign policy that allows to assign the pipeline to one of the running system node ( MASTER node, for example). It is convenient as no additional instances (waiting or initializing ones) are required to perform a job prepared special docker image that includes pre-installed packages such as system packages ( curl , nano , git , etc.), kubectl , pipe CLI, Cloud CLI ( AWS / Azure / GCP ), LustreFS client When admin launches a system job - the system instance according to specified assign policy ( MASTER instance, by default) is found for the system job pipeline performing At the selected system instance, the docker-container is launched from the special docker-image for system jobs In the launched docker-container, the system job script is being performed","title":"Overview"},{"location":"manual/12_Manage_Settings/12.15._System_jobs/#configuration","text":"The following System Preferences are currently used to configure the System jobs behaviour: system.jobs.pipeline.id - ID of the prepared system pipeline that contains system jobs scripts system.jobs.scripts.location - path to the system scripts directory inside the pipeline repo. Default value is src/system-jobs system.jobs.output.pipeline.task - name of the task at the Run logs page of the system pipeline that is launched for the system job. Task contains system job output results. Default value is SystemJob For example:","title":"Configuration"},{"location":"manual/12_Manage_Settings/12.15._System_jobs/#create-a-new-system-job","text":"To create a new System job, admin shall: Open the pipeline defined in the system.jobs.pipeline.id System Preference, e.g.: Open the CODE tab inside the pipeline: Navigate to the folder defined as the folder for system jobs scripts (specified via system.jobs.scripts.location System Preference) In the opened folder, you can view previously created scripts: Add a new system script - you can create it manually or upload the existing one from the local workstation. We will create a new script manually. Click the \" + NEW FILE \" button: In the appeared pop-up, specify a new script name and commit message for the pipeline changes ( optionally ). For our example, we will create a simple bash script that lists s3 object storages using AWS Cloud CLI or outputs storage content if the storage name was specified as a parameter. So, the script will be called storages_listing : Click the OK button to confirm. Click the just-created file to edit it: In the appeared pop-up, specify the script itself and save changes: Specify a commit message, e.g.: New system script is created:","title":"Create a new System job"},{"location":"manual/12_Manage_Settings/12.15._System_jobs/#system-jobs-panel","text":"Entry point for the usage of existing System script jobs is the System jobs subtab of the SYSTEM MANAGEMENT tab in the System settings: Here, admin can view the whole list of stored system scripts, select any script and launch it or observe script's runs history. This panel contains: a - list of all stored scripts. Click any to select it b - selected script name c - list of selected script's runs history d - button to refresh runs history e - button to launch the script: click the button itself to launch the script as is click v button near to launch the script with specifying parameters f - view output logs of the script's specific launch g - view run's details that used for the script launch","title":"System jobs panel"},{"location":"manual/12_Manage_Settings/12.15._System_jobs/#run-a-script-and-view-results","text":"For our example, we will use a simple bash script created in the section above . To run a script from the System jobs panel: Click the script in the list. Click the LAUNCH button to perform a script as is (without parameters), e.g.: Just-launched script run will appear in the runs history: Jobs states are similar to pipelines states . Once the script is performed, the state will be changed to Success : Click the script run's row or the button LOG to view the script performing output: Script logs will appear: If needed, you may download these logs as a text file by click the corresponding button - DOWNLOAD : To run a script with parameters: Click the script in the list. Click the v button near the LAUNCH and select the item Launch with parameters , e.g.: In the appeared form, specify parameters for the script separated by spaces (in the format <parameter_1> <parameter_2> ... ) and click the LAUNCH button: Just-launched script will appear in the runs history: Script performing logs can be viewed in the same way as was described in the example above. To view run's details that was used for the script launch: Click the DETAILS button in the script run's row: The Run logs page will be opened: Here you can also view system job results - click the task SystemJob (default name, it can be changed via the System Preference system.jobs.output.pipeline.task ):","title":"Run a script and view results"},{"location":"manual/12_Manage_Settings/12.2._Edit_a_system_event/","text":"12.2. Edit a system event An administrator can edit System events notifications only. Navigate to System events tab . Click the Edit button. Change any field: Title of the notification. Body of the notification. Notification Severity (\" info \", \" warning \" or \" critical \"). Blocking box. Blocking event emerges in the middle of the window and requires confirmation from the user to disappear. Active box. Active notifications will be shown for all users of the Cloud Pipeline until admin sets them inactive. Click Save . To delete a system event: Click the Delete button. Confirm the deletion.","title":"12.2. Edit a system event"},{"location":"manual/12_Manage_Settings/12.2._Edit_a_system_event/#122-edit-a-system-event","text":"An administrator can edit System events notifications only. Navigate to System events tab . Click the Edit button. Change any field: Title of the notification. Body of the notification. Notification Severity (\" info \", \" warning \" or \" critical \"). Blocking box. Blocking event emerges in the middle of the window and requires confirmation from the user to disappear. Active box. Active notifications will be shown for all users of the Cloud Pipeline until admin sets them inactive. Click Save . To delete a system event: Click the Delete button. Confirm the deletion.","title":"12.2. Edit a system event"},{"location":"manual/12_Manage_Settings/12.3._Create_a_new_user/","text":"12.3. Create a new user Create a new user manually Users batch import CSV format Batch import approach Example CSV Import users Import results review User shall have ROLE_ADMIN to create a new user. Create a new user manually Navigate to User management tab. Click + Create user control. The Create user form will be opened. This form contains the following sections: Name - a new user's name. Default data storage - drop-down list suggested a default data storage to the created user. Assign group or role - drop-down list suggested the existing roles and groups assign. View of roles and groups that are assigned to a new user. Note : the groups and roles, marked as default, will be shown. Enter a name for the new user. Note : there is no restriction to username format, but it is highly recommended to name a user according to your SSO scheme. Select a default data storage if it is necessary. Select desired groups and roles to assign the new user. Click the Create button and the new user will be displayed in the Users tab table. Users batch import Described above mechanism allows creating users one-by-one via the GUI. If a number of users shall be created - it can be quite complicated to perform those operation multiple times. To address this, the special ability is implemented in the Cloud Pipeline - an import users from a CSV file using GUI and CLI. CSV format CSV format of the file for the batch import (with examples): UserName,Groups,<AttributeItem1>,<AttributeItem2>,<AttributeItemN> <user1>,<group1>,<Value1>,<Value2>,<ValueN> <user2>,<group2>|<group3>,<Value3>,<Value4>,<ValueN> <user3>,,<Value3>,<Value4>,<ValueN> <user4>,<group4>,,, Where: UserName ( mandatory column) - contains the user name. This column can't contain empty values Groups ( mandatory column) - contains the \"permission\" groups, which shall be assigned to the user. This column may contain empty values for the users (in this case - no groups will be added). Also this column may contain several values for one user separated by vertical line ( | ) <AttributeItem1> , <AttributeItem2> , <AttributeItemN> - set of other optional columns, which correspond to the user attributes (they could be existing or new) Batch import approach The import process takes a number of inputs: CSV file Users/Groups/Attributes creation options , which control if a corresponding object shall be created if not found in the database. If a creation option is not specified - the object creation won't happen: \" create-user \" \" create-group \" \" create-<ATTRIBUTE_ITEM_NAME> \" The inputs are then processed in the following manner: If the user with UserName does not exist: If \" create-user \" is specified - the user shall be created Otherwise - skip such user If the user is not a member of one of the groups, listed in Groups - the user shall be added to the group(s) If one of the group(s) does not exist: If \" create-group \" is specified - the group shall be created and a user shall be assigned to that group Otherwise - skip the group For each of the <AttributeItemX> the following actions shall be performed: If the user does not have an attribute with the name <AttributeItemX> and the attribute's name does not match any existing \" SYSTEM DICTIONARY \" - it shall be added with a corresponding value If an attribute already exists, but the value is different - it shall be updated with a corresponding value (for the cases, when the attribute's name does not match any existing \" SYSTEM DICTIONARY \") If an attribute is an existing \" SYSTEM DICTIONARY \" and a value is linked to another dictionary - this link shall be handled in the same manner as GUI does (i.e. another attribute shall be added/updated, according to the link) If an attribute is an existing \" SYSTEM DICTIONARY \" and a value does not exist in that dictionary: If \" create-<ATTRIBUTE_ITEM_NAME> \" is specified (where \" <ATTRIBUTE_ITEM_NAME> \" shall match the column name) - the value shall be added as a new dictionary entry Otherwise - skip the Attribute item If one of the values is empty (i.e. Groups , <AttributeItemX> ) for a specific user - this field shall be skipped (like \" user3 \" and \" user4 \" in the example above) Example CSV The following CSV file will be used for the further import example: UserName,Groups,billing-center,import_attr1 IMPORT_USER1,IMPORT_GROUP1,Center1,import_attr1_val1 IMPORT_USER2,IMPORT_GROUP2,Center1,import_attr1_val2 IMPORT_USER3,IMPORT_GROUP1|IMPORT_GROUP2,Center2,import_attr1_val3 Let's assume that the System Dictionaries configuration contains two dictionaries: billing-group Group1 Group2 billing-center Center1 \u2192 Group1 Center2 \u2192 Group1 Center3 \u2192 Group2 Import users Open the USER MANAGEMENT tab of the System Settings page. Click the \" Import users \" button: You will be prompted to select a CSV file for the import ( here we use the example CSV, shown above ) The GUI will ask to the creation options selection (see Processing logic section for the details): Select the creation options ( here we are ok to create everything: users, groups, billing centers and assign new attribute to the users ) and click the IMPORT button: Note : to import users via CLI see here . Import results review Once the import is done (via GUI or CLI) - review the import results: Users and groups have been created Users were assigned to the newly create groups Attributes were assigned to the users as well: A random import_attr1 attribute was assigned as a plain text value, as there is no corresponding SYSTEM DICTIONARY billing-center attribute was assigned from the corresponding SYSTEM DICTIONARY billing-group was assigned as well even if it is not specified in the CSV file, as the Center1 is mapped to the Group1 (see example CSV description section above)","title":"12.3. Create a new user"},{"location":"manual/12_Manage_Settings/12.3._Create_a_new_user/#123-create-a-new-user","text":"Create a new user manually Users batch import CSV format Batch import approach Example CSV Import users Import results review User shall have ROLE_ADMIN to create a new user.","title":"12.3. Create a new user"},{"location":"manual/12_Manage_Settings/12.3._Create_a_new_user/#create-a-new-user-manually","text":"Navigate to User management tab. Click + Create user control. The Create user form will be opened. This form contains the following sections: Name - a new user's name. Default data storage - drop-down list suggested a default data storage to the created user. Assign group or role - drop-down list suggested the existing roles and groups assign. View of roles and groups that are assigned to a new user. Note : the groups and roles, marked as default, will be shown. Enter a name for the new user. Note : there is no restriction to username format, but it is highly recommended to name a user according to your SSO scheme. Select a default data storage if it is necessary. Select desired groups and roles to assign the new user. Click the Create button and the new user will be displayed in the Users tab table.","title":"Create a new user manually"},{"location":"manual/12_Manage_Settings/12.3._Create_a_new_user/#users-batch-import","text":"Described above mechanism allows creating users one-by-one via the GUI. If a number of users shall be created - it can be quite complicated to perform those operation multiple times. To address this, the special ability is implemented in the Cloud Pipeline - an import users from a CSV file using GUI and CLI.","title":"Users batch import"},{"location":"manual/12_Manage_Settings/12.3._Create_a_new_user/#csv-format","text":"CSV format of the file for the batch import (with examples): UserName,Groups,<AttributeItem1>,<AttributeItem2>,<AttributeItemN> <user1>,<group1>,<Value1>,<Value2>,<ValueN> <user2>,<group2>|<group3>,<Value3>,<Value4>,<ValueN> <user3>,,<Value3>,<Value4>,<ValueN> <user4>,<group4>,,, Where: UserName ( mandatory column) - contains the user name. This column can't contain empty values Groups ( mandatory column) - contains the \"permission\" groups, which shall be assigned to the user. This column may contain empty values for the users (in this case - no groups will be added). Also this column may contain several values for one user separated by vertical line ( | ) <AttributeItem1> , <AttributeItem2> , <AttributeItemN> - set of other optional columns, which correspond to the user attributes (they could be existing or new)","title":"CSV format"},{"location":"manual/12_Manage_Settings/12.3._Create_a_new_user/#batch-import-approach","text":"The import process takes a number of inputs: CSV file Users/Groups/Attributes creation options , which control if a corresponding object shall be created if not found in the database. If a creation option is not specified - the object creation won't happen: \" create-user \" \" create-group \" \" create-<ATTRIBUTE_ITEM_NAME> \" The inputs are then processed in the following manner: If the user with UserName does not exist: If \" create-user \" is specified - the user shall be created Otherwise - skip such user If the user is not a member of one of the groups, listed in Groups - the user shall be added to the group(s) If one of the group(s) does not exist: If \" create-group \" is specified - the group shall be created and a user shall be assigned to that group Otherwise - skip the group For each of the <AttributeItemX> the following actions shall be performed: If the user does not have an attribute with the name <AttributeItemX> and the attribute's name does not match any existing \" SYSTEM DICTIONARY \" - it shall be added with a corresponding value If an attribute already exists, but the value is different - it shall be updated with a corresponding value (for the cases, when the attribute's name does not match any existing \" SYSTEM DICTIONARY \") If an attribute is an existing \" SYSTEM DICTIONARY \" and a value is linked to another dictionary - this link shall be handled in the same manner as GUI does (i.e. another attribute shall be added/updated, according to the link) If an attribute is an existing \" SYSTEM DICTIONARY \" and a value does not exist in that dictionary: If \" create-<ATTRIBUTE_ITEM_NAME> \" is specified (where \" <ATTRIBUTE_ITEM_NAME> \" shall match the column name) - the value shall be added as a new dictionary entry Otherwise - skip the Attribute item If one of the values is empty (i.e. Groups , <AttributeItemX> ) for a specific user - this field shall be skipped (like \" user3 \" and \" user4 \" in the example above)","title":"Batch import approach"},{"location":"manual/12_Manage_Settings/12.3._Create_a_new_user/#example-csv","text":"The following CSV file will be used for the further import example: UserName,Groups,billing-center,import_attr1 IMPORT_USER1,IMPORT_GROUP1,Center1,import_attr1_val1 IMPORT_USER2,IMPORT_GROUP2,Center1,import_attr1_val2 IMPORT_USER3,IMPORT_GROUP1|IMPORT_GROUP2,Center2,import_attr1_val3 Let's assume that the System Dictionaries configuration contains two dictionaries: billing-group Group1 Group2 billing-center Center1 \u2192 Group1 Center2 \u2192 Group1 Center3 \u2192 Group2","title":"Example CSV"},{"location":"manual/12_Manage_Settings/12.3._Create_a_new_user/#import-users","text":"Open the USER MANAGEMENT tab of the System Settings page. Click the \" Import users \" button: You will be prompted to select a CSV file for the import ( here we use the example CSV, shown above ) The GUI will ask to the creation options selection (see Processing logic section for the details): Select the creation options ( here we are ok to create everything: users, groups, billing centers and assign new attribute to the users ) and click the IMPORT button: Note : to import users via CLI see here .","title":"Import users"},{"location":"manual/12_Manage_Settings/12.3._Create_a_new_user/#import-results-review","text":"Once the import is done (via GUI or CLI) - review the import results: Users and groups have been created Users were assigned to the newly create groups Attributes were assigned to the users as well: A random import_attr1 attribute was assigned as a plain text value, as there is no corresponding SYSTEM DICTIONARY billing-center attribute was assigned from the corresponding SYSTEM DICTIONARY billing-group was assigned as well even if it is not specified in the CSV file, as the Center1 is mapped to the Group1 (see example CSV description section above)","title":"Import results review"},{"location":"manual/12_Manage_Settings/12.4._Edit_delete_a_user/","text":"12.4. Edit/delete a user Edit a user Default data storage Groups (roles) management Attributes Launch options Possibility to revert changes Block/unblock a user GUI impersonation Delete a user User shall have ROLE_ADMIN to edit/delete users. Edit a user For edit a user: Open the Users subtab in the User management section of the system-level settings. Find a user. Click the Edit button in the row opposite the user name: Pop-up window will appear: At this form, there are several blocks of details/settings for a user. User info Main details' section about the user, that contain general info fields like user name, date of the user registration in the Platform, user email address, first and last names, e.g.: Default data storage Here you can select default data storage for a user: Groups (roles) management In this block you can set groups and roles for the selected user: For more information about changing a set of the roles/groups for the specific user see 12.8. Change a set of roles/groups for a user . Attributes In this block you can set metadata tags (attributes) for a user. These tags represent key/value pairs, same as pipeline/folder tags. For more information see 17. CP objects tagging by additional attributes . \"Blocking\" notifications track One of the special attribute that is set automatically - information about the notifications confirmation: Via that attribute you can view, which \"blocking\" notifications were confirmed by the user (about system notifications see here ). This attribute is shown only for users that confirmed at least one \"blocking\" notification. By default, this attribute has the following pair: KEY - confirmed_notifications (that name could be changed via the system-level preference system.events.confirmation.metadata.key ) VALUE - link that shows summary count of confirmed notifications for the user To open the detailed table with confirmed notifications for the user: Click the VALUE link: Here you can view detailed information about confirmed notifications - their titles, messages and datetime of the confirmation: Also you can open \"raw\" JSON view of the detailed table, if necessary. For that, click the EDIT button under the detailed table: Here you can edit the contents. Click the SAVE button to save changes: Launch options In this block you can specify some restrictions for a specific user on allowed instance types, price types, jobs visibility, etc. To apply configured launch options for a user, click button in the right bottom corner of the pop-up. This action will save all made changes in the user settings (not only launch options) and close the pop-up. Allowed instance count This setting ( Allowed instance max count ) allows to restrict the number of instances a user can run at the same time. This is useful to address any bugs as the users' scripts may spawn hundreds of machines without a real need. Note : this restriction is not applied to the users with the ROLE_ADMIN role. Behavior is configured by the following way: for example, if this setting for the user is specified to 5 - they can launch 5 jobs at a maximum. This includes worker nodes of the clusters. If the user tries to launch a job, but it exceeds a current limit - the warning and errors will be shown. Imagine that user has already launched 5 jobs with the set limit also equal 5. The user starts a new instance (which is going to be a 6th job): GUI will warn the user at the Launch page: GUI will warn the user, before submitting a job: If the user confirms a run operation - it will be rejected: Similar warnings are shown if the user will try to start a cluster or an autoscaled cluster. In this case, even if there are some spare slots, but the preparing cluster may exceed it eventually - the corresponding warning will appear. E.g., imagine that user has already launched 3 from 5 allowed jobs and tries to start an autoscaled cluster: Note : if user has reached the limit, warning will be also shown in case when the user will try to start a new job via pipe CLI. Job will be rejected, e.g.: Via pipe CLI, user can view the allowed instances count to be launched - using the command pipe users instances . Such restrictions could be set not only for a user, but on another levels too. Next hierarchy is set for applying of instances max count (in descending order of priority): User-level - i.e. specified for a user - as described above. This overrides any other limit for a particular user. User group level - i.e. specified for a group/role. Count of jobs of each member of the group/role is summed and compared to this parameter. If a number of jobs exceeds a limit - the job submission is rejected. How to configure see here . This overrides a global limit for a specific user group. (global) launch.max.runs.user.global - can be used to set a global default restriction for all the users. I.e. if it set to 5, each Platform user can launch 5 jobs at a maximum. See 12.10. Manage system-level settings for details. Allowed instance types Here, there are two settings that can restrict allowed instance types for a specific user: Allowed instance types mask - this mask restricts allowed instance types for launching tools, pipelines and configurations (i.e. any user's runs). Example of usage : If you want the user to be able to launch runs with only \"m5...\" instances types, mask would be m5* : In that case, before that user will launch any tool/pipeline/configuration, dropdown list of available node types for them will be like this: Allowed tool instance types mask - this mask restricts allowed instance types only for tools' runs. This mask has higher priority for launching tool than Allowed instance types mask . I.e. when both masks are set - for the launching tool, Allowed tool instance types mask will be applied. Example of usage : if you want the user to be able to launch tools with only \"large m5...\" instances types, mask would be m5*.large* : In that case, before that user will launch any tool, dropdown list of available node types for them will be like this: Setting restrictions on allowed instance types is a convenient way to minimize a number of invalid configurations' runs. Such restrictions could be set not only for a user, but on another levels too. Next hierarchy is set for applying of specified allowed instance types (in descending order of priority): User level - i.e. specified for a user - as described above. User group level - i.e. specified for a group/role of a user. If user is a member of several groups - list of allowed instances will be summarized across all the groups. How to configure see here . Tool level - i.e. specified for a tool, that a user tries to launch. How to configure see 10.5. Launch a Tool . (global) cluster.allowed.instance.types.docker - system preference that defines allowed instance types for tools. See 12.10. Manage system-level settings for details. (global) cluster.allowed.instance.types - system preference that defines allowed instance types for pipelines/tools/configurations. See 12.10. Manage system-level settings for details. After specifying allowed instance types, all GUI forms that allow to select the list of instance types (configurations/launch forms) - will display only valid instance types, according to hierarchy above. Allowed price types This field may restrict, what price types are allowed for a specific user (for launching any job). Example of usage : if you want the user to be able to launch only \"On-demand\" runs, select it: In that case, before that user will launch any run, dropdown list of price types for them will be like this: Setting restrictions on allowed price types is a convenient way to minimize a number of invalid configurations' runs. Such restrictions could be set not only for a user, but on another levels too. Next hierarchy is set for applying of specified allowed price types (in descending order of priority): User level - i.e. specified for a user - as described above. User group level - i.e. specified for a group/role of a user. If user is a member of several groups - list of allowed price types will be summarized across all the groups. How to configure see here . Tool level - i.e. specified for a tool, that a user tries to launch. How to configure see 10.5. Launch a Tool . Jobs visibility This field may restrict the visibility of runs at the Active Runs page for users who are non-owners of these runs. Note : this restriction is not applied to the users with the ROLE_ADMIN role. Example of usage : if you want the user to be able to view all pipeline runs (for that pipelines on which user has corresponding permissions), select \" Inherit \": Therefore USER4, that is owner of the pipeline, will view all runs of that pipeline, e.g.: If you want the user to be able to view only own runs launched, select \" Only owner \": Therefore USER4, that is owner of the pipeline, will view only own runs of that pipeline, e.g.: If this setting is not specified for a user - for them \" Inherit \" behavior is applied by default. Next hierarchy is set for applying of jobs visibility (in descending order of priority): User level - i.e. specified for a user - as described above. Group level - i.e. specified for a group/role. How to configure see here . (global) launch.run.visibility - system preference that defines jobs visibility globally. See 12.10. Manage system-level settings for details. Credentials profiles In these fields, specific interfaces can be specified for a user: Cloud Credentials Profiles - here, specific interfaces can be assigned for a user. Such interfaces allow to use seamless authentication in Cloud Provider services. For details and examples see here . Default Credentials Profile - setting allows to select one of Cloud Credentials Profiles assigned to the user as the default profile. For details and examples see here . Possibility to revert changes In certain cases, there could be convenient to undo all changes in a user profile when modifying it - without closing the form. The admin has such ability: open the User management tab select the desired user to modify, click the Edit button to open the popup with the user's settings edit some settings if needed to revert done changes - click the REVERT button at the bottom of the form ( Note : it's possible only before saving!): all done unsaved changes are reverted. The REVERT button becomes disabled: Note : in such way all unsaved changes of user settings could be reverted - Default data storage , Roles & Groups list, Attributes and Launch options . Block/unblock a user To block user: Open the Users subtab in the User management section of the system-level settings. Find a user. Click Edit button in the row opposite the user name. In the opened pop-up window click the BLOCK button in the left bottom corner. Confirm the blocking: To unblock user: Open the Users subtab in the User management section of the system-level settings. Find a user. Click Edit button in the row opposite the user name. In the opened pop-up window click the UNBLOCK button in the left bottom corner. Confirm the unblocking: GUI Impersonation Cloud Pipeline supports \"Impersonation\" feature. It allows admins (users with the ROLE_ADMIN role) to login as a selected user into the Cloud Pipeline GUI and have the same permissions/level of access as the user. Note : while running any job in the \"impersonated\" mode - the costs will go to that user and a linked billing-group. Start/Stop impersonation Login to the Cloud Pipeline using an administrative account Open the Users subtab in the User management section of the system-level settings. Find a user. Click the Edit button in the row opposite the user name - to load a user's profile: Click the Impersonate button in the top-right corner. Cloud Pipeline GUI will be reloaded using the selected user: While in the \"Impersonation\" mode, the following changes happen to the GUI: Main menu will turn orange, indicating that the impersonation mode is ON Logout button will be changed to the Stop impersonation button: Hovering over the Stop impersonation button will bring the ID of the user, whose account is currently active, e.g.: To stop the impersonation and revert back to the own admin account - click the Stop impersonation button. Delete a user To delete a user: Open the Users subtab in the User management section of the system-level settings. Find a user. Click the Edit button in the row opposite the user name. In the opened pop-up window click the Delete button in the left bottom corner. Confirm the deletion:","title":"12.4. Edit/delete a user"},{"location":"manual/12_Manage_Settings/12.4._Edit_delete_a_user/#124-editdelete-a-user","text":"Edit a user Default data storage Groups (roles) management Attributes Launch options Possibility to revert changes Block/unblock a user GUI impersonation Delete a user User shall have ROLE_ADMIN to edit/delete users.","title":"12.4. Edit/delete a user"},{"location":"manual/12_Manage_Settings/12.4._Edit_delete_a_user/#edit-a-user","text":"For edit a user: Open the Users subtab in the User management section of the system-level settings. Find a user. Click the Edit button in the row opposite the user name: Pop-up window will appear: At this form, there are several blocks of details/settings for a user.","title":"Edit a user"},{"location":"manual/12_Manage_Settings/12.4._Edit_delete_a_user/#user-info","text":"Main details' section about the user, that contain general info fields like user name, date of the user registration in the Platform, user email address, first and last names, e.g.:","title":"User info"},{"location":"manual/12_Manage_Settings/12.4._Edit_delete_a_user/#default-data-storage","text":"Here you can select default data storage for a user:","title":"Default data storage"},{"location":"manual/12_Manage_Settings/12.4._Edit_delete_a_user/#groups-roles-management","text":"In this block you can set groups and roles for the selected user: For more information about changing a set of the roles/groups for the specific user see 12.8. Change a set of roles/groups for a user .","title":"Groups (roles) management"},{"location":"manual/12_Manage_Settings/12.4._Edit_delete_a_user/#attributes","text":"In this block you can set metadata tags (attributes) for a user. These tags represent key/value pairs, same as pipeline/folder tags. For more information see 17. CP objects tagging by additional attributes .","title":"Attributes"},{"location":"manual/12_Manage_Settings/12.4._Edit_delete_a_user/#blocking-notifications-track","text":"One of the special attribute that is set automatically - information about the notifications confirmation: Via that attribute you can view, which \"blocking\" notifications were confirmed by the user (about system notifications see here ). This attribute is shown only for users that confirmed at least one \"blocking\" notification. By default, this attribute has the following pair: KEY - confirmed_notifications (that name could be changed via the system-level preference system.events.confirmation.metadata.key ) VALUE - link that shows summary count of confirmed notifications for the user To open the detailed table with confirmed notifications for the user: Click the VALUE link: Here you can view detailed information about confirmed notifications - their titles, messages and datetime of the confirmation: Also you can open \"raw\" JSON view of the detailed table, if necessary. For that, click the EDIT button under the detailed table: Here you can edit the contents. Click the SAVE button to save changes:","title":"\"Blocking\" notifications track"},{"location":"manual/12_Manage_Settings/12.4._Edit_delete_a_user/#launch-options","text":"In this block you can specify some restrictions for a specific user on allowed instance types, price types, jobs visibility, etc. To apply configured launch options for a user, click button in the right bottom corner of the pop-up. This action will save all made changes in the user settings (not only launch options) and close the pop-up.","title":"Launch options"},{"location":"manual/12_Manage_Settings/12.4._Edit_delete_a_user/#allowed-instance-count","text":"This setting ( Allowed instance max count ) allows to restrict the number of instances a user can run at the same time. This is useful to address any bugs as the users' scripts may spawn hundreds of machines without a real need. Note : this restriction is not applied to the users with the ROLE_ADMIN role. Behavior is configured by the following way: for example, if this setting for the user is specified to 5 - they can launch 5 jobs at a maximum. This includes worker nodes of the clusters. If the user tries to launch a job, but it exceeds a current limit - the warning and errors will be shown. Imagine that user has already launched 5 jobs with the set limit also equal 5. The user starts a new instance (which is going to be a 6th job): GUI will warn the user at the Launch page: GUI will warn the user, before submitting a job: If the user confirms a run operation - it will be rejected: Similar warnings are shown if the user will try to start a cluster or an autoscaled cluster. In this case, even if there are some spare slots, but the preparing cluster may exceed it eventually - the corresponding warning will appear. E.g., imagine that user has already launched 3 from 5 allowed jobs and tries to start an autoscaled cluster: Note : if user has reached the limit, warning will be also shown in case when the user will try to start a new job via pipe CLI. Job will be rejected, e.g.: Via pipe CLI, user can view the allowed instances count to be launched - using the command pipe users instances . Such restrictions could be set not only for a user, but on another levels too. Next hierarchy is set for applying of instances max count (in descending order of priority): User-level - i.e. specified for a user - as described above. This overrides any other limit for a particular user. User group level - i.e. specified for a group/role. Count of jobs of each member of the group/role is summed and compared to this parameter. If a number of jobs exceeds a limit - the job submission is rejected. How to configure see here . This overrides a global limit for a specific user group. (global) launch.max.runs.user.global - can be used to set a global default restriction for all the users. I.e. if it set to 5, each Platform user can launch 5 jobs at a maximum. See 12.10. Manage system-level settings for details.","title":"Allowed instance count"},{"location":"manual/12_Manage_Settings/12.4._Edit_delete_a_user/#allowed-instance-types","text":"Here, there are two settings that can restrict allowed instance types for a specific user: Allowed instance types mask - this mask restricts allowed instance types for launching tools, pipelines and configurations (i.e. any user's runs). Example of usage : If you want the user to be able to launch runs with only \"m5...\" instances types, mask would be m5* : In that case, before that user will launch any tool/pipeline/configuration, dropdown list of available node types for them will be like this: Allowed tool instance types mask - this mask restricts allowed instance types only for tools' runs. This mask has higher priority for launching tool than Allowed instance types mask . I.e. when both masks are set - for the launching tool, Allowed tool instance types mask will be applied. Example of usage : if you want the user to be able to launch tools with only \"large m5...\" instances types, mask would be m5*.large* : In that case, before that user will launch any tool, dropdown list of available node types for them will be like this: Setting restrictions on allowed instance types is a convenient way to minimize a number of invalid configurations' runs. Such restrictions could be set not only for a user, but on another levels too. Next hierarchy is set for applying of specified allowed instance types (in descending order of priority): User level - i.e. specified for a user - as described above. User group level - i.e. specified for a group/role of a user. If user is a member of several groups - list of allowed instances will be summarized across all the groups. How to configure see here . Tool level - i.e. specified for a tool, that a user tries to launch. How to configure see 10.5. Launch a Tool . (global) cluster.allowed.instance.types.docker - system preference that defines allowed instance types for tools. See 12.10. Manage system-level settings for details. (global) cluster.allowed.instance.types - system preference that defines allowed instance types for pipelines/tools/configurations. See 12.10. Manage system-level settings for details. After specifying allowed instance types, all GUI forms that allow to select the list of instance types (configurations/launch forms) - will display only valid instance types, according to hierarchy above.","title":"Allowed instance types"},{"location":"manual/12_Manage_Settings/12.4._Edit_delete_a_user/#allowed-price-types","text":"This field may restrict, what price types are allowed for a specific user (for launching any job). Example of usage : if you want the user to be able to launch only \"On-demand\" runs, select it: In that case, before that user will launch any run, dropdown list of price types for them will be like this: Setting restrictions on allowed price types is a convenient way to minimize a number of invalid configurations' runs. Such restrictions could be set not only for a user, but on another levels too. Next hierarchy is set for applying of specified allowed price types (in descending order of priority): User level - i.e. specified for a user - as described above. User group level - i.e. specified for a group/role of a user. If user is a member of several groups - list of allowed price types will be summarized across all the groups. How to configure see here . Tool level - i.e. specified for a tool, that a user tries to launch. How to configure see 10.5. Launch a Tool .","title":"Allowed price types"},{"location":"manual/12_Manage_Settings/12.4._Edit_delete_a_user/#jobs-visibility","text":"This field may restrict the visibility of runs at the Active Runs page for users who are non-owners of these runs. Note : this restriction is not applied to the users with the ROLE_ADMIN role. Example of usage : if you want the user to be able to view all pipeline runs (for that pipelines on which user has corresponding permissions), select \" Inherit \": Therefore USER4, that is owner of the pipeline, will view all runs of that pipeline, e.g.: If you want the user to be able to view only own runs launched, select \" Only owner \": Therefore USER4, that is owner of the pipeline, will view only own runs of that pipeline, e.g.: If this setting is not specified for a user - for them \" Inherit \" behavior is applied by default. Next hierarchy is set for applying of jobs visibility (in descending order of priority): User level - i.e. specified for a user - as described above. Group level - i.e. specified for a group/role. How to configure see here . (global) launch.run.visibility - system preference that defines jobs visibility globally. See 12.10. Manage system-level settings for details.","title":"Jobs visibility"},{"location":"manual/12_Manage_Settings/12.4._Edit_delete_a_user/#credentials-profiles","text":"In these fields, specific interfaces can be specified for a user: Cloud Credentials Profiles - here, specific interfaces can be assigned for a user. Such interfaces allow to use seamless authentication in Cloud Provider services. For details and examples see here . Default Credentials Profile - setting allows to select one of Cloud Credentials Profiles assigned to the user as the default profile. For details and examples see here .","title":"Credentials profiles"},{"location":"manual/12_Manage_Settings/12.4._Edit_delete_a_user/#possibility-to-revert-changes","text":"In certain cases, there could be convenient to undo all changes in a user profile when modifying it - without closing the form. The admin has such ability: open the User management tab select the desired user to modify, click the Edit button to open the popup with the user's settings edit some settings if needed to revert done changes - click the REVERT button at the bottom of the form ( Note : it's possible only before saving!): all done unsaved changes are reverted. The REVERT button becomes disabled: Note : in such way all unsaved changes of user settings could be reverted - Default data storage , Roles & Groups list, Attributes and Launch options .","title":"Possibility to revert changes"},{"location":"manual/12_Manage_Settings/12.4._Edit_delete_a_user/#blockunblock-a-user","text":"To block user: Open the Users subtab in the User management section of the system-level settings. Find a user. Click Edit button in the row opposite the user name. In the opened pop-up window click the BLOCK button in the left bottom corner. Confirm the blocking: To unblock user: Open the Users subtab in the User management section of the system-level settings. Find a user. Click Edit button in the row opposite the user name. In the opened pop-up window click the UNBLOCK button in the left bottom corner. Confirm the unblocking:","title":"Block/unblock a user"},{"location":"manual/12_Manage_Settings/12.4._Edit_delete_a_user/#gui-impersonation","text":"Cloud Pipeline supports \"Impersonation\" feature. It allows admins (users with the ROLE_ADMIN role) to login as a selected user into the Cloud Pipeline GUI and have the same permissions/level of access as the user. Note : while running any job in the \"impersonated\" mode - the costs will go to that user and a linked billing-group.","title":"GUI Impersonation"},{"location":"manual/12_Manage_Settings/12.4._Edit_delete_a_user/#startstop-impersonation","text":"Login to the Cloud Pipeline using an administrative account Open the Users subtab in the User management section of the system-level settings. Find a user. Click the Edit button in the row opposite the user name - to load a user's profile: Click the Impersonate button in the top-right corner. Cloud Pipeline GUI will be reloaded using the selected user: While in the \"Impersonation\" mode, the following changes happen to the GUI: Main menu will turn orange, indicating that the impersonation mode is ON Logout button will be changed to the Stop impersonation button: Hovering over the Stop impersonation button will bring the ID of the user, whose account is currently active, e.g.: To stop the impersonation and revert back to the own admin account - click the Stop impersonation button.","title":"Start/Stop impersonation"},{"location":"manual/12_Manage_Settings/12.4._Edit_delete_a_user/#delete-a-user","text":"To delete a user: Open the Users subtab in the User management section of the system-level settings. Find a user. Click the Edit button in the row opposite the user name. In the opened pop-up window click the Delete button in the left bottom corner. Confirm the deletion:","title":"Delete a user"},{"location":"manual/12_Manage_Settings/12.5._Create_a_group/","text":"12.5. Create a group User shall have ROLE_ADMIN to create a new group. Navigate to the User management tab of the system-level settings. Click the Groups subtab. Click + Create group button. Specify a name for the new group (e.g. NEW_GROUP ). If you want to grant the group and its permissions to all new users mark the group as Default . Select a default data storage for the created group if it is necessary. Click the Create button. Just-created group will appear in the list:","title":"12.5. Create a group"},{"location":"manual/12_Manage_Settings/12.5._Create_a_group/#125-create-a-group","text":"User shall have ROLE_ADMIN to create a new group. Navigate to the User management tab of the system-level settings. Click the Groups subtab. Click + Create group button. Specify a name for the new group (e.g. NEW_GROUP ). If you want to grant the group and its permissions to all new users mark the group as Default . Select a default data storage for the created group if it is necessary. Click the Create button. Just-created group will appear in the list:","title":"12.5. Create a group"},{"location":"manual/12_Manage_Settings/12.6._Edit_a_group_role/","text":"12.6. Edit a group (role) Edit a group (role) Default data storage User management Attributes Launch options Possibility to revert changes Block/unblock a group User shall have the ROLE_ADMIN role to edit groups/roles. Edit a group (role) For edit a group/role: Open the Groups/Roles subtab in the User management section of the system-level settings. Find a group (role). Click the Edit button in the row opposite the group/role name: Pop-up window will be shown: At this form, there are several blocks of the settings for a group/role. Default data storage Here you can select default data storage for a group/role: User management In this block, you can change a members list of the opened group/role: For more information see 12.8. Change a set of roles/groups for a user . Attributes In this block you can set metadata tags (attributes) for a group. These tags represent key/value pairs, same as pipeline/folder tags. For more information see 17. CP objects tagging by additional attributes . Launch options In this block you can specify some restrictions for a group of users/role on allowed instance types, price types, jobs visibility, etc. To apply configured launch options for a group/role, click button in the right bottom corner of the pop-up. This action will save all made changes in the group/role settings (not only launch options) and close the pop-up. Allowed instance count This setting ( Allowed instance max count ) allows to restrict the number of instances that members of a particular group/role can run at the same time. This is useful to address any bugs as the users' scripts may spawn hundreds of machines without a real need. Note : this restriction is not applied to the users with the ROLE_ADMIN role. Behavior is configured by the following way: for example, if this setting for the group/role is specified to 5 - then each member of that group/role can launch only 5 jobs simultaneously. This includes worker nodes of the clusters. If the member of the group/role tries to launch a job, but it exceeds a current limit for members of a such group/role - the warning and errors will be shown. Imagine that the member of a group has already launched 5 jobs with the set groups's limit also equal 5. That user starts a new instance (which is going to be a 6th job from this user): GUI will warn the user at the Launch page: GUI will warn the user, before submitting a job: If the user confirms a run operation - it will be rejected: Similar warnings are shown if the member of that group will try to start a cluster or an autoscaled cluster. In this case, even if there are some spare slots, but the preparing cluster may exceed it eventually - the corresponding warning will appear. E.g., imagine that member of the group has already launched 3 from 5 allowed jobs and tries to start an autoscaled cluster: Note : if user has reached the group/role limit, warning will be also shown in case when that user will try to start a new job via pipe CLI. Job will be rejected, e.g.: Such restrictions could be set not only for a group/role, but on another levels too. Next hierarchy is set for applying of instances max count (in descending order of priority): User-level - i.e. specified for a user. This overrides any other set limits for a particular user. How to configure see here . User group level - i.e. specified for a group/role. Count of jobs of each member of the group/role is summed and compared to this parameter. If a number of jobs exceeds a limit - the job submission is rejected. How to configure - described above. (global) launch.max.runs.user.global - can be used to set a global default restriction for all the users. I.e. if it set to 5, each Platform user can launch 5 jobs at a maximum. See 12.10. Manage system-level settings for details. Allowed instance types Here, there are two settings that can restrict allowed instance types for a specific group/role: Allowed instance types mask - this mask restricts allowed instance types for launching tools, pipelines and configurations (i.e. for any run from the member of this group/role). Example of usage : if you want members of the group/role to be able to launch runs with only \"m5...\" instances types, mask would be m5* : In that case, before the member of such group/role will launch any tool/pipeline/configuration, dropdown list of available node types for them will be like this: Allowed tool instance types mask - this mask restricts allowed instance types only for tools' runs. This mask has higher priority for launching tool than Allowed instance types mask . I.e. when both masks are set - for the launching tool, Allowed tool instance types mask will be applied. Example of usage : if you want members of the group/role to be able to launch tools with only \"large m5...\" instances types, mask would be m5*.large* : In that case, before the member of such group/role will launch any tool, dropdown list of available node types for them will be like this: Setting restrictions on allowed instance types is a convenient way to minimize a number of invalid configurations' runs. Such restrictions could be set not only for a group/role, but on another levels too. Next hierarchy is set for applying of specified allowed instance types (in descending order of priority): User level - i.e. specified for a particular user. How to configure see here . User group level - i.e. specified for a group/role of a particular user. If user is a member of several groups - list of allowed instances will be summarized across all the groups. How to configure - described above. Tool level - i.e. specified for a tool, that a particular user tries to launch. How to configure see 10.5. Launch a Tool . (global) cluster.allowed.instance.types.docker - system preference that defines allowed instance types for tools. See 12.10. Manage system-level settings for details. (global) cluster.allowed.instance.types - system preference that defines allowed instance types for pipelines/tools/configurations. See 12.10. Manage system-level settings for details. After specifying allowed instance types, all GUI forms that allow to select the list of instance types (configurations/launch forms) - will display only valid instance types, according to hierarchy above. Allowed price types This field may restrict, what price types are allowed for users of a specific group/role (for launching any job). Example of usage : if you want members of the group/role to be able to launch only \"On-demand\" runs, select it: In that case, before members of that group/role will launch any run, dropdown list of price types for them will be like this: Setting restrictions on allowed price types is a convenient way to minimize a number of invalid configurations' runs. Such restrictions could be set not only for a group/role, but on another levels too. Next hierarchy is set for applying of specified allowed price types (in descending order of priority): User level - i.e. specified for a particular user. How to configure see here . User group level - i.e. specified for a particular group/role. If user is a member of several groups - list of allowed price types will be summarized across all the groups. How to configure - described above. Tool level - i.e. specified for a tool, that a user tries to launch. How to configure see 10.5. Launch a Tool . Jobs visibility This field may restrict the visibility of runs at the Active Runs page for users who are non-owners of these runs. Note : this restriction is not applied to the users with the ROLE_ADMIN role. Example of usage : if you want members of a particular group/role to be able to view all pipeline runs (for that pipelines on which members have corresponding permissions), select \" Inherit \": Therefore USER4, that is owner of the pipeline, will view all runs of that pipeline, e.g.: If you want members of a particular group/role to be able to view only own runs, select \" Only owner \": Therefore USER4, that is owner of the pipeline, will view only own runs of that pipeline, e.g.: If this setting is not specified for a group/role - for members of that group/role \" Inherit \" behavior is applied by default. Next hierarchy is set for applying of jobs visibility (in descending order of priority): User level - i.e. specified for a user. How to configure see here . Group level - i.e. specified for a group/role - as decribed above. (global) launch.run.visibility - system preference that defines jobs visibility globally. See 12.10. Manage system-level settings for details. Credentials profiles In these fields, specific interfaces can be specified for a group/role: Cloud Credentials Profiles - here, specific interfaces can be assigned for members of a particular group/role. Such interfaces allow to use seamless authentication in Cloud Provider services. For details and examples see here . Default Credentials Profile - setting allows to select one of Cloud Credentials Profiles assigned to members of a particular group/role as the default profile. For details and examples see here . Possibility to revert changes In certain cases, there could be convenient to undo all changes in a group/role profile when modifying it - without closing the form. The admin has such ability: open the User management tab and then the Groups / Roles tab select the desired group to modify, click the Edit button to open the popup with the group's settings edit some settings if needed to revert done changes - click the REVERT button at the bottom of the form ( Note : it's possible only before saving!): all done unsaved changes are reverted. The REVERT button becomes disabled: Note : in such way all unsaved changes of user settings could be reverted - Default data storage , Users list, Attributes and Launch options . Block/unblock a group To block a group: Open the Groups subtab at the User management section of the system-level settings. Click the Edit button next to the group's name. Note : system groups are created by the SSO authentication system automatically and can not be found here. Pop-up with the group settings will be opened. Click the BLOCK button in the left bottom corner: Confirm the blocking: To unblock a group: Open the Groups subtab at the User management section of the system-level settings. Click the Edit button next to the blocked group: Pop-up with the group settings will be opened. Click the UNBLOCK button in the left bottom corner: Confirm the unblocking:","title":"12.6. Edit a group/role"},{"location":"manual/12_Manage_Settings/12.6._Edit_a_group_role/#126-edit-a-group-role","text":"Edit a group (role) Default data storage User management Attributes Launch options Possibility to revert changes Block/unblock a group User shall have the ROLE_ADMIN role to edit groups/roles.","title":"12.6. Edit a group (role)"},{"location":"manual/12_Manage_Settings/12.6._Edit_a_group_role/#edit-a-group-role","text":"For edit a group/role: Open the Groups/Roles subtab in the User management section of the system-level settings. Find a group (role). Click the Edit button in the row opposite the group/role name: Pop-up window will be shown: At this form, there are several blocks of the settings for a group/role.","title":"Edit a group (role)"},{"location":"manual/12_Manage_Settings/12.6._Edit_a_group_role/#default-data-storage","text":"Here you can select default data storage for a group/role:","title":"Default data storage"},{"location":"manual/12_Manage_Settings/12.6._Edit_a_group_role/#user-management","text":"In this block, you can change a members list of the opened group/role: For more information see 12.8. Change a set of roles/groups for a user .","title":"User management"},{"location":"manual/12_Manage_Settings/12.6._Edit_a_group_role/#attributes","text":"In this block you can set metadata tags (attributes) for a group. These tags represent key/value pairs, same as pipeline/folder tags. For more information see 17. CP objects tagging by additional attributes .","title":"Attributes"},{"location":"manual/12_Manage_Settings/12.6._Edit_a_group_role/#launch-options","text":"In this block you can specify some restrictions for a group of users/role on allowed instance types, price types, jobs visibility, etc. To apply configured launch options for a group/role, click button in the right bottom corner of the pop-up. This action will save all made changes in the group/role settings (not only launch options) and close the pop-up.","title":"Launch options"},{"location":"manual/12_Manage_Settings/12.6._Edit_a_group_role/#allowed-instance-count","text":"This setting ( Allowed instance max count ) allows to restrict the number of instances that members of a particular group/role can run at the same time. This is useful to address any bugs as the users' scripts may spawn hundreds of machines without a real need. Note : this restriction is not applied to the users with the ROLE_ADMIN role. Behavior is configured by the following way: for example, if this setting for the group/role is specified to 5 - then each member of that group/role can launch only 5 jobs simultaneously. This includes worker nodes of the clusters. If the member of the group/role tries to launch a job, but it exceeds a current limit for members of a such group/role - the warning and errors will be shown. Imagine that the member of a group has already launched 5 jobs with the set groups's limit also equal 5. That user starts a new instance (which is going to be a 6th job from this user): GUI will warn the user at the Launch page: GUI will warn the user, before submitting a job: If the user confirms a run operation - it will be rejected: Similar warnings are shown if the member of that group will try to start a cluster or an autoscaled cluster. In this case, even if there are some spare slots, but the preparing cluster may exceed it eventually - the corresponding warning will appear. E.g., imagine that member of the group has already launched 3 from 5 allowed jobs and tries to start an autoscaled cluster: Note : if user has reached the group/role limit, warning will be also shown in case when that user will try to start a new job via pipe CLI. Job will be rejected, e.g.: Such restrictions could be set not only for a group/role, but on another levels too. Next hierarchy is set for applying of instances max count (in descending order of priority): User-level - i.e. specified for a user. This overrides any other set limits for a particular user. How to configure see here . User group level - i.e. specified for a group/role. Count of jobs of each member of the group/role is summed and compared to this parameter. If a number of jobs exceeds a limit - the job submission is rejected. How to configure - described above. (global) launch.max.runs.user.global - can be used to set a global default restriction for all the users. I.e. if it set to 5, each Platform user can launch 5 jobs at a maximum. See 12.10. Manage system-level settings for details.","title":"Allowed instance count"},{"location":"manual/12_Manage_Settings/12.6._Edit_a_group_role/#allowed-instance-types","text":"Here, there are two settings that can restrict allowed instance types for a specific group/role: Allowed instance types mask - this mask restricts allowed instance types for launching tools, pipelines and configurations (i.e. for any run from the member of this group/role). Example of usage : if you want members of the group/role to be able to launch runs with only \"m5...\" instances types, mask would be m5* : In that case, before the member of such group/role will launch any tool/pipeline/configuration, dropdown list of available node types for them will be like this: Allowed tool instance types mask - this mask restricts allowed instance types only for tools' runs. This mask has higher priority for launching tool than Allowed instance types mask . I.e. when both masks are set - for the launching tool, Allowed tool instance types mask will be applied. Example of usage : if you want members of the group/role to be able to launch tools with only \"large m5...\" instances types, mask would be m5*.large* : In that case, before the member of such group/role will launch any tool, dropdown list of available node types for them will be like this: Setting restrictions on allowed instance types is a convenient way to minimize a number of invalid configurations' runs. Such restrictions could be set not only for a group/role, but on another levels too. Next hierarchy is set for applying of specified allowed instance types (in descending order of priority): User level - i.e. specified for a particular user. How to configure see here . User group level - i.e. specified for a group/role of a particular user. If user is a member of several groups - list of allowed instances will be summarized across all the groups. How to configure - described above. Tool level - i.e. specified for a tool, that a particular user tries to launch. How to configure see 10.5. Launch a Tool . (global) cluster.allowed.instance.types.docker - system preference that defines allowed instance types for tools. See 12.10. Manage system-level settings for details. (global) cluster.allowed.instance.types - system preference that defines allowed instance types for pipelines/tools/configurations. See 12.10. Manage system-level settings for details. After specifying allowed instance types, all GUI forms that allow to select the list of instance types (configurations/launch forms) - will display only valid instance types, according to hierarchy above.","title":"Allowed instance types"},{"location":"manual/12_Manage_Settings/12.6._Edit_a_group_role/#allowed-price-types","text":"This field may restrict, what price types are allowed for users of a specific group/role (for launching any job). Example of usage : if you want members of the group/role to be able to launch only \"On-demand\" runs, select it: In that case, before members of that group/role will launch any run, dropdown list of price types for them will be like this: Setting restrictions on allowed price types is a convenient way to minimize a number of invalid configurations' runs. Such restrictions could be set not only for a group/role, but on another levels too. Next hierarchy is set for applying of specified allowed price types (in descending order of priority): User level - i.e. specified for a particular user. How to configure see here . User group level - i.e. specified for a particular group/role. If user is a member of several groups - list of allowed price types will be summarized across all the groups. How to configure - described above. Tool level - i.e. specified for a tool, that a user tries to launch. How to configure see 10.5. Launch a Tool .","title":"Allowed price types"},{"location":"manual/12_Manage_Settings/12.6._Edit_a_group_role/#jobs-visibility","text":"This field may restrict the visibility of runs at the Active Runs page for users who are non-owners of these runs. Note : this restriction is not applied to the users with the ROLE_ADMIN role. Example of usage : if you want members of a particular group/role to be able to view all pipeline runs (for that pipelines on which members have corresponding permissions), select \" Inherit \": Therefore USER4, that is owner of the pipeline, will view all runs of that pipeline, e.g.: If you want members of a particular group/role to be able to view only own runs, select \" Only owner \": Therefore USER4, that is owner of the pipeline, will view only own runs of that pipeline, e.g.: If this setting is not specified for a group/role - for members of that group/role \" Inherit \" behavior is applied by default. Next hierarchy is set for applying of jobs visibility (in descending order of priority): User level - i.e. specified for a user. How to configure see here . Group level - i.e. specified for a group/role - as decribed above. (global) launch.run.visibility - system preference that defines jobs visibility globally. See 12.10. Manage system-level settings for details.","title":"Jobs visibility"},{"location":"manual/12_Manage_Settings/12.6._Edit_a_group_role/#credentials-profiles","text":"In these fields, specific interfaces can be specified for a group/role: Cloud Credentials Profiles - here, specific interfaces can be assigned for members of a particular group/role. Such interfaces allow to use seamless authentication in Cloud Provider services. For details and examples see here . Default Credentials Profile - setting allows to select one of Cloud Credentials Profiles assigned to members of a particular group/role as the default profile. For details and examples see here .","title":"Credentials profiles"},{"location":"manual/12_Manage_Settings/12.6._Edit_a_group_role/#possibility-to-revert-changes","text":"In certain cases, there could be convenient to undo all changes in a group/role profile when modifying it - without closing the form. The admin has such ability: open the User management tab and then the Groups / Roles tab select the desired group to modify, click the Edit button to open the popup with the group's settings edit some settings if needed to revert done changes - click the REVERT button at the bottom of the form ( Note : it's possible only before saving!): all done unsaved changes are reverted. The REVERT button becomes disabled: Note : in such way all unsaved changes of user settings could be reverted - Default data storage , Users list, Attributes and Launch options .","title":"Possibility to revert changes"},{"location":"manual/12_Manage_Settings/12.6._Edit_a_group_role/#blockunblock-a-group","text":"To block a group: Open the Groups subtab at the User management section of the system-level settings. Click the Edit button next to the group's name. Note : system groups are created by the SSO authentication system automatically and can not be found here. Pop-up with the group settings will be opened. Click the BLOCK button in the left bottom corner: Confirm the blocking: To unblock a group: Open the Groups subtab at the User management section of the system-level settings. Click the Edit button next to the blocked group: Pop-up with the group settings will be opened. Click the UNBLOCK button in the left bottom corner: Confirm the unblocking:","title":"Block/unblock a group"},{"location":"manual/12_Manage_Settings/12.7._Delete_a_group/","text":"12.7. Delete a group User shall have ROLE_ADMIN to delete a group. Navigate to the User management tab of the system-level settings. Open the Groups subtab. Click the Delete button next to the group's name. Note : system groups are created by the SSO authentication system automatically and can not be found/deleted here. Confirm the deletion:","title":"12.7. Delete a group"},{"location":"manual/12_Manage_Settings/12.7._Delete_a_group/#127-delete-a-group","text":"User shall have ROLE_ADMIN to delete a group. Navigate to the User management tab of the system-level settings. Open the Groups subtab. Click the Delete button next to the group's name. Note : system groups are created by the SSO authentication system automatically and can not be found/deleted here. Confirm the deletion:","title":"12.7. Delete a group"},{"location":"manual/12_Manage_Settings/12.8._Change_a_set_of_roles_groups_for_a_user/","text":"12.8. Change a set of roles/groups for a user User shall have ROLE_ADMIN to change groups(roles) for a user. There are two ways to set a role or group to a user: Change a set of roles and groups to a selected user from the Users tab . Change a member list for a selected role or group . Note : the scenarios below shows a process using Roles as an example. Setting groups for a user happens in the same manner. Change a set of roles and groups to a selected user Navigate to User management tab. Make sure that you are in the Users tab area. Find user on the list (you can use Search field - see the picture below, 1 ). Click the Edit button (see the picture below, 2 ). The editing form is open. To assign a role or group to the user, click on \"Add role or group\" field and select the desired item from the drop-down list. When the desired item is selected, the + Add control will be enabled. To delete roles or groups, use the Delete button. Click OK and all changes will be saved and displayed in the Users tab table. Change a member list for a selected role or group Navigate to the User management tab. Move to the Roles tab. Click the Edit button next to Role's name. You'll see a list of users assigned to the role. Look for the desired user via Search field. When the user is selected, the +Add user control will be enabled. Click the +Add user control to add a new user to the role member list. To delete a user from the role member list, click the Delete button next to a user name. Click OK and all changes will be saved and displayed in the Users tab table.","title":"12.8. Change a set of roles/groups for a user"},{"location":"manual/12_Manage_Settings/12.8._Change_a_set_of_roles_groups_for_a_user/#128-change-a-set-of-rolesgroups-for-a-user","text":"User shall have ROLE_ADMIN to change groups(roles) for a user. There are two ways to set a role or group to a user: Change a set of roles and groups to a selected user from the Users tab . Change a member list for a selected role or group . Note : the scenarios below shows a process using Roles as an example. Setting groups for a user happens in the same manner.","title":"12.8. Change a set of roles/groups for a user"},{"location":"manual/12_Manage_Settings/12.8._Change_a_set_of_roles_groups_for_a_user/#change-a-set-of-roles-and-groups-to-a-selected-user","text":"Navigate to User management tab. Make sure that you are in the Users tab area. Find user on the list (you can use Search field - see the picture below, 1 ). Click the Edit button (see the picture below, 2 ). The editing form is open. To assign a role or group to the user, click on \"Add role or group\" field and select the desired item from the drop-down list. When the desired item is selected, the + Add control will be enabled. To delete roles or groups, use the Delete button. Click OK and all changes will be saved and displayed in the Users tab table.","title":"Change a set of roles and groups to a selected user"},{"location":"manual/12_Manage_Settings/12.8._Change_a_set_of_roles_groups_for_a_user/#change-a-member-list-for-a-selected-role-or-group","text":"Navigate to the User management tab. Move to the Roles tab. Click the Edit button next to Role's name. You'll see a list of users assigned to the role. Look for the desired user via Search field. When the user is selected, the +Add user control will be enabled. Click the +Add user control to add a new user to the role member list. To delete a user from the role member list, click the Delete button next to a user name. Click OK and all changes will be saved and displayed in the Users tab table.","title":"Change a member list for a selected role or group"},{"location":"manual/12_Manage_Settings/12.9._Change_email_notification/","text":"12.9. Email notifications User shall have ROLE_ADMIN to manage Email notifications. Change the email notification Configure automatic email notifications on users' runs IDLE runs Notifications for long paused runs Example of the scenario for automatic notifications for \"idle\" runs HIGH-CONSUMED runs Push notifications View push notifications Notifications section Settings Change the email notification Navigate to the Settings tab. Select the Email notifications section. Choose any of the email notification types (e.g. LONG_INIT ) on the left: Remove Keep admins informed and Keep owners informed options. This email notification type will no longer inform admins and owners. Add a new user to the Informed users . While typing system will suggest you users. When you selected all users, click outside this field. Change the Threshold parameter to e.g. 1400. Press the Save button to save all changes to the LONG_INIT email notification template. Configure automatic email notifications on users' runs Here, the mechanism of automatic email notifications for IDLE and HIGH-CONSUMED runs will be described. IDLE runs The system behavior for the IDLE runs is defined by the set of the following parameters: Setting Preference-duplicate Description Max duration of idle (min) system.max.idle.timeout.minutes Specifies the duration in minutes after that the system should check node's activity. If after this duration node's CPU utilization will be below CPU idle threshold - email notification IDLE_RUN will be sent to the user and the run itself will be marked by label Action delay (min) system.idle.action.timeout.minutes Specifies the duration in minutes. If node's CPU utilization is still below CPU idle threshold for this duration after the Max duration of idle is over - an action, specified in Action field will be performed CPU idle threshold (%) system.idle.cpu.threshold Specifies percentage of the node's CPU utilization, below which an Action shall be taken Action system.idle.action Sets which action to perform with the node, that has the CPU utilization below than CPU idle threshold : NOTIFY - only send email notification IDLE_RUN . This action will be repeated every Resend delay if the node's CPU utilization will be still below than CPU idle threshold PAUSE - pause an instance if possible (only if the instance is On-Demand , Spot instances are skipped) and send single email notification IDLE_RUN_PAUSED PAUSE_OR_STOP - pause an instance if it is On-Demand or stop an instance if it is Spot and send the corresponding single email notification IDLE_RUN_PAUSED / IDLE_RUN_STOPPED STOP - Stop an instance, disregarding price-type, and send single email notification IDLE_RUN_STOPPED Resend delay (sec) Only for IDLE_RUN notification Specifies the duration in seconds. Defines a delay after which the IDLE_RUN notification will be sent repeatedly in case when node's CPU utilization is still below CPU idle threshold . IDLE_RUN notifications will be repeated until Action delay is over. Then Action will be performed. Note : if Action is set as NOTIFY - sending notifications will be continued every Resend delay . Default value : -1 . In that case, IDLE_RUN notifications will not be resending. system.resource.monitoring.period Specifies period (in milliseconds) between the scannings of running instances to collect the monitoring metrics. After each such period, it's defined to display label for the specific instance or not Described parameters are configured in \"common\" panel for all IDLE notifications - IDLE_RUN , IDLE_RUN_PAUSED , IDLE_RUN_STOP , e.g.: Saving of the changes in described parameters values at the EMAIL NOTIFICATIONS form automatically changes the corresponding values in PREFERENCES , and vice versa. Notifications for long paused runs There are two notification types for the runs that are in the PAUSED state for a long time: LONG_PAUSED - this notification will be sent if the run has been paused for a long time. LONG_PAUSED_STOPPED - this notification will be sent if the run has been paused for a long time and should be terminated. The system behavior for the \"long paused\" runs is defined by the set of the following parameters: Setting Preference-duplicate Description Action Common setting for both notification types system.long.paused.action Sets which action to perform with the node, that has been paused for a long time: NOTIFY - only send email notification LONG_PAUSED . Then this action will be repeated every Resend delay if the run will be still paused STOP - terminate the paused run and send single email notification LONG_PAUSED_STOPPED Threshold (sec) Separately sets for each notification type Specifies the duration in seconds. Defines the delay after which an action, specified in Action field, will be performed and the corresponding notification will be sent if the run is still paused. Resend delay (sec) Only for LONG_PAUSED notification type Specifies the duration in seconds. Defines a delay after which the LONG_PAUSED notification will be sent repeatedly in case when the run is still paused after a Threshold duration. Note : if Action is set as NOTIFY - sending notifications will be continued every Resend delay . Default value : -1 . In that case, LONG_PAUSED notifications will not be resending. Saving of the changes in described parameter value (for Action ) at the EMAIL NOTIFICATIONS form automatically changes the corresponding value in PREFERENCES , and vice versa. For example, LONG_PAUSED notification settings: Example of the scenario for automatic notifications for \"idle\" runs In general, the system behavior is the following: User launches a run After Max duration of idle ( system.max.idle.timeout.minutes ) period, the system starts to check the node's activity. Then, the state of the node is checked every system.resource.monitoring.period If the node's CPU utilization becomes below the CPU idle threshold ( system.idle.cpu.threshold ): email notification IDLE_RUN is being sent, the run itself is being marked by the label IDLE_RUN notification is being repeated every Resend delay (configured for IDLE_RUN ) until the Action delay ( system.idle.action.timeout.minutes ) period is over. After Action delay ( system.idle.action.timeout.minutes ), if the node's CPU utilization is still below CPU idle threshold ( system.idle.cpu.threshold ): email notification IDLE_RUN is being sent (in case when Action ( system.idle.action ) is set as NOTIFY ) run is being paused/stopped and the corresponding email notification IDLE_RUN_PAUSED or IDLE_RUN_STOPPED is being sent (in case when Action ( system.idle.action ) is set as PAUSE / PAUSE_OR_STOP / STOP ) In case when Action ( system.idle.action ) is set as NOTIFY , email notifications IDLE_RUN continue to be sent every Resend delay , if the node's CPU utilization remains below the CPU idle threshold ( system.idle.cpu.threshold ). In case when run has been paused: if the Action (for Long Paused notifications - system.long.paused.action ) is set as NOTIFY , and the run has been paused for a Threshold period (configured for LONG_PAUSED ), the notification LONG_PAUSED is being sent. and if Resend delay (for LONG_PAUSED ) is configured, then email notifications LONG_PAUSED continue to be sent every Resend delay , if the run is still paused if the Action (for Long Paused notifications - system.long.paused.action ) is set as STOP , and the run has been paused for a Threshold period (configured for LONG_PAUSED_STOPPED ), run is being stopped (terminated) and the corresponding email notification LONG_PAUSED_STOPPED is being sent. Note : users can manually disable the automatic pausing of on-demand instances if they aren't used. For that the \" Auto pause \" checkbox at the Launch page shall be unchecked before the run: This action cancels only the auto pause, but the RUN_IDLE email notifications will be being sent (if the corresponding conditions will be met). HIGH-CONSUMED runs All settings that configure actions/notifications for \"high-consumed\" runs are moved to the EMAIL NOTIFICATIONS section. These settings duplicate some System Preferences and also can be configured from the PREFERENCES section, but it is more comfortable to configure them from the NOTIFICATIONS . HIGH_CONSUMED_RESOURCES notification settings: The system behavior for the \"high-consumed\" runs is defined by the set of the following System-level parameters (Preferences): Setting Preference name Description Threshold of disk consumption (%) system.disk.consume.threshold Specifies the node's disk threshold (in %) above which the email notification HIGH_CONSUMED_RESOURCES will be sent and the corresponding run will be marked by label Threshold of memory consumption (%) system.memory.consume.threshold Specifies the node's memory threshold (in %) above which the email notification HIGH_CONSUMED_RESOURCES will be sent and the corresponding run will be marked by the label system.monitoring.time.range Specifies the threshold timeout (if a real average resource consumption is higher then any of the thresholds above for that timeout - a notification shall be sent). Also defines the period after which the notification will be sent again, if the problem is still in place So, when memory or disk consuming will be higher than a threshold value for a specified period of time (in average) - a notification will be sent (and resent after a delay, if the problem is still in place). Saving of the changes in described parameters values at the EMAIL NOTIFICATIONS form automatically changes the corresponding values in PREFERENCES , and vice versa. Push notifications All email notifications, that are sending by Cloud Pipeline platform, are also duplicated as push notifications. This allows to view notifications right in the platform GUI. Push notifications do not require additional configuring - they are fully the same as corresponding email notifications, i.e. have the same header, content, recipients list, frequency and trigger of sending, etc. Therefore, administrator shall only configure email notifications once - platform will automatically duplicate emails sending as push notifications on the GUI. View push notifications Once any system event is occurred and its trigger for sending email notification has fired, email will be sent to the configured recipients (according to the notification template settings). Simultaneously, the push notification (with the same subject and body as in the email) will be \"sent\" to the same recipients. If these recipients are working in the Cloud Pipeline GUI at that moment or when they will open the platform GUI next time, they will view the corresponding push notification - in the right-upper corner, over any layout Cloud Pipeline page, e.g.: At the same moment, near the Notifications icon in the main menu will appear a count of new unread messages: To view details and full notification body, click the notification tile. Notification will be opened in a pop-up: Once that pop-up is closed, the notification will be marked as \"read\", the counter near the Notifications icon in the main menu will decrease. If you close the push notification by click the cross-button on it, such notification also will be marked as \"read\": If there are several push notifications were received, a couple of them will be shown with short details and the rest will be collapsed into a tile, e.g.: Such collapsed tile contains: a label with a count of collapsed notifications button hide - to hide all appeared push notifications but left them \" unread \" button read all - to hide all appeared push notifications and mark them as \" read \" Notifications section Notifications section allows to view all push notifications/emails sent to the current user. To open the section, click the corresponding button in the main menu: This page allows to view notifications. By default, new \"unread\" notifications list will be shown (as on the picture above). To view \"read\" notifications list, select the corresponding item in the dropdown list near the page title: \"Read\" notifications list will appear: Notifications list has the following fields: Status - status of the notification (\"unread\" or \"read\", distinct by the icon color) Title - notification title - it corresponds to the original email subject Text - first row of the notification text - it corresponds to the beginning of the original email body Created date - date and time of the notification push Read date - date and time when the notification is read Additionally, the page contains controls: Read all button - to mark all new \"unread\" notifications as \"read\" Refresh button - to refresh notifications lists and update a counter of \"unread\" notifications To view the notification - click it in the list, e.g.: Notification will be opened in a pop-up: If the notification was new and \"unread\" - once that pop-up is closed, the notification will be marked as \"read\" and will be placed to the corresponding notifications list. Settings Mute notifications If user do not want to receive push notifications over all forms and pages in the platform GUI - they can be \"muted\". For that, user shall: Open System settings Select the tab My profile In the Profile sub-tab, tick the checkbox \" Mute email notifications \": After, new notifications will be appearing only in the Notifications section, but corresponding pushs will not be showing. Storing period Admins can configure the period for which described GUI notifications will be stored. It can be done via the system preference system.notifications.exp.period . This preference set the duration of the period in days after which old received notifications will be removed. The default value is not set (blank). This means that notifications are not being removed at all.","title":"12.9. Email notifications"},{"location":"manual/12_Manage_Settings/12.9._Change_email_notification/#129-email-notifications","text":"User shall have ROLE_ADMIN to manage Email notifications. Change the email notification Configure automatic email notifications on users' runs IDLE runs Notifications for long paused runs Example of the scenario for automatic notifications for \"idle\" runs HIGH-CONSUMED runs Push notifications View push notifications Notifications section Settings","title":"12.9. Email notifications"},{"location":"manual/12_Manage_Settings/12.9._Change_email_notification/#change-the-email-notification","text":"Navigate to the Settings tab. Select the Email notifications section. Choose any of the email notification types (e.g. LONG_INIT ) on the left: Remove Keep admins informed and Keep owners informed options. This email notification type will no longer inform admins and owners. Add a new user to the Informed users . While typing system will suggest you users. When you selected all users, click outside this field. Change the Threshold parameter to e.g. 1400. Press the Save button to save all changes to the LONG_INIT email notification template.","title":"Change the email notification"},{"location":"manual/12_Manage_Settings/12.9._Change_email_notification/#configure-automatic-email-notifications-on-users-runs","text":"Here, the mechanism of automatic email notifications for IDLE and HIGH-CONSUMED runs will be described.","title":"Configure automatic email notifications on users' runs"},{"location":"manual/12_Manage_Settings/12.9._Change_email_notification/#idle-runs","text":"The system behavior for the IDLE runs is defined by the set of the following parameters: Setting Preference-duplicate Description Max duration of idle (min) system.max.idle.timeout.minutes Specifies the duration in minutes after that the system should check node's activity. If after this duration node's CPU utilization will be below CPU idle threshold - email notification IDLE_RUN will be sent to the user and the run itself will be marked by label Action delay (min) system.idle.action.timeout.minutes Specifies the duration in minutes. If node's CPU utilization is still below CPU idle threshold for this duration after the Max duration of idle is over - an action, specified in Action field will be performed CPU idle threshold (%) system.idle.cpu.threshold Specifies percentage of the node's CPU utilization, below which an Action shall be taken Action system.idle.action Sets which action to perform with the node, that has the CPU utilization below than CPU idle threshold : NOTIFY - only send email notification IDLE_RUN . This action will be repeated every Resend delay if the node's CPU utilization will be still below than CPU idle threshold PAUSE - pause an instance if possible (only if the instance is On-Demand , Spot instances are skipped) and send single email notification IDLE_RUN_PAUSED PAUSE_OR_STOP - pause an instance if it is On-Demand or stop an instance if it is Spot and send the corresponding single email notification IDLE_RUN_PAUSED / IDLE_RUN_STOPPED STOP - Stop an instance, disregarding price-type, and send single email notification IDLE_RUN_STOPPED Resend delay (sec) Only for IDLE_RUN notification Specifies the duration in seconds. Defines a delay after which the IDLE_RUN notification will be sent repeatedly in case when node's CPU utilization is still below CPU idle threshold . IDLE_RUN notifications will be repeated until Action delay is over. Then Action will be performed. Note : if Action is set as NOTIFY - sending notifications will be continued every Resend delay . Default value : -1 . In that case, IDLE_RUN notifications will not be resending. system.resource.monitoring.period Specifies period (in milliseconds) between the scannings of running instances to collect the monitoring metrics. After each such period, it's defined to display label for the specific instance or not Described parameters are configured in \"common\" panel for all IDLE notifications - IDLE_RUN , IDLE_RUN_PAUSED , IDLE_RUN_STOP , e.g.: Saving of the changes in described parameters values at the EMAIL NOTIFICATIONS form automatically changes the corresponding values in PREFERENCES , and vice versa.","title":"IDLE runs"},{"location":"manual/12_Manage_Settings/12.9._Change_email_notification/#notifications-for-long-paused-runs","text":"There are two notification types for the runs that are in the PAUSED state for a long time: LONG_PAUSED - this notification will be sent if the run has been paused for a long time. LONG_PAUSED_STOPPED - this notification will be sent if the run has been paused for a long time and should be terminated. The system behavior for the \"long paused\" runs is defined by the set of the following parameters: Setting Preference-duplicate Description Action Common setting for both notification types system.long.paused.action Sets which action to perform with the node, that has been paused for a long time: NOTIFY - only send email notification LONG_PAUSED . Then this action will be repeated every Resend delay if the run will be still paused STOP - terminate the paused run and send single email notification LONG_PAUSED_STOPPED Threshold (sec) Separately sets for each notification type Specifies the duration in seconds. Defines the delay after which an action, specified in Action field, will be performed and the corresponding notification will be sent if the run is still paused. Resend delay (sec) Only for LONG_PAUSED notification type Specifies the duration in seconds. Defines a delay after which the LONG_PAUSED notification will be sent repeatedly in case when the run is still paused after a Threshold duration. Note : if Action is set as NOTIFY - sending notifications will be continued every Resend delay . Default value : -1 . In that case, LONG_PAUSED notifications will not be resending. Saving of the changes in described parameter value (for Action ) at the EMAIL NOTIFICATIONS form automatically changes the corresponding value in PREFERENCES , and vice versa. For example, LONG_PAUSED notification settings:","title":"Notifications for long paused runs"},{"location":"manual/12_Manage_Settings/12.9._Change_email_notification/#example-of-the-scenario-for-automatic-notifications-for-idle-runs","text":"In general, the system behavior is the following: User launches a run After Max duration of idle ( system.max.idle.timeout.minutes ) period, the system starts to check the node's activity. Then, the state of the node is checked every system.resource.monitoring.period If the node's CPU utilization becomes below the CPU idle threshold ( system.idle.cpu.threshold ): email notification IDLE_RUN is being sent, the run itself is being marked by the label IDLE_RUN notification is being repeated every Resend delay (configured for IDLE_RUN ) until the Action delay ( system.idle.action.timeout.minutes ) period is over. After Action delay ( system.idle.action.timeout.minutes ), if the node's CPU utilization is still below CPU idle threshold ( system.idle.cpu.threshold ): email notification IDLE_RUN is being sent (in case when Action ( system.idle.action ) is set as NOTIFY ) run is being paused/stopped and the corresponding email notification IDLE_RUN_PAUSED or IDLE_RUN_STOPPED is being sent (in case when Action ( system.idle.action ) is set as PAUSE / PAUSE_OR_STOP / STOP ) In case when Action ( system.idle.action ) is set as NOTIFY , email notifications IDLE_RUN continue to be sent every Resend delay , if the node's CPU utilization remains below the CPU idle threshold ( system.idle.cpu.threshold ). In case when run has been paused: if the Action (for Long Paused notifications - system.long.paused.action ) is set as NOTIFY , and the run has been paused for a Threshold period (configured for LONG_PAUSED ), the notification LONG_PAUSED is being sent. and if Resend delay (for LONG_PAUSED ) is configured, then email notifications LONG_PAUSED continue to be sent every Resend delay , if the run is still paused if the Action (for Long Paused notifications - system.long.paused.action ) is set as STOP , and the run has been paused for a Threshold period (configured for LONG_PAUSED_STOPPED ), run is being stopped (terminated) and the corresponding email notification LONG_PAUSED_STOPPED is being sent. Note : users can manually disable the automatic pausing of on-demand instances if they aren't used. For that the \" Auto pause \" checkbox at the Launch page shall be unchecked before the run: This action cancels only the auto pause, but the RUN_IDLE email notifications will be being sent (if the corresponding conditions will be met).","title":"Example of the scenario for automatic notifications for \"idle\" runs"},{"location":"manual/12_Manage_Settings/12.9._Change_email_notification/#high-consumed-runs","text":"All settings that configure actions/notifications for \"high-consumed\" runs are moved to the EMAIL NOTIFICATIONS section. These settings duplicate some System Preferences and also can be configured from the PREFERENCES section, but it is more comfortable to configure them from the NOTIFICATIONS . HIGH_CONSUMED_RESOURCES notification settings: The system behavior for the \"high-consumed\" runs is defined by the set of the following System-level parameters (Preferences): Setting Preference name Description Threshold of disk consumption (%) system.disk.consume.threshold Specifies the node's disk threshold (in %) above which the email notification HIGH_CONSUMED_RESOURCES will be sent and the corresponding run will be marked by label Threshold of memory consumption (%) system.memory.consume.threshold Specifies the node's memory threshold (in %) above which the email notification HIGH_CONSUMED_RESOURCES will be sent and the corresponding run will be marked by the label system.monitoring.time.range Specifies the threshold timeout (if a real average resource consumption is higher then any of the thresholds above for that timeout - a notification shall be sent). Also defines the period after which the notification will be sent again, if the problem is still in place So, when memory or disk consuming will be higher than a threshold value for a specified period of time (in average) - a notification will be sent (and resent after a delay, if the problem is still in place). Saving of the changes in described parameters values at the EMAIL NOTIFICATIONS form automatically changes the corresponding values in PREFERENCES , and vice versa.","title":"HIGH-CONSUMED runs"},{"location":"manual/12_Manage_Settings/12.9._Change_email_notification/#push-notifications","text":"All email notifications, that are sending by Cloud Pipeline platform, are also duplicated as push notifications. This allows to view notifications right in the platform GUI. Push notifications do not require additional configuring - they are fully the same as corresponding email notifications, i.e. have the same header, content, recipients list, frequency and trigger of sending, etc. Therefore, administrator shall only configure email notifications once - platform will automatically duplicate emails sending as push notifications on the GUI.","title":"Push notifications"},{"location":"manual/12_Manage_Settings/12.9._Change_email_notification/#view-push-notifications","text":"Once any system event is occurred and its trigger for sending email notification has fired, email will be sent to the configured recipients (according to the notification template settings). Simultaneously, the push notification (with the same subject and body as in the email) will be \"sent\" to the same recipients. If these recipients are working in the Cloud Pipeline GUI at that moment or when they will open the platform GUI next time, they will view the corresponding push notification - in the right-upper corner, over any layout Cloud Pipeline page, e.g.: At the same moment, near the Notifications icon in the main menu will appear a count of new unread messages: To view details and full notification body, click the notification tile. Notification will be opened in a pop-up: Once that pop-up is closed, the notification will be marked as \"read\", the counter near the Notifications icon in the main menu will decrease. If you close the push notification by click the cross-button on it, such notification also will be marked as \"read\": If there are several push notifications were received, a couple of them will be shown with short details and the rest will be collapsed into a tile, e.g.: Such collapsed tile contains: a label with a count of collapsed notifications button hide - to hide all appeared push notifications but left them \" unread \" button read all - to hide all appeared push notifications and mark them as \" read \"","title":"View push notifications"},{"location":"manual/12_Manage_Settings/12.9._Change_email_notification/#notifications-section","text":"Notifications section allows to view all push notifications/emails sent to the current user. To open the section, click the corresponding button in the main menu: This page allows to view notifications. By default, new \"unread\" notifications list will be shown (as on the picture above). To view \"read\" notifications list, select the corresponding item in the dropdown list near the page title: \"Read\" notifications list will appear: Notifications list has the following fields: Status - status of the notification (\"unread\" or \"read\", distinct by the icon color) Title - notification title - it corresponds to the original email subject Text - first row of the notification text - it corresponds to the beginning of the original email body Created date - date and time of the notification push Read date - date and time when the notification is read Additionally, the page contains controls: Read all button - to mark all new \"unread\" notifications as \"read\" Refresh button - to refresh notifications lists and update a counter of \"unread\" notifications To view the notification - click it in the list, e.g.: Notification will be opened in a pop-up: If the notification was new and \"unread\" - once that pop-up is closed, the notification will be marked as \"read\" and will be placed to the corresponding notifications list.","title":"Notifications section"},{"location":"manual/12_Manage_Settings/12.9._Change_email_notification/#settings","text":"","title":"Settings"},{"location":"manual/12_Manage_Settings/12.9._Change_email_notification/#mute-notifications","text":"If user do not want to receive push notifications over all forms and pages in the platform GUI - they can be \"muted\". For that, user shall: Open System settings Select the tab My profile In the Profile sub-tab, tick the checkbox \" Mute email notifications \": After, new notifications will be appearing only in the Notifications section, but corresponding pushs will not be showing.","title":"Mute notifications"},{"location":"manual/12_Manage_Settings/12.9._Change_email_notification/#storing-period","text":"Admins can configure the period for which described GUI notifications will be stored. It can be done via the system preference system.notifications.exp.period . This preference set the duration of the period in days after which old received notifications will be removed. The default value is not set (blank). This means that notifications are not being removed at all.","title":"Storing period"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/","text":"12. Manage Settings System Settings consist of CLI , System events , User management , Email notifications , Preferences , Cloud Regions and System logs tabs. To open System Settings click the gear icon on the main menu in the left side of the Cloud Pipeline application: CLI tab Pipe CLI Git CLI System events User management Users Groups Roles Usage report Email notifications Preferences Cloud Regions System dictionaries System management System logs NAT gateway My profile Profile Appearance CLI tab \" CLI \" tab generates two types of CLI installation and configuration commands to set CLI for the Cloud Pipeline - Pipe CLI and Git CLI . You can select each of them by click the corresponding option in the \" CLI \" tab menu. Pipe CLI Control Description Operation system Choose an operation system from drop-down list and the instruction how to install the Cloud Pipeline CLI will appear in the window below. Generate access key Generates access token to be used by CLI. Valid till A date access key expires. For more details see 14.1. Install and setup CLI . Git CLI Here you can see instructions how to configure your Git client to work with the Cloud Pipeline. System events This tab is visible only for users with the ROLE_ADMIN role. System events tab represents system events notifications. Here you can create, edit, delete system events notifications. System events notifications are organized into a table. It represents the body of the notification , its severity status (\" info \", \" warning \" or \" critical \") and date of creation , activity status . Note : Variants of activity status: Blocking event emerges in the middle of the window and requires confirmation from the user to disappear. Active notifications will be shown for all users of the Cloud Pipeline until admin sets them inactive. Administrator can edit and delete notifications via corresponding buttons. System events controls Controls are at the top right of the table. Control Description 1 Expand/Collapse This button shows/hides the body of the event. 2 Refresh To refresh a list of notifications. 3 + ADD To create a new notification. 4 Edit To open the edit form for the event. 5 Delete To delete an event. For more detais see 12.1. Add a new system event . User management This tab is visible only for users with the ROLE_ADMIN role or the ROLE_USER_READER role. The User management tab helps to manage user groups and system roles. To grant or refuse permissions to a specific group of users (e.g. project team members), you can just create a user group and grant or refuse permissions to the specific set of objects to the whole group. System roles is one of the principal tool for managing security access to the objects. Even if you have WRITE permission for a folder object, you might be not able to create a pipeline there, if you don't have the ROLE_PIPELINE_MANAGER role. Note : About permissions, you can read more here . User management consists of the following subtabs: Users , Groups , Roles , Usage report . Users This table view displays a list of users and their additional information: Name - an authenticated domain account (SAML/OAuth/OpenID), e.g. e-mail. Groups - a set of groups assigned to a user. It could be whether CP's user's groups and groups, given to each user automatically by SSO authentication system . Note : automatically created groups based on SSO authentication system are light-grey colored. Roles - a set of system roles assigned to a user. Additionally, near the name user's state is shown. Possible states: Online - for users who are logged in and use the Platform in the moment Offline - for users who are not logged in at the moment/do not use the Platform for some time By hover over the Offline icons - the tooltip will be shown that contains the info when the specific user has utilized the Platform the last time, e.g.: Please note, that users' states are visible only for user with the ROLE_ADMIN role. Users tab controls Control Description 1 Search field Field to search a particular user from a list of users. 2 Show users This control allows to filter which users to show in the list: all ( by default ) only blocked users. More details about blocking/unblocking see here only online users - i.e. users that are working at the Platform in the current moment only users that exceeded any spending quota 3 + Create user By click this button, the \"Create user\" form will be opened - to create a new user. More details see here . 4 Import users Allows to import users from the list in .csv format. 5 Export users Allows to export all users with selected set of attributes. More details see below . 6 Edit Allows to change a list of roles/groups assigned to a user. More details see here . Export users The Export users button allows administrator to export user list in .csv format. There are 2 export options: Default configuration In this case, the file with all users and full list of their properties ( ID , username , attributes (first and last name, email), registration date , first login date , list of groups , list of roles , blocked / unblocked state, default data storage ) will be downloaded to the local workstation. To export full user list with default configuration: click the Export users button in the right upper corner of the Users tab in the User management dashboard or hover the v button next to the Export users button and click the Default configuration item in the appeared drop-down menu Custom configuration Custom configuration allows admins to select which user properties should be downloaded: hover the v button next to the Export users button and click the Custom configuration item in the appeared drop-down menu the modal window with the list of available user properties to export will appear, e.g.: User can select any set of attributes by marking/unmarking corresponding checkboxes. At least 1 checkbox should be marked to export user list. \"Fields\" section contains the list of all users' attributes (by default, all attributes are selected). \"Metadata\" section contains the list of all users' metadata attributes (by default, all attributes are deselected). Note : enabled Header checkbox adds the headers row into the exporting file. to download the result file with full user list and custom set of their properties click the Download CSV button. Note : not all user's metadata have to be exported. Some of them may contains the sensitive data. To restrict the list of the metadata to export the following system preference should be used - misc.metadata.sensitive.keys . In this preference, all metadata keys that shouldn't be exported are being specified (i.e. if these metadata keys are specified in the preference, they will not be displayed at the export pop-up for selection). Example of that configured preference: Groups The \"Groups\" subtab shows a set of user groups created in CP. Here you can grant or refuse users in a group membership. Note that this tab displays groups created in CP only, not given by SSO authentication system . Groups tab controls Control Description 1 Search field To search a particular group from a list of groups. 2 + Create group To create a new group. More details see here . 3 Edit This control allows to change a list of users owning this group. More details see here . 4 Delete Delete a group. More details see here . Roles The \" Roles \" subtab shows a set of predefined system roles that couldn't be extended or reduced. Here you can grant or refuse users in a role. There is a list of CP system roles: Role Description ROLE_ADMIN The user gets Read/Write/Execute/Owner permissions to all objects in the system. Note : The owner of the object can manage its Access Control List. OWNER property is assigned to a user has created an object by default. ROLE_USER basic user. ROLE_ADVANCED_USER ROLE_ANONYMOUS_USER specific role for the ability of sharing interactive runs endpoints to the anonymous users (users, who have successfully passed the IdP authentication but is not registered in the Cloud Pipeline). ROLE_BILLING_MANAGER users who are granted this role are able to view the whole Billing reports info of the platform (as if they were admins). ROLE_CONFIGURATION_MANAGER allows to create/delete Cluster Configurations (given to each user by default). ROLE_DTS_MANAGER ROLE_ENTITIES_MANAGER allows to create/delete Entities . ROLE_FOLDER_MANAGER allows to create/delete Folders (given to each user by default). ROLE_PIPELINE_MANAGER allows to create/delete Pipelines (given to each user by default). ROLE_STORAGE_MANAGER allows to create/delete Data Storages . ROLE_TOOL_GROUP_MANAGER allows to create/delete Tool groups . ROLE_USER_READER allows read-only access to the users, groups, roles information and export users feature. Set of user's roles combined with permission settings defines allowed actions for the user and therefore the layout of GUI buttons. A user sees GUI options in appliance with his rights. Note : roles 6-12 are being checked if a user has WRITE permission for the parent object. Roles tab controls Control Description Search field To search particular group from a list of roles, start to enter the role name (see the picture above, 1 ). Edit Allows changing a list of users assigned the role (see the picture above, 2 ). More details see here . Usage report The \" Usage report \" subtab shows the Platform's statistics of users activity. At this subtab, the summary info about total count of Platform users that were online at different time moments during the certain period is displayed in a chart form: Usage report tab controls Control Description 1 Calendar control To select a specific day/month from the calendar. 2 Period selector To select the type of period of view - day ( by default ) or month. 3 User filter To restrict the displayed data by specific user(s) or user group(s)/role(s). Multiselect is supported Day period By default, the day report is generated for the today (see example on the picture above). You can select another day to view statistics by the Calendar control. Report contains the chart with the average count of the Platform users that were online in the selected day - by hours division. By click any point - info tooltip appears including the time moment and the full list of users that were online in this moment, e.g.: Month period By default, the month report is generated for the current month. You can select another month to view statistics by the Calendar control. There are 2 possible views for the month period: Average ( default ) - chart shows the number of the Platform online users by days division in the selected month. For each day, the count of online users is being calculated as median value between all hourly values: Unique - chart shows the summary number of the unique Platform online users by days division during the selected month. For each day, the count of users is being calculated as summary accumulative count of the unique Platform online users at the end of the day: By click any point - info tooltip appears including the date and the full list of unique users that were online in this day, e.g.: Email notifications This tab is visible only for users with the ROLE_ADMIN role. The email notifications helps to keep track of what's happening in the Cloud Pipeline. On the left you can see a list of the email notification templates. Email notifications tab controls Control Descriptions Enabled checkbox If set, email distribution of the selected type will be enabled. Keep admins informed checkbox If set, all emails with such type will be sent to all users with ROLE_ADMIN role. Keep owners informed checkbox If set, all emails with such type will be sent to the OWNERS of the corresponding Cloud Pipeline objects. Informed users text field Select users that will get such email types. Threshold text field Amount of seconds that is required for the process to generate email. Resend delay text field Amount of seconds that is required for the process to generate a repeat email notification on that subject. Subject text field Email notification subject. Body text field Body of the email notification. Revert button Return an email settings to the previous unsaved state. Save button Saves current email notification settings. Also you can switch from the Edit to the Preview mode to see how the Subject and the Body of the email notification will actually look: Templates Note : this is the current list of notification templates. It might be extended in the future. Notification type Description BILLING_QUOTA_EXCEEDING tells that billing quota is higher than a threshold value DATASTORAGE_LIFECYCLE_ACTION tells that datastorage lifecycle event (data transition) is approaching or has happened DATASTORAGE_LIFECYCLE_RESTORE_ACTION tells that datastorage lifecycle event (data restoring) has happened FULL_NODE_POOL tells that all nodes of some node pool(s) are already in use HIGH_CONSUMED_RESOURCES tells that memory or disk consuming is higher than a threshold value for a specific period of time IDLE_RUN tells that the job is idle for a long time IDLE_RUN_PAUSED tells that the job was paused because it was idle for a long time IDLE_RUN_STOPPED tells that the job was stopped because it was idle for a long time INACTIVE_USERS tells that some users are not active for a long period of time LDAP_BLOCKED_USERS tells that specific users were blocked in the Platform due to their blocking in LDAP LONG_INIT tells that the job is initializing for a long time LONG_PAUSED tells that the job has been paused for a long time LONG_PAUSED_STOPPED tells that the job, that has been paused for a long time, is stopped at the moment LONG_RUNNING tells that the job is running for a long time LONG_STATUS tells that the job is been waiting in a certain status for a long time NEW_ISSUE notifies about new issue NEW_ISSUE_COMMENT tells that an issue was commented PIPELINE_RUN_STATUS notifies about current pipeline status STORAGE_QUOTA_EXCEEDING tells that a storage has reached volume consumption threshold Preferences This tab is visible only for users with the ROLE_ADMIN role. The Preferences tab contains different global settings for the Cloud Pipeline. These settings determine default behavior of the Cloud Pipeline. On the left you can see a set of sections. Each section contains a list of global settings. See more information here . Cloud Regions This tab is visible only for users with the ROLE_ADMIN role. The Cloud Regions tab contains different settings for the specific Cloud Regions and Cloud Providers. You could manage regions, add or remove them from the Cloud Pipeline. System dictionaries This tab is visible only for users with the ROLE_ADMIN role. The System dictionaries tab contains the list of predefined \"dictionaries\" - sets of \"key-value(s)\" that could be used by admins when configuring Cloud Pipeline objects attributes (metadata). For example, admins have to set attributes (metadata) for \"general\" users manually. In case, when such metadata keys aren't different for each user and has certain amount of values, it is convenient to select these values from the predefined values list, not to specify them manually each time. Each dictionary is the categorical attribute. I.e. it is attribute which values are predefined. Each dictionary has its name and values: If the dictionary exists in the system, then admin can use it when specifying attributes for any Platform object ( Pipeline , Folder , Storage , Project , Tool ), and also for User , Group or Role . In this case, it is enough to specify only the dictionary name as the attribute key, the list of dictionary values will appear automatically in the value field: Also, the different dictionaries may be connected (linked). I.e. admins can create two dictionaries, which values are mapped 1-1 or 1-many , e.g.: In the GUI, such connection is being handled in the following way: Admin specifies the links between the dictionaries items (e.g. for the example above ProjectID : BRCA1 -> DataStorage : <path> ). Links have the \"autofill\" attribute. If the admin selects the source key ( ProjectID : BRCA1 ) as attribute key for any object - the destination key will be specified automatically ( DataStorage will be added with the <path> selection): For more details see here . Note : all functionality described above is available only for admins. \"General\" users can't view/edit System Dictionaries. If non-admin user tries to specify any existing dictionary as the attribute \"Key\" - it will be displayed as plain text attribute. No dictionary values will be loaded/displayed. System management This tab is visible only for users with the ROLE_ADMIN role. The System management tab includes several possibilities to configure/observe the Platform deployment in general. System logs The System logs subtab contains the following audit trail events: users' authentication attempts users' profiles modifications platform objects' permissions management access to interactive applications from pipeline runs access to the data stored in the object storages management of the transition rules for storages lifecycle other platform functionality features For more details see here . NAT gateway The NAT gateway subtab allows to configure network routes. This is nessasary when the Cloud Pipeline Platform is deployed in some private subnet and the admin needs to expose a network endpoint for some service. Via this form, admins can create/remove network routes from private subnet(s) to the external network resources. The NAT gateway subtab contains: the list of existing routes (port forwarding map). Each route record contains: info about external resources info about corresponding internal config (mapping details) comment controls to manage the routes list (add/remove/save etc.) For more details see here . My profile The My profile tab allows: to view main info about the current user to configure the view of the Platform pages (for the current user) Profile The Profile subtab allows to view the following info for the current loginned user: registration user info (username, name, last name, etc.) roles assigned to the user user attributes (metadata) Appearance The Appearance subtab allows to select the UI theme(s) - to configure the view of the Platform pages for the current user. User can select a single theme that will always shown: Or configure synchronization with system preferences and automatically switch between day and night themes:","title":"12.0. Overview"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#12-manage-settings","text":"System Settings consist of CLI , System events , User management , Email notifications , Preferences , Cloud Regions and System logs tabs. To open System Settings click the gear icon on the main menu in the left side of the Cloud Pipeline application: CLI tab Pipe CLI Git CLI System events User management Users Groups Roles Usage report Email notifications Preferences Cloud Regions System dictionaries System management System logs NAT gateway My profile Profile Appearance","title":"12. Manage Settings"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#cli-tab","text":"\" CLI \" tab generates two types of CLI installation and configuration commands to set CLI for the Cloud Pipeline - Pipe CLI and Git CLI . You can select each of them by click the corresponding option in the \" CLI \" tab menu.","title":"CLI tab"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#pipe-cli","text":"Control Description Operation system Choose an operation system from drop-down list and the instruction how to install the Cloud Pipeline CLI will appear in the window below. Generate access key Generates access token to be used by CLI. Valid till A date access key expires. For more details see 14.1. Install and setup CLI .","title":"Pipe CLI"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#git-cli","text":"Here you can see instructions how to configure your Git client to work with the Cloud Pipeline.","title":"Git CLI"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#system-events","text":"This tab is visible only for users with the ROLE_ADMIN role. System events tab represents system events notifications. Here you can create, edit, delete system events notifications. System events notifications are organized into a table. It represents the body of the notification , its severity status (\" info \", \" warning \" or \" critical \") and date of creation , activity status . Note : Variants of activity status: Blocking event emerges in the middle of the window and requires confirmation from the user to disappear. Active notifications will be shown for all users of the Cloud Pipeline until admin sets them inactive. Administrator can edit and delete notifications via corresponding buttons.","title":"System events"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#system-events-controls","text":"Controls are at the top right of the table. Control Description 1 Expand/Collapse This button shows/hides the body of the event. 2 Refresh To refresh a list of notifications. 3 + ADD To create a new notification. 4 Edit To open the edit form for the event. 5 Delete To delete an event. For more detais see 12.1. Add a new system event .","title":"System events controls"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#user-management","text":"This tab is visible only for users with the ROLE_ADMIN role or the ROLE_USER_READER role. The User management tab helps to manage user groups and system roles. To grant or refuse permissions to a specific group of users (e.g. project team members), you can just create a user group and grant or refuse permissions to the specific set of objects to the whole group. System roles is one of the principal tool for managing security access to the objects. Even if you have WRITE permission for a folder object, you might be not able to create a pipeline there, if you don't have the ROLE_PIPELINE_MANAGER role. Note : About permissions, you can read more here . User management consists of the following subtabs: Users , Groups , Roles , Usage report .","title":"User management"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#users","text":"This table view displays a list of users and their additional information: Name - an authenticated domain account (SAML/OAuth/OpenID), e.g. e-mail. Groups - a set of groups assigned to a user. It could be whether CP's user's groups and groups, given to each user automatically by SSO authentication system . Note : automatically created groups based on SSO authentication system are light-grey colored. Roles - a set of system roles assigned to a user. Additionally, near the name user's state is shown. Possible states: Online - for users who are logged in and use the Platform in the moment Offline - for users who are not logged in at the moment/do not use the Platform for some time By hover over the Offline icons - the tooltip will be shown that contains the info when the specific user has utilized the Platform the last time, e.g.: Please note, that users' states are visible only for user with the ROLE_ADMIN role.","title":"Users"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#users-tab-controls","text":"Control Description 1 Search field Field to search a particular user from a list of users. 2 Show users This control allows to filter which users to show in the list: all ( by default ) only blocked users. More details about blocking/unblocking see here only online users - i.e. users that are working at the Platform in the current moment only users that exceeded any spending quota 3 + Create user By click this button, the \"Create user\" form will be opened - to create a new user. More details see here . 4 Import users Allows to import users from the list in .csv format. 5 Export users Allows to export all users with selected set of attributes. More details see below . 6 Edit Allows to change a list of roles/groups assigned to a user. More details see here .","title":"Users tab controls"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#export-users","text":"The Export users button allows administrator to export user list in .csv format. There are 2 export options: Default configuration In this case, the file with all users and full list of their properties ( ID , username , attributes (first and last name, email), registration date , first login date , list of groups , list of roles , blocked / unblocked state, default data storage ) will be downloaded to the local workstation. To export full user list with default configuration: click the Export users button in the right upper corner of the Users tab in the User management dashboard or hover the v button next to the Export users button and click the Default configuration item in the appeared drop-down menu Custom configuration Custom configuration allows admins to select which user properties should be downloaded: hover the v button next to the Export users button and click the Custom configuration item in the appeared drop-down menu the modal window with the list of available user properties to export will appear, e.g.: User can select any set of attributes by marking/unmarking corresponding checkboxes. At least 1 checkbox should be marked to export user list. \"Fields\" section contains the list of all users' attributes (by default, all attributes are selected). \"Metadata\" section contains the list of all users' metadata attributes (by default, all attributes are deselected). Note : enabled Header checkbox adds the headers row into the exporting file. to download the result file with full user list and custom set of their properties click the Download CSV button. Note : not all user's metadata have to be exported. Some of them may contains the sensitive data. To restrict the list of the metadata to export the following system preference should be used - misc.metadata.sensitive.keys . In this preference, all metadata keys that shouldn't be exported are being specified (i.e. if these metadata keys are specified in the preference, they will not be displayed at the export pop-up for selection). Example of that configured preference:","title":"Export users"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#groups","text":"The \"Groups\" subtab shows a set of user groups created in CP. Here you can grant or refuse users in a group membership. Note that this tab displays groups created in CP only, not given by SSO authentication system .","title":"Groups"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#groups-tab-controls","text":"Control Description 1 Search field To search a particular group from a list of groups. 2 + Create group To create a new group. More details see here . 3 Edit This control allows to change a list of users owning this group. More details see here . 4 Delete Delete a group. More details see here .","title":"Groups tab controls"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#roles","text":"The \" Roles \" subtab shows a set of predefined system roles that couldn't be extended or reduced. Here you can grant or refuse users in a role. There is a list of CP system roles: Role Description ROLE_ADMIN The user gets Read/Write/Execute/Owner permissions to all objects in the system. Note : The owner of the object can manage its Access Control List. OWNER property is assigned to a user has created an object by default. ROLE_USER basic user. ROLE_ADVANCED_USER ROLE_ANONYMOUS_USER specific role for the ability of sharing interactive runs endpoints to the anonymous users (users, who have successfully passed the IdP authentication but is not registered in the Cloud Pipeline). ROLE_BILLING_MANAGER users who are granted this role are able to view the whole Billing reports info of the platform (as if they were admins). ROLE_CONFIGURATION_MANAGER allows to create/delete Cluster Configurations (given to each user by default). ROLE_DTS_MANAGER ROLE_ENTITIES_MANAGER allows to create/delete Entities . ROLE_FOLDER_MANAGER allows to create/delete Folders (given to each user by default). ROLE_PIPELINE_MANAGER allows to create/delete Pipelines (given to each user by default). ROLE_STORAGE_MANAGER allows to create/delete Data Storages . ROLE_TOOL_GROUP_MANAGER allows to create/delete Tool groups . ROLE_USER_READER allows read-only access to the users, groups, roles information and export users feature. Set of user's roles combined with permission settings defines allowed actions for the user and therefore the layout of GUI buttons. A user sees GUI options in appliance with his rights. Note : roles 6-12 are being checked if a user has WRITE permission for the parent object.","title":"Roles"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#roles-tab-controls","text":"Control Description Search field To search particular group from a list of roles, start to enter the role name (see the picture above, 1 ). Edit Allows changing a list of users assigned the role (see the picture above, 2 ). More details see here .","title":"Roles tab controls"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#usage-report","text":"The \" Usage report \" subtab shows the Platform's statistics of users activity. At this subtab, the summary info about total count of Platform users that were online at different time moments during the certain period is displayed in a chart form:","title":"Usage report"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#usage-report-tab-controls","text":"Control Description 1 Calendar control To select a specific day/month from the calendar. 2 Period selector To select the type of period of view - day ( by default ) or month. 3 User filter To restrict the displayed data by specific user(s) or user group(s)/role(s). Multiselect is supported","title":"Usage report tab controls"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#day-period","text":"By default, the day report is generated for the today (see example on the picture above). You can select another day to view statistics by the Calendar control. Report contains the chart with the average count of the Platform users that were online in the selected day - by hours division. By click any point - info tooltip appears including the time moment and the full list of users that were online in this moment, e.g.:","title":"Day period"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#month-period","text":"By default, the month report is generated for the current month. You can select another month to view statistics by the Calendar control. There are 2 possible views for the month period: Average ( default ) - chart shows the number of the Platform online users by days division in the selected month. For each day, the count of online users is being calculated as median value between all hourly values: Unique - chart shows the summary number of the unique Platform online users by days division during the selected month. For each day, the count of users is being calculated as summary accumulative count of the unique Platform online users at the end of the day: By click any point - info tooltip appears including the date and the full list of unique users that were online in this day, e.g.:","title":"Month period"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#email-notifications","text":"This tab is visible only for users with the ROLE_ADMIN role. The email notifications helps to keep track of what's happening in the Cloud Pipeline. On the left you can see a list of the email notification templates.","title":"Email notifications"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#email-notifications-tab-controls","text":"Control Descriptions Enabled checkbox If set, email distribution of the selected type will be enabled. Keep admins informed checkbox If set, all emails with such type will be sent to all users with ROLE_ADMIN role. Keep owners informed checkbox If set, all emails with such type will be sent to the OWNERS of the corresponding Cloud Pipeline objects. Informed users text field Select users that will get such email types. Threshold text field Amount of seconds that is required for the process to generate email. Resend delay text field Amount of seconds that is required for the process to generate a repeat email notification on that subject. Subject text field Email notification subject. Body text field Body of the email notification. Revert button Return an email settings to the previous unsaved state. Save button Saves current email notification settings. Also you can switch from the Edit to the Preview mode to see how the Subject and the Body of the email notification will actually look:","title":"Email notifications tab controls"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#templates","text":"Note : this is the current list of notification templates. It might be extended in the future. Notification type Description BILLING_QUOTA_EXCEEDING tells that billing quota is higher than a threshold value DATASTORAGE_LIFECYCLE_ACTION tells that datastorage lifecycle event (data transition) is approaching or has happened DATASTORAGE_LIFECYCLE_RESTORE_ACTION tells that datastorage lifecycle event (data restoring) has happened FULL_NODE_POOL tells that all nodes of some node pool(s) are already in use HIGH_CONSUMED_RESOURCES tells that memory or disk consuming is higher than a threshold value for a specific period of time IDLE_RUN tells that the job is idle for a long time IDLE_RUN_PAUSED tells that the job was paused because it was idle for a long time IDLE_RUN_STOPPED tells that the job was stopped because it was idle for a long time INACTIVE_USERS tells that some users are not active for a long period of time LDAP_BLOCKED_USERS tells that specific users were blocked in the Platform due to their blocking in LDAP LONG_INIT tells that the job is initializing for a long time LONG_PAUSED tells that the job has been paused for a long time LONG_PAUSED_STOPPED tells that the job, that has been paused for a long time, is stopped at the moment LONG_RUNNING tells that the job is running for a long time LONG_STATUS tells that the job is been waiting in a certain status for a long time NEW_ISSUE notifies about new issue NEW_ISSUE_COMMENT tells that an issue was commented PIPELINE_RUN_STATUS notifies about current pipeline status STORAGE_QUOTA_EXCEEDING tells that a storage has reached volume consumption threshold","title":"Templates"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#preferences","text":"This tab is visible only for users with the ROLE_ADMIN role. The Preferences tab contains different global settings for the Cloud Pipeline. These settings determine default behavior of the Cloud Pipeline. On the left you can see a set of sections. Each section contains a list of global settings. See more information here .","title":"Preferences"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#cloud-regions","text":"This tab is visible only for users with the ROLE_ADMIN role. The Cloud Regions tab contains different settings for the specific Cloud Regions and Cloud Providers. You could manage regions, add or remove them from the Cloud Pipeline.","title":"Cloud Regions"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#system-dictionaries","text":"This tab is visible only for users with the ROLE_ADMIN role. The System dictionaries tab contains the list of predefined \"dictionaries\" - sets of \"key-value(s)\" that could be used by admins when configuring Cloud Pipeline objects attributes (metadata). For example, admins have to set attributes (metadata) for \"general\" users manually. In case, when such metadata keys aren't different for each user and has certain amount of values, it is convenient to select these values from the predefined values list, not to specify them manually each time. Each dictionary is the categorical attribute. I.e. it is attribute which values are predefined. Each dictionary has its name and values: If the dictionary exists in the system, then admin can use it when specifying attributes for any Platform object ( Pipeline , Folder , Storage , Project , Tool ), and also for User , Group or Role . In this case, it is enough to specify only the dictionary name as the attribute key, the list of dictionary values will appear automatically in the value field: Also, the different dictionaries may be connected (linked). I.e. admins can create two dictionaries, which values are mapped 1-1 or 1-many , e.g.: In the GUI, such connection is being handled in the following way: Admin specifies the links between the dictionaries items (e.g. for the example above ProjectID : BRCA1 -> DataStorage : <path> ). Links have the \"autofill\" attribute. If the admin selects the source key ( ProjectID : BRCA1 ) as attribute key for any object - the destination key will be specified automatically ( DataStorage will be added with the <path> selection): For more details see here . Note : all functionality described above is available only for admins. \"General\" users can't view/edit System Dictionaries. If non-admin user tries to specify any existing dictionary as the attribute \"Key\" - it will be displayed as plain text attribute. No dictionary values will be loaded/displayed.","title":"System dictionaries"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#system-management","text":"This tab is visible only for users with the ROLE_ADMIN role. The System management tab includes several possibilities to configure/observe the Platform deployment in general.","title":"System management"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#system-logs","text":"The System logs subtab contains the following audit trail events: users' authentication attempts users' profiles modifications platform objects' permissions management access to interactive applications from pipeline runs access to the data stored in the object storages management of the transition rules for storages lifecycle other platform functionality features For more details see here .","title":"System logs"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#nat-gateway","text":"The NAT gateway subtab allows to configure network routes. This is nessasary when the Cloud Pipeline Platform is deployed in some private subnet and the admin needs to expose a network endpoint for some service. Via this form, admins can create/remove network routes from private subnet(s) to the external network resources. The NAT gateway subtab contains: the list of existing routes (port forwarding map). Each route record contains: info about external resources info about corresponding internal config (mapping details) comment controls to manage the routes list (add/remove/save etc.) For more details see here .","title":"NAT gateway"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#my-profile","text":"The My profile tab allows: to view main info about the current user to configure the view of the Platform pages (for the current user)","title":"My profile"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#profile","text":"The Profile subtab allows to view the following info for the current loginned user: registration user info (username, name, last name, etc.) roles assigned to the user user attributes (metadata)","title":"Profile"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#appearance","text":"The Appearance subtab allows to select the UI theme(s) - to configure the view of the Platform pages for the current user. User can select a single theme that will always shown: Or configure synchronization with system preferences and automatically switch between day and night themes:","title":"Appearance"},{"location":"manual/13_Permissions/13._Permissions/","text":"13. Permissions Overview Owner property How to change an owner Admin role Permission settings Overview Security Policies Scheme is organized by 2 principal tools: groups (user groups and system roles) and Access Control List defined for each CP's object. Note : About groups and system roles you can read more here . Object's Access Control List specifies who can work with the object and what he can do with it. It is defined as a pair of attributes: a User or User Group ID Permissions The permission settings are divided into the following options which can be combined for the object: Read Write Execute Below is a mapping of the objects' possible actions to permissions which demonstrates what actions will be allowed or denied to a user or user group. Note : according to Security Policies Scheme WRITE permission is not enough to add/delete any Cloud Pipeline object. A specific *_MANAGER role is required also (about roles see here ). Object User Action Permission Folder View folder Read List folder contents Create object (e.g folder, pipeline, etc.) Write Delete folder Rename folder Change parent Upload metadata Pipeline *Permissions for a pipeline version are inherited from the pipeline View pipeline Read List pipeline attributes Delete a pipeline Write Edit pipeline attributes Change parent Run a pipeline Execute DataStorage *Permissions for files in data storage are inherited from the data storage View datastorage Read List datastorage contents Delete a datastorage Write Edit datastorage attributes/contents Change parent Pipeline run View runs Inherited from a run pipeline View run logs Launch a pipeline Stop a run Rerun Tool run View runs Admin and Owner only View run logs Launch a pipeline Stop a run Rerun Cluster node View a cluster Inherited from a currently assigned run View node details Terminate a node Docker Registry View registry Read Add registry Write Delete registry Edit registry attributes Run a child tool Execute Tool group View tool group Read Create tool group Write Edit tool group Delete tool group Run a child tool Execute Tool View enabled tool Read View disabled tools list Edit tool attributes Write Run tool without a pipeline Execute Instance management Admin and Owner only Run configuration View run configuration Read Delete a run configuration Write Edit run configuration attributes Change parent Run a run configuration Execute Note : also you can manage permissions on the Cloud Pipeline objects via CLI. See 14.7. View and manage Permissions via CLI . Owner property Each object has an additional \"Owner\" property. The owner of the object can manage its Access Control List. Owner property is assigned to a user that created an object. How to change an owner The Owner of an object can be changed easily for: Folders; Pipelines and pipelines versions; Data storages; Run configurations; Docker registries, Tool Groups and Tools. Note : you shall have Owner or Admin role. To change an owner of an object: Select an object. Click \"Gear\" icon in the top-right corner of the screen. Navigate to Permissions tab. Note : To edit permissions: for a Folder - click the \"Gear\" icon \u2192 Edit folder for a Docker registry - click the \"Gear\" icon \u2192 Registry \u2192 Edit . for a Tool group - click the \"Gear\" icon \u2192 Group \u2192 Edit for a Tool - click the \"Gear\" icon \u2192 Permissions . Click owner's name. Now you can edit it: Start to enter a desired username and system will suggest you the existing users. Click the desired username. Click \"Apply\" control and the changes will be saved. Also you can change an owner of an object via pipe CLI - see here . Admin role Admin property can be given by assigning ROLE_ADMIN to a user (about roles see here ). The user gets Read/Write/Execute/Owner permissions to all objects in the system. Initially, a user with ROLE_ADMIN shall be an authenticated domain account (SAML/OAuth/OpenID) defined during a system deployment (in some properties file or database). Permission settings The permissions could be granted to a user in one of the following ways: the system has a \"default\" system role or user group. This type of system roles or groups assigned by default once a user is created; assigned user groups or system role where every member has the same permissions for specific objects; granted permissions for specific user. The priority of permissions granted for specific object explicitly is higher than the group or role permissions, e.g. if a basic user is included into the group that doesn't have an access to some folder but he has permissions explicitly defined for himself that allow him to work with that folder, he will have an access to it. To assign object's permissions to a user or user group you shall move to the object's page and click the \"Gear\" icon in the top-right corner of the screen and select \"Permissions\" tab. Note : To edit permissions: for a Folder - click the \"Gear\" icon \u2192 Edit folder for a Docker registry - click the \"Gear\" icon \u2192 Registry \u2192 Edit . for a Tool group - click the \"Gear\" icon \u2192 Group \u2192 Edit for a Tool - click the \"Gear\" icon \u2192 Permissions . You can explicitly define permissions for the object for a particular user or group of users (users within the same Group) in the \"Permission\" form by clicking on its name in the \"Groups and users\" list. Note : if you couldn't find the desired user or user group, you can add it via \"Add a user\" and \"Add a user group\" controls (see the picture below, 1 ). The additional section for a particular user or user group suggests you tick the desired grants. Here you can allow or deny specific permission options (see the picture above, 2 ). Note : if you don't tick any possible variant, it will be inherited from the parent object (e.g. a pipeline in a folder inherits permissions from it) (see the picture above, 3 ). Example 1: according to the picture above, a user will get WRITE and EXECUTE permissions for an object, and the READ permission will be inherited from the parent object. Example 2: on the picture below we see Permission form of a run configuration. We grant a user READ and EXECUTE permissions, but deny WRITE permission. So the user is able to see the run configuration and run it, but he can not edit its parameters:","title":"13. Permissions"},{"location":"manual/13_Permissions/13._Permissions/#13-permissions","text":"Overview Owner property How to change an owner Admin role Permission settings","title":"13. Permissions"},{"location":"manual/13_Permissions/13._Permissions/#overview","text":"Security Policies Scheme is organized by 2 principal tools: groups (user groups and system roles) and Access Control List defined for each CP's object. Note : About groups and system roles you can read more here . Object's Access Control List specifies who can work with the object and what he can do with it. It is defined as a pair of attributes: a User or User Group ID Permissions The permission settings are divided into the following options which can be combined for the object: Read Write Execute Below is a mapping of the objects' possible actions to permissions which demonstrates what actions will be allowed or denied to a user or user group. Note : according to Security Policies Scheme WRITE permission is not enough to add/delete any Cloud Pipeline object. A specific *_MANAGER role is required also (about roles see here ). Object User Action Permission Folder View folder Read List folder contents Create object (e.g folder, pipeline, etc.) Write Delete folder Rename folder Change parent Upload metadata Pipeline *Permissions for a pipeline version are inherited from the pipeline View pipeline Read List pipeline attributes Delete a pipeline Write Edit pipeline attributes Change parent Run a pipeline Execute DataStorage *Permissions for files in data storage are inherited from the data storage View datastorage Read List datastorage contents Delete a datastorage Write Edit datastorage attributes/contents Change parent Pipeline run View runs Inherited from a run pipeline View run logs Launch a pipeline Stop a run Rerun Tool run View runs Admin and Owner only View run logs Launch a pipeline Stop a run Rerun Cluster node View a cluster Inherited from a currently assigned run View node details Terminate a node Docker Registry View registry Read Add registry Write Delete registry Edit registry attributes Run a child tool Execute Tool group View tool group Read Create tool group Write Edit tool group Delete tool group Run a child tool Execute Tool View enabled tool Read View disabled tools list Edit tool attributes Write Run tool without a pipeline Execute Instance management Admin and Owner only Run configuration View run configuration Read Delete a run configuration Write Edit run configuration attributes Change parent Run a run configuration Execute Note : also you can manage permissions on the Cloud Pipeline objects via CLI. See 14.7. View and manage Permissions via CLI .","title":"Overview"},{"location":"manual/13_Permissions/13._Permissions/#owner-property","text":"Each object has an additional \"Owner\" property. The owner of the object can manage its Access Control List. Owner property is assigned to a user that created an object.","title":"Owner property"},{"location":"manual/13_Permissions/13._Permissions/#how-to-change-an-owner","text":"The Owner of an object can be changed easily for: Folders; Pipelines and pipelines versions; Data storages; Run configurations; Docker registries, Tool Groups and Tools. Note : you shall have Owner or Admin role. To change an owner of an object: Select an object. Click \"Gear\" icon in the top-right corner of the screen. Navigate to Permissions tab. Note : To edit permissions: for a Folder - click the \"Gear\" icon \u2192 Edit folder for a Docker registry - click the \"Gear\" icon \u2192 Registry \u2192 Edit . for a Tool group - click the \"Gear\" icon \u2192 Group \u2192 Edit for a Tool - click the \"Gear\" icon \u2192 Permissions . Click owner's name. Now you can edit it: Start to enter a desired username and system will suggest you the existing users. Click the desired username. Click \"Apply\" control and the changes will be saved. Also you can change an owner of an object via pipe CLI - see here .","title":"How to change an owner"},{"location":"manual/13_Permissions/13._Permissions/#admin-role","text":"Admin property can be given by assigning ROLE_ADMIN to a user (about roles see here ). The user gets Read/Write/Execute/Owner permissions to all objects in the system. Initially, a user with ROLE_ADMIN shall be an authenticated domain account (SAML/OAuth/OpenID) defined during a system deployment (in some properties file or database).","title":"Admin role"},{"location":"manual/13_Permissions/13._Permissions/#permission-settings","text":"The permissions could be granted to a user in one of the following ways: the system has a \"default\" system role or user group. This type of system roles or groups assigned by default once a user is created; assigned user groups or system role where every member has the same permissions for specific objects; granted permissions for specific user. The priority of permissions granted for specific object explicitly is higher than the group or role permissions, e.g. if a basic user is included into the group that doesn't have an access to some folder but he has permissions explicitly defined for himself that allow him to work with that folder, he will have an access to it. To assign object's permissions to a user or user group you shall move to the object's page and click the \"Gear\" icon in the top-right corner of the screen and select \"Permissions\" tab. Note : To edit permissions: for a Folder - click the \"Gear\" icon \u2192 Edit folder for a Docker registry - click the \"Gear\" icon \u2192 Registry \u2192 Edit . for a Tool group - click the \"Gear\" icon \u2192 Group \u2192 Edit for a Tool - click the \"Gear\" icon \u2192 Permissions . You can explicitly define permissions for the object for a particular user or group of users (users within the same Group) in the \"Permission\" form by clicking on its name in the \"Groups and users\" list. Note : if you couldn't find the desired user or user group, you can add it via \"Add a user\" and \"Add a user group\" controls (see the picture below, 1 ). The additional section for a particular user or user group suggests you tick the desired grants. Here you can allow or deny specific permission options (see the picture above, 2 ). Note : if you don't tick any possible variant, it will be inherited from the parent object (e.g. a pipeline in a folder inherits permissions from it) (see the picture above, 3 ). Example 1: according to the picture above, a user will get WRITE and EXECUTE permissions for an object, and the READ permission will be inherited from the parent object. Example 2: on the picture below we see Permission form of a run configuration. We grant a user READ and EXECUTE permissions, but deny WRITE permission. So the user is able to see the run configuration and run it, but he can not edit its parameters:","title":"Permission settings"},{"location":"manual/14_CLI/14.1._Install_and_setup_CLI/","text":"14.1. Install and setup CLI How to install and setup pipe CLI configure command options pipe configuration for using NTLM Authentication Proxy Allow to run pipe commands on behalf of the other user Using pipe token command Using '--user' option Update the CLI How to install and setup pipe CLI Go to Settings \u2192 CLI tab. Select Pipe CLI item at the left panel. Select your Operation System from the list. Follow the installation instructions for your OS (e.g. Linux). Commands below shall be executed in the Terminal. When installation is finished, type pipe in the Terminal to test pipe installation. This command shall produce short description of pipe CLI and pipe CLI commands (it is the same as the pipe --help command execution). Then type the pipe --version command - this command will output only the Cloud Pipeline CLI version: Return to the Web GUI and press the Generate access key button. Copy CLI configure command Paste copied command into the Terminal and run it to configure: Now Cloud Pipeline CLI is ready to use. To check it, run the command pipe --version again: This time the command will output the Cloud Pipeline CLI version, API version, short info about received access token (the user-owner name, usage dates) Note : when pipe CLI is being configured JWT token is given for one month, if user didn't select another expiration date. The warning about the expiration date of the provided token is printed, if it is less than 7 days left: after pipe configure command executing: when any other command is running, e.g.: Note : If any exceptions occur during installation, follow the instructions in the Terminal. Notice that Python 2 / Python 3 has to be installed to run CLI. Python can be downloaded here https://www.python.org/downloads/ . Note : pip package manager is required for CLI installation if you selected Operation System \u2192 Other on step 2. Modern Python versions come bundled with pip . On top of that, with this type of installation you'll also need internet connection to install dependencies. configure command options Options Description Required options -a / --auth-token Token for API authentication -s / --api URL of a Pipeline API endpoint -tz / --timezone [local|utc] Sets presentation timezone. Default: local Non-required options -p / --proxy URL of a proxy for all calls -nt / --proxy-ntlm Enables NTLM proxy support -nu / --proxy-ntlm-user Sets username for NTLM proxy authorization -np / --proxy-ntlm-pass Sets password for NTLM proxy authorization -nd / --proxy-ntlm-domain Sets domain for NTLM proxy authorization Note : there is not necessary to set all options while input that command. If some options are not set directly - user shall be prompted for them in an interactive manner, if they will not be set in an interactive manner, default values will be used (where it is possible). pipe configuration for using NTLM Authentication Proxy CLI pipe can be configured for using NTLM Authentication Proxy, when running in Linux. For that, use the ntlm options described above while execute pipe configure command. If pipe configure command is executing with specified --proxy-ntlm option, pipe will try to get the proxy value from the --proxy option or the environment variables ( --proxy option has a higher priority). Example: pipe configure --proxy-ntlm --proxy \"http://myproxy:3128\" Allow to run pipe commands on behalf of the other user Note : the functionality is able only for users with the ROLE_ADMIN role. It could be convenient/useful for administrators to perform some operations on behalf of the other user (e.g. check permissions/act as a service account/etc.). For that in pipe CLI, there are two abilities: separate pipe token command common option -u ( --user ) for all pipe commands Using pipe token command This command prints the JWT token for a specified user. JWT token could be used manually as authentication API token with the pipe configure command (as described above ) - to configure pipe CLI on behalf of the desired user. The format of the command: pipe token <USER_ID> [OPTIONS] <USER_ID> is the name of the user account. Options Description Non-required options -d / --duration The number of days the token will be valid. If it's not set - the default value will be used, same as in the GUI For example, to get a JWT token for the user USER3 for 5 days: Using '--user' option To run separate pipe commands on behalf of the other user - the common option, that was added to all pipe commands, can be used: --user|-u <USER_ID> (where <USER_ID> is the name of the user account). Note : the option isn't available for the commands configure , --version , --help . If this option is specified - operation (command execution) will be performed using the corresponding user account. Some examples: view active runs on behalf of the user without ROLE_ADMIN role: list storage content on behalf of the user without ROLE_ADMIN role that hasn't read permission on that storage: list storage content and the attempt to upload the file on behalf of the user without ROLE_ADMIN role that has read permission and hasn't write permission on that storage: Update the CLI The command to update the Cloud Pipeline CLI version: pipe update [PATH] PATH - defines the API URL path to download Cloud Pipeline CLI source (optional argument). This command compare the CLI and API versions. If the CLI version is less than the API one, it will update the CLI - the latest Cloud Pipeline CLI version will be installed. Otherwise no actions will be performed. Example:","title":"14.1. Install and setup"},{"location":"manual/14_CLI/14.1._Install_and_setup_CLI/#141-install-and-setup-cli","text":"How to install and setup pipe CLI configure command options pipe configuration for using NTLM Authentication Proxy Allow to run pipe commands on behalf of the other user Using pipe token command Using '--user' option Update the CLI","title":"14.1. Install and setup CLI"},{"location":"manual/14_CLI/14.1._Install_and_setup_CLI/#how-to-install-and-setup-pipe-cli","text":"Go to Settings \u2192 CLI tab. Select Pipe CLI item at the left panel. Select your Operation System from the list. Follow the installation instructions for your OS (e.g. Linux). Commands below shall be executed in the Terminal. When installation is finished, type pipe in the Terminal to test pipe installation. This command shall produce short description of pipe CLI and pipe CLI commands (it is the same as the pipe --help command execution). Then type the pipe --version command - this command will output only the Cloud Pipeline CLI version: Return to the Web GUI and press the Generate access key button. Copy CLI configure command Paste copied command into the Terminal and run it to configure: Now Cloud Pipeline CLI is ready to use. To check it, run the command pipe --version again: This time the command will output the Cloud Pipeline CLI version, API version, short info about received access token (the user-owner name, usage dates) Note : when pipe CLI is being configured JWT token is given for one month, if user didn't select another expiration date. The warning about the expiration date of the provided token is printed, if it is less than 7 days left: after pipe configure command executing: when any other command is running, e.g.: Note : If any exceptions occur during installation, follow the instructions in the Terminal. Notice that Python 2 / Python 3 has to be installed to run CLI. Python can be downloaded here https://www.python.org/downloads/ . Note : pip package manager is required for CLI installation if you selected Operation System \u2192 Other on step 2. Modern Python versions come bundled with pip . On top of that, with this type of installation you'll also need internet connection to install dependencies.","title":"How to install and setup pipe CLI"},{"location":"manual/14_CLI/14.1._Install_and_setup_CLI/#configure-command-options","text":"Options Description Required options -a / --auth-token Token for API authentication -s / --api URL of a Pipeline API endpoint -tz / --timezone [local|utc] Sets presentation timezone. Default: local Non-required options -p / --proxy URL of a proxy for all calls -nt / --proxy-ntlm Enables NTLM proxy support -nu / --proxy-ntlm-user Sets username for NTLM proxy authorization -np / --proxy-ntlm-pass Sets password for NTLM proxy authorization -nd / --proxy-ntlm-domain Sets domain for NTLM proxy authorization Note : there is not necessary to set all options while input that command. If some options are not set directly - user shall be prompted for them in an interactive manner, if they will not be set in an interactive manner, default values will be used (where it is possible).","title":"configure command options"},{"location":"manual/14_CLI/14.1._Install_and_setup_CLI/#pipe-configuration-for-using-ntlm-authentication-proxy","text":"CLI pipe can be configured for using NTLM Authentication Proxy, when running in Linux. For that, use the ntlm options described above while execute pipe configure command. If pipe configure command is executing with specified --proxy-ntlm option, pipe will try to get the proxy value from the --proxy option or the environment variables ( --proxy option has a higher priority). Example: pipe configure --proxy-ntlm --proxy \"http://myproxy:3128\"","title":"pipe configuration for using NTLM Authentication Proxy"},{"location":"manual/14_CLI/14.1._Install_and_setup_CLI/#allow-to-run-pipe-commands-on-behalf-of-the-other-user","text":"Note : the functionality is able only for users with the ROLE_ADMIN role. It could be convenient/useful for administrators to perform some operations on behalf of the other user (e.g. check permissions/act as a service account/etc.). For that in pipe CLI, there are two abilities: separate pipe token command common option -u ( --user ) for all pipe commands","title":"Allow to run pipe commands on behalf of the other user"},{"location":"manual/14_CLI/14.1._Install_and_setup_CLI/#using-pipe-token-command","text":"This command prints the JWT token for a specified user. JWT token could be used manually as authentication API token with the pipe configure command (as described above ) - to configure pipe CLI on behalf of the desired user. The format of the command: pipe token <USER_ID> [OPTIONS] <USER_ID> is the name of the user account. Options Description Non-required options -d / --duration The number of days the token will be valid. If it's not set - the default value will be used, same as in the GUI For example, to get a JWT token for the user USER3 for 5 days:","title":"Using pipe token command"},{"location":"manual/14_CLI/14.1._Install_and_setup_CLI/#using-user-option","text":"To run separate pipe commands on behalf of the other user - the common option, that was added to all pipe commands, can be used: --user|-u <USER_ID> (where <USER_ID> is the name of the user account). Note : the option isn't available for the commands configure , --version , --help . If this option is specified - operation (command execution) will be performed using the corresponding user account. Some examples: view active runs on behalf of the user without ROLE_ADMIN role: list storage content on behalf of the user without ROLE_ADMIN role that hasn't read permission on that storage: list storage content and the attempt to upload the file on behalf of the user without ROLE_ADMIN role that has read permission and hasn't write permission on that storage:","title":"Using '--user' option"},{"location":"manual/14_CLI/14.1._Install_and_setup_CLI/#update-the-cli","text":"The command to update the Cloud Pipeline CLI version: pipe update [PATH] PATH - defines the API URL path to download Cloud Pipeline CLI source (optional argument). This command compare the CLI and API versions. If the CLI version is less than the API one, it will update the CLI - the latest Cloud Pipeline CLI version will be installed. Otherwise no actions will be performed. Example:","title":"Update the CLI"},{"location":"manual/14_CLI/14.10._SSH_tunnel/","text":"14.10. Create SSH tunnel to the running compute instance Overview Cloud Pipeline run instances can be accessed via SSH directly from on-premises local workstations using special network tunnels (see picture below). Such tunnels can be established between a local Windows or Linux workstation and a Cloud Pipeline run. pipe CLI provides a set of command to manage such network tunnels. Notice that pipe CLI automatically manages SSH keys and configures passwordless SSH access . As a result no manual SSH keys management is required to access Cloud Pipeline run from the local workstation. SSH tunnels to Cloud Pipeline runs can be used for interactive SSH sessions, files transferring and third-party applications which depends on SSH protocol. The command that runs ports tunnelling operations: pipe tunnel COMMAND [ARGS] Where COMMAND - one of the following commands: start <RUN_ID> - establishes tunnel connection to specified run instance port and serves it as a local port stop <RUN_ID - stops background tunnel processes with specified run start command possible options: Option Description Required options -lp / --local-port INTEGER Local port to establish connection from -rp / --remote-port INTEGER Remote port to establish connection to Non-required options -ct / --connection-timeout FLOAT Socket connection timeout in seconds -s / --ssh Configures passwordless ssh to specified run instance -sp / --ssh-path TEXT Path to \".ssh\" directory for passwordless ssh configuration on Linux -sh / --ssh-host TEXT Host name for passwordless ssh configuration -sk / --ssh-keep Keeps passwordless ssh configuration after tunnel stopping -l / --log-file TEXT Logs file for tunnel in background mode -v / --log-level TEXT Logs level for tunnel: CRITICAL, ERROR, WARNING, INFO or DEBUG -t / --timeout INTEGER Time period in ms for background tunnel process health check -f / --foreground Establishes tunnel in foreground mode -r / --retries INTEGER Number of retries to connect to specified pipeline run. Default is 10 --trace Enables error stack traces displaying stop command possible options: Option Description Non-required options -lp / --local-port INTEGER Local port to stop tunnel for -t / --timeout INTEGER Tunnels stopping timeout in ms -f / --force Killing tunnels rather than stopping them -v / --log-level TEXT Explicit logging level: CRITICAL, ERROR, WARNING, INFO or DEBUG --trace Enables error stack traces displaying Usage example Firstly, launch a tool in Cloud Pipeline . In our example we will use library/centos tool launched with default settings. How to launch a tool see here . The launched run will be used in the following sections to establish SSH tunnels to. When the tool is launched, open its Run logs page. Wait for the SSH hyperlink to appear. Once the SSH hyperlink appears then run is initialized and SSH tunnels can be established: Follow to the corresponding section below to configure local workstation to be able to connect to Cloud Pipeline run via SSH. Connect from Windows workstation The tutorial describes how to connect to Cloud Pipeline run via SSH from Windows workstation. Note : The tutorial requires PuTTY application being installed locally. Additionally plink.exe and pscp.exe CLI clients are used in the tutorial. Download both PuTTY , plink.exe and pscp.exe from the official site using this link: https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html . Open PowerShell console: press Win + R , type powershell and press Enter Install pipe CLI if it is not installed yet (see details here ) Establish SSH tunnel using the command below. In the command below 56819 is the Run ID of the tool run launched before this example execution, 4567 is just a random free local port and 22 is the Cloud Pipeline run SSH port . Additional --ssh flag enables passwordless SSH access: pipe tunnel start 56819 -lp 4567 -rp 22 --ssh Once the tunnel is established either PuTTY application or its CLI client can be used to interact with Cloud Pipeline run via SSH. Usually a name of an automatically created PuTTY session is pipeline-<RUN_ID> . Notice that no actions later on require a password. This happens because pipe CLI automatically manages SSH keys while configuring a network tunnel. Open PuTTY application and double click the corresponding session to open an interactive SSH session. There may be a delay between a tunnel establishing and a session creation. In case a corresponding session is missing please try reopen PuTTY application a little bit later: After double clicking the corresponding item in PuTTY an interactive SSH session appears: Return to PowerShell console and use the commands below to interact with Cloud Pipeline run via SSH using PuTTY CLI clients. Notice that commands below expects plink.exe and pscp.exe executables to be in the current folder. Note : use an actual Run ID rather then the example one ( 56819 ) # Open an interactive SSH session .\\plink.exe pipeline-56819 # Execute a single command via SSH .\\plink.exe pipeline-56819 -batch echo '$HOSTNAME' # Upload a local file to Cloud Pipeline run via SSH New-Item file-pscp.txt Set-Content file-pscp.txt 'Content' .\\pscp.exe file-pscp.txt pipeline-56819:/common/workdir/file-pscp.txt # Download a file from Cloud Pipeline run to a local directory via SSH .\\pscp.exe pipeline-56819:/common/workdir/file-pscp.txt file-pscp-copy.txt Get-Content file-pscp-copy.txt After, stop the SSH tunnel and the Cloud Pipeline run using the commands below. Note : use an actual Run ID rather then the example one ( 56819 ) pipe tunnel stop 56819 pipe stop 56819 Connect from Linux workstation The tutorial describes how to connect to Cloud Pipeline run via SSH from Linux workstation. The tutorial requires openssh library being installed locally. It can be installed using platform's package manager. Install pipe CLI if it is not installed yet (see details here ) Establish SSH tunnel using the command below. In the command below 75376 is the Run ID of the tool run launched before this example execution, 4567 is just a random free local port and 22 is the Cloud Pipeline run SSH port . Additional --ssh flag enables passwordless SSH access: pipe tunnel start 75376 -lp 4567 -rp 22 --ssh Once the tunnel is established either ssh or scp CLI clients can be used to interact with Cloud Pipeline run via SSH. Usually a name of an automatically created SSH host is pipeline-<RUN_ID> . Notice that no actions later on require a password. This happens because pipe CLI automatically manages SSH keys while configuring a network tunnel. Use the commands below to interact with Cloud Pipeline run via SSH using openssh default clients. Note : use an actual Run ID rather then the example one ( 75376 ) # Open an interactive SSH session ssh pipeline-75376 # Execute a single command via SSH ssh pipeline-75376 echo \\$HOSTNAME # Upload a local file to Cloud Pipeline run via SSH echo \"Content\" > file-scp.txt scp file-scp.txt pipeline-75376:/common/workdir/file-scp.txt # Download a file from Cloud Pipeline run to a local directory via SSH scp pipeline-75376:/common/workdir/file-scp.txt file-scp-copy.txt cat file-scp-copy.txt After, stop the SSH tunnel and the Cloud Pipeline run using the commands below. Note : use an actual Run ID rather then the example one ( 75376 ) pipe tunnel stop 75376 pipe stop 75376","title":"14.10. SSH tunnel"},{"location":"manual/14_CLI/14.10._SSH_tunnel/#1410-create-ssh-tunnel-to-the-running-compute-instance","text":"","title":"14.10. Create SSH tunnel to the running compute instance"},{"location":"manual/14_CLI/14.10._SSH_tunnel/#overview","text":"Cloud Pipeline run instances can be accessed via SSH directly from on-premises local workstations using special network tunnels (see picture below). Such tunnels can be established between a local Windows or Linux workstation and a Cloud Pipeline run. pipe CLI provides a set of command to manage such network tunnels. Notice that pipe CLI automatically manages SSH keys and configures passwordless SSH access . As a result no manual SSH keys management is required to access Cloud Pipeline run from the local workstation. SSH tunnels to Cloud Pipeline runs can be used for interactive SSH sessions, files transferring and third-party applications which depends on SSH protocol. The command that runs ports tunnelling operations: pipe tunnel COMMAND [ARGS] Where COMMAND - one of the following commands: start <RUN_ID> - establishes tunnel connection to specified run instance port and serves it as a local port stop <RUN_ID - stops background tunnel processes with specified run start command possible options: Option Description Required options -lp / --local-port INTEGER Local port to establish connection from -rp / --remote-port INTEGER Remote port to establish connection to Non-required options -ct / --connection-timeout FLOAT Socket connection timeout in seconds -s / --ssh Configures passwordless ssh to specified run instance -sp / --ssh-path TEXT Path to \".ssh\" directory for passwordless ssh configuration on Linux -sh / --ssh-host TEXT Host name for passwordless ssh configuration -sk / --ssh-keep Keeps passwordless ssh configuration after tunnel stopping -l / --log-file TEXT Logs file for tunnel in background mode -v / --log-level TEXT Logs level for tunnel: CRITICAL, ERROR, WARNING, INFO or DEBUG -t / --timeout INTEGER Time period in ms for background tunnel process health check -f / --foreground Establishes tunnel in foreground mode -r / --retries INTEGER Number of retries to connect to specified pipeline run. Default is 10 --trace Enables error stack traces displaying stop command possible options: Option Description Non-required options -lp / --local-port INTEGER Local port to stop tunnel for -t / --timeout INTEGER Tunnels stopping timeout in ms -f / --force Killing tunnels rather than stopping them -v / --log-level TEXT Explicit logging level: CRITICAL, ERROR, WARNING, INFO or DEBUG --trace Enables error stack traces displaying","title":"Overview"},{"location":"manual/14_CLI/14.10._SSH_tunnel/#usage-example","text":"Firstly, launch a tool in Cloud Pipeline . In our example we will use library/centos tool launched with default settings. How to launch a tool see here . The launched run will be used in the following sections to establish SSH tunnels to. When the tool is launched, open its Run logs page. Wait for the SSH hyperlink to appear. Once the SSH hyperlink appears then run is initialized and SSH tunnels can be established: Follow to the corresponding section below to configure local workstation to be able to connect to Cloud Pipeline run via SSH.","title":"Usage example"},{"location":"manual/14_CLI/14.10._SSH_tunnel/#connect-from-windows-workstation","text":"The tutorial describes how to connect to Cloud Pipeline run via SSH from Windows workstation. Note : The tutorial requires PuTTY application being installed locally. Additionally plink.exe and pscp.exe CLI clients are used in the tutorial. Download both PuTTY , plink.exe and pscp.exe from the official site using this link: https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html . Open PowerShell console: press Win + R , type powershell and press Enter Install pipe CLI if it is not installed yet (see details here ) Establish SSH tunnel using the command below. In the command below 56819 is the Run ID of the tool run launched before this example execution, 4567 is just a random free local port and 22 is the Cloud Pipeline run SSH port . Additional --ssh flag enables passwordless SSH access: pipe tunnel start 56819 -lp 4567 -rp 22 --ssh Once the tunnel is established either PuTTY application or its CLI client can be used to interact with Cloud Pipeline run via SSH. Usually a name of an automatically created PuTTY session is pipeline-<RUN_ID> . Notice that no actions later on require a password. This happens because pipe CLI automatically manages SSH keys while configuring a network tunnel. Open PuTTY application and double click the corresponding session to open an interactive SSH session. There may be a delay between a tunnel establishing and a session creation. In case a corresponding session is missing please try reopen PuTTY application a little bit later: After double clicking the corresponding item in PuTTY an interactive SSH session appears: Return to PowerShell console and use the commands below to interact with Cloud Pipeline run via SSH using PuTTY CLI clients. Notice that commands below expects plink.exe and pscp.exe executables to be in the current folder. Note : use an actual Run ID rather then the example one ( 56819 ) # Open an interactive SSH session .\\plink.exe pipeline-56819 # Execute a single command via SSH .\\plink.exe pipeline-56819 -batch echo '$HOSTNAME' # Upload a local file to Cloud Pipeline run via SSH New-Item file-pscp.txt Set-Content file-pscp.txt 'Content' .\\pscp.exe file-pscp.txt pipeline-56819:/common/workdir/file-pscp.txt # Download a file from Cloud Pipeline run to a local directory via SSH .\\pscp.exe pipeline-56819:/common/workdir/file-pscp.txt file-pscp-copy.txt Get-Content file-pscp-copy.txt After, stop the SSH tunnel and the Cloud Pipeline run using the commands below. Note : use an actual Run ID rather then the example one ( 56819 ) pipe tunnel stop 56819 pipe stop 56819","title":"Connect from Windows workstation"},{"location":"manual/14_CLI/14.10._SSH_tunnel/#connect-from-linux-workstation","text":"The tutorial describes how to connect to Cloud Pipeline run via SSH from Linux workstation. The tutorial requires openssh library being installed locally. It can be installed using platform's package manager. Install pipe CLI if it is not installed yet (see details here ) Establish SSH tunnel using the command below. In the command below 75376 is the Run ID of the tool run launched before this example execution, 4567 is just a random free local port and 22 is the Cloud Pipeline run SSH port . Additional --ssh flag enables passwordless SSH access: pipe tunnel start 75376 -lp 4567 -rp 22 --ssh Once the tunnel is established either ssh or scp CLI clients can be used to interact with Cloud Pipeline run via SSH. Usually a name of an automatically created SSH host is pipeline-<RUN_ID> . Notice that no actions later on require a password. This happens because pipe CLI automatically manages SSH keys while configuring a network tunnel. Use the commands below to interact with Cloud Pipeline run via SSH using openssh default clients. Note : use an actual Run ID rather then the example one ( 75376 ) # Open an interactive SSH session ssh pipeline-75376 # Execute a single command via SSH ssh pipeline-75376 echo \\$HOSTNAME # Upload a local file to Cloud Pipeline run via SSH echo \"Content\" > file-scp.txt scp file-scp.txt pipeline-75376:/common/workdir/file-scp.txt # Download a file from Cloud Pipeline run to a local directory via SSH scp pipeline-75376:/common/workdir/file-scp.txt file-scp-copy.txt cat file-scp-copy.txt After, stop the SSH tunnel and the Cloud Pipeline run using the commands below. Note : use an actual Run ID rather then the example one ( 75376 ) pipe tunnel stop 75376 pipe stop 75376","title":"Connect from Linux workstation"},{"location":"manual/14_CLI/14.2._View_and_manage_Attributes_via_CLI/","text":"14.2. View and manage Attributes via CLI View attributes Manage attributes Add and Edit attributes Delete attributes Cloud Pipeline CLI has to be installed. See 14.1. Install and setup CLI . View attributes To view attributes of the object you need READ permission for the object. See 13. Permissions . Command to list all tags for a specific object: pipe tag get <Object class> <Object id/name> Two parameters shall be specified: Object class - defines a name of the object class. Possible values: data_storage , docker_registry , folder , metadata_entity , pipeline , tool , tool_group , configuration . Object id or name - defines an ID or name of an object of the specified object class. Note : full path to the object has to be specified. Paths to Docker registry and Tool objects should include registry IP address. The example below lists attributes of the data storage with ID 3976 : pipe tag get data_storage 3976 To list attributes of the tool group library : Manage attributes A user has to be an administrator ( ROLE_ADMIN ) or an owner ( OWNER ) of the object to edit attributes. See 13. Permissions . A user can add new attributes, edit or delete existing attributes via CLI. Add and Edit attributes To add new and edit existing attributes the following command is used: pipe tag set <Object class> <Object id/name> <List of KEY=VALUE> Three parameters shall be specified: Object class - defines a name of the object class. Possible values: data_storage , docker_registry , folder , metadata_entity , pipeline , tool , tool_group , configuration . Object id/name - defines an ID or name of an object of the specified object class. Note : full path to the object has to be specified. Paths to Docker registry and Tool objects should include registry IP address. List of KEY=VALUE - list of tags to set. Can be specified as a single KEY=VALUE pair or a list of them. Note : if a specific tag key already exists for an object, it will be overwritten . The example below sets attributes ds_key2 = testvalue2_update and ds_key3 = testvalue3 for the data storage with ID 3976 : pipe tag set data_storage 3976 ds_key2=testvalue2_update ds_key3=testvalue3 Delete attributes To delete attributes the following command is used: pipe tag delete <Object class> <Object id/name> <List of KEYs> Three parameters shall be specified: Object class - defines a name of the object class. Possible values: data_storage , docker_registry , folder , metadata_entity , pipeline , tool , tool_group , configuration . Object id/name - defines an ID or name of an object of the specified object class. Note : full path to the object has to be specified. Paths to Docker registry and Tool objects should include registry IP address. List of KEYs - list of attribute keys to delete. The example below deletes attributes ds_key1 , ds_key3 from the the data storage with ID 3976 : pipe tag delete data_storage 3976 ds_key1 ds_key3","title":"14.2. View and manage Attributes"},{"location":"manual/14_CLI/14.2._View_and_manage_Attributes_via_CLI/#142-view-and-manage-attributes-via-cli","text":"View attributes Manage attributes Add and Edit attributes Delete attributes Cloud Pipeline CLI has to be installed. See 14.1. Install and setup CLI .","title":"14.2. View and manage Attributes via CLI"},{"location":"manual/14_CLI/14.2._View_and_manage_Attributes_via_CLI/#view-attributes","text":"To view attributes of the object you need READ permission for the object. See 13. Permissions . Command to list all tags for a specific object: pipe tag get <Object class> <Object id/name> Two parameters shall be specified: Object class - defines a name of the object class. Possible values: data_storage , docker_registry , folder , metadata_entity , pipeline , tool , tool_group , configuration . Object id or name - defines an ID or name of an object of the specified object class. Note : full path to the object has to be specified. Paths to Docker registry and Tool objects should include registry IP address. The example below lists attributes of the data storage with ID 3976 : pipe tag get data_storage 3976 To list attributes of the tool group library :","title":"View attributes"},{"location":"manual/14_CLI/14.2._View_and_manage_Attributes_via_CLI/#manage-attributes","text":"A user has to be an administrator ( ROLE_ADMIN ) or an owner ( OWNER ) of the object to edit attributes. See 13. Permissions . A user can add new attributes, edit or delete existing attributes via CLI.","title":"Manage attributes"},{"location":"manual/14_CLI/14.2._View_and_manage_Attributes_via_CLI/#add-and-edit-attributes","text":"To add new and edit existing attributes the following command is used: pipe tag set <Object class> <Object id/name> <List of KEY=VALUE> Three parameters shall be specified: Object class - defines a name of the object class. Possible values: data_storage , docker_registry , folder , metadata_entity , pipeline , tool , tool_group , configuration . Object id/name - defines an ID or name of an object of the specified object class. Note : full path to the object has to be specified. Paths to Docker registry and Tool objects should include registry IP address. List of KEY=VALUE - list of tags to set. Can be specified as a single KEY=VALUE pair or a list of them. Note : if a specific tag key already exists for an object, it will be overwritten . The example below sets attributes ds_key2 = testvalue2_update and ds_key3 = testvalue3 for the data storage with ID 3976 : pipe tag set data_storage 3976 ds_key2=testvalue2_update ds_key3=testvalue3","title":"Add and Edit attributes"},{"location":"manual/14_CLI/14.2._View_and_manage_Attributes_via_CLI/#delete-attributes","text":"To delete attributes the following command is used: pipe tag delete <Object class> <Object id/name> <List of KEYs> Three parameters shall be specified: Object class - defines a name of the object class. Possible values: data_storage , docker_registry , folder , metadata_entity , pipeline , tool , tool_group , configuration . Object id/name - defines an ID or name of an object of the specified object class. Note : full path to the object has to be specified. Paths to Docker registry and Tool objects should include registry IP address. List of KEYs - list of attribute keys to delete. The example below deletes attributes ds_key1 , ds_key3 from the the data storage with ID 3976 : pipe tag delete data_storage 3976 ds_key1 ds_key3","title":"Delete attributes"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/","text":"14.3. Manage Storage via CLI Create a datastorage List storages/storage content Show storage usage Edit a datastorage Change backup duration, STS/LTS duration, versioning Change a parent folder for a datastorage Delete a datastorage Create a folder in a storage Upload and download data Control File versions Show files versions Restore files Delete an object from a datastorage Manage datastorage objects attributes Get object attributes Set object attributes Delete object attributes Mounting of storages Mount a storage Unmount a storage Cloud Pipeline CLI has to be installed. See 14.1. Install and setup CLI . To perform different operations with object storages the command set pipe storage is used. Create a datastorage The command to create an object storage: pipe storage create [OPTIONS] Options Description Required options -n / --name Alias of the new object storage -p / --path Datastorage path Non-required options -d / --description Description of the object storage -sts / --short_term_storage Number of days for storing data in the short term storage. Note : This option is available not for all Cloud Providers -lts / --long_term_storage Number of days for storing data in the long term storage. Note : This option is available not for all Cloud Providers -v / --versioning Enable versioning for this object storage. Note : This option is available not for all Cloud Providers -b / --backup_duration Number of days for storing backups of the storage. Note : This option is available not for all Cloud Providers -t / --type Type of the Cloud for the object storage. Depends on the Cloud Provider. Possible values - S3 (for AWS), AZ (for MS Azure), GS (for GCP) and NFS (for FS mounts). Additionally, for AWS HealthOmics Storages - AWS_OMICS_REF (for reference store ) and AWS_OMICS_SEQ (for sequence store ). -f / --parent_folder Name/ID of the folder which will contain the object storage. Default value - library root folder -c / --on_cloud To create datastorage on the Cloud. This flag shall be specified, only if a new datastorage is being created. If user want to add an existing storage this flag should not be specified -r / --region_id Cloud region ID where storage shall be created Note : there is not necessary to set all options while input that command. If some options are not set - user shall be prompted for them in an interactive manner, if they will not be set in an interactive manner, default values will be used. In the example below the object storage output_results for the new datastorage output_results_storage will be created in the folder with ID 297 with the following options: STS duration - 20 days, LTS duration - 45 days, backup duration - 30 days. pipe storage create -n output-results -p output-results-storage -c -sts 20 -lts 45 -b 30 -f 297 -c flag was specify to create a storage in the Cloud. As you can see - Description , Type of the Cloud and Cloud Region ID fields were left empty. So, values for these options will be set as default. List storages/storage content The command to view the full storage list of the current platform deployment: pipe storage ls [OPTIONS] Options Description Non-required options -l / --show_details Show details Perform the following command to view full storage list and to check that the object storage from the example above was created: pipe storage ls -l The command to view the content of the datastorage: pipe storage ls [OPTIONS] <Path> Path - defines a full path to the datastorage. Path value must begin from the Cloud prefix: s3 (for AWS) / az (for MS Azure) / gs (for GCP) / common cp (instead of described ones, regardless of Provider) OR omics (for AWS HealthOmics Storages ) Options Description Non-required options -l / --show_details Show details -v / --show_versions Show object versions. Only for storages with enabled versioning -r / --recursive Recursive listing -p / --page Maximum number of records to show -a / --all Show all results at once ignoring page settings Example of the detailed datastorage content view: pipe storage ls --show_details s3://objstor Or you can use the common CP prefix instead of S3 : pipe storage ls --show_details cp://objstor Result will be the same: Show storage usage To obtain a \"disk usage\" information on the supported data storage or its inner folder(s) use the command: pipe storage du [OPTIONS] [STORAGE] The command prints for the data storages/path: summary number of files in the storage/path summary size of files in the storage/path Options Description Non-required options -p / --relative-path The relative path inside the storage -f / --format The size unit format (default: Mb ). Possible values: K , Kb , KB - for kilobytes; M , Mb , MB - for megabytes; G , Gb , GB - for gigabytes -d / --depth The maximum depth level of the nesting folders. Note : this option isn't supported for the FS storages yet STORAGE - defines the datastorage name/path. Without specifying any options and storage this command prints the full list of the available storages (both types - object and FS) with the \"usage\" information for each of them: pipe storage du With specifying the storage name this command prints the \"usage\" information only by that storage, e.g.: pipe storage du objectdatastorage With -p ( --relative-path ) option the command prints the \"usage\" information for the specified path in the required storage, e.g.: pipe storage du objectdatastorage -p innerdir1 With -d ( --depth ) option the command prints the \"usage\" information in the required storage (and path) for the specified folders nesting depth, e.g.: pipe storage du objectdatastorage -p innerdir2 -d 1 Specified command will print the \"usage\" information for all folders with the nesting depth no more 1 relative to the path objectdatastorage/innerdir2/ : With -f ( --format ) option user can change the unit for the size value, e.g. to print the storage usage in gigabytes: pipe storage du objectdatastorage -f GB Edit a datastorage Change backup duration, STS/LTS duration, versioning The Command to change backup duration, select STS/LTS duration or enable versioning: pipe storage policy [OPTIONS] Options Description Required options -n / --name Alias/path of the storage to update the policy. Specified without Cloud prefix Non-required options -sts / --short_term_storage Number of days for storing data in the short term storage. Note : This option is available not for all Cloud Providers -lts / --long_term_storage Number of days for storing data in the long term storage. Note : This option is available not for all Cloud Providers -v / --versioning Enable versioning for this object storage. Note : This option is available not for all Cloud Providers -b / --backup_duration Number of days for storing backups of the storage. Note : This option is available not for all Cloud Providers In the example below backup duration is set to 25 days, STS and LTS durations are set to 50 days and 100 days respectively for the datastorage objstor . Also, we enable versioning for that datastorage: pipe storage policy -n objstor -b 25 -sts 50 -lts 100 -v Note : there is not necessary to set all options while input that command. If some options are not set - user shall be prompted for them in an interactive manner, if they will not be set in an interactive manner, default values will be used. You can check via the GUI that parameters were changed: Change a parent folder for a datastorage The command to move a datastorage to a new parent folder: pipe storage mvtodir <Storage> <Directory> Directory - name of the folder to which the object storage will be moved. Storage - alias/path of the storage to move. Specified without Cloud prefix. In the example below we will move the storage objstor to the folder \" InnerFolder \": pipe storage mvtodir objstor InnerFolder Delete a datastorage The command to delete an object storage: pipe storage delete [OPTIONS] Options Description Required options -n / --name Alias/path of the storage to delete. Specified without Cloud prefix Non-required options -c / --on_cloud To delete datastorage from a Cloud. If this option isn't set, datastorage will just become unregistered -y / --yes Do not ask confirmation In the example below we will delete the output-results-storage without asking confirmation: pipe storage delete -n output-results-storage -y As the command above was performed withoud --on_cloud option, output-results-storage wasn't deleted from a Cloud actually and it might be added again to the Cloud Pipeline platform via described storage create command: Create a folder in a storage The command to create a folder in a storage: pipe storage mkdir <List of FOLDERs> List of FOLDERs - defines a list of the folders paths. Each path in the list shall be a full path to a new folder in the specific datastorage. Path value must begin from the Cloud prefix ( S3 (for AWS), AZ (for MS Azure), GS (for GCP) or common CP (instead of described ones, regardless of Provider)). In the example below we will create folders \" new-folder1 \", \" new-folder2 \" in the storage output-results-storage and then will check that they're exist via described storage ls command: pipe storage mkdir cp://output-results-storage/new-folder1 cp://output-results-storage/new-folder2 Upload and download data There are two commands to upload/download data: pipe storage cp [OPTIONS] <Source> <Destination> pipe storage mv [OPTIONS] <Source> <Destination> By cp command you can copy files from one datastorage to another one or between local filesystem and a datastorage (in both directions). By mv command you can move files from one datastorage to another one or between local filesystem and a datastorage (in both directions). Source - defines a path (in the local filesystem or datastorage) to the object to be copied/moved. Destination - defines a path (in the local filesystem or datastorage) to the object where source object will be copied/moved. Options Description Non-required options -r / --recursive Recursive source scan. This option is not needed when you copy/move a single file -f / --force Rewrite files in destination -e / --exclude Exclude all files matching this pattern from processing -i / --include Include only files matching this pattern into processing -q / --quiet Quiet mode -s / --skip-existing Skip files existing in destination, if they have size matching source -t / --tags Set object attributes during processing. attributes can be specified as single KEY=VALUE pair or a list of them. If this option specified all existent attributes will be overwritten -l / --file-list Path to the file with file paths that should be copied/moved. This file should be tab delimited and consist of two columns: relative path to file and size -sl / --symlinks [follow|filter|skip] Describe symlinks processing strategy for local sources. Possible values: follow - follow symlinks (default); skip - do not follow symlinks; filter - follow symlinks but check for cyclic links -a / --additional-options For cp command only . Comma-separated list of additional arguments to be used during file copy In the example below we will upload files from the local filesystem to the storage objstor into the folder \" upload \": pipe storage cp ~/data cp://objstor/upload --recursive Application will start uploading files and print progress. You can view it: After uploading is complete, check that they're uploaded - via described storage ls command: In the example below we will upload the file \" 2.fastq \" from the local filesystem in the quiet mode, rewrite it to the storage objstor into the folder \" upload \" and set two attributes on it: storage cp -f -q ~/data/2.fastq cp://objstor/upload/ -t testkey1=testvalue1 -t testkey2=testvalue2 After that, we will check attributes of the uploaded file - via storage get-object-tags command: Note : Files uploaded via CLI will have the following attributes and values automatically set: CP_OWNER . The value of the attribute will be set as a user ID. CP_SOURCE . The value of the attribute will be set as a local path used to upload. The example demonstrates automatic file tagging after data uploading: The example below demonstrates how to download files from the datastorage folder to the local filesystem. Also we will not download files which names starts from 1. : pipe storage cp -r -e 1.* cp://objstor/upload/ ~/data/input/ Control File versions Note : This feature is available not for all Cloud Providers. Currently, it is supported by AWS and GCP . Show files versions To view file versions for the storage with enabled versioning use the storage ls command with the both specified -l and -v options, e.g.: pipe storage ls -l -v cp://versioning-storage As you can see - \" file1 \" and \" file3 \" each has 1 version. \" file2 \" has 3 versions. \" file4 \" has 2 verions. To view versions of a specific file specify its full path, e.g.: pipe storage ls -l -v cp://versioning-storage/file4 Restore files The command to restore a previous version of a file: pipe storage restore [OPTIONS] <Path> Path - defines a full path to the file/directory in a datastorage. Options Description Non-required options -v / --version To restore a specified version -r / --recursive To restore the whole directory hierarchy. Note : this feature is yet supported for the AWS Cloud Provider only -i / --include Include only files matching this pattern into processing -e / --exclude Exclude all files matching this pattern from processing By this command you can restore file version in a datastorage. If version is not specified via -v option it will try to restore the latest non-deleted version. Otherwise a specified version will be restored. If the Path is a directory, the command gets the top-level deleted files from the Path directory and restore them to the latest version. The example below shows how to set one of the previous versions for the \" file2 \" as the latest: pipe storage restore -v <Version> cp://versioning-storage/file2 Note : When a specified version of the \" file2 \" is restored, a copy of that version is created to become the latest version of the file. You can restore a deleted file without specifying a version. It works only for files with a Delete marker as the latest version (\" file4 \" in the example below). In such case the command will be, e.g.: pipe storage restore cp://versioning-storage/file4 Note : Before we restored the file \" file4 \" its latest version was a Delete marker . After restoration this marker disappeared. The example below shows how to restore the latest version for only \"*.txt\" files recursively in the deleted directory (the directory that was marked with a Delete marker previously), e.g.: pipe storage restore --recursive --include *.txt s3://objstor/examples/ After restoring, check that the \"*.txt\" file in the subfolder also restored: Delete an object from a datastorage The command to delete an object from a storage: pipe storage rm [OPTIONS] <Path> Path - defines a full path to the object in a datastorage. Options Description Non-required options -y / --yes Do not ask confirmation -v / --version Delete a specified version of an object -d / --hard-delete Completely delete an object from a storage -r / --recursive Recursive deletion (required for deleting folders) -e / --exclude Exclude all files matching this pattern from processing -i / --include Include only files matching this pattern into processing The example below demonstrates how to delete a file from the storage objstor : pipe storage rm cp://objstor/simplefile If this command is performed without options over the object from the storage with enabled versioning, that object will not be removed completely, it will remain in a datastorage and get a Delete marker . Such objects can be restored via storage restore command. In the example below we set a Delete marker to the file \" file1 \": Note : the latest version of the file \" file1 \" is marked with Delete marker now. To completely delete an object from a datastorage use -d option. In the example below we will completely delete the file \" file2 \" from the storage with enabled versioning without asking confirmation: pipe storage rm -y -d cp://versioning-storage/file2 To delete a specific version of an object use -v option. In the example below we will delete one of the versions of the file \" file4 \" from the storage with enabled versioning without asking confirmation: pipe storage rm -y -v <Version> cp://versioning-storage/file4 In the example below we will completely delete files from the folder \" initial-data \" - for that we will use --recursive option. But we will delete only files whose names contain symbol 3 - for that we will use --include option: pipe storage rm --yes --hard-delete --include *3* --recursive cp://versioning-storage/initial-data/ Manage datastorage objects attributes This section is about attribute management of the inner datastorages files via CLI. To manage attributes of other Cloud Pipeline objects - see 14.2. View and manage Attributes via CLI . Get object attributes The command to list attributes of a specific file in a datastorage: pipe storage get-object-tags [OPTIONS] <Path> Path - defines a full path to a file in a datastorage. Path value must begin from the Cloud prefix ( S3 (for AWS), AZ (for MS Azure), GS (for GCP) or common CP (instead of described ones, regardless of Provider)). Options Description Non-required options -v / --version To get attributes for a specified file version. If option is not set, but datastorage versioning is enabled - processing will be performed for the latest file version In the example below we will get attributes for the file \" file1 \" from the datastorage objstor : pipe storage get-object-tags cp://objstor/file1 In the example below we will get attributes for the previous version of the file \" file1 \" from the datastorage objstor : pipe storage get-object-tags -v <Version> cp://objstor/file1 Set object attributes The command to set attributes for a specific file in a datastorage: pipe storage set-object-tags [OPTIONS] <Path> <List of KEY=VALUE> Path - defines a full path to a file in a datastorage. Path value must begin from the Cloud prefix ( S3 (for AWS), AZ (for MS Azure), GS (for GCP) or common CP (instead of described ones, regardless of Provider)). List of KEY=VALUE - list of attributes to set. Can be specified as a single KEY=VALUE pair or a list of them. Options Description Non-required options -v / --version To set attributes for a specified file version. If option is not set, but datastorage versioning is enabled - processing will be performed for the latest file version Note : if a specific attribute key already exists for a file, it will be overwritten . In the example below we will set attributes for the file \" file2 \" from the datastorage objstor and then check that attributes are set by the storage get-object-tags command: pipe storage set-object-tags cp://objstor/file2 example_key1=example_value1 example_key2=example_value2 Delete object attributes The command to delete attributes for a specific file in a datastorage: pipe storage delete-object-tags [OPTIONS] <Path> <List of KEYs> Path - defines a full path to a file in a datastorage. Path value must begin from the Cloud prefix ( S3 (for AWS), AZ (for MS Azure), GS (for GCP) or common CP (instead of described ones, regardless of Provider)). List of KEYs - list of attribute keys to delete. Options Description Non-required options -v / --version To delete attributes for a specified file version. If option is not set, but datastorage versioning is enabled - processing will be performed for the latest file version In the example below we will delete attribute tagkey1 for the previous version of the file \" file1 \" from the datastorage objstor : pipe storage delete-object-tags -v <Version> cp://objstor/file1 tagkey1 Mounting of storages pipe cli supports mounting data storages (both - File Storages and Object Storages) to Linux and Mac workstations (requires FUSE installed). Note : This feature is available not for all Cloud Providers. Currently, it is supported only by AWS For the mounted storages, regular listing/read/write commands are supported (e.g. cp , mv , ls , mkdir , rm , fallocate , truncate , dd , etc. - according to the corresponding OS), users can manage files/folders as with any general hard drive. Mount a storage The command to mount a data storage into the mountpoint: pipe storage mount [OPTIONS] <Mountpoint> Mountpoint - defines a full path to a directory on the workstation where the data storage shall be mounted. Options Description Required options -f / --file File System mode. In this mode, all available File Storages will be mounted into the mountpoint. This option is mutually exclusive with -b option -b / --bucket Object Storage mode. In this mode, the specified Object Storage will be mounted into the mountpoint. This option is mutually exclusive with -f option Non-required options -m / --mode Allows to set a mask that defines access permissions (to the owner / group / others ). This mask is set in the numerical view (three-digit octal number) -o / --options Allows to specify any mount options supported by underlying FUSE implementation -l / --log-file If set, standard/error output of mount operations will be written into the specified file -q / --quiet Quiet mode -t / --threads Enable multithreading - allows several processes simultaneously interact with the mount point In the example below we will mount the Object Storage named objstor into the directory \" mountdir \" and specify the file \" mount.log \" as log-file for mount operations: pipe storage mount -l ~/mount.log -b objstor ~/mountdir In the example below we will mount the Object Storage named objectdatastorage into the directory \" mountdir \" and set full access permissons (read, write, execute) to the owner, read and execute permissions to the group and no permissions to others: pipe storage mount -b objectdatastorage -m 750 ~/mountdir Note : if -m option isn't specified, during mounting the default permissions mask will be set - 700 (full access to the owner and no permissions to the group and others) Unmount a storage The command to unmount a mountpoint: pipe storage umount [OPTIONS] <Mountpoint> Mountpoint - defines a full path to a directory on the workstation where the data storage was mounted. Options Description Non-required options -q / --quiet Quiet mode In the example below we will unmount a storage from the directory \" mountdir \": pipe storage umount ~/mountdir","title":"14.3. Manage Data Storage"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#143-manage-storage-via-cli","text":"Create a datastorage List storages/storage content Show storage usage Edit a datastorage Change backup duration, STS/LTS duration, versioning Change a parent folder for a datastorage Delete a datastorage Create a folder in a storage Upload and download data Control File versions Show files versions Restore files Delete an object from a datastorage Manage datastorage objects attributes Get object attributes Set object attributes Delete object attributes Mounting of storages Mount a storage Unmount a storage Cloud Pipeline CLI has to be installed. See 14.1. Install and setup CLI . To perform different operations with object storages the command set pipe storage is used.","title":"14.3. Manage Storage via CLI"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#create-a-datastorage","text":"The command to create an object storage: pipe storage create [OPTIONS] Options Description Required options -n / --name Alias of the new object storage -p / --path Datastorage path Non-required options -d / --description Description of the object storage -sts / --short_term_storage Number of days for storing data in the short term storage. Note : This option is available not for all Cloud Providers -lts / --long_term_storage Number of days for storing data in the long term storage. Note : This option is available not for all Cloud Providers -v / --versioning Enable versioning for this object storage. Note : This option is available not for all Cloud Providers -b / --backup_duration Number of days for storing backups of the storage. Note : This option is available not for all Cloud Providers -t / --type Type of the Cloud for the object storage. Depends on the Cloud Provider. Possible values - S3 (for AWS), AZ (for MS Azure), GS (for GCP) and NFS (for FS mounts). Additionally, for AWS HealthOmics Storages - AWS_OMICS_REF (for reference store ) and AWS_OMICS_SEQ (for sequence store ). -f / --parent_folder Name/ID of the folder which will contain the object storage. Default value - library root folder -c / --on_cloud To create datastorage on the Cloud. This flag shall be specified, only if a new datastorage is being created. If user want to add an existing storage this flag should not be specified -r / --region_id Cloud region ID where storage shall be created Note : there is not necessary to set all options while input that command. If some options are not set - user shall be prompted for them in an interactive manner, if they will not be set in an interactive manner, default values will be used. In the example below the object storage output_results for the new datastorage output_results_storage will be created in the folder with ID 297 with the following options: STS duration - 20 days, LTS duration - 45 days, backup duration - 30 days. pipe storage create -n output-results -p output-results-storage -c -sts 20 -lts 45 -b 30 -f 297 -c flag was specify to create a storage in the Cloud. As you can see - Description , Type of the Cloud and Cloud Region ID fields were left empty. So, values for these options will be set as default.","title":"Create a datastorage"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#list-storagesstorage-content","text":"The command to view the full storage list of the current platform deployment: pipe storage ls [OPTIONS] Options Description Non-required options -l / --show_details Show details Perform the following command to view full storage list and to check that the object storage from the example above was created: pipe storage ls -l The command to view the content of the datastorage: pipe storage ls [OPTIONS] <Path> Path - defines a full path to the datastorage. Path value must begin from the Cloud prefix: s3 (for AWS) / az (for MS Azure) / gs (for GCP) / common cp (instead of described ones, regardless of Provider) OR omics (for AWS HealthOmics Storages ) Options Description Non-required options -l / --show_details Show details -v / --show_versions Show object versions. Only for storages with enabled versioning -r / --recursive Recursive listing -p / --page Maximum number of records to show -a / --all Show all results at once ignoring page settings Example of the detailed datastorage content view: pipe storage ls --show_details s3://objstor Or you can use the common CP prefix instead of S3 : pipe storage ls --show_details cp://objstor Result will be the same:","title":"List storages/storage content"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#show-storage-usage","text":"To obtain a \"disk usage\" information on the supported data storage or its inner folder(s) use the command: pipe storage du [OPTIONS] [STORAGE] The command prints for the data storages/path: summary number of files in the storage/path summary size of files in the storage/path Options Description Non-required options -p / --relative-path The relative path inside the storage -f / --format The size unit format (default: Mb ). Possible values: K , Kb , KB - for kilobytes; M , Mb , MB - for megabytes; G , Gb , GB - for gigabytes -d / --depth The maximum depth level of the nesting folders. Note : this option isn't supported for the FS storages yet STORAGE - defines the datastorage name/path. Without specifying any options and storage this command prints the full list of the available storages (both types - object and FS) with the \"usage\" information for each of them: pipe storage du With specifying the storage name this command prints the \"usage\" information only by that storage, e.g.: pipe storage du objectdatastorage With -p ( --relative-path ) option the command prints the \"usage\" information for the specified path in the required storage, e.g.: pipe storage du objectdatastorage -p innerdir1 With -d ( --depth ) option the command prints the \"usage\" information in the required storage (and path) for the specified folders nesting depth, e.g.: pipe storage du objectdatastorage -p innerdir2 -d 1 Specified command will print the \"usage\" information for all folders with the nesting depth no more 1 relative to the path objectdatastorage/innerdir2/ : With -f ( --format ) option user can change the unit for the size value, e.g. to print the storage usage in gigabytes: pipe storage du objectdatastorage -f GB","title":"Show storage usage"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#edit-a-datastorage","text":"","title":"Edit a datastorage"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#change-backup-duration-stslts-duration-versioning","text":"The Command to change backup duration, select STS/LTS duration or enable versioning: pipe storage policy [OPTIONS] Options Description Required options -n / --name Alias/path of the storage to update the policy. Specified without Cloud prefix Non-required options -sts / --short_term_storage Number of days for storing data in the short term storage. Note : This option is available not for all Cloud Providers -lts / --long_term_storage Number of days for storing data in the long term storage. Note : This option is available not for all Cloud Providers -v / --versioning Enable versioning for this object storage. Note : This option is available not for all Cloud Providers -b / --backup_duration Number of days for storing backups of the storage. Note : This option is available not for all Cloud Providers In the example below backup duration is set to 25 days, STS and LTS durations are set to 50 days and 100 days respectively for the datastorage objstor . Also, we enable versioning for that datastorage: pipe storage policy -n objstor -b 25 -sts 50 -lts 100 -v Note : there is not necessary to set all options while input that command. If some options are not set - user shall be prompted for them in an interactive manner, if they will not be set in an interactive manner, default values will be used. You can check via the GUI that parameters were changed:","title":"Change backup duration, STS/LTS duration, versioning"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#change-a-parent-folder-for-a-datastorage","text":"The command to move a datastorage to a new parent folder: pipe storage mvtodir <Storage> <Directory> Directory - name of the folder to which the object storage will be moved. Storage - alias/path of the storage to move. Specified without Cloud prefix. In the example below we will move the storage objstor to the folder \" InnerFolder \": pipe storage mvtodir objstor InnerFolder","title":"Change a parent folder for a datastorage"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#delete-a-datastorage","text":"The command to delete an object storage: pipe storage delete [OPTIONS] Options Description Required options -n / --name Alias/path of the storage to delete. Specified without Cloud prefix Non-required options -c / --on_cloud To delete datastorage from a Cloud. If this option isn't set, datastorage will just become unregistered -y / --yes Do not ask confirmation In the example below we will delete the output-results-storage without asking confirmation: pipe storage delete -n output-results-storage -y As the command above was performed withoud --on_cloud option, output-results-storage wasn't deleted from a Cloud actually and it might be added again to the Cloud Pipeline platform via described storage create command:","title":"Delete a datastorage"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#create-a-folder-in-a-storage","text":"The command to create a folder in a storage: pipe storage mkdir <List of FOLDERs> List of FOLDERs - defines a list of the folders paths. Each path in the list shall be a full path to a new folder in the specific datastorage. Path value must begin from the Cloud prefix ( S3 (for AWS), AZ (for MS Azure), GS (for GCP) or common CP (instead of described ones, regardless of Provider)). In the example below we will create folders \" new-folder1 \", \" new-folder2 \" in the storage output-results-storage and then will check that they're exist via described storage ls command: pipe storage mkdir cp://output-results-storage/new-folder1 cp://output-results-storage/new-folder2","title":"Create a folder in a\u00a0storage"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#upload-and-download-data","text":"There are two commands to upload/download data: pipe storage cp [OPTIONS] <Source> <Destination> pipe storage mv [OPTIONS] <Source> <Destination> By cp command you can copy files from one datastorage to another one or between local filesystem and a datastorage (in both directions). By mv command you can move files from one datastorage to another one or between local filesystem and a datastorage (in both directions). Source - defines a path (in the local filesystem or datastorage) to the object to be copied/moved. Destination - defines a path (in the local filesystem or datastorage) to the object where source object will be copied/moved. Options Description Non-required options -r / --recursive Recursive source scan. This option is not needed when you copy/move a single file -f / --force Rewrite files in destination -e / --exclude Exclude all files matching this pattern from processing -i / --include Include only files matching this pattern into processing -q / --quiet Quiet mode -s / --skip-existing Skip files existing in destination, if they have size matching source -t / --tags Set object attributes during processing. attributes can be specified as single KEY=VALUE pair or a list of them. If this option specified all existent attributes will be overwritten -l / --file-list Path to the file with file paths that should be copied/moved. This file should be tab delimited and consist of two columns: relative path to file and size -sl / --symlinks [follow|filter|skip] Describe symlinks processing strategy for local sources. Possible values: follow - follow symlinks (default); skip - do not follow symlinks; filter - follow symlinks but check for cyclic links -a / --additional-options For cp command only . Comma-separated list of additional arguments to be used during file copy In the example below we will upload files from the local filesystem to the storage objstor into the folder \" upload \": pipe storage cp ~/data cp://objstor/upload --recursive Application will start uploading files and print progress. You can view it: After uploading is complete, check that they're uploaded - via described storage ls command: In the example below we will upload the file \" 2.fastq \" from the local filesystem in the quiet mode, rewrite it to the storage objstor into the folder \" upload \" and set two attributes on it: storage cp -f -q ~/data/2.fastq cp://objstor/upload/ -t testkey1=testvalue1 -t testkey2=testvalue2 After that, we will check attributes of the uploaded file - via storage get-object-tags command: Note : Files uploaded via CLI will have the following attributes and values automatically set: CP_OWNER . The value of the attribute will be set as a user ID. CP_SOURCE . The value of the attribute will be set as a local path used to upload. The example demonstrates automatic file tagging after data uploading: The example below demonstrates how to download files from the datastorage folder to the local filesystem. Also we will not download files which names starts from 1. : pipe storage cp -r -e 1.* cp://objstor/upload/ ~/data/input/","title":"Upload and download data"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#control-file-versions","text":"Note : This feature is available not for all Cloud Providers. Currently, it is supported by AWS and GCP .","title":"Control File versions"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#show-files-versions","text":"To view file versions for the storage with enabled versioning use the storage ls command with the both specified -l and -v options, e.g.: pipe storage ls -l -v cp://versioning-storage As you can see - \" file1 \" and \" file3 \" each has 1 version. \" file2 \" has 3 versions. \" file4 \" has 2 verions. To view versions of a specific file specify its full path, e.g.: pipe storage ls -l -v cp://versioning-storage/file4","title":"Show files versions"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#restore-files","text":"The command to restore a previous version of a file: pipe storage restore [OPTIONS] <Path> Path - defines a full path to the file/directory in a datastorage. Options Description Non-required options -v / --version To restore a specified version -r / --recursive To restore the whole directory hierarchy. Note : this feature is yet supported for the AWS Cloud Provider only -i / --include Include only files matching this pattern into processing -e / --exclude Exclude all files matching this pattern from processing By this command you can restore file version in a datastorage. If version is not specified via -v option it will try to restore the latest non-deleted version. Otherwise a specified version will be restored. If the Path is a directory, the command gets the top-level deleted files from the Path directory and restore them to the latest version. The example below shows how to set one of the previous versions for the \" file2 \" as the latest: pipe storage restore -v <Version> cp://versioning-storage/file2 Note : When a specified version of the \" file2 \" is restored, a copy of that version is created to become the latest version of the file. You can restore a deleted file without specifying a version. It works only for files with a Delete marker as the latest version (\" file4 \" in the example below). In such case the command will be, e.g.: pipe storage restore cp://versioning-storage/file4 Note : Before we restored the file \" file4 \" its latest version was a Delete marker . After restoration this marker disappeared. The example below shows how to restore the latest version for only \"*.txt\" files recursively in the deleted directory (the directory that was marked with a Delete marker previously), e.g.: pipe storage restore --recursive --include *.txt s3://objstor/examples/ After restoring, check that the \"*.txt\" file in the subfolder also restored:","title":"Restore files"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#delete-an-object-from-a-datastorage","text":"The command to delete an object from a storage: pipe storage rm [OPTIONS] <Path> Path - defines a full path to the object in a datastorage. Options Description Non-required options -y / --yes Do not ask confirmation -v / --version Delete a specified version of an object -d / --hard-delete Completely delete an object from a storage -r / --recursive Recursive deletion (required for deleting folders) -e / --exclude Exclude all files matching this pattern from processing -i / --include Include only files matching this pattern into processing The example below demonstrates how to delete a file from the storage objstor : pipe storage rm cp://objstor/simplefile If this command is performed without options over the object from the storage with enabled versioning, that object will not be removed completely, it will remain in a datastorage and get a Delete marker . Such objects can be restored via storage restore command. In the example below we set a Delete marker to the file \" file1 \": Note : the latest version of the file \" file1 \" is marked with Delete marker now. To completely delete an object from a datastorage use -d option. In the example below we will completely delete the file \" file2 \" from the storage with enabled versioning without asking confirmation: pipe storage rm -y -d cp://versioning-storage/file2 To delete a specific version of an object use -v option. In the example below we will delete one of the versions of the file \" file4 \" from the storage with enabled versioning without asking confirmation: pipe storage rm -y -v <Version> cp://versioning-storage/file4 In the example below we will completely delete files from the folder \" initial-data \" - for that we will use --recursive option. But we will delete only files whose names contain symbol 3 - for that we will use --include option: pipe storage rm --yes --hard-delete --include *3* --recursive cp://versioning-storage/initial-data/","title":"Delete an object from a datastorage"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#manage-datastorage-objects-attributes","text":"This section is about attribute management of the inner datastorages files via CLI. To manage attributes of other Cloud Pipeline objects - see 14.2. View and manage Attributes via CLI .","title":"Manage datastorage objects attributes"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#get-object-attributes","text":"The command to list attributes of a specific file in a datastorage: pipe storage get-object-tags [OPTIONS] <Path> Path - defines a full path to a file in a datastorage. Path value must begin from the Cloud prefix ( S3 (for AWS), AZ (for MS Azure), GS (for GCP) or common CP (instead of described ones, regardless of Provider)). Options Description Non-required options -v / --version To get attributes for a specified file version. If option is not set, but datastorage versioning is enabled - processing will be performed for the latest file version In the example below we will get attributes for the file \" file1 \" from the datastorage objstor : pipe storage get-object-tags cp://objstor/file1 In the example below we will get attributes for the previous version of the file \" file1 \" from the datastorage objstor : pipe storage get-object-tags -v <Version> cp://objstor/file1","title":"Get object attributes"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#set-object-attributes","text":"The command to set attributes for a specific file in a datastorage: pipe storage set-object-tags [OPTIONS] <Path> <List of KEY=VALUE> Path - defines a full path to a file in a datastorage. Path value must begin from the Cloud prefix ( S3 (for AWS), AZ (for MS Azure), GS (for GCP) or common CP (instead of described ones, regardless of Provider)). List of KEY=VALUE - list of attributes to set. Can be specified as a single KEY=VALUE pair or a list of them. Options Description Non-required options -v / --version To set attributes for a specified file version. If option is not set, but datastorage versioning is enabled - processing will be performed for the latest file version Note : if a specific attribute key already exists for a file, it will be overwritten . In the example below we will set attributes for the file \" file2 \" from the datastorage objstor and then check that attributes are set by the storage get-object-tags command: pipe storage set-object-tags cp://objstor/file2 example_key1=example_value1 example_key2=example_value2","title":"Set object attributes"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#delete-object-attributes","text":"The command to delete attributes for a specific file in a datastorage: pipe storage delete-object-tags [OPTIONS] <Path> <List of KEYs> Path - defines a full path to a file in a datastorage. Path value must begin from the Cloud prefix ( S3 (for AWS), AZ (for MS Azure), GS (for GCP) or common CP (instead of described ones, regardless of Provider)). List of KEYs - list of attribute keys to delete. Options Description Non-required options -v / --version To delete attributes for a specified file version. If option is not set, but datastorage versioning is enabled - processing will be performed for the latest file version In the example below we will delete attribute tagkey1 for the previous version of the file \" file1 \" from the datastorage objstor : pipe storage delete-object-tags -v <Version> cp://objstor/file1 tagkey1","title":"Delete object attributes"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#mounting-of-storages","text":"pipe cli supports mounting data storages (both - File Storages and Object Storages) to Linux and Mac workstations (requires FUSE installed). Note : This feature is available not for all Cloud Providers. Currently, it is supported only by AWS For the mounted storages, regular listing/read/write commands are supported (e.g. cp , mv , ls , mkdir , rm , fallocate , truncate , dd , etc. - according to the corresponding OS), users can manage files/folders as with any general hard drive.","title":"Mounting of storages"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#mount-a-storage","text":"The command to mount a data storage into the mountpoint: pipe storage mount [OPTIONS] <Mountpoint> Mountpoint - defines a full path to a directory on the workstation where the data storage shall be mounted. Options Description Required options -f / --file File System mode. In this mode, all available File Storages will be mounted into the mountpoint. This option is mutually exclusive with -b option -b / --bucket Object Storage mode. In this mode, the specified Object Storage will be mounted into the mountpoint. This option is mutually exclusive with -f option Non-required options -m / --mode Allows to set a mask that defines access permissions (to the owner / group / others ). This mask is set in the numerical view (three-digit octal number) -o / --options Allows to specify any mount options supported by underlying FUSE implementation -l / --log-file If set, standard/error output of mount operations will be written into the specified file -q / --quiet Quiet mode -t / --threads Enable multithreading - allows several processes simultaneously interact with the mount point In the example below we will mount the Object Storage named objstor into the directory \" mountdir \" and specify the file \" mount.log \" as log-file for mount operations: pipe storage mount -l ~/mount.log -b objstor ~/mountdir In the example below we will mount the Object Storage named objectdatastorage into the directory \" mountdir \" and set full access permissons (read, write, execute) to the owner, read and execute permissions to the group and no permissions to others: pipe storage mount -b objectdatastorage -m 750 ~/mountdir Note : if -m option isn't specified, during mounting the default permissions mask will be set - 700 (full access to the owner and no permissions to the group and others)","title":"Mount a storage"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#unmount-a-storage","text":"The command to unmount a mountpoint: pipe storage umount [OPTIONS] <Mountpoint> Mountpoint - defines a full path to a directory on the workstation where the data storage was mounted. Options Description Non-required options -q / --quiet Quiet mode In the example below we will unmount a storage from the directory \" mountdir \": pipe storage umount ~/mountdir","title":"Unmount a storage"},{"location":"manual/14_CLI/14.4._View_pipeline_definitions_via_CLI/","text":"14.4. View pipeline definitions via CLI The command to view pipeline definitions: pipe view-pipes [OPTIONS] [PIPELINE] PIPELINE - pipeline name or ID. Options Description Non-required options -v / --versions List versions of a pipeline -p / --parameters List parameters of a pipeline -s / --storage-rules List storage rules of a pipeline -r / --permissions List user permissions of a pipeline Without any arguments that command will output the list of all pipelines, e.g.: With specifying pipeline name/ID that command will output the definition of a specific pipeline. E.g., to view info about the pipeline with ID 3212 : pipe view-pipes 3212 To view pipeline versions, parameter list and storage rules - use -v , -p and -s options accordingly: Note : you can view pipeline parameter list by another command - run -n <Pipeline name/ID> -p . See more details here . To view permissions on a specific pipeline - use the -r option: Note : you can view pipeline permissions by another command - view-acl -t pipeline <Pipeline name/ID> . See more details here .","title":"14.4. View pipeline definitions"},{"location":"manual/14_CLI/14.4._View_pipeline_definitions_via_CLI/#144-view-pipeline-definitions-via-cli","text":"The command to view pipeline definitions: pipe view-pipes [OPTIONS] [PIPELINE] PIPELINE - pipeline name or ID. Options Description Non-required options -v / --versions List versions of a pipeline -p / --parameters List parameters of a pipeline -s / --storage-rules List storage rules of a pipeline -r / --permissions List user permissions of a pipeline Without any arguments that command will output the list of all pipelines, e.g.: With specifying pipeline name/ID that command will output the definition of a specific pipeline. E.g., to view info about the pipeline with ID 3212 : pipe view-pipes 3212 To view pipeline versions, parameter list and storage rules - use -v , -p and -s options accordingly: Note : you can view pipeline parameter list by another command - run -n <Pipeline name/ID> -p . See more details here . To view permissions on a specific pipeline - use the -r option: Note : you can view pipeline permissions by another command - view-acl -t pipeline <Pipeline name/ID> . See more details here .","title":"14.4. View pipeline definitions via CLI"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/","text":"14.5. Manage pipeline executions via CLI View pipeline runs Schedule a pipeline execution Change execution environment Change advanced options Setting parameters for a launch Launch a cluster Launch a pipeline in a synchronized mode Launch a job on the existing running instance Run a tool Generate pipeline launch command via the GUI Runs sharing View sharing status Share a run Unshare a run Run a single command or an interactive session over the SSH protocol Pause a pipeline execution Resume paused pipeline execution Stop a pipeline execution Terminate a node Cloud Pipeline CLI has to be installed. See 14. Command-line interface (CLI) . View pipeline runs The command to view runs info: pipe view-runs [OPTIONS] [RUN_ID] Options Description Non-required options -s / --status [ANY | FAILURE | PAUSED | PAUSING | RESUMING | RUNNING | STOPPED | SUCCESS] List pipeline runs with a specific status -df / --date-from List pipeline runs started after specified date -dt / --date-to List pipeline runs completed before specified date -p / --pipeline List history of runs for a specific pipeline -pid / --parent-id List runs for a specific parent pipeline run -f / --find Search runs with a specific substring in run parameters values -t / --top Display top N records -nd / --node-details Display node details -pd / --parameters-details Display parameters -td / --tasks-details Display tasks That command without any arguments will list all active runs available for the current user (more about permissions see here ): To view information about specific run - enter its RunID at the end of the command, e.g.: You can additionally display information about instance used by a specific run - by the -nd ( --node-details ) flag: You can additionally display information about pipeline parameters used by a specific run - by the -pd ( --parameters-details ) flag: You can additionally display information about run tasks - by the -td ( --tasks-details ) flag: Note : by default, the view-runs command outputs information about active runs. To view a list of all runs with a specific status (see possible values in the table above) - use the -s ( --status ) option, e.g. to view a list of failed runs: pipe view-runs -s FAILURE By default, only last 100 records are displayed. If you want to change this count, use the -t ( --top ) option with specifying a number of records to show, e.g.: Option -df ( --date-from ) allows to output runs started after specified datetime. Datetime shall be specified in one of the following formats: yyyy-MM-dd HH:mm:ss or yyyy-MM-dd , e.g.: Option -dt ( --date-to ) allows to output runs completed before specified datetime. Note : since the view-runs command outputs by default information about active runs, then for using that command with the -dt option one of the completed statuses must be specified, e.g.: To list history of runs for a specific pipeline use the -p ( --pipeline ) option and a pipeline name. By default it will print a list of active runs for the latest pipeline version. If you want to launch non-latest pipeline version, specify version name after the pipeline name using the @ symbol, e.g. to list STOPPED runs for the v1 version of the simplepipeline pipeline: pipe view-runs -p simplepipeline@v1 -s STOPPED To list child runs use the -pid ( --parent-id ) option and a parent run ID, e.g.: To find runs with a specific substring in run parameters values use the -f ( --find ) option. E.g. to find among all stopped runs only those which parameter values contain the word \"Changed\": pipe view-runs -s STOPPED -f \"Changed\" Schedule a pipeline execution The command to shedule a pipeline/version execution: pipe run [OPTIONS] [RUN_PARAMETERS] Options Description Non-required options -n / --pipeline Pipeline name or ID -c / --config Pipeline configuration name -di / --docker-image Docker image -it / --instance-type Node type in terms of the Cloud Provider -id / --instance-disk Instance disk size -ic / --instance-count Number of worker instances to launch in a cluster -nc / --cores Number of cores that a cluster shall contain. This option will be ignored if -ic ( --instance-count ) option was specified -r / --region-id Instance Cloud region -pt / --price-type [spot | on-demand] Price type -t / --timeout Timeout (in minutes), when elapsed - run will be stopped -cmd / --cmd-template Command template -p / --parameters Returns a parameter list for the specified pipeline -pn / --parent-node Parent instance Run ID. Allows to run a pipeline as a child job on the existing running instance -s / --sync Allows a pipeline to be run in a sync mode. When set - terminal will be blocked until the pipeline won't finish the run. After that the pipeline's finish status will be returned into the terminal output and the terminal will be unblocked -np / --non-pause Allows to switch off the auto-pause option. Note : this option is supported for on-demand runs only -y / --yes Do not ask confirmation -q / --quiet Quiet mode RUN_PARAMETERS - list of the pipeline parameters to set. Can be specified as a single --<Parameter name> <value> pair or a list of them. In the example below the pipeline with the name simplepipeline will be launched: pipe run -n simplepipeline -y And then we'll check that it was launched by the view-runs command: If not additionally specified, described command will launch default configuration of the pipeline latest version. If you want to launch non-default pipeline configuration, specify its name with the -c ( --config ) option, e.g.: pipe run -n simplepipeline -c new-config -y In the example above the configuration with the name new-config of the pipeline simplepipeline was launched. If you want to launch non-latest pipeline version, specify version name after the pipeline name using the @ symbol, e.g.: pipe run -n simplepipeline@v1 -y In the example above default configuration of the version with the name v1 of the pipeline simplepipeline was launched. Change execution environment You can change execution environment for a pipeline run by setting the following options: -di ( --docker-image ), -it ( --instance-type ), -id ( --instance-disk ), -r ( --region-id ). In the example below the pipeline simplepipeline will be launched on the instance n1-highcpu-2 with the disk size 25 Gb: pipe run -n simplepipeline --instance-type n1-highcpu-2 --instance-disk 25 -y Note : for the -di ( --docker-image ) option shall specify a full path to the Docker image including its version. Change advanced options You can change advanced options for a pipeline execution by setting the following options: -pt ( --price-type ), -t ( --timeout ), -cmd ( --cmd-template ). E.g. the command below will launch the pipeline simplepipeline with the spot price type and changed command template: pipe run -n simplepipeline -pt spot -cmd \"sleep 1d\" Note : -pt flag could take only one from two values - on-demand and spot , independent on the Cloud Provider. Setting parameters for a launch To view all parameter list for a pipeline use -p ( --parameters ) flag, e.g. to view parameters of the simplepipeline pipeline: pipe run -n simplepipeline -p As you can see, that flag allows to view all parameters with their types. If parameters have default values they will be also printed. To set parameters for a pipeline launch, enter them after all options in the following manner: --<Parameter1 name> <value> --<Parameter2 name> <value> ... , e.g.: pipe run -n simplepipeline --booleanParamExample false --stringParamExample \"Changed test value\" -y Launch a cluster You can launch a cluster using -ic ( --instance-count ) option. It sets a number of worker instances. In the example below the cluster with two child nodes will be launched: pipe run -n simplepipeline -ic 2 Also you can launch a cluster using the -nc ( --number-cores ) option. With this option you specify a number of cores that a cluster shall contain. In that case worker instance count of the cluster will be calculated automatically based on the supported instance types. Note : This option will be ignored if -ic ( --instance-count ) option was specified. In the example below the cluster runs with 16 cores will be launched: pipe run -n simplepipeline -nc 16 As you can see, for that run single instance with 16 cores was scheduled. Launch a pipeline in a synchronized mode You can launch a pipeline in a synchronized mode by setting the -s ( --sync ) flag. In that mode terminal will be blocked until the pipeline won't finish the run. After the pipeline's finish, its status will be returned into the terminal output and the terminal will be unblocked. Example of the pipeline simplepipeline execution in a sync mode: pipe run -n simplepipeline --sync -y Launch a job on the existing running instance You can launch a pipeline as a child job on the existing running instance by setting the -pn ( --parent-node ) option. It allows not to initialize a new node for a such job but use already existing one. Example of the launching the pipeline simplepipeline as a child of the previously launched and initialized instance: pipe run -n simplepipeline --parent-node <Parent RunID> -q -y Run a tool Also you can launch a tool - without providing a pipeline name/ID. In that case Docker image shall be mandatory specified. If instance type, instance disk and cmd template aren't specified, default tool settings for them will be used. In the example below we will launch the Docker image ubuntu:latest on the n1-highcpu-2 instance with the disk size 27 GB and sleep infinity command template: pipe run -di <Docker image path> -it n1-highcpu-2 -id 27 -cmd \"sleep infinity\" -y Note : Docker image path shall be specified in a full manner including version. Generate pipeline launch command via the GUI The construction of the correct run command sometimes could be hard for users, so users can generate necessary launch commands via the GUI. For that: Via the GUI open the Launch page of a pipeline/tool you want to run At the Launch page, configure all settings for the run as you want and then click the \" CLI command \" button in the right-upper corner: In the appeared popup copy the pipe run command, e.g.: Paste the copied command into the terminal and perform it Note : instead of step 2 described above, you may open the Run logs page of any active/completed run and click the LAUNCH COMMAND button in the right-upper corner to view the launch command of the selected run: Also, user can select the API tab in the \"Launch commands\" popup and get the POST request for a job launch: Runs sharing Note : to share a run with other users/groups, user shall be the OWNER of that run or has an ADMIN role. About sharing run via the GUI see here . View sharing status The command to view for whom a run is shared: pipe share get RUN_ID The example below lists users/groups for whom the run with ID 38081 is shared: pipe share get 38081 Share a run The command to share a running job with other users/groups: pipe share add RUN_ID [OPTIONS] Options Description -su / --shared-user Specifies the user for whom the run will be shared. Multiple options are supported -sg / --shared-group Specifies the group/role for which the run will be shared. Multiple options are supported -ssh / --share-ssh Share SSH-session of the run. Non-required option for the runs with endpoints One of the option -su or -sg shall be necessarily specified. Both options also can be used simultaneously. -ssh option is necessarily required only in cases when the run hasn't endpoints. In the example below we will share the SSH-session of the run with ID 38081 with the user USER1 and then check it by the command pipe share get : pipe share add 38081 -ssh -su USER1 Unshare a run The command to unshare a running job from other users/groups for whom the access was shared before: pipe share remove RUN_ID [OPTIONS] Options Description Non-required -su / --shared-user Specifies the user for whom the sharing of the run will be disabled. Multiple options are supported -sg / --shared-group Specifies the group/role for which the sharing of the run will be disabled. Multiple options are supported -ssh / --share-ssh Remove only SSH-sharing for all users/groups for runs with endpoint(s) or remove all users/groups of the run shared list for runs without endpoints Without any additional options the command will remove all sharings of the run. In the example below we will unshare the run with ID 38081 with the role ROLE_USER and check it by the command pipe share get : pipe share remove 38081 -sg ROLE_USER Run a single command or an interactive session over the SSH protocol Note : to perform a command or run an interactive session over the running job, user shall be the OWNER of that job or has an ADMIN role. The command to run a single command or an interactive session over the SSH protocol for the specified job run: pipe ssh RUN_ID [COMMAND] COMMAND - a single command to execute over the running instance with the specified Run ID using the SSH protocol. In the example below we will list the content of the root directory of the job run with ID 12370 : pipe ssh 12370 'ls -l /root' If COMMAND isn't specified, it will start an interactive session over the running instance with the specified Run ID using the SSH protocol. To exit from the interactive session use commands exit or logout . We will perform the same command as was in the example above but in the interactive session: Pause a pipeline execution Note : to pause a run via CLI, the same rules are applied as on the GUI - only non-cluster On-demand runs can be paused The command to pause a specific running pipeline: pipe pause [OPTIONS] RUN_ID Options Description Non-required options --check-size If set - firstly, checks if free disk space is enough for the commit operation -s / --sync Allows to perform pause operation in a sync mode. When set - terminal will be blocked until the PAUSED status won't be returned for the pipeline In the example below we will launch a pipeline run and then pause it. Before the pausing, we will additionally check free disk space for the commit: Resume paused pipeline execution The command to resume a specific paused pipeline: pipe resume [OPTIONS] RUN_ID Options Description Non-required options -s / --sync Allows to perform resume operation in a sync mode. When set - terminal will be blocked until the RUNNING status won't be returned for the pipeline In the example below we will resume a previously paused pipeline run: Stop a pipeline execution The command to stop a specific running pipeline: pipe stop [OPTIONS] RUN_ID Options Description Non-required options -y / --yes Do not ask confirmation In the example below we will run a pipeline and then stop it: Terminate a node The command to terminate a specific calculation node: pipe terminate-node [OPTIONS] NODE_NAME Options Description Non-required options -y / --yes Do not ask confirmation NODE_NAME - calculation node name (ID). You can know it, for example, by the view-runs command with the -nd option, the view-cluster command or via the GUI. In the example below we will terminate a node of the running pipeline:","title":"14.5. Manage pipeline executions"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/#145-manage-pipeline-executions-via-cli","text":"View pipeline runs Schedule a pipeline execution Change execution environment Change advanced options Setting parameters for a launch Launch a cluster Launch a pipeline in a synchronized mode Launch a job on the existing running instance Run a tool Generate pipeline launch command via the GUI Runs sharing View sharing status Share a run Unshare a run Run a single command or an interactive session over the SSH protocol Pause a pipeline execution Resume paused pipeline execution Stop a pipeline execution Terminate a node Cloud Pipeline CLI has to be installed. See 14. Command-line interface (CLI) .","title":"14.5. Manage pipeline executions via CLI"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/#view-pipeline-runs","text":"The command to view runs info: pipe view-runs [OPTIONS] [RUN_ID] Options Description Non-required options -s / --status [ANY | FAILURE | PAUSED | PAUSING | RESUMING | RUNNING | STOPPED | SUCCESS] List pipeline runs with a specific status -df / --date-from List pipeline runs started after specified date -dt / --date-to List pipeline runs completed before specified date -p / --pipeline List history of runs for a specific pipeline -pid / --parent-id List runs for a specific parent pipeline run -f / --find Search runs with a specific substring in run parameters values -t / --top Display top N records -nd / --node-details Display node details -pd / --parameters-details Display parameters -td / --tasks-details Display tasks That command without any arguments will list all active runs available for the current user (more about permissions see here ): To view information about specific run - enter its RunID at the end of the command, e.g.: You can additionally display information about instance used by a specific run - by the -nd ( --node-details ) flag: You can additionally display information about pipeline parameters used by a specific run - by the -pd ( --parameters-details ) flag: You can additionally display information about run tasks - by the -td ( --tasks-details ) flag: Note : by default, the view-runs command outputs information about active runs. To view a list of all runs with a specific status (see possible values in the table above) - use the -s ( --status ) option, e.g. to view a list of failed runs: pipe view-runs -s FAILURE By default, only last 100 records are displayed. If you want to change this count, use the -t ( --top ) option with specifying a number of records to show, e.g.: Option -df ( --date-from ) allows to output runs started after specified datetime. Datetime shall be specified in one of the following formats: yyyy-MM-dd HH:mm:ss or yyyy-MM-dd , e.g.: Option -dt ( --date-to ) allows to output runs completed before specified datetime. Note : since the view-runs command outputs by default information about active runs, then for using that command with the -dt option one of the completed statuses must be specified, e.g.: To list history of runs for a specific pipeline use the -p ( --pipeline ) option and a pipeline name. By default it will print a list of active runs for the latest pipeline version. If you want to launch non-latest pipeline version, specify version name after the pipeline name using the @ symbol, e.g. to list STOPPED runs for the v1 version of the simplepipeline pipeline: pipe view-runs -p simplepipeline@v1 -s STOPPED To list child runs use the -pid ( --parent-id ) option and a parent run ID, e.g.: To find runs with a specific substring in run parameters values use the -f ( --find ) option. E.g. to find among all stopped runs only those which parameter values contain the word \"Changed\": pipe view-runs -s STOPPED -f \"Changed\"","title":"View pipeline runs"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/#schedule-a-pipeline-execution","text":"The command to shedule a pipeline/version execution: pipe run [OPTIONS] [RUN_PARAMETERS] Options Description Non-required options -n / --pipeline Pipeline name or ID -c / --config Pipeline configuration name -di / --docker-image Docker image -it / --instance-type Node type in terms of the Cloud Provider -id / --instance-disk Instance disk size -ic / --instance-count Number of worker instances to launch in a cluster -nc / --cores Number of cores that a cluster shall contain. This option will be ignored if -ic ( --instance-count ) option was specified -r / --region-id Instance Cloud region -pt / --price-type [spot | on-demand] Price type -t / --timeout Timeout (in minutes), when elapsed - run will be stopped -cmd / --cmd-template Command template -p / --parameters Returns a parameter list for the specified pipeline -pn / --parent-node Parent instance Run ID. Allows to run a pipeline as a child job on the existing running instance -s / --sync Allows a pipeline to be run in a sync mode. When set - terminal will be blocked until the pipeline won't finish the run. After that the pipeline's finish status will be returned into the terminal output and the terminal will be unblocked -np / --non-pause Allows to switch off the auto-pause option. Note : this option is supported for on-demand runs only -y / --yes Do not ask confirmation -q / --quiet Quiet mode RUN_PARAMETERS - list of the pipeline parameters to set. Can be specified as a single --<Parameter name> <value> pair or a list of them. In the example below the pipeline with the name simplepipeline will be launched: pipe run -n simplepipeline -y And then we'll check that it was launched by the view-runs command: If not additionally specified, described command will launch default configuration of the pipeline latest version. If you want to launch non-default pipeline configuration, specify its name with the -c ( --config ) option, e.g.: pipe run -n simplepipeline -c new-config -y In the example above the configuration with the name new-config of the pipeline simplepipeline was launched. If you want to launch non-latest pipeline version, specify version name after the pipeline name using the @ symbol, e.g.: pipe run -n simplepipeline@v1 -y In the example above default configuration of the version with the name v1 of the pipeline simplepipeline was launched.","title":"Schedule a pipeline execution"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/#change-execution-environment","text":"You can change execution environment for a pipeline run by setting the following options: -di ( --docker-image ), -it ( --instance-type ), -id ( --instance-disk ), -r ( --region-id ). In the example below the pipeline simplepipeline will be launched on the instance n1-highcpu-2 with the disk size 25 Gb: pipe run -n simplepipeline --instance-type n1-highcpu-2 --instance-disk 25 -y Note : for the -di ( --docker-image ) option shall specify a full path to the Docker image including its version.","title":"Change execution environment"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/#change-advanced-options","text":"You can change advanced options for a pipeline execution by setting the following options: -pt ( --price-type ), -t ( --timeout ), -cmd ( --cmd-template ). E.g. the command below will launch the pipeline simplepipeline with the spot price type and changed command template: pipe run -n simplepipeline -pt spot -cmd \"sleep 1d\" Note : -pt flag could take only one from two values - on-demand and spot , independent on the Cloud Provider.","title":"Change advanced options"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/#setting-parameters-for-a-launch","text":"To view all parameter list for a pipeline use -p ( --parameters ) flag, e.g. to view parameters of the simplepipeline pipeline: pipe run -n simplepipeline -p As you can see, that flag allows to view all parameters with their types. If parameters have default values they will be also printed. To set parameters for a pipeline launch, enter them after all options in the following manner: --<Parameter1 name> <value> --<Parameter2 name> <value> ... , e.g.: pipe run -n simplepipeline --booleanParamExample false --stringParamExample \"Changed test value\" -y","title":"Setting parameters for a launch"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/#launch-a-cluster","text":"You can launch a cluster using -ic ( --instance-count ) option. It sets a number of worker instances. In the example below the cluster with two child nodes will be launched: pipe run -n simplepipeline -ic 2 Also you can launch a cluster using the -nc ( --number-cores ) option. With this option you specify a number of cores that a cluster shall contain. In that case worker instance count of the cluster will be calculated automatically based on the supported instance types. Note : This option will be ignored if -ic ( --instance-count ) option was specified. In the example below the cluster runs with 16 cores will be launched: pipe run -n simplepipeline -nc 16 As you can see, for that run single instance with 16 cores was scheduled.","title":"Launch a cluster"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/#launch-a-pipeline-in-a-synchronized-mode","text":"You can launch a pipeline in a synchronized mode by setting the -s ( --sync ) flag. In that mode terminal will be blocked until the pipeline won't finish the run. After the pipeline's finish, its status will be returned into the terminal output and the terminal will be unblocked. Example of the pipeline simplepipeline execution in a sync mode: pipe run -n simplepipeline --sync -y","title":"Launch a pipeline in a synchronized mode"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/#launch-a-job-on-the-existing-running-instance","text":"You can launch a pipeline as a child job on the existing running instance by setting the -pn ( --parent-node ) option. It allows not to initialize a new node for a such job but use already existing one. Example of the launching the pipeline simplepipeline as a child of the previously launched and initialized instance: pipe run -n simplepipeline --parent-node <Parent RunID> -q -y","title":"Launch a job on the existing running instance"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/#run-a-tool","text":"Also you can launch a tool - without providing a pipeline name/ID. In that case Docker image shall be mandatory specified. If instance type, instance disk and cmd template aren't specified, default tool settings for them will be used. In the example below we will launch the Docker image ubuntu:latest on the n1-highcpu-2 instance with the disk size 27 GB and sleep infinity command template: pipe run -di <Docker image path> -it n1-highcpu-2 -id 27 -cmd \"sleep infinity\" -y Note : Docker image path shall be specified in a full manner including version.","title":"Run a tool"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/#generate-pipeline-launch-command-via-the-gui","text":"The construction of the correct run command sometimes could be hard for users, so users can generate necessary launch commands via the GUI. For that: Via the GUI open the Launch page of a pipeline/tool you want to run At the Launch page, configure all settings for the run as you want and then click the \" CLI command \" button in the right-upper corner: In the appeared popup copy the pipe run command, e.g.: Paste the copied command into the terminal and perform it Note : instead of step 2 described above, you may open the Run logs page of any active/completed run and click the LAUNCH COMMAND button in the right-upper corner to view the launch command of the selected run: Also, user can select the API tab in the \"Launch commands\" popup and get the POST request for a job launch:","title":"Generate pipeline launch command via the GUI"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/#runs-sharing","text":"Note : to share a run with other users/groups, user shall be the OWNER of that run or has an ADMIN role. About sharing run via the GUI see here .","title":"Runs sharing"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/#view-sharing-status","text":"The command to view for whom a run is shared: pipe share get RUN_ID The example below lists users/groups for whom the run with ID 38081 is shared: pipe share get 38081","title":"View sharing status"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/#share-a-run","text":"The command to share a running job with other users/groups: pipe share add RUN_ID [OPTIONS] Options Description -su / --shared-user Specifies the user for whom the run will be shared. Multiple options are supported -sg / --shared-group Specifies the group/role for which the run will be shared. Multiple options are supported -ssh / --share-ssh Share SSH-session of the run. Non-required option for the runs with endpoints One of the option -su or -sg shall be necessarily specified. Both options also can be used simultaneously. -ssh option is necessarily required only in cases when the run hasn't endpoints. In the example below we will share the SSH-session of the run with ID 38081 with the user USER1 and then check it by the command pipe share get : pipe share add 38081 -ssh -su USER1","title":"Share a run"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/#unshare-a-run","text":"The command to unshare a running job from other users/groups for whom the access was shared before: pipe share remove RUN_ID [OPTIONS] Options Description Non-required -su / --shared-user Specifies the user for whom the sharing of the run will be disabled. Multiple options are supported -sg / --shared-group Specifies the group/role for which the sharing of the run will be disabled. Multiple options are supported -ssh / --share-ssh Remove only SSH-sharing for all users/groups for runs with endpoint(s) or remove all users/groups of the run shared list for runs without endpoints Without any additional options the command will remove all sharings of the run. In the example below we will unshare the run with ID 38081 with the role ROLE_USER and check it by the command pipe share get : pipe share remove 38081 -sg ROLE_USER","title":"Unshare a run"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/#run-a-single-command-or-an-interactive-session-over-the-ssh-protocol","text":"Note : to perform a command or run an interactive session over the running job, user shall be the OWNER of that job or has an ADMIN role. The command to run a single command or an interactive session over the SSH protocol for the specified job run: pipe ssh RUN_ID [COMMAND] COMMAND - a single command to execute over the running instance with the specified Run ID using the SSH protocol. In the example below we will list the content of the root directory of the job run with ID 12370 : pipe ssh 12370 'ls -l /root' If COMMAND isn't specified, it will start an interactive session over the running instance with the specified Run ID using the SSH protocol. To exit from the interactive session use commands exit or logout . We will perform the same command as was in the example above but in the interactive session:","title":"Run a single command or an interactive session over the SSH protocol"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/#pause-a-pipeline-execution","text":"Note : to pause a run via CLI, the same rules are applied as on the GUI - only non-cluster On-demand runs can be paused The command to pause a specific running pipeline: pipe pause [OPTIONS] RUN_ID Options Description Non-required options --check-size If set - firstly, checks if free disk space is enough for the commit operation -s / --sync Allows to perform pause operation in a sync mode. When set - terminal will be blocked until the PAUSED status won't be returned for the pipeline In the example below we will launch a pipeline run and then pause it. Before the pausing, we will additionally check free disk space for the commit:","title":"Pause a pipeline execution"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/#resume-paused-pipeline-execution","text":"The command to resume a specific paused pipeline: pipe resume [OPTIONS] RUN_ID Options Description Non-required options -s / --sync Allows to perform resume operation in a sync mode. When set - terminal will be blocked until the RUNNING status won't be returned for the pipeline In the example below we will resume a previously paused pipeline run:","title":"Resume paused pipeline execution"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/#stop-a-pipeline-execution","text":"The command to stop a specific running pipeline: pipe stop [OPTIONS] RUN_ID Options Description Non-required options -y / --yes Do not ask confirmation In the example below we will run a pipeline and then stop it:","title":"Stop a pipeline execution"},{"location":"manual/14_CLI/14.5._Manage_pipeline_executions_via_CLI/#terminate-a-node","text":"The command to terminate a specific calculation node: pipe terminate-node [OPTIONS] NODE_NAME Options Description Non-required options -y / --yes Do not ask confirmation NODE_NAME - calculation node name (ID). You can know it, for example, by the view-runs command with the -nd option, the view-cluster command or via the GUI. In the example below we will terminate a node of the running pipeline:","title":"Terminate a node"},{"location":"manual/14_CLI/14.6._View_cluster_nodes_via_CLI/","text":"14.6. View cluster nodes via CLI Export cluster utilization The command to view working nodes info: pipe view-cluster [NODE_NAME] NODE_NAME - calculation node name (ID). That command without any arguments will list all working nodes: Or you can view full information about specific node by specifying its name/ID at the end of the command, e.g.: Export cluster utilization Users can export Cluster Node Monitor reports by pipe abilities. The command to download the node usage metrics: pipe cluster monitor [OPTIONS] Options Description Required options -i / --instance-id ID Specifies the cloud instance ID. This option cannot be used in conjunction with the --run-id option -r / --run-id ID Specifies the pipeline run ID. This option cannot be used in conjunction with the --instance-id option Non-required -o / --output TEXT Set the output file for monitoring report. If not specified the report file will be automatically generated in the current folder -df / --date-from TEXT Set the start date for monitoring data collection. If not specified a --date-to option value minus 1 day will be used. Note : data format shall be specified like yyyy-MM-dd HH:mm:ss or yyyy-MM-dd -dt / --date-to TEXT Set the end date for monitoring data collection. If not specified the current date and time will be used. Note : data format shall be specified like yyyy-MM-dd HH:mm:ss or yyyy-MM-dd -p / --interval TEXT Set the time interval of resources utilization statistics that will be exported. Note : data format shall be specified like <N>m for minutes or <N>h for hours, where <N> is the required number of minutes/hours. Default value: 1m -rt / --report-type TEXT Exported report type (case insensitive). Currently CSV and XLS are supported. Default value: CSV The following command will download the monitoring report of the run with ID \"56440\" from \"26th January 2021\" till \"28th January 2021\" in the Excel-format, data will be split into 5-minute intervals: pipe cluster monitor -r 56440 --report-type XLS --interval 5m -df 2021-01-26 -dt 2021-01-28 The report will have the same view as one made from the GUI:","title":"14.6. View cluster nodes"},{"location":"manual/14_CLI/14.6._View_cluster_nodes_via_CLI/#146-view-cluster-nodes-via-cli","text":"Export cluster utilization The command to view working nodes info: pipe view-cluster [NODE_NAME] NODE_NAME - calculation node name (ID). That command without any arguments will list all working nodes: Or you can view full information about specific node by specifying its name/ID at the end of the command, e.g.:","title":"14.6. View cluster nodes via CLI"},{"location":"manual/14_CLI/14.6._View_cluster_nodes_via_CLI/#export-cluster-utilization","text":"Users can export Cluster Node Monitor reports by pipe abilities. The command to download the node usage metrics: pipe cluster monitor [OPTIONS] Options Description Required options -i / --instance-id ID Specifies the cloud instance ID. This option cannot be used in conjunction with the --run-id option -r / --run-id ID Specifies the pipeline run ID. This option cannot be used in conjunction with the --instance-id option Non-required -o / --output TEXT Set the output file for monitoring report. If not specified the report file will be automatically generated in the current folder -df / --date-from TEXT Set the start date for monitoring data collection. If not specified a --date-to option value minus 1 day will be used. Note : data format shall be specified like yyyy-MM-dd HH:mm:ss or yyyy-MM-dd -dt / --date-to TEXT Set the end date for monitoring data collection. If not specified the current date and time will be used. Note : data format shall be specified like yyyy-MM-dd HH:mm:ss or yyyy-MM-dd -p / --interval TEXT Set the time interval of resources utilization statistics that will be exported. Note : data format shall be specified like <N>m for minutes or <N>h for hours, where <N> is the required number of minutes/hours. Default value: 1m -rt / --report-type TEXT Exported report type (case insensitive). Currently CSV and XLS are supported. Default value: CSV The following command will download the monitoring report of the run with ID \"56440\" from \"26th January 2021\" till \"28th January 2021\" in the Excel-format, data will be split into 5-minute intervals: pipe cluster monitor -r 56440 --report-type XLS --interval 5m -df 2021-01-26 -dt 2021-01-28 The report will have the same view as one made from the GUI:","title":"Export cluster utilization"},{"location":"manual/14_CLI/14.7._View_and_manage_Permissions_via_CLI/","text":"14.7. View and manage Permissions via CLI View permissions Manage permissions Example: set permissions for a folder Example: set permissions for a pipeline View the list of objects accessible by a user View the list of objects accessible by a group Change OWNER property Cloud Pipeline CLI has to be installed. See 14. Command-line interface (CLI) . View permissions To view permissions for the object you need READ permission for the object. See 13. Permissions . Command to list all permissions for a specific object: pipe view-acl -t|--object-type <Object type> <Object id/name> Two parameters are required: Object type - defines a name of the object class. Possible values: pipeline , folder , data_storage . Object id/name - defines a name of an object of a specified class. Note : full path to the object has to be specified. In the example below we check permissions for the folder \" workfolder/manage-permissions-folder \": pipe view-acl -t folder \"workfolder/manage-permissions-folder\" To check permissions for the Data Storage with ID 3976 : pipe view-acl --object-type data_storage 3976 Manage permissions To manage permissions for the object you need to be an OWNER of that object or you need to have the ADMIN role. See 13. Permissions . Command to set permissions for the object: pipe set-acl -t|--object-type <Object type> -s|--sid <User/Group name> [-g|--group] -a|--allow/-d|--deny/-i|--inherit <w>/<x>/<r> <Object id/name> The following parameters are required: Object type - defines a name of the object class. Possible values: pipeline , folder , data_storage . User or Group name - defines a name of an user or a group (role) for whom permissions will be set. Note : the option -g (or --group ) shall be necessarily specified when permissions are being set for a group (role) Allow ( -a or --allow ), Deny ( -d or --deny ), Inherit ( -i or --inherit ) - actions that could be performed with permissions. WRITE ( w ), READ ( r ) and EXECUTE ( x ) - permissions for setting. Object id or name - defines an ID or name of an object of the specified class to set permissions for. Note : full path to the object has to be specified if the name is not unique (in cases for Data Storage , Pipeline ). Note : permissions and actions over them could be written in command in any combinations. See examples below. Example: set permissions for a folder Here we demonstrate how to set permissions for a folder . You can set permissions for other CP objects in the same way. In the example below we grant the user USER3 READ access and deny WRITE and EXECUTE access to the directory \" workfolder/manage-permissions-folder \". pipe set-acl -t folder -s USER3 -d wx -a r \"workfolder/manage-permissions-folder\" Example: set permissions for a pipeline In the example below we grant the role ROLE_USER READ and WRITE access to the pipeline with ID 5937 . pipe set-acl --object-type pipeline --sid ROLE_USER --group --allow rw 5937 View the list of objects accessible by a user To view objects accessible for a user you shall have the ROLE_ADMIN role. The command to view the full list of objects accessible by a user: pipe view-user-objects <Username> [OPTIONS] Where <Username> defines the name of the user for which you wish to view the accessible object list. Options Description Non-required options -t / --object-type <OBJECT_TYPE> Defines a name of the object class. If specified, the command output will contain only the list of accessible objects of the specific type by a user. Possible values: pipeline , folder , data_storage , configuration , docker_registry , tool , tool_group In the example below we'll print the list of objects accessible to the user user3 : pipe view-user-objects user3 In the example below we'll print the list of pipelines accessible to the user demo : pipe view-user-objects -t pipeline demo View the list of objects accessible by a group To view objects accessible for a group you shall have the ROLE_ADMIN role. The command to view the full list of objects accessible by a user group/role: pipe view-group-objects <Groupname> [OPTIONS] Where <Groupname> defines the name of the user group/role for which you wish to view the accessible object list. Options Description Non-required options -t / --object-type <OBJECT_TYPE> Defines a name of the object class. If specified, the command output will contain only the list of accessible objects of the specific type by a user group. Possible values: pipeline , folder , data_storage , configuration , docker_registry , tool , tool_group In the example below we'll print the list of objects accessible to the role ROLE_USER : pipe view-group-objects ROLE_USER In the example below we'll print the list of pipelines accessible to the group LIBRARY : pipe view-group-objects -t pipeline ROLE_LIBRARY Change OWNER property Each object has a mandatory OWNER property. You can change an owner of the Cloud Pipeline object via CLI. Please note, for do that, you shall be an object OWNER or have the ROLE_ADMIN role. Note : how to change an object owner via the GUI see here . Command to change an owner of the object: pipe chown <User name> <Object class> <Object id/name> Three parameters shall be specified: User name - defines a user name of a desired object owner. Object class - defines a name of the object class. Possible values: data_storage , docker_registry , folder , pipeline , tool , tool_group , configuration . Object id/name - defines an ID or name of an object of the specified object class. Note : full path to the object has to be specified. Paths to Docker registry and Tool objects should include registry IP address. The example below will change an owner to USER3 for the pipeline with ID 5937 : pipe chown USER3 pipeline 5937","title":"14.7. View and manage Permissions"},{"location":"manual/14_CLI/14.7._View_and_manage_Permissions_via_CLI/#147-view-and-manage-permissions-via-cli","text":"View permissions Manage permissions Example: set permissions for a folder Example: set permissions for a pipeline View the list of objects accessible by a user View the list of objects accessible by a group Change OWNER property Cloud Pipeline CLI has to be installed. See 14. Command-line interface (CLI) .","title":"14.7. View and manage Permissions via CLI"},{"location":"manual/14_CLI/14.7._View_and_manage_Permissions_via_CLI/#view-permissions","text":"To view permissions for the object you need READ permission for the object. See 13. Permissions . Command to list all permissions for a specific object: pipe view-acl -t|--object-type <Object type> <Object id/name> Two parameters are required: Object type - defines a name of the object class. Possible values: pipeline , folder , data_storage . Object id/name - defines a name of an object of a specified class. Note : full path to the object has to be specified. In the example below we check permissions for the folder \" workfolder/manage-permissions-folder \": pipe view-acl -t folder \"workfolder/manage-permissions-folder\" To check permissions for the Data Storage with ID 3976 : pipe view-acl --object-type data_storage 3976","title":"View permissions"},{"location":"manual/14_CLI/14.7._View_and_manage_Permissions_via_CLI/#manage-permissions","text":"To manage permissions for the object you need to be an OWNER of that object or you need to have the ADMIN role. See 13. Permissions . Command to set permissions for the object: pipe set-acl -t|--object-type <Object type> -s|--sid <User/Group name> [-g|--group] -a|--allow/-d|--deny/-i|--inherit <w>/<x>/<r> <Object id/name> The following parameters are required: Object type - defines a name of the object class. Possible values: pipeline , folder , data_storage . User or Group name - defines a name of an user or a group (role) for whom permissions will be set. Note : the option -g (or --group ) shall be necessarily specified when permissions are being set for a group (role) Allow ( -a or --allow ), Deny ( -d or --deny ), Inherit ( -i or --inherit ) - actions that could be performed with permissions. WRITE ( w ), READ ( r ) and EXECUTE ( x ) - permissions for setting. Object id or name - defines an ID or name of an object of the specified class to set permissions for. Note : full path to the object has to be specified if the name is not unique (in cases for Data Storage , Pipeline ). Note : permissions and actions over them could be written in command in any combinations. See examples below.","title":"Manage permissions"},{"location":"manual/14_CLI/14.7._View_and_manage_Permissions_via_CLI/#example-set-permissions-for-a-folder","text":"Here we demonstrate how to set permissions for a folder . You can set permissions for other CP objects in the same way. In the example below we grant the user USER3 READ access and deny WRITE and EXECUTE access to the directory \" workfolder/manage-permissions-folder \". pipe set-acl -t folder -s USER3 -d wx -a r \"workfolder/manage-permissions-folder\"","title":"Example: set permissions for a folder"},{"location":"manual/14_CLI/14.7._View_and_manage_Permissions_via_CLI/#example-set-permissions-for-a-pipeline","text":"In the example below we grant the role ROLE_USER READ and WRITE access to the pipeline with ID 5937 . pipe set-acl --object-type pipeline --sid ROLE_USER --group --allow rw 5937","title":"Example: set permissions for a pipeline"},{"location":"manual/14_CLI/14.7._View_and_manage_Permissions_via_CLI/#view-the-list-of-objects-accessible-by-a-user","text":"To view objects accessible for a user you shall have the ROLE_ADMIN role. The command to view the full list of objects accessible by a user: pipe view-user-objects <Username> [OPTIONS] Where <Username> defines the name of the user for which you wish to view the accessible object list. Options Description Non-required options -t / --object-type <OBJECT_TYPE> Defines a name of the object class. If specified, the command output will contain only the list of accessible objects of the specific type by a user. Possible values: pipeline , folder , data_storage , configuration , docker_registry , tool , tool_group In the example below we'll print the list of objects accessible to the user user3 : pipe view-user-objects user3 In the example below we'll print the list of pipelines accessible to the user demo : pipe view-user-objects -t pipeline demo","title":"View the list of objects accessible by a user"},{"location":"manual/14_CLI/14.7._View_and_manage_Permissions_via_CLI/#view-the-list-of-objects-accessible-by-a-group","text":"To view objects accessible for a group you shall have the ROLE_ADMIN role. The command to view the full list of objects accessible by a user group/role: pipe view-group-objects <Groupname> [OPTIONS] Where <Groupname> defines the name of the user group/role for which you wish to view the accessible object list. Options Description Non-required options -t / --object-type <OBJECT_TYPE> Defines a name of the object class. If specified, the command output will contain only the list of accessible objects of the specific type by a user group. Possible values: pipeline , folder , data_storage , configuration , docker_registry , tool , tool_group In the example below we'll print the list of objects accessible to the role ROLE_USER : pipe view-group-objects ROLE_USER In the example below we'll print the list of pipelines accessible to the group LIBRARY : pipe view-group-objects -t pipeline ROLE_LIBRARY","title":"View the list of objects accessible by a group"},{"location":"manual/14_CLI/14.7._View_and_manage_Permissions_via_CLI/#change-owner-property","text":"Each object has a mandatory OWNER property. You can change an owner of the Cloud Pipeline object via CLI. Please note, for do that, you shall be an object OWNER or have the ROLE_ADMIN role. Note : how to change an object owner via the GUI see here . Command to change an owner of the object: pipe chown <User name> <Object class> <Object id/name> Three parameters shall be specified: User name - defines a user name of a desired object owner. Object class - defines a name of the object class. Possible values: data_storage , docker_registry , folder , pipeline , tool , tool_group , configuration . Object id/name - defines an ID or name of an object of the specified object class. Note : full path to the object has to be specified. Paths to Docker registry and Tool objects should include registry IP address. The example below will change an owner to USER3 for the pipeline with ID 5937 : pipe chown USER3 pipeline 5937","title":"Change OWNER property"},{"location":"manual/14_CLI/14.8._View_tools_definitions_via_CLI/","text":"14.8. View tools definitions via CLI View a list of groups in the registry View a list of tools in the group View a tool definition View tool version's details Using the \"path\" to view the object Cloud Pipeline CLI has to be installed. See 14. Command-line interface (CLI) . Via the CLI users can view details of a tool/specific tool version or tools groups. The general command to perform these operations: pipe view-tools [OPTIONS] Options Description Non-required options -r / --registry Defines a specific Docker registry -g / --group Defines a specific tool group in a registry -t / --tool Defines a specific tool in a tool group -v / --version Defines a specific version of a tool Without any arguments that command will output a list of the tools contained in: In a personal tool group If not personal group is available - in the library or default tool group If none of the above is available - a corresponding warning will be printed Note : If more than one registry exists on the current Cloud Pipeline deployment - Docker registry shall be forcibly specified ( -r becomes a mandatory option) or the corresponding error message will be printed. View a list of groups in the registry With specifying a Docker registry that command will output a list of tools groups of that registry. Note : Docker registry shall be specified as <registry_name>:<port> . E.g.: pipe view-tools --registry <registry_name>:<port> View a list of tools in the group With specifying a tools group that command will output a list of tools in the specific group. Note : If more than one registry exists in the current Cloud Pipeline deployment - Docker registry shall be also specified. E.g.: pipe view-tools [--registry <registry_name>] --group <group_name> In the example above, the list of tools in user's personal tools group was printed. View a tool definition To view a specific tool's definition and the list of tool versions - use the tool name together with its group name. Note : If more than one registry exists in the current Cloud Pipeline deployment - Docker registry shall be also specified. E.g.: pipe view-tools [--registry <registry_name>] --group <group_name> --tool <tool_name> View tool version's details To view details of a specific tool version - use the tool name together with the version and the group name. Note : If more than one registry exists in the current Cloud Pipeline deployment - Docker registry shall be also specified. E.g.: pipe view-tools [--registry <registry_name>] --group <group_name> --tool <tool_name> --version <version_name> Details of a specific tool version contain: tool definition list of the tool version execution settings (if specified) list of the tool version vulnerabilities list of the tool version packages Using the \"path\" to view the object Users also can view definitions via the \"path\" to the object (registry/group/tool). The \"full path\" format is: <registry_name>:<port>/<group_name>/<tool_name>:<verion_name> . In that case, the specifying of command options ( -r / -g / -t / -v ) is not required. So: pipe view-tools <registry_name>:<port> will show a list of tools groups in the specified registry pipe view-tools <registry_name>:<port>/<group_name> will show a list of tools in the specified group pipe view-tools <registry_name>:<port>/<group_name>/<tool_name> will show a definition of the specified tool pipe view-tools <registry_name>:<port>/<group_name>/<tool_name>:<verion_name> will show details of the specified tool version","title":"14.8. View tools definitions"},{"location":"manual/14_CLI/14.8._View_tools_definitions_via_CLI/#148-view-tools-definitions-via-cli","text":"View a list of groups in the registry View a list of tools in the group View a tool definition View tool version's details Using the \"path\" to view the object Cloud Pipeline CLI has to be installed. See 14. Command-line interface (CLI) . Via the CLI users can view details of a tool/specific tool version or tools groups. The general command to perform these operations: pipe view-tools [OPTIONS] Options Description Non-required options -r / --registry Defines a specific Docker registry -g / --group Defines a specific tool group in a registry -t / --tool Defines a specific tool in a tool group -v / --version Defines a specific version of a tool Without any arguments that command will output a list of the tools contained in: In a personal tool group If not personal group is available - in the library or default tool group If none of the above is available - a corresponding warning will be printed Note : If more than one registry exists on the current Cloud Pipeline deployment - Docker registry shall be forcibly specified ( -r becomes a mandatory option) or the corresponding error message will be printed.","title":"14.8. View tools definitions via CLI"},{"location":"manual/14_CLI/14.8._View_tools_definitions_via_CLI/#view-a-list-of-groups-in-the-registry","text":"With specifying a Docker registry that command will output a list of tools groups of that registry. Note : Docker registry shall be specified as <registry_name>:<port> . E.g.: pipe view-tools --registry <registry_name>:<port>","title":"View a list of groups in the registry"},{"location":"manual/14_CLI/14.8._View_tools_definitions_via_CLI/#view-a-list-of-tools-in-the-group","text":"With specifying a tools group that command will output a list of tools in the specific group. Note : If more than one registry exists in the current Cloud Pipeline deployment - Docker registry shall be also specified. E.g.: pipe view-tools [--registry <registry_name>] --group <group_name> In the example above, the list of tools in user's personal tools group was printed.","title":"View a list of tools in the group"},{"location":"manual/14_CLI/14.8._View_tools_definitions_via_CLI/#view-a-tool-definition","text":"To view a specific tool's definition and the list of tool versions - use the tool name together with its group name. Note : If more than one registry exists in the current Cloud Pipeline deployment - Docker registry shall be also specified. E.g.: pipe view-tools [--registry <registry_name>] --group <group_name> --tool <tool_name>","title":"View a tool definition"},{"location":"manual/14_CLI/14.8._View_tools_definitions_via_CLI/#view-tool-versions-details","text":"To view details of a specific tool version - use the tool name together with the version and the group name. Note : If more than one registry exists in the current Cloud Pipeline deployment - Docker registry shall be also specified. E.g.: pipe view-tools [--registry <registry_name>] --group <group_name> --tool <tool_name> --version <version_name> Details of a specific tool version contain: tool definition list of the tool version execution settings (if specified) list of the tool version vulnerabilities list of the tool version packages","title":"View tool version's details"},{"location":"manual/14_CLI/14.8._View_tools_definitions_via_CLI/#using-the-path-to-view-the-object","text":"Users also can view definitions via the \"path\" to the object (registry/group/tool). The \"full path\" format is: <registry_name>:<port>/<group_name>/<tool_name>:<verion_name> . In that case, the specifying of command options ( -r / -g / -t / -v ) is not required. So: pipe view-tools <registry_name>:<port> will show a list of tools groups in the specified registry pipe view-tools <registry_name>:<port>/<group_name> will show a list of tools in the specified group pipe view-tools <registry_name>:<port>/<group_name>/<tool_name> will show a definition of the specified tool pipe view-tools <registry_name>:<port>/<group_name>/<tool_name>:<verion_name> will show details of the specified tool version","title":"Using the \"path\" to view the object"},{"location":"manual/14_CLI/14.9._User_management_via_CLI/","text":"14.9. User management via CLI Batch import Instances usage Cloud Pipeline CLI has to be installed. See 14. Command-line interface (CLI) . Batch import pipe CLI offers the same options to import the users from a CSV file as it can be performed via GUI . Command to import users from CSV file: pipe users import [OPTIONS] FILE_PATH Where FILE_PATH - defines a path to the CSV file with users list Possible options: Options Description Non-required options -cu / --create-user Allows new user creation -cg / --create-group Allows new group creation -cm / --create-metadata <KEY> Allows to create a new metadata with specified key. Multiple options supported To mimic the GUI scenario, described here , the following command will be used (it imports an example CSV and allows creation of any object, that does not exist yet): pipe users import --create-user \\ --create-group \\ --create-metadata billing-center \\ --create-metadata import_attr1 \\ ~/import-test.csv During the execution - pipe command prints detailed logs, regarding the operations, which were performed and how the CSV was parsed: [INFO] User 'IMPORT_USER1' successfully created. [INFO] Role 'ROLE_IMPORT_GROUP1' successfully created. [INFO] Role 'ROLE_IMPORT_GROUP1' successfully assigned to user 'IMPORT_USER1'. [INFO] A new metadata 'billing-center'='Center1' added to user 'IMPORT_USER1'. [INFO] A new metadata 'billing-group'='Group1' added to user 'IMPORT_USER1'. [INFO] A new metadata 'import_attr1'='import_attr1_val1' added to user 'IMPORT_USER1'. [INFO] User 'IMPORT_USER2' successfully created. [INFO] Role 'ROLE_IMPORT_GROUP2' successfully created. [INFO] Role 'ROLE_IMPORT_GROUP2' successfully assigned to user 'IMPORT_USER2'. [INFO] A new metadata 'billing-center'='Center1' added to user 'IMPORT_USER2'. [INFO] A new metadata 'billing-group'='Group1' added to user 'IMPORT_USER2'. [INFO] A new metadata 'import_attr1'='import_attr1_val2' added to user 'IMPORT_USER2'. [INFO] User 'IMPORT_USER3' successfully created. [INFO] Role 'ROLE_IMPORT_GROUP1' successfully assigned to user 'IMPORT_USER3'. [INFO] Role 'ROLE_IMPORT_GROUP2' successfully assigned to user 'IMPORT_USER3'. [INFO] A new metadata 'billing-center'='Center2' added to user 'IMPORT_USER3'. [INFO] A new metadata 'billing-group'='Group1' added to user 'IMPORT_USER3'. [INFO] A new metadata 'import_attr1'='import_attr1_val3' added to user 'IMPORT_USER3'. To view results see here . They will be the same as on GUI. Instances usage Via the pipe CLI users can view count of instances running by them at the moment. The general command to perform this operation: pipe users instances [OPTIONS] Options Description Non-required options -v / --verbose Shows all user's active restrictions by the instances count in a table view This command will show: summary number of instances running by the user at the moment (including cluster's worker nodes) the configured restriction by the instances count (i.e. maximal count of the instances that current user can launch simultaneously). By default, without -v option, only the restriction with the highest priority will be shown. Example of the command outputs: For more details about user's restrictions by the launched instances count see here .","title":"14.9. User management"},{"location":"manual/14_CLI/14.9._User_management_via_CLI/#149-user-management-via-cli","text":"Batch import Instances usage Cloud Pipeline CLI has to be installed. See 14. Command-line interface (CLI) .","title":"14.9. User management via CLI"},{"location":"manual/14_CLI/14.9._User_management_via_CLI/#batch-import","text":"pipe CLI offers the same options to import the users from a CSV file as it can be performed via GUI . Command to import users from CSV file: pipe users import [OPTIONS] FILE_PATH Where FILE_PATH - defines a path to the CSV file with users list Possible options: Options Description Non-required options -cu / --create-user Allows new user creation -cg / --create-group Allows new group creation -cm / --create-metadata <KEY> Allows to create a new metadata with specified key. Multiple options supported To mimic the GUI scenario, described here , the following command will be used (it imports an example CSV and allows creation of any object, that does not exist yet): pipe users import --create-user \\ --create-group \\ --create-metadata billing-center \\ --create-metadata import_attr1 \\ ~/import-test.csv During the execution - pipe command prints detailed logs, regarding the operations, which were performed and how the CSV was parsed: [INFO] User 'IMPORT_USER1' successfully created. [INFO] Role 'ROLE_IMPORT_GROUP1' successfully created. [INFO] Role 'ROLE_IMPORT_GROUP1' successfully assigned to user 'IMPORT_USER1'. [INFO] A new metadata 'billing-center'='Center1' added to user 'IMPORT_USER1'. [INFO] A new metadata 'billing-group'='Group1' added to user 'IMPORT_USER1'. [INFO] A new metadata 'import_attr1'='import_attr1_val1' added to user 'IMPORT_USER1'. [INFO] User 'IMPORT_USER2' successfully created. [INFO] Role 'ROLE_IMPORT_GROUP2' successfully created. [INFO] Role 'ROLE_IMPORT_GROUP2' successfully assigned to user 'IMPORT_USER2'. [INFO] A new metadata 'billing-center'='Center1' added to user 'IMPORT_USER2'. [INFO] A new metadata 'billing-group'='Group1' added to user 'IMPORT_USER2'. [INFO] A new metadata 'import_attr1'='import_attr1_val2' added to user 'IMPORT_USER2'. [INFO] User 'IMPORT_USER3' successfully created. [INFO] Role 'ROLE_IMPORT_GROUP1' successfully assigned to user 'IMPORT_USER3'. [INFO] Role 'ROLE_IMPORT_GROUP2' successfully assigned to user 'IMPORT_USER3'. [INFO] A new metadata 'billing-center'='Center2' added to user 'IMPORT_USER3'. [INFO] A new metadata 'billing-group'='Group1' added to user 'IMPORT_USER3'. [INFO] A new metadata 'import_attr1'='import_attr1_val3' added to user 'IMPORT_USER3'. To view results see here . They will be the same as on GUI.","title":"Batch import"},{"location":"manual/14_CLI/14.9._User_management_via_CLI/#instances-usage","text":"Via the pipe CLI users can view count of instances running by them at the moment. The general command to perform this operation: pipe users instances [OPTIONS] Options Description Non-required options -v / --verbose Shows all user's active restrictions by the instances count in a table view This command will show: summary number of instances running by the user at the moment (including cluster's worker nodes) the configured restriction by the instances count (i.e. maximal count of the instances that current user can launch simultaneously). By default, without -v option, only the restriction with the highest priority will be shown. Example of the command outputs: For more details about user's restrictions by the launched instances count see here .","title":"Instances usage"},{"location":"manual/14_CLI/14._Command-line_interface/","text":"14. Command-line interface (CLI) Introduction Working with CLI CLI options and commands Examples Cloud Pipeline CLI has to be installed. See 14.1. Install and setup CLI . Introduction Working with a Cloud Pipeline from CLI has numerous benefits compared to GUI: It has extra features which are not accessible from GUI such copying or moving files from one storage to another. The uploading file size could exceed 10 Mb. It is more convenient for System administrators. Working with CLI All CLI commands have to be typed into the command line (Terminal) of the computer. pipe [OPTIONS] COMMAND [ARGS]... The pipe is a command-line interface for Cloud Pipeline. It has a number of available commands. Each command has a number of arguments as an input. To get a list of available options and commands type pipe or pipe --help into the command line. CLI options and commands Options --help Show help message and exit --version Show CLI version and exit. Details of the command using see here Commands chown Changes current owner to specified. Details of the command using see here . cluster Cluster nodes operations. Details of the command using see here . configure Configures CLI parameters. This command can be automatically generated . Details of the command using see here . pause Allows to pause pipeline execution. Details of the command using see here . resume Allows to resume previously paused pipeline execution. Details of the command using see here . run Schedules a pipeline execution. Details of the command using see here . set-acl Set object permissions. Details of the command using see here . share Allows to share launched run with users/groups. Details of the command using see here . ssh Runs a single command or an interactive session over the SSH protocol for the specified job run. Details of the command using see here . stop Stops a running pipeline. Details of the command using see here . storage Storage operations. Details of the command using see here . tag Operations with attributes. Details of the command using see here . terminate-node Terminates calculation node. Details of the command using see here . token Prints the authentication token for a specified user. Details of the command using see here . tunnel Run ports tunnelling operations. Details of the command using see here . update Checks the Cloud Pipeline CLI version and updates it to the latest one if required. Details of the command using see here . users User management operations. Details of the command using see here . view-acl View object permissions. Details of the command using see here . view-cluster Lists cluster nodes. Details of the command using see here . view-group-objects Lists the objects accessible to the specific users group. Details of the command using see here . view-pipes Lists pipelines definitions. Details of the command using see here . view-runs Lists pipeline runs. Details of the command using see here . view-tools Displays details of a tool/tool version. Details of the command using see here . view-user-objects Lists the objects accessible to the specific user. Details of the command using see here . Note : To see command's arguments and options type pipe command --help . Examples To see a list of available CLI commands type pipe or pipe --help in the terminal. Note : each command might have its own set of commands that consequently might have their own set of commands... To learn more about a specific command, type the following in the terminal: pipe COMMAND --help . For instance, we can list a number of pipe storage commands with pipe storage --help . Another example - a user can see a list of pipelines runs by pipe view-runs command.","title":"14.0. Overview"},{"location":"manual/14_CLI/14._Command-line_interface/#14-command-line-interface-cli","text":"Introduction Working with CLI CLI options and commands Examples Cloud Pipeline CLI has to be installed. See 14.1. Install and setup CLI .","title":"14. Command-line interface (CLI)"},{"location":"manual/14_CLI/14._Command-line_interface/#introduction","text":"Working with a Cloud Pipeline from CLI has numerous benefits compared to GUI: It has extra features which are not accessible from GUI such copying or moving files from one storage to another. The uploading file size could exceed 10 Mb. It is more convenient for System administrators.","title":"Introduction"},{"location":"manual/14_CLI/14._Command-line_interface/#working-with-cli","text":"All CLI commands have to be typed into the command line (Terminal) of the computer. pipe [OPTIONS] COMMAND [ARGS]... The pipe is a command-line interface for Cloud Pipeline. It has a number of available commands. Each command has a number of arguments as an input. To get a list of available options and commands type pipe or pipe --help into the command line.","title":"Working with CLI"},{"location":"manual/14_CLI/14._Command-line_interface/#cli-options-and-commands","text":"Options --help Show help message and exit --version Show CLI version and exit. Details of the command using see here Commands chown Changes current owner to specified. Details of the command using see here . cluster Cluster nodes operations. Details of the command using see here . configure Configures CLI parameters. This command can be automatically generated . Details of the command using see here . pause Allows to pause pipeline execution. Details of the command using see here . resume Allows to resume previously paused pipeline execution. Details of the command using see here . run Schedules a pipeline execution. Details of the command using see here . set-acl Set object permissions. Details of the command using see here . share Allows to share launched run with users/groups. Details of the command using see here . ssh Runs a single command or an interactive session over the SSH protocol for the specified job run. Details of the command using see here . stop Stops a running pipeline. Details of the command using see here . storage Storage operations. Details of the command using see here . tag Operations with attributes. Details of the command using see here . terminate-node Terminates calculation node. Details of the command using see here . token Prints the authentication token for a specified user. Details of the command using see here . tunnel Run ports tunnelling operations. Details of the command using see here . update Checks the Cloud Pipeline CLI version and updates it to the latest one if required. Details of the command using see here . users User management operations. Details of the command using see here . view-acl View object permissions. Details of the command using see here . view-cluster Lists cluster nodes. Details of the command using see here . view-group-objects Lists the objects accessible to the specific users group. Details of the command using see here . view-pipes Lists pipelines definitions. Details of the command using see here . view-runs Lists pipeline runs. Details of the command using see here . view-tools Displays details of a tool/tool version. Details of the command using see here . view-user-objects Lists the objects accessible to the specific user. Details of the command using see here . Note : To see command's arguments and options type pipe command --help .","title":"CLI options and commands"},{"location":"manual/14_CLI/14._Command-line_interface/#examples","text":"To see a list of available CLI commands type pipe or pipe --help in the terminal. Note : each command might have its own set of commands that consequently might have their own set of commands... To learn more about a specific command, type the following in the terminal: pipe COMMAND --help . For instance, we can list a number of pipe storage commands with pipe storage --help . Another example - a user can see a list of pipelines runs by pipe view-runs command.","title":"Examples"},{"location":"manual/15_Interactive_services/15.1._Starting_an_Interactive_application/","text":"15.1. Starting an Interactive application Starting a service Terminating a service To run a Tool or a Pipeline as an Interactive service you need to have EXECUTE permissions for that Tool/Pipeline. For more information see 13. Permissions . On this page, you'll find an example of launching an Interactive application. Launching steps remain the same for all applications ( Jupiter Notebook , Rstudio , etc.). All launching steps on this page are illustrated with the example of the Rstudio application. Rstudio is a popular IDE for R language that: is designed to make it easy to write scripts; makes it easy to set your working directory and access files on your computer; makes graphics much more accessible to a casual user. Starting a service Both Pipelines and Tools can be run as interactive services. The example below shows launching Tool scenario. Search for a Tool that implements a service (type a service name in the search box - list will be filtered). Navigate to the Tool information page by clicking the Tool's name and click the Run button then press the OK (for more information see 10.5. Launch a Tool ). Service will begin to set up. A service instance will be shown in the same way as batch jobs - within Active Runs menu. Once an instance is created for a service, a link(s) to the web GUI will be shown within \"Run Log\" form. Clicking the link will load the application web interface. In this example, we show the Rstudio web interface. Service configuration includes the following items: For all out of the box services in the Cloud Pipeline, a user will be automatically authenticated within a service. Note : authentication within all new services added by the users shall be configured by themselves. Below is an example of the authentication within the Rstudio service. Besides, for all out of the box services, you'll also find that all STS data storages, that are available to the user, are available within a service. Data from the STS storages will be available as a local file system and you will be able to work with it just as you do on your laptop. Note : In case of the Rstudio application, you can find all available data storages in the Home/cloud-data directory in the bottom-right corner of the screen. Only a user that launched a service can access it. Other users (even if a direct link to the service's GUI is known) will have 401 - Unauthorized error . Terminating a service Stopping a service is performed in the same manner as with a batch job. Option 1 . Load a list of \" Active Runs \" and click the STOP button. Option 2 . Load \"Run Logs\" form and click the STOP button. More information about runs lifecycle see here .","title":"15.1. Starting an interactive application"},{"location":"manual/15_Interactive_services/15.1._Starting_an_Interactive_application/#151-starting-an-interactive-application","text":"Starting a service Terminating a service To run a Tool or a Pipeline as an Interactive service you need to have EXECUTE permissions for that Tool/Pipeline. For more information see 13. Permissions . On this page, you'll find an example of launching an Interactive application. Launching steps remain the same for all applications ( Jupiter Notebook , Rstudio , etc.). All launching steps on this page are illustrated with the example of the Rstudio application. Rstudio is a popular IDE for R language that: is designed to make it easy to write scripts; makes it easy to set your working directory and access files on your computer; makes graphics much more accessible to a casual user.","title":"15.1. Starting an Interactive application"},{"location":"manual/15_Interactive_services/15.1._Starting_an_Interactive_application/#starting-a-service","text":"Both Pipelines and Tools can be run as interactive services. The example below shows launching Tool scenario. Search for a Tool that implements a service (type a service name in the search box - list will be filtered). Navigate to the Tool information page by clicking the Tool's name and click the Run button then press the OK (for more information see 10.5. Launch a Tool ). Service will begin to set up. A service instance will be shown in the same way as batch jobs - within Active Runs menu. Once an instance is created for a service, a link(s) to the web GUI will be shown within \"Run Log\" form. Clicking the link will load the application web interface. In this example, we show the Rstudio web interface. Service configuration includes the following items: For all out of the box services in the Cloud Pipeline, a user will be automatically authenticated within a service. Note : authentication within all new services added by the users shall be configured by themselves. Below is an example of the authentication within the Rstudio service. Besides, for all out of the box services, you'll also find that all STS data storages, that are available to the user, are available within a service. Data from the STS storages will be available as a local file system and you will be able to work with it just as you do on your laptop. Note : In case of the Rstudio application, you can find all available data storages in the Home/cloud-data directory in the bottom-right corner of the screen. Only a user that launched a service can access it. Other users (even if a direct link to the service's GUI is known) will have 401 - Unauthorized error .","title":"Starting a service"},{"location":"manual/15_Interactive_services/15.1._Starting_an_Interactive_application/#terminating-a-service","text":"Stopping a service is performed in the same manner as with a batch job. Option 1 . Load a list of \" Active Runs \" and click the STOP button. Option 2 . Load \"Run Logs\" form and click the STOP button. More information about runs lifecycle see here .","title":"Terminating a service"},{"location":"manual/15_Interactive_services/15.2._Using_Terminal_access/","text":"15.2. Using Terminal access Using Terminal access Terminal view Example: using of Environment Modules for the Cloud Pipeline runs Example: using of Slurm for the Cloud Pipeline's clusters Terminal access is available to the OWNER of the running job and users with ADMIN role. With sufficient permissions, Terminal access can be achieved to any running job. For more information see 13. Permissions . Also you can get a terminal access to the running job using the pipe CLI. For more details see here . All software in the Cloud Pipeline is located in Docker containers , and we can use Terminal access to the Docker container via the Interactive services . This can be useful when: usage of a new bioinformatics tool shall be tested; batch job scripts shall be tested within a real execution environment; docker image shall be extended and saved (install more packages/bioinformatics tools) - see 10.4. Edit a Tool . Using Terminal access Both Pipelines and Tools can be run as interactive services . The example below shows launching tool scenario: Navigate to the list of registered Tools and search for the Tool required (e.g. \"base-generic-centos7\" ). Go to the Tool page and click the arrow near the Run button \u2192 Select \"Custom Settings\" . Launch Tool page form will load (it's the same form that is used to configure a batch run). The following fields shall be filled: Node type Disk size Cloud Region \" Start idle \" box should be chosen. Click the Launch button when all above parameters are set. Once a run is scheduled and configured SSH hyperlink will appear in the \"Run Log\" form in the right upper corner of the form. Note : This link is only visible to the owner of the run and users with ROLE_ADMIN role assigned. Note : Also you can find this link at the Active Runs panel of the main Dashboard: Clicking the SSH link will load a new browser tab with an authenticated Terminal . Note : If an unauthorized user will load a direct link, \"Permission denied\" error will be returned. Terminal view The view of the SSH terminal session can be configured. At the moment, two color schemas can be used: Dark (default) Light There are two ways to configure a required color schema: Persistent - schema will be stored in the user profile and used any time SSH session is opened: Navigate to the Settings -> My Profile -> Profile Choose the schema from the SSH terminal theme menu: Any SSH windows already opened - will still use the older parameter until reloaded (i.e. F5). Temporary - will be used during a current SSH session only. Any other sessions will use the settings from the My Profile (see Persistent option above): Open SSH session Click the icon to toggle the color schema Dark <-> Light : Example: using of Environment Modules for the Cloud Pipeline runs Configure of Environment Modules using is available only for users with ADMIN role. The Environment Modules package provides for the dynamic modification of a user's environment via modulefiles . In the example below, we will use Modules to switch between two versions of Java Development Kit . At the beginning we will create a storage for all JDK versions files and modulefiles . For that: open the Library , click Create + \u2192 Storages \u2192 Create new object storage While creating - specify a storage name and mount point, e.g. /opt/software : Click the Create button. Open the created storage and create two folders in it: app - here we will upload JDK files modulefiles - here we will create modulefiles for each JDK version Open the modulefiles folder, create the jdk folder in it. Open the jdk folder, create modulefile for the JDK ver. 9.0.4 - name it 9.0.4 : Click the file name, click the Fullscreen button at the file content panel: At the popup click the EDIT button and input the modulefile content, e.g. for the JDK ver. 9.0.4 : Save it. Repeat steps 5-7 for the JDK ver. 11.0.2 . At the end you will have two modulefiles in the jdk folder: Open System Settings popup, click the Preferences tab, select Launch section. Into the launch.env.properties field add a new variable - CP_CAP_MODULES_FILES_DIR . That variable specify path to the source modulefiles . As you can see - during the run, when the storage created at step 2 will be mounted to the node in the specified mount-point ( /opt/software ), created above JDK modulefiles will be available in the modulefiles folder created at step 3 - by the path /opt/software/modulefiles . Save and close the Settings popup. Go to the Tool page, open the tool page you want to use the Environment Modules with and click the arrow near the Run button \u2192 Select \"Custom Settings\" . At the Launch page expand Advanced section. In the Limit mounts field select the storage created at step 2 (see more details here ). Click the Add system parameter button In the popup select the CP_CAP_MODULES item and click the OK button: CP_CAP_MODULES parameter enables installation and using the Modules for the current run. While installing, Modules will be configured to the source modulefiles path from the CP_CAP_MODULES_FILES_DIR launch environment variable (that was set at step 9). If CP_CAP_MODULES_FILES_DIR is not set - default modulefiles location will be used. Launch the tool. Open Run logs page, wait until InstallEnvironmentModules task will appear and check that the Modules was installed successfully: Wait until SSH hyperlink will appear in the right upper corner. Click it. In the terminal run the command module use to check the ource path to the modulefiles : Now, we will install JDK . For the ver. 9.0.4 run the following commands: # Download \"jdk 9.0.4\" archive wget https://download.java.net/java/GA/jdk9/9.0.4/binaries/openjdk-9.0.4_linux-x64_bin.tar.gz # Extract archive content tar -zxf openjdk-9.0.4_linux-x64_bin.tar.gz # Copy \"jdk 9.0.4\" files into the mounted data storage cp -r jdk-9.0.4 /opt/software/app/jdk-9.0.4 For the ver. 11.0.2 run the following commands: # Download \"jdk 11.0.2\" archive wget https://download.java.net/java/GA/jdk11/9/GPL/openjdk-11.0.2_linux-x64_bin.tar.gz # Extract archive content tar -zxf openjdk-11.0.2_linux-x64_bin.tar.gz # Copy \"jdk 11.0.2\" files into the mounted data storage cp -r jdk-11.0.2 /opt/software/app/jdk-11.0.2 Now, you can check the facilities of the Environment Modules package. Load the available modulefiles list: Load the JDK ver. 11.0.2 : Switch to the JDK ver. 9.0.4 : Unload all JDK versions: Example: using of Slurm for the Cloud Pipeline's clusters Slurm is an open source, highly scalable cluster management and job scheduling system for large and small Linux clusters. In the example below, we will use Slurm for performing the simplest batch job. Open the Tools page, select a tool and its version ( Note : in our example we will use Ubuntu 18.04 ). Hover over the v button near the Run button and click the \"Custom settings\" item in the dropdown list. At the Launch page expand \"Exec environment\" section and click the \" Configure cluster \" button: In the appeared popup click the Cluster tab. Set the count of \"child\" nodes, tick the \"Enable Slurm\" checkbox and click the OK button to confirm: Launch the tool: Open the main Dashboard and wait until the SSH hyperlink will appear at the Active Runs panel for the just-launched tool, then click it: The terminal web GUI will appear. At the beginning, let's check general system state, view existing partitions in the system and the list of available nodes. For that, perform the sinfo command: Only main.q partition is created. All cluster nodes are attached to this partition. To report more detailed information about partition - the scontrol command can be used: And to display detailed information about one of the nodes, e.g.: Now, we'll parallely execute /bin/hostname on all three nodes ( -N3 option) and include task numbers in the output ( -l option) via the srun command. The default partition will be used. One task per node will be used by default: For the batch job, create the following script: This script contains a timelimit for the job embedded within itself (via the --time option after the #SBATCH prefix). Script contains the command /bin/hostname that will be executed on the first node in the allocation (where the script runs) plus two job steps initiated using the srun command and executed sequentially. To submit a job script for execution over all three nodes use the sbatch command, result will be written to the file ( -o option): During the script execution you can check the queue of running jobs in priority order via the squeue command: The result of the sbatch command performing:","title":"15.2. Using terminal access"},{"location":"manual/15_Interactive_services/15.2._Using_Terminal_access/#152-using-terminal-access","text":"Using Terminal access Terminal view Example: using of Environment Modules for the Cloud Pipeline runs Example: using of Slurm for the Cloud Pipeline's clusters Terminal access is available to the OWNER of the running job and users with ADMIN role. With sufficient permissions, Terminal access can be achieved to any running job. For more information see 13. Permissions . Also you can get a terminal access to the running job using the pipe CLI. For more details see here . All software in the Cloud Pipeline is located in Docker containers , and we can use Terminal access to the Docker container via the Interactive services . This can be useful when: usage of a new bioinformatics tool shall be tested; batch job scripts shall be tested within a real execution environment; docker image shall be extended and saved (install more packages/bioinformatics tools) - see 10.4. Edit a Tool .","title":"15.2. Using Terminal access"},{"location":"manual/15_Interactive_services/15.2._Using_Terminal_access/#using-terminal-access","text":"Both Pipelines and Tools can be run as interactive services . The example below shows launching tool scenario: Navigate to the list of registered Tools and search for the Tool required (e.g. \"base-generic-centos7\" ). Go to the Tool page and click the arrow near the Run button \u2192 Select \"Custom Settings\" . Launch Tool page form will load (it's the same form that is used to configure a batch run). The following fields shall be filled: Node type Disk size Cloud Region \" Start idle \" box should be chosen. Click the Launch button when all above parameters are set. Once a run is scheduled and configured SSH hyperlink will appear in the \"Run Log\" form in the right upper corner of the form. Note : This link is only visible to the owner of the run and users with ROLE_ADMIN role assigned. Note : Also you can find this link at the Active Runs panel of the main Dashboard: Clicking the SSH link will load a new browser tab with an authenticated Terminal . Note : If an unauthorized user will load a direct link, \"Permission denied\" error will be returned.","title":"Using Terminal access"},{"location":"manual/15_Interactive_services/15.2._Using_Terminal_access/#terminal-view","text":"The view of the SSH terminal session can be configured. At the moment, two color schemas can be used: Dark (default) Light There are two ways to configure a required color schema: Persistent - schema will be stored in the user profile and used any time SSH session is opened: Navigate to the Settings -> My Profile -> Profile Choose the schema from the SSH terminal theme menu: Any SSH windows already opened - will still use the older parameter until reloaded (i.e. F5). Temporary - will be used during a current SSH session only. Any other sessions will use the settings from the My Profile (see Persistent option above): Open SSH session Click the icon to toggle the color schema Dark <-> Light :","title":"Terminal view"},{"location":"manual/15_Interactive_services/15.2._Using_Terminal_access/#example-using-of-environment-modules-for-the-cloud-pipeline-runs","text":"Configure of Environment Modules using is available only for users with ADMIN role. The Environment Modules package provides for the dynamic modification of a user's environment via modulefiles . In the example below, we will use Modules to switch between two versions of Java Development Kit . At the beginning we will create a storage for all JDK versions files and modulefiles . For that: open the Library , click Create + \u2192 Storages \u2192 Create new object storage While creating - specify a storage name and mount point, e.g. /opt/software : Click the Create button. Open the created storage and create two folders in it: app - here we will upload JDK files modulefiles - here we will create modulefiles for each JDK version Open the modulefiles folder, create the jdk folder in it. Open the jdk folder, create modulefile for the JDK ver. 9.0.4 - name it 9.0.4 : Click the file name, click the Fullscreen button at the file content panel: At the popup click the EDIT button and input the modulefile content, e.g. for the JDK ver. 9.0.4 : Save it. Repeat steps 5-7 for the JDK ver. 11.0.2 . At the end you will have two modulefiles in the jdk folder: Open System Settings popup, click the Preferences tab, select Launch section. Into the launch.env.properties field add a new variable - CP_CAP_MODULES_FILES_DIR . That variable specify path to the source modulefiles . As you can see - during the run, when the storage created at step 2 will be mounted to the node in the specified mount-point ( /opt/software ), created above JDK modulefiles will be available in the modulefiles folder created at step 3 - by the path /opt/software/modulefiles . Save and close the Settings popup. Go to the Tool page, open the tool page you want to use the Environment Modules with and click the arrow near the Run button \u2192 Select \"Custom Settings\" . At the Launch page expand Advanced section. In the Limit mounts field select the storage created at step 2 (see more details here ). Click the Add system parameter button In the popup select the CP_CAP_MODULES item and click the OK button: CP_CAP_MODULES parameter enables installation and using the Modules for the current run. While installing, Modules will be configured to the source modulefiles path from the CP_CAP_MODULES_FILES_DIR launch environment variable (that was set at step 9). If CP_CAP_MODULES_FILES_DIR is not set - default modulefiles location will be used. Launch the tool. Open Run logs page, wait until InstallEnvironmentModules task will appear and check that the Modules was installed successfully: Wait until SSH hyperlink will appear in the right upper corner. Click it. In the terminal run the command module use to check the ource path to the modulefiles : Now, we will install JDK . For the ver. 9.0.4 run the following commands: # Download \"jdk 9.0.4\" archive wget https://download.java.net/java/GA/jdk9/9.0.4/binaries/openjdk-9.0.4_linux-x64_bin.tar.gz # Extract archive content tar -zxf openjdk-9.0.4_linux-x64_bin.tar.gz # Copy \"jdk 9.0.4\" files into the mounted data storage cp -r jdk-9.0.4 /opt/software/app/jdk-9.0.4 For the ver. 11.0.2 run the following commands: # Download \"jdk 11.0.2\" archive wget https://download.java.net/java/GA/jdk11/9/GPL/openjdk-11.0.2_linux-x64_bin.tar.gz # Extract archive content tar -zxf openjdk-11.0.2_linux-x64_bin.tar.gz # Copy \"jdk 11.0.2\" files into the mounted data storage cp -r jdk-11.0.2 /opt/software/app/jdk-11.0.2 Now, you can check the facilities of the Environment Modules package. Load the available modulefiles list: Load the JDK ver. 11.0.2 : Switch to the JDK ver. 9.0.4 : Unload all JDK versions:","title":"Example: using of Environment Modules for the Cloud Pipeline runs"},{"location":"manual/15_Interactive_services/15.2._Using_Terminal_access/#example-using-of-slurm-for-the-cloud-pipelines-clusters","text":"Slurm is an open source, highly scalable cluster management and job scheduling system for large and small Linux clusters. In the example below, we will use Slurm for performing the simplest batch job. Open the Tools page, select a tool and its version ( Note : in our example we will use Ubuntu 18.04 ). Hover over the v button near the Run button and click the \"Custom settings\" item in the dropdown list. At the Launch page expand \"Exec environment\" section and click the \" Configure cluster \" button: In the appeared popup click the Cluster tab. Set the count of \"child\" nodes, tick the \"Enable Slurm\" checkbox and click the OK button to confirm: Launch the tool: Open the main Dashboard and wait until the SSH hyperlink will appear at the Active Runs panel for the just-launched tool, then click it: The terminal web GUI will appear. At the beginning, let's check general system state, view existing partitions in the system and the list of available nodes. For that, perform the sinfo command: Only main.q partition is created. All cluster nodes are attached to this partition. To report more detailed information about partition - the scontrol command can be used: And to display detailed information about one of the nodes, e.g.: Now, we'll parallely execute /bin/hostname on all three nodes ( -N3 option) and include task numbers in the output ( -l option) via the srun command. The default partition will be used. One task per node will be used by default: For the batch job, create the following script: This script contains a timelimit for the job embedded within itself (via the --time option after the #SBATCH prefix). Script contains the command /bin/hostname that will be executed on the first node in the allocation (where the script runs) plus two job steps initiated using the srun command and executed sequentially. To submit a job script for execution over all three nodes use the sbatch command, result will be written to the file ( -o option): During the script execution you can check the queue of running jobs in priority order via the squeue command: The result of the sbatch command performing:","title":"Example: using of Slurm for the Cloud Pipeline's clusters"},{"location":"manual/15_Interactive_services/15.3._Expose_node_filesystem/","text":"15.3. Expose node filesystem Upload files through node's file browser Download files/directories through node's file browser Delete files/directories through node's file browser Search files/directories at node's file browser Filesystem browser is available for the Active runs only. The system preference storage.fsbrowser.enabled should be set. To browse node filesystem go to the Run logs page. After a job had been initialized the Browse hyperlink become available to the users: Click the hyperlink to view the node's filesystem. By default, the \"root\" directory for the FSBrowser is / directory of the node: Admin can change the \"root\" directory for the FSBrowser by the system preference storage.fsbrowser.wd . Other abilities of FSBrowser behavior management see here . User has an ability to view, download, upload, delete and search files and directories. To transfer between folders just click the folder and the system will open this folder to browse: Note : Storage has an auto-cleanup policy (to remove the temp files). Upload files through node's file browser To upload files to the node filesystem press the Upload button: The local File Manager window will be opened. Select files that you want to upload and press the Open button: The Cloud Pipeline will display Operations list with all selected files and uploading statuses: After the uploading will be completed the system will update files statuses: To collapse the Operations window - press the cross button in the right-upper corner. The window will be collapsed: Download files/directories through node's file browser Download operation supports both files and directories. If a directory is requested for a download the result of such operation will be a gzipped tarball. To download files or directories on your local machine from the node filesystem press the Download button: The Operations window will be open: The file or directory will be downloaded to your local machine after it's status in Operations window became Downloaded : If some directory was downloaded it will be saved as a gzipped tarball file: Delete files/directories through node's file browser To delete file or directory user needs to press the Delete button: The confirmation window will be displayed. Press OK to delete file or directory: Search files/directories at node's file browser To search inside the current directory through files and directories - click the address panel and type desired file or directory name: The result will be displayed to you. The string in the address panel is a path to the searched file or directory. So you can type a path to the file or directory and the result will be displayed to you right away.","title":"15.3. Expose node filesystem"},{"location":"manual/15_Interactive_services/15.3._Expose_node_filesystem/#153-expose-node-filesystem","text":"Upload files through node's file browser Download files/directories through node's file browser Delete files/directories through node's file browser Search files/directories at node's file browser Filesystem browser is available for the Active runs only. The system preference storage.fsbrowser.enabled should be set. To browse node filesystem go to the Run logs page. After a job had been initialized the Browse hyperlink become available to the users: Click the hyperlink to view the node's filesystem. By default, the \"root\" directory for the FSBrowser is / directory of the node: Admin can change the \"root\" directory for the FSBrowser by the system preference storage.fsbrowser.wd . Other abilities of FSBrowser behavior management see here . User has an ability to view, download, upload, delete and search files and directories. To transfer between folders just click the folder and the system will open this folder to browse: Note : Storage has an auto-cleanup policy (to remove the temp files).","title":"15.3. Expose node filesystem"},{"location":"manual/15_Interactive_services/15.3._Expose_node_filesystem/#upload-files-through-nodes-file-browser","text":"To upload files to the node filesystem press the Upload button: The local File Manager window will be opened. Select files that you want to upload and press the Open button: The Cloud Pipeline will display Operations list with all selected files and uploading statuses: After the uploading will be completed the system will update files statuses: To collapse the Operations window - press the cross button in the right-upper corner. The window will be collapsed:","title":"Upload files through node's file browser"},{"location":"manual/15_Interactive_services/15.3._Expose_node_filesystem/#download-filesdirectories-through-nodes-file-browser","text":"Download operation supports both files and directories. If a directory is requested for a download the result of such operation will be a gzipped tarball. To download files or directories on your local machine from the node filesystem press the Download button: The Operations window will be open: The file or directory will be downloaded to your local machine after it's status in Operations window became Downloaded : If some directory was downloaded it will be saved as a gzipped tarball file:","title":"Download files/directories through node's file browser"},{"location":"manual/15_Interactive_services/15.3._Expose_node_filesystem/#delete-filesdirectories-through-nodes-file-browser","text":"To delete file or directory user needs to press the Delete button: The confirmation window will be displayed. Press OK to delete file or directory:","title":"Delete files/directories through node's file browser"},{"location":"manual/15_Interactive_services/15.3._Expose_node_filesystem/#search-filesdirectories-at-nodes-file-browser","text":"To search inside the current directory through files and directories - click the address panel and type desired file or directory name: The result will be displayed to you. The string in the address panel is a path to the searched file or directory. So you can type a path to the file or directory and the result will be displayed to you right away.","title":"Search files/directories at node's file browser"},{"location":"manual/15_Interactive_services/15.4._Interactive_service_examples/","text":"15.4. Interactive service examples Running Apache Spark cluster with RStudio Web GUI Launch the RStudio tool with Apache Cluster Example of sparklyr script Monitoring execution via Spark UI To run a Tool or a Pipeline as an Interactive service you need to have EXECUTE permissions for that Tool/Pipeline. For more information see 13. Permissions . On this page, you'll find examples of launching Interactive applications with various features. Running Apache Spark cluster with RStudio Web GUI Launch the RStudio tool with Apache Cluster Navigate to the list of registered Tools and search for the RStudio Tool: Go to the Tool page and click the arrow near the Run button \u2192 Select \"Custom Settings\" . At the Launch page select Node type (from which the subsequent cluster will consist). Note : it is recommended to select nodes with more memory volume, because it is critical for Spark's in-memory processing Click the Configure cluster button: At the popup select the Cluster tab, set the number of Child nodes to create (e.g. 2 or more), then tick the Enable Apache Spark checkbox and confirm by the OK button: Or you could enable Apache Spark by manually adding the system parameter CP_CAP_SPARK with true value: Click the Launch button: At the popup confirm the launch. Check that the cluster has appeared in the ACTIVE RUNS tab of the Runs page: Wait until all components are initialized. The cluster tile in the ACTIVE RUNS tab will turn into yellow. Click on the parent run to open the Run logs page: At the Run logs page there are two endpoints: RStudio - it exposes RStudio's Web IDE SparkUI - it exposes Web GUI of the Spark. It allows to monitor Spark master/workers/application via the web-browser. Details are available in the Spark UI manual Click the RStudio endpoint. This will load RStudio Web GUI with the pre-installed sparklyr package in the new tab of the web-browser: From here one can start coding in R using sparklyr to run the workload over the cluster. Example of sparklyr script It is assumed that a Spark cluster with the RStudio Web GUI and sparklyr package is up and running, as shown in the previous section. Accessing datasets from the Cloud Pipeline's Spark Access to data via File Storages If the user has an access to the FS storage - then datasets from such storage can be accessed via the file:// schema or without a schema at all. But this approach may start to degrade once there is 100+ cores cluster with a lot of I/O operations. Access to data via Object Storages Note : this feature is available only for AWS Cloud Provider. Spark cluster configuration uses the stable version of the Hadoop-AWS module , that allows to directly access (read/write) the datasets in the S3 buckets using Spark jobs. The only difference with the filesystem access - is the URL schema. s3a:// prefix shall be used instead of s3:// . E.g. if there is a user's S3 bucket named \" test-object-storage \" with the \" test_dataset.parquet \" dataset - then it can be accessed as \" s3a://test-object-storage/test_dataset.parquet \" from the sparklyr code (or any other Spark job). Prepare and run sparklyr script This section provides prepare and run of R script that shows how to connect to the Spark cluster, deployed in the Cloud Pipeline, and read/write the data from/to object data storage. Note : we will use S3 bucket with directly access to data as described above. This script will not work for other Cloud Providers. For this example, a small public VCF file will be used. It is located in some S3 bucket e.g.: So, prepare the following script: library(sparklyr) # Cloud Pipeline provides the SPARK's master URL in SPARK_MASTER variable # SPARK_HOME variabe is set by the Cloud Pipeline and will be used by sparklyr - no need to specify it explicitly # Spark version will be retrieved by sparklyr from the $SPARK_HOME/RELEASE file - no need to specify it explicitly master <- Sys.getenv(\"SPARK_MASTER\") sc <- spark_connect(master=master) # Get the current Cloud Pipeline's unique Run ID to write the results into the unique directory unique_id <- Sys.getenv(\"RUN_ID\") # Setup input VCF (tab-delimited) file location and the resulting parquet file # Note that both input and output are located in the S3 bucket and are addressed via s3a:// schema example_data_vcf_path <- \"s3a://objstor/TestSample.vcf\" example_data_parquet_path <- paste(\"s3a://objstor/results\", unique_id, \"example_data.parquet\", sep=\"/\") # Read VCF from the storage and convert to the DataFrame example_data_df <- spark_read_csv(sc = sc, name = \"example_data_vcf\", path = example_data_vcf_path, header = F, delimiter = \"\\t\") # Write DataFrame as a parquet to the storage spark_write_parquet(example_data_df, path = example_data_parquet_path) Paste that script into the RStudio console and launch it: Once script is finished - resulting parquet will be written to the storage. To check it open in the Library the storage, that was specified for output results: Open the path for output results - you will see the directory with the name equal to the Run ID that contains resulting files: Monitoring execution via Spark UI To view the details of the jobs being executed in Spark, how the memory is used and get other useful information - the SparkUI endpoint from the Run logs page shall be opened. While executing the example script , open the Spark UI endpoint: A list of active applications and workers will be shown: To get the details of the underlying jobs, executed by the Spark instance, click the application name: The following page will be opened: For more details about Spark UI opportunities see here .","title":"15.4. Interactive service examples"},{"location":"manual/15_Interactive_services/15.4._Interactive_service_examples/#154-interactive-service-examples","text":"Running Apache Spark cluster with RStudio Web GUI Launch the RStudio tool with Apache Cluster Example of sparklyr script Monitoring execution via Spark UI To run a Tool or a Pipeline as an Interactive service you need to have EXECUTE permissions for that Tool/Pipeline. For more information see 13. Permissions . On this page, you'll find examples of launching Interactive applications with various features.","title":"15.4. Interactive service examples"},{"location":"manual/15_Interactive_services/15.4._Interactive_service_examples/#running-apache-spark-cluster-with-rstudio-web-gui","text":"","title":"Running Apache Spark cluster with RStudio Web GUI"},{"location":"manual/15_Interactive_services/15.4._Interactive_service_examples/#launch-the-rstudio-tool-with-apache-cluster","text":"Navigate to the list of registered Tools and search for the RStudio Tool: Go to the Tool page and click the arrow near the Run button \u2192 Select \"Custom Settings\" . At the Launch page select Node type (from which the subsequent cluster will consist). Note : it is recommended to select nodes with more memory volume, because it is critical for Spark's in-memory processing Click the Configure cluster button: At the popup select the Cluster tab, set the number of Child nodes to create (e.g. 2 or more), then tick the Enable Apache Spark checkbox and confirm by the OK button: Or you could enable Apache Spark by manually adding the system parameter CP_CAP_SPARK with true value: Click the Launch button: At the popup confirm the launch. Check that the cluster has appeared in the ACTIVE RUNS tab of the Runs page: Wait until all components are initialized. The cluster tile in the ACTIVE RUNS tab will turn into yellow. Click on the parent run to open the Run logs page: At the Run logs page there are two endpoints: RStudio - it exposes RStudio's Web IDE SparkUI - it exposes Web GUI of the Spark. It allows to monitor Spark master/workers/application via the web-browser. Details are available in the Spark UI manual Click the RStudio endpoint. This will load RStudio Web GUI with the pre-installed sparklyr package in the new tab of the web-browser: From here one can start coding in R using sparklyr to run the workload over the cluster.","title":"Launch the RStudio tool with Apache Cluster"},{"location":"manual/15_Interactive_services/15.4._Interactive_service_examples/#example-of-sparklyr-script","text":"It is assumed that a Spark cluster with the RStudio Web GUI and sparklyr package is up and running, as shown in the previous section.","title":"Example of sparklyr script"},{"location":"manual/15_Interactive_services/15.4._Interactive_service_examples/#accessing-datasets-from-the-cloud-pipelines-spark","text":"Access to data via File Storages If the user has an access to the FS storage - then datasets from such storage can be accessed via the file:// schema or without a schema at all. But this approach may start to degrade once there is 100+ cores cluster with a lot of I/O operations. Access to data via Object Storages Note : this feature is available only for AWS Cloud Provider. Spark cluster configuration uses the stable version of the Hadoop-AWS module , that allows to directly access (read/write) the datasets in the S3 buckets using Spark jobs. The only difference with the filesystem access - is the URL schema. s3a:// prefix shall be used instead of s3:// . E.g. if there is a user's S3 bucket named \" test-object-storage \" with the \" test_dataset.parquet \" dataset - then it can be accessed as \" s3a://test-object-storage/test_dataset.parquet \" from the sparklyr code (or any other Spark job).","title":"Accessing datasets from the Cloud Pipeline's Spark"},{"location":"manual/15_Interactive_services/15.4._Interactive_service_examples/#prepare-and-run-sparklyr-script","text":"This section provides prepare and run of R script that shows how to connect to the Spark cluster, deployed in the Cloud Pipeline, and read/write the data from/to object data storage. Note : we will use S3 bucket with directly access to data as described above. This script will not work for other Cloud Providers. For this example, a small public VCF file will be used. It is located in some S3 bucket e.g.: So, prepare the following script: library(sparklyr) # Cloud Pipeline provides the SPARK's master URL in SPARK_MASTER variable # SPARK_HOME variabe is set by the Cloud Pipeline and will be used by sparklyr - no need to specify it explicitly # Spark version will be retrieved by sparklyr from the $SPARK_HOME/RELEASE file - no need to specify it explicitly master <- Sys.getenv(\"SPARK_MASTER\") sc <- spark_connect(master=master) # Get the current Cloud Pipeline's unique Run ID to write the results into the unique directory unique_id <- Sys.getenv(\"RUN_ID\") # Setup input VCF (tab-delimited) file location and the resulting parquet file # Note that both input and output are located in the S3 bucket and are addressed via s3a:// schema example_data_vcf_path <- \"s3a://objstor/TestSample.vcf\" example_data_parquet_path <- paste(\"s3a://objstor/results\", unique_id, \"example_data.parquet\", sep=\"/\") # Read VCF from the storage and convert to the DataFrame example_data_df <- spark_read_csv(sc = sc, name = \"example_data_vcf\", path = example_data_vcf_path, header = F, delimiter = \"\\t\") # Write DataFrame as a parquet to the storage spark_write_parquet(example_data_df, path = example_data_parquet_path) Paste that script into the RStudio console and launch it: Once script is finished - resulting parquet will be written to the storage. To check it open in the Library the storage, that was specified for output results: Open the path for output results - you will see the directory with the name equal to the Run ID that contains resulting files:","title":"Prepare and run sparklyr script"},{"location":"manual/15_Interactive_services/15.4._Interactive_service_examples/#monitoring-execution-via-spark-ui","text":"To view the details of the jobs being executed in Spark, how the memory is used and get other useful information - the SparkUI endpoint from the Run logs page shall be opened. While executing the example script , open the Spark UI endpoint: A list of active applications and workers will be shown: To get the details of the underlying jobs, executed by the Spark instance, click the application name: The following page will be opened: For more details about Spark UI opportunities see here .","title":"Monitoring execution via Spark UI"},{"location":"manual/15_Interactive_services/15._Interactive_services/","text":"15. Interactive services Overview Interactive services - a feature of the Cloud Pipeline that allows to set up an interactive application in a cloud infrastructure and access it via the web interface, leveraging cloud large instances. This is useful when some analysis steps shall be performed in interactive mode. It can also be useful for navigating through the execution environment and for testing purposes. Examples: Debug scripts, that shall be used in batch jobs Perform data post-processing using IDE Install/Delete/Update software. In the Cloud Pipeline, both Tools and Pipelines can be run as interactive services. Supported services Out of the box, Cloud Pipeline provides the following interactive services: RStudio - IDE for R language that helps to make work with R a great deal more convenient. For details see https://www.rstudio.com/ . Jupiter Notebook - a web application that allows creating documents with code pieces, visualizations and narrative text inside. For more information see http://jupyter.org/ . Terminal - a window with a command line (shell). Note : Terminal access is available for all Tools or Pipelines with these Tools. On the other side, you can't get an access to the e.g. Rstudio application if a Tool doesn't contain it. List of services can be extended by users.","title":"15.0. Overview"},{"location":"manual/15_Interactive_services/15._Interactive_services/#15-interactive-services","text":"","title":"15. Interactive services"},{"location":"manual/15_Interactive_services/15._Interactive_services/#overview","text":"Interactive services - a feature of the Cloud Pipeline that allows to set up an interactive application in a cloud infrastructure and access it via the web interface, leveraging cloud large instances. This is useful when some analysis steps shall be performed in interactive mode. It can also be useful for navigating through the execution environment and for testing purposes. Examples: Debug scripts, that shall be used in batch jobs Perform data post-processing using IDE Install/Delete/Update software. In the Cloud Pipeline, both Tools and Pipelines can be run as interactive services.","title":"Overview"},{"location":"manual/15_Interactive_services/15._Interactive_services/#supported-services","text":"Out of the box, Cloud Pipeline provides the following interactive services: RStudio - IDE for R language that helps to make work with R a great deal more convenient. For details see https://www.rstudio.com/ . Jupiter Notebook - a web application that allows creating documents with code pieces, visualizations and narrative text inside. For more information see http://jupyter.org/ . Terminal - a window with a command line (shell). Note : Terminal access is available for all Tools or Pipelines with these Tools. On the other side, you can't get an access to the e.g. Rstudio application if a Tool doesn't contain it. List of services can be extended by users.","title":"Supported services"},{"location":"manual/16_Issues/16._Issues/","text":"16. Issues Open an issue Change an issue title Leave a comment Edit a comment Delete a comment Delete an issue Issues is a great tool to share results with other users or get feedback. It allows keeping the discussion in one place - traceable and linked to specific data. The feature is available for: Folders, including Projects Pipelines Tools. Open an issue To open an issue, a user shall have READ permissions for a discussed object. For more information see 13. Permissions . To open an issue the following steps shall be performed: Navigate to the object you want to discuss and click icon \u2192 Issues . Note : the second way is to navigate to a folder that contains the object you want to discuss and click icon in the desired object's line. The Issues pane will be opened. Click button to create a new issue. Fill up the open form: Title (e.g. new issue ), Description (e.g. error ). Description supports MARKDOWN formatting, thus you can write your text in the Write tab with special symbols and then preview it in the Preview tab. Note : you can address your topic to a specific user if you put @ symbol and start to write username. The system will suggest you choose from the list. After you save your topic, the user will receive e-mail notification. When you finish, press the Create button - the topic will be created. You'll see topic title, the author and how much time past since the topic was created. You can click on it to open and see description. If you want to go back to the list of all discussions, click . Change an issue title To edit an issue title a user shall be OWNER of the discussion. For more information see 13. Permissions . To change an issue title the following steps shall be performed: Navigate to the issue which title you want to change and open it. Click the issue title - the Title field will be open for editing. Enter new title and click out of the field. The new title will be saved. Leave a comment To leave a comment a user shall have READ permissions for a discussioned object. For more information see 13. Permissions . To leave a comment on an issue the following steps shall be performed: Navigate to the object you want to discuss and click icon \u2192 Issues . Note : the second way is to navigate to a folder that contains the object you want to discuss and click icon in the desired object's line. The Issues pane will be opened. Click on an issue you interested in. Fill up the Comment form. Comment supports MARKDOWN formatting, thus you can write your text in the Write tab with special symbols and then preview it in the Preview tab. Note : you can address your comment to a specific user if you put @ symbol and start to write username. The system will suggest you choose from the list. After you save your comment, the user will receive an e-mail notification. Note : you can also drag and drop pictures here, so that everyone can see the issue more clearly: When you finish, press the Send button - the comment will be created. You'll see your comment, the author and how much time past since the comment was created. Edit a comment To edit a comment a user shall be OWNER of the comment. For more information see 13. Permissions . Note : to edit a topic description the same steps should be performed. To edit a comment the following steps shall be performed: Navigate to the comment you want to edit. Press icon - the editing form will be shown. Change your comment and click to save your changes. Note : won't be available until you change something. Note : if you change your mind and want to leave your comment as is, click . The changes will be saved. Delete a comment To delete a comment a user shall be OWNER of the comment. For more information see 13. Permissions . To delete a comment the following steps shall be performed: Navigate to the comment you want to delete. Click icon. Confirm your action in the dialog window. The comment is deleted. Delete an issue To delete an issue a user shall be OWNER of the issue. For more information see 13. Permissions . To delete an issue the following steps shall be performed: Navigate to the issue you want to delete and open it. Click icon. Confirm your action in the dialog window. The issue is deleted.","title":"16. Issues"},{"location":"manual/16_Issues/16._Issues/#16-issues","text":"Open an issue Change an issue title Leave a comment Edit a comment Delete a comment Delete an issue Issues is a great tool to share results with other users or get feedback. It allows keeping the discussion in one place - traceable and linked to specific data. The feature is available for: Folders, including Projects Pipelines Tools.","title":"16. Issues"},{"location":"manual/16_Issues/16._Issues/#open-an-issue","text":"To open an issue, a user shall have READ permissions for a discussed object. For more information see 13. Permissions . To open an issue the following steps shall be performed: Navigate to the object you want to discuss and click icon \u2192 Issues . Note : the second way is to navigate to a folder that contains the object you want to discuss and click icon in the desired object's line. The Issues pane will be opened. Click button to create a new issue. Fill up the open form: Title (e.g. new issue ), Description (e.g. error ). Description supports MARKDOWN formatting, thus you can write your text in the Write tab with special symbols and then preview it in the Preview tab. Note : you can address your topic to a specific user if you put @ symbol and start to write username. The system will suggest you choose from the list. After you save your topic, the user will receive e-mail notification. When you finish, press the Create button - the topic will be created. You'll see topic title, the author and how much time past since the topic was created. You can click on it to open and see description. If you want to go back to the list of all discussions, click .","title":"Open an issue"},{"location":"manual/16_Issues/16._Issues/#change-an-issue-title","text":"To edit an issue title a user shall be OWNER of the discussion. For more information see 13. Permissions . To change an issue title the following steps shall be performed: Navigate to the issue which title you want to change and open it. Click the issue title - the Title field will be open for editing. Enter new title and click out of the field. The new title will be saved.","title":"Change an issue title"},{"location":"manual/16_Issues/16._Issues/#leave-a-comment","text":"To leave a comment a user shall have READ permissions for a discussioned object. For more information see 13. Permissions . To leave a comment on an issue the following steps shall be performed: Navigate to the object you want to discuss and click icon \u2192 Issues . Note : the second way is to navigate to a folder that contains the object you want to discuss and click icon in the desired object's line. The Issues pane will be opened. Click on an issue you interested in. Fill up the Comment form. Comment supports MARKDOWN formatting, thus you can write your text in the Write tab with special symbols and then preview it in the Preview tab. Note : you can address your comment to a specific user if you put @ symbol and start to write username. The system will suggest you choose from the list. After you save your comment, the user will receive an e-mail notification. Note : you can also drag and drop pictures here, so that everyone can see the issue more clearly: When you finish, press the Send button - the comment will be created. You'll see your comment, the author and how much time past since the comment was created.","title":"Leave a comment"},{"location":"manual/16_Issues/16._Issues/#edit-a-comment","text":"To edit a comment a user shall be OWNER of the comment. For more information see 13. Permissions . Note : to edit a topic description the same steps should be performed. To edit a comment the following steps shall be performed: Navigate to the comment you want to edit. Press icon - the editing form will be shown. Change your comment and click to save your changes. Note : won't be available until you change something. Note : if you change your mind and want to leave your comment as is, click . The changes will be saved.","title":"Edit a comment"},{"location":"manual/16_Issues/16._Issues/#delete-a-comment","text":"To delete a comment a user shall be OWNER of the comment. For more information see 13. Permissions . To delete a comment the following steps shall be performed: Navigate to the comment you want to delete. Click icon. Confirm your action in the dialog window. The comment is deleted.","title":"Delete a comment"},{"location":"manual/16_Issues/16._Issues/#delete-an-issue","text":"To delete an issue a user shall be OWNER of the issue. For more information see 13. Permissions . To delete an issue the following steps shall be performed: Navigate to the issue you want to delete and open it. Click icon. Confirm your action in the dialog window. The issue is deleted.","title":"Delete an issue"},{"location":"manual/17_Tagging_by_attributes/17.1._Faceted_filters_search_by_tags/","text":"17.1. Faceted filters search using tags Preliminary preparation Configure faceted filters Faceted filters at the search page Filters displaying Using faceted filters to search In Cloud Pipeline , besides the simple Global Search, there is also the advanced search included search by tags (attributes). Search by tags is being performed via faceted filters panel. Tags' keys and values displayed in this panel are loaded from System Dictionaries marked as filter sources. User can restrict (filter) search results - checking/unchecking desired filters. Let's take a closer look. Preliminary preparation \"Faceted filters\" search is convenient, when users have well-structured data in the platform and want to search by the attributes. But not all specified attributes are being loaded to the faceted filters, only attributes that were added to objects using System Dictionaries will be loaded to these filters. Firstly, the sets of key and possible values should be created as corresponding dictionaries in the special section of the System Settings , e.g.: About System Dictionaries see details here . After the dictionary is created, it can be used for the attribute (tag) creation. For that: Navigate to the object you wish to tag Open the \"Attributes\" panel for the object Click the \" + Add \" button at the \"Attributes\" panel: Click the \"Key\" field. In the appeared list, all available dictionaries list will appear: Select the dictionary name that you want to use as the attribute key In the \"Value\" field, corresponding values of the selected dictionary will appear: Select the dictionary value that you want to use as the attribute value Click the \" v Add \" button to confirm: Added attribute will appear in the \"Attributes\" panel: About the tagging objects via System Dictionaries see details here . Configure faceted filters To determine which of the available System Dictionaries should be used as faceted filters, there is a special system preference faceted.filter.dictionaries . Here, admin shall manually specify dictionaries and some settings of displaying specified dictionaries at the \"Faceted filters\" panel: This preference has JSON -format. Features: \"dictionaries\" - array of the existing System Dictionaries that shall be used and displayed in the \"Faceted filters\" panel \"dictionary\" - key to specify a name of the existing dictionary \"order\" - key to specify an order number by which dictionaries are being sorted in the \"Faceted filters\" panel. This order number is relative - if dictionary entries are not associated with any object, such entries or even dictionary itself will not be visible at the \"Faceted filters\" panel \"defaultDictEntriesToDisplay\" - key to specify a number of the dictionary values that will be always visible (if the count of the dictionary values exceeds this number - extra values will be hidden). This setting is being configured for all faceted filters simultaneously, but can be configured for the specific dictionary separately (individual dictionary setting has higher priority). Possible values for that setting - number or string \"all\" - in last case, all dictionary values will be always displayed Example of the specific dictionary settings: Here: the system dictionary used as faceted filter has name \"File formats\" the relative order of that dictionary at the \"Faceted filters\" panel is 7 the default count of that dictionary values that should be always displayed at the \"Faceted filters\" panel is 5 Faceted filters at the search page Filters displaying To open the \"Advanced Search\" page - click the Search icon in the main menu: The \"Faceted filters\" panel is placed on the left side of the \"Advanced Search\" page: Here all dictionaries specified in the system preference faceted.filter.dictionaries are displayed. Please note, that only dictionaries which entries associated with any object are displayed. For example, let some dictionary Dictionary1 has entries Value1 and Value2 . If any platform object is tagged by Dictionary1:Value1 and no objects are tagged by Dictionary1:Value2 , in the \"Faceted filters\" panel Value2 will not be shown for the filter Dictionary1 . If there are no platform objects tagged by Dictionary1:Value1 and Dictionary1:Value2 - in the \"Faceted filters\" panel, the filter Dictionary1 will not be shown at all. Displaying of filters: Where: a - header with the filter name (dictionary name). All filter sections are expanded by default. To collapse the section - click the header with the filter name, e.g.: b - filter values (dictionary entries associated with objects) c - footer that allows to show all dictionary entries associated with objects. This element is displayed only for dictionaries which count of associated entries is more than configured \"defaultDictEntriesToDisplay\" value. For the dictionary in example above, \"defaultDictEntriesToDisplay\" value is 4 . Click the footer to show all entries associated with objects: Click the footer again to hide \"extra\" entries d - checkbox that allows to select only objects from the search results that are associated with the certain dictionary entry (i.e. only objects tagged by the certain attribute) e - the count of the objects associated with the certain dictionary entry. Filter values are being sorted by this count descending If any filter has more than 10 associated entries, under its header the search bar appears - to quick search among entries of this filter, e.g.: Using faceted filters to search To use faceted filter to search objects by their attributes (tags) - just click the checkbox near the desired filter value. In the search results, only objects were associated with the checked filter value (dictionary entry) will remain, e.g.: Only objects tagged by this dictionary entry remained in the search results. After any faceted filter value's checkbox is being changed its state (manually enabled/disabled): other filters are being updated - count of associated objects changes according to the new selected condition: if any value of other faceted filter has no common associated objects with the selected filter value - such value becomes disabled, being colored in grey and has 0 value as a count of associated objects (i.e. for values from different faceted filters (different dictionaries) the logical operation AND is being performed), e.g.: but for the same faceted filter which value is selected (same dictionary) other filter values remain without changes - you can selected them, their counts of associated objects doesn't change (i.e. for values from the same faceted filter (the same dictionary) the logical operation OR is being performed), e.g.: in the panel of object types (under the main search bar), the count of displayed objects is being changed also paging is updated You can select several filters values from different dictionaries. Each time, other filters will be updated according to the rules above, and also displayed search results will be changed according to the selected filters, e.g.: You can hover over any displayed search result and click the Info icon to check that the object is really tagged by selected filters (attributes), e.g.: To clear all filters and remove restrictions click the corresponding button at the top of the \"Faceted filters\" panel:","title":"17.1. Faceted filters search using tags"},{"location":"manual/17_Tagging_by_attributes/17.1._Faceted_filters_search_by_tags/#171-faceted-filters-search-using-tags","text":"Preliminary preparation Configure faceted filters Faceted filters at the search page Filters displaying Using faceted filters to search In Cloud Pipeline , besides the simple Global Search, there is also the advanced search included search by tags (attributes). Search by tags is being performed via faceted filters panel. Tags' keys and values displayed in this panel are loaded from System Dictionaries marked as filter sources. User can restrict (filter) search results - checking/unchecking desired filters. Let's take a closer look.","title":"17.1. Faceted filters search using tags"},{"location":"manual/17_Tagging_by_attributes/17.1._Faceted_filters_search_by_tags/#preliminary-preparation","text":"\"Faceted filters\" search is convenient, when users have well-structured data in the platform and want to search by the attributes. But not all specified attributes are being loaded to the faceted filters, only attributes that were added to objects using System Dictionaries will be loaded to these filters. Firstly, the sets of key and possible values should be created as corresponding dictionaries in the special section of the System Settings , e.g.: About System Dictionaries see details here . After the dictionary is created, it can be used for the attribute (tag) creation. For that: Navigate to the object you wish to tag Open the \"Attributes\" panel for the object Click the \" + Add \" button at the \"Attributes\" panel: Click the \"Key\" field. In the appeared list, all available dictionaries list will appear: Select the dictionary name that you want to use as the attribute key In the \"Value\" field, corresponding values of the selected dictionary will appear: Select the dictionary value that you want to use as the attribute value Click the \" v Add \" button to confirm: Added attribute will appear in the \"Attributes\" panel: About the tagging objects via System Dictionaries see details here .","title":"Preliminary preparation"},{"location":"manual/17_Tagging_by_attributes/17.1._Faceted_filters_search_by_tags/#configure-faceted-filters","text":"To determine which of the available System Dictionaries should be used as faceted filters, there is a special system preference faceted.filter.dictionaries . Here, admin shall manually specify dictionaries and some settings of displaying specified dictionaries at the \"Faceted filters\" panel: This preference has JSON -format. Features: \"dictionaries\" - array of the existing System Dictionaries that shall be used and displayed in the \"Faceted filters\" panel \"dictionary\" - key to specify a name of the existing dictionary \"order\" - key to specify an order number by which dictionaries are being sorted in the \"Faceted filters\" panel. This order number is relative - if dictionary entries are not associated with any object, such entries or even dictionary itself will not be visible at the \"Faceted filters\" panel \"defaultDictEntriesToDisplay\" - key to specify a number of the dictionary values that will be always visible (if the count of the dictionary values exceeds this number - extra values will be hidden). This setting is being configured for all faceted filters simultaneously, but can be configured for the specific dictionary separately (individual dictionary setting has higher priority). Possible values for that setting - number or string \"all\" - in last case, all dictionary values will be always displayed Example of the specific dictionary settings: Here: the system dictionary used as faceted filter has name \"File formats\" the relative order of that dictionary at the \"Faceted filters\" panel is 7 the default count of that dictionary values that should be always displayed at the \"Faceted filters\" panel is 5","title":"Configure faceted filters"},{"location":"manual/17_Tagging_by_attributes/17.1._Faceted_filters_search_by_tags/#faceted-filters-at-the-search-page","text":"","title":"Faceted filters at the search page"},{"location":"manual/17_Tagging_by_attributes/17.1._Faceted_filters_search_by_tags/#filters-displaying","text":"To open the \"Advanced Search\" page - click the Search icon in the main menu: The \"Faceted filters\" panel is placed on the left side of the \"Advanced Search\" page: Here all dictionaries specified in the system preference faceted.filter.dictionaries are displayed. Please note, that only dictionaries which entries associated with any object are displayed. For example, let some dictionary Dictionary1 has entries Value1 and Value2 . If any platform object is tagged by Dictionary1:Value1 and no objects are tagged by Dictionary1:Value2 , in the \"Faceted filters\" panel Value2 will not be shown for the filter Dictionary1 . If there are no platform objects tagged by Dictionary1:Value1 and Dictionary1:Value2 - in the \"Faceted filters\" panel, the filter Dictionary1 will not be shown at all. Displaying of filters: Where: a - header with the filter name (dictionary name). All filter sections are expanded by default. To collapse the section - click the header with the filter name, e.g.: b - filter values (dictionary entries associated with objects) c - footer that allows to show all dictionary entries associated with objects. This element is displayed only for dictionaries which count of associated entries is more than configured \"defaultDictEntriesToDisplay\" value. For the dictionary in example above, \"defaultDictEntriesToDisplay\" value is 4 . Click the footer to show all entries associated with objects: Click the footer again to hide \"extra\" entries d - checkbox that allows to select only objects from the search results that are associated with the certain dictionary entry (i.e. only objects tagged by the certain attribute) e - the count of the objects associated with the certain dictionary entry. Filter values are being sorted by this count descending If any filter has more than 10 associated entries, under its header the search bar appears - to quick search among entries of this filter, e.g.:","title":"Filters displaying"},{"location":"manual/17_Tagging_by_attributes/17.1._Faceted_filters_search_by_tags/#using-faceted-filters-to-search","text":"To use faceted filter to search objects by their attributes (tags) - just click the checkbox near the desired filter value. In the search results, only objects were associated with the checked filter value (dictionary entry) will remain, e.g.: Only objects tagged by this dictionary entry remained in the search results. After any faceted filter value's checkbox is being changed its state (manually enabled/disabled): other filters are being updated - count of associated objects changes according to the new selected condition: if any value of other faceted filter has no common associated objects with the selected filter value - such value becomes disabled, being colored in grey and has 0 value as a count of associated objects (i.e. for values from different faceted filters (different dictionaries) the logical operation AND is being performed), e.g.: but for the same faceted filter which value is selected (same dictionary) other filter values remain without changes - you can selected them, their counts of associated objects doesn't change (i.e. for values from the same faceted filter (the same dictionary) the logical operation OR is being performed), e.g.: in the panel of object types (under the main search bar), the count of displayed objects is being changed also paging is updated You can select several filters values from different dictionaries. Each time, other filters will be updated according to the rules above, and also displayed search results will be changed according to the selected filters, e.g.: You can hover over any displayed search result and click the Info icon to check that the object is really tagged by selected filters (attributes), e.g.: To clear all filters and remove restrictions click the corresponding button at the top of the \"Faceted filters\" panel:","title":"Using faceted filters to search"},{"location":"manual/17_Tagging_by_attributes/17._CP_objects_tagging_by_additional_attributes/","text":"17. CP objects tagging by additional attributes Add attributes Add JSON object into the attribute value Add predefined values from dictionaries Edit attributes Delete attributes Automatic tagging A user can manage custom sets of \" key-values \" attributes for data storage and files. These custom attributes could be used for an additional description of the object and make the search process easier by using attributes as tags. To edit object's attributes, you need to be an OWNER of the object. For more information see 13. Permissions . You can also manage attributes via CLI. See 14.2. View and manage Attributes via CLI . How to navigate to Attributes panel of different objects: Folder Metadata Pipeline Data storage Tool groups and tools User Group of users/role Note : if you were changing the data storage file's attributes, you could return to data storage's attribute by clicking control. Add attributes Navigate to the Attributes panel of a selected object. Click the + Add button. Enter an attribute key and value. Click the Add button: Added attribute will appear at the Attributes panel: Add JSON object into the attribute value Also, you can add more complex attributes than just strings. In the \"Value\" field you can specify a raw JSON object, that will be transformed into the pretty-view table. View an example: Navigate to the Attributes panel of a selected object. Click the + Add button. Enter an attribute key. Enter the JSON object as the attribute value, click the Add button: Added attribute will appear at the Attributes panel: As the value - the link will be displayed that shows the summary count of first-level JSON records. Click the value link - the pretty-view detailed table will be opened for the added attribute: If the raw JSON has more than one level, downstream records will be shown as the link Object . You can hover over it and view the downstream records: If you want to edit such attribute value - click the EDIT button. The raw JSON will be opened: You can edit it and click the SAVE button to save changes. Add predefined values from dictionaries Another way to add attribute key and value - not to specify them manually and select from predefined list of System Dictionaries . Firstly, the sets of key and possible values should be created by admins as corresponding dictionaries in special section of the System Settings , e.g.: After the dictionary is created, it can be used for the tag creation. For that, you should click the \"Key\" field during the attribute creation and select the dictionary name in the list. After that, the corresponding list of dictionary values becomes available for specifying as the attribute \"Value\": Note : currently, described functionality is available for admins only See more details and examples in System Dictionaries . Edit attributes Navigate to the Attributes panel of a selected object. Click the attribute key or value field: Change the attribute key or value: Press the Enter key or just click out of the active field. Edited attribute will appear at the Attributes panel: Delete attributes Navigate to the Attributes panel of a selected object. Click the Trash icon to delete a particular attribute. Note : click the Remove all button to delete all attributes. Automatic tagging In the Cloud Pipeline files are automatically tagged with the following attributes when uploading them to the data storage via CLI/GUI (see a CLI example 14.3. Manage Storage via CLI ): Name in GUI Name in CLI Value Owner CP_OWNER The value of the attribute will be set as a user ID. Source CP_SOURCE The value of the attribute will be set as a local path used to upload. Note : this attribute is set only if a file is uploaded through CLI. Note : The exception is that the storage is based on FS share. Files in such data storage don't have attributes at all. Besides, files are automatically tagged with the following attributes when uploading them to the data storage as a result of a pipeline run: Name in GUI Name in CLI Value Owner CP_OWNER User ID Source CP_SOURCE Local path used to upload data RunID CP_RUN_ID Run ID Pipeline CP_JOB_NAME Pipeline name CP_JOB_ID Pipeline ID Pipeline version CP_JOB_VERSION Pipeline version Pipeline configuration CP_JOB_CONFIGURATION Pipeline configuration Docker image CP_DOCKER_IMAGE Tool (docker image) that was used Compute CP_CALC_CONFIG Instance type Example of the Attributes panel for the such file:","title":"17.0. CP objects tagging by additional attributes"},{"location":"manual/17_Tagging_by_attributes/17._CP_objects_tagging_by_additional_attributes/#17-cp-objects-tagging-by-additional-attributes","text":"Add attributes Add JSON object into the attribute value Add predefined values from dictionaries Edit attributes Delete attributes Automatic tagging A user can manage custom sets of \" key-values \" attributes for data storage and files. These custom attributes could be used for an additional description of the object and make the search process easier by using attributes as tags. To edit object's attributes, you need to be an OWNER of the object. For more information see 13. Permissions . You can also manage attributes via CLI. See 14.2. View and manage Attributes via CLI . How to navigate to Attributes panel of different objects: Folder Metadata Pipeline Data storage Tool groups and tools User Group of users/role Note : if you were changing the data storage file's attributes, you could return to data storage's attribute by clicking control.","title":"17. CP objects tagging by additional attributes"},{"location":"manual/17_Tagging_by_attributes/17._CP_objects_tagging_by_additional_attributes/#add-attributes","text":"Navigate to the Attributes panel of a selected object. Click the + Add button. Enter an attribute key and value. Click the Add button: Added attribute will appear at the Attributes panel:","title":"Add attributes"},{"location":"manual/17_Tagging_by_attributes/17._CP_objects_tagging_by_additional_attributes/#add-json-object-into-the-attribute-value","text":"Also, you can add more complex attributes than just strings. In the \"Value\" field you can specify a raw JSON object, that will be transformed into the pretty-view table. View an example: Navigate to the Attributes panel of a selected object. Click the + Add button. Enter an attribute key. Enter the JSON object as the attribute value, click the Add button: Added attribute will appear at the Attributes panel: As the value - the link will be displayed that shows the summary count of first-level JSON records. Click the value link - the pretty-view detailed table will be opened for the added attribute: If the raw JSON has more than one level, downstream records will be shown as the link Object . You can hover over it and view the downstream records: If you want to edit such attribute value - click the EDIT button. The raw JSON will be opened: You can edit it and click the SAVE button to save changes.","title":"Add JSON object into the attribute value"},{"location":"manual/17_Tagging_by_attributes/17._CP_objects_tagging_by_additional_attributes/#add-predefined-values-from-dictionaries","text":"Another way to add attribute key and value - not to specify them manually and select from predefined list of System Dictionaries . Firstly, the sets of key and possible values should be created by admins as corresponding dictionaries in special section of the System Settings , e.g.: After the dictionary is created, it can be used for the tag creation. For that, you should click the \"Key\" field during the attribute creation and select the dictionary name in the list. After that, the corresponding list of dictionary values becomes available for specifying as the attribute \"Value\": Note : currently, described functionality is available for admins only See more details and examples in System Dictionaries .","title":"Add predefined values from dictionaries"},{"location":"manual/17_Tagging_by_attributes/17._CP_objects_tagging_by_additional_attributes/#edit-attributes","text":"Navigate to the Attributes panel of a selected object. Click the attribute key or value field: Change the attribute key or value: Press the Enter key or just click out of the active field. Edited attribute will appear at the Attributes panel:","title":"Edit attributes"},{"location":"manual/17_Tagging_by_attributes/17._CP_objects_tagging_by_additional_attributes/#delete-attributes","text":"Navigate to the Attributes panel of a selected object. Click the Trash icon to delete a particular attribute. Note : click the Remove all button to delete all attributes.","title":"Delete attributes"},{"location":"manual/17_Tagging_by_attributes/17._CP_objects_tagging_by_additional_attributes/#automatic-tagging","text":"In the Cloud Pipeline files are automatically tagged with the following attributes when uploading them to the data storage via CLI/GUI (see a CLI example 14.3. Manage Storage via CLI ): Name in GUI Name in CLI Value Owner CP_OWNER The value of the attribute will be set as a user ID. Source CP_SOURCE The value of the attribute will be set as a local path used to upload. Note : this attribute is set only if a file is uploaded through CLI. Note : The exception is that the storage is based on FS share. Files in such data storage don't have attributes at all. Besides, files are automatically tagged with the following attributes when uploading them to the data storage as a result of a pipeline run: Name in GUI Name in CLI Value Owner CP_OWNER User ID Source CP_SOURCE Local path used to upload data RunID CP_RUN_ID Run ID Pipeline CP_JOB_NAME Pipeline name CP_JOB_ID Pipeline ID Pipeline version CP_JOB_VERSION Pipeline version Pipeline configuration CP_JOB_CONFIGURATION Pipeline configuration Docker image CP_DOCKER_IMAGE Tool (docker image) that was used Compute CP_CALC_CONFIG Instance type Example of the Attributes panel for the such file:","title":"Automatic tagging"},{"location":"manual/18_Home_page/18._Home_page/","text":"18. Home page Home page widgets Activities Data Notifications Tools Pipelines Projects Recently completed runs Active runs Services Adjust Home page view Start a Run from the Home tab Home page widgets Activities This widget lists recent comments/issues/posts that occured for the items that you own (e.g. models, pipelines, projects, folders, etc.). Data List of the data storages that are available to you for READ/WRITE operations. These data storages are available from the Platform GUI. Click an item to navigate to the data storage contents. Note : personal Data storages (i.e. a user is an OWNER of this Storage) will be shown on top, WRITE - second priority, READ - third priority. Note : data storages are tagged with the Cloud Region flag to visually distinguish storage locations. Note : if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - corresponding auxiliary Cloud Provider icons are additionally displayed, e.g.: Notifications List of system-wide notifications from administrators posted. These are the same notifications as shown at a login time in the top right corner of the main page. Tools Tools/Compute stacks/Docker images that are added in your PERSONAL repository or available to your group. To run a Tool - please use a RUN button that appears when hovering an item with a mouse. Use a search bar to find tools that are shared by other users and groups. To get a full list of available stacks - please use the Tools menu item in the left toolbar. Note : group-level Tools will be shown on the top of the Tools list. Pipelines Pipelines that are available to you for READ/WRITE operations. This is the same list, as available in the Library hierarchy. Pipeline can be run right from this widget using a RUN button that appears when hovering an item with a mouse. Press the HISTORY control to view runs history of a chosen pipeline. Projects Projects that are available to you. This is the same list, as available in the Library hierarchy. Projects could be opened from this widget. Press the HISTORY control to view runs history of a chosen Project. Press the DATA STORAGE control to open a data storage included into a chosen Project. Recently completed runs This widget lists your runs that were recently completed. Click a corresponding entry in this widget to navigate to the run details/logs page. Use the Rerun control to rerun selected item. Note : completed runs are tagged with the Cloud Region flag to visually distinguish compute instance locations. Note : if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - corresponding auxiliary Cloud Provider icons are additionally displayed, e.g.: Help tooltips are provided when hovering a run state icon, e.g.: Active runs List of the jobs/tools that are currently in RUNNING or PAUSED state with some information about instance, elapsed time and estimated price, that is calculated based on the run duration and instance type. If this list is empty - start a new run from the Tools or Pipelines widgets. Hover a run item to view a list of available action. Use STOP / PAUSE / RESUME / TERMINATE actions to change the state of the run, use OPEN to navigate to the GUI of the interactive job or use SSH to open SSH session over the running container. Select the LINKS button to view/navigate the run input/output parameters. Click a run item to navigate to the details and logs page. Note : active runs are tagged with the Cloud Region flag to visually distinguish compute instance locations. Only top 50 active runs will be shown, if more than 50 jobs/tools are running - use Explore all active runs link. Note : if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - corresponding auxiliary Cloud Provider icons are additionally displayed, e.g.: Note : if specific run CPU/Memory/Disk consumption is lower or higher specified in the configurations values, the IDLE or PRESSURE labels will be displayed respectively: Help tooltips are provided when hovering a run state icon, e.g.: Services This widget lists direct links to the Interactive services, that are exposed by the launched Tools. Compared to the ACTIVE RUNS widget - this one does NOT show all the active jobs/tools, only links to the web/desktop GUI. If this list is empty - start a new run of an interactive compute stack from the Tools widget. Click an item within this widget to navigate to the corresponding service. Adjust Home page view Dashboard view can be adjusted by clicking the Configure button. In the opened window a user can adjust view of the Home tab by selecting widgets. Selected items will be displayed after you click OK : Show only favourites checkbox allows to restrict displaying items in selected widgets and show only ones ticked by favourite mark (\"star\" icon in the left side of the items - ). This feature works only for DATA , PIPELINES , PROJECTS and TOOLS widgets. Restore default layout control is used to restore default tab configuration. Also, user can remove widgets by clicking the \"Delete\" icon on them. Start a Run from the Home tab User shall have EXECUTE rights to run selected Tool/pipeline. Although Runs can be started from the Tools or Library tabs respectively (see 10.5. Launch a Tool and 6.2. Launch a pipeline ), we can start them from the Home tab as well. In the example below we will start a Run from the Tools widget of the Home tab: Navigate to the Home tab. Select a Tool in the Tools widget. Click Run . Run a Tool with custom settings with Run custom control or run them with default settings with Run button. Note : Active runs with endpoints are highlighted in yellow. They expose the Open button that shows endpoints. Clicking them will navigate you to the endpoint URL: Note : all widgets that display Runs show input/output links via LINKS control. Clicking a link will navigate you to the appropriate Data Storage.","title":"18. Home page"},{"location":"manual/18_Home_page/18._Home_page/#18-home-page","text":"Home page widgets Activities Data Notifications Tools Pipelines Projects Recently completed runs Active runs Services Adjust Home page view Start a Run from the Home tab","title":"18. Home page"},{"location":"manual/18_Home_page/18._Home_page/#home-page-widgets","text":"","title":"Home page widgets"},{"location":"manual/18_Home_page/18._Home_page/#activities","text":"This widget lists recent comments/issues/posts that occured for the items that you own (e.g. models, pipelines, projects, folders, etc.).","title":"Activities"},{"location":"manual/18_Home_page/18._Home_page/#data","text":"List of the data storages that are available to you for READ/WRITE operations. These data storages are available from the Platform GUI. Click an item to navigate to the data storage contents. Note : personal Data storages (i.e. a user is an OWNER of this Storage) will be shown on top, WRITE - second priority, READ - third priority. Note : data storages are tagged with the Cloud Region flag to visually distinguish storage locations. Note : if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - corresponding auxiliary Cloud Provider icons are additionally displayed, e.g.:","title":"Data"},{"location":"manual/18_Home_page/18._Home_page/#notifications","text":"List of system-wide notifications from administrators posted. These are the same notifications as shown at a login time in the top right corner of the main page.","title":"Notifications"},{"location":"manual/18_Home_page/18._Home_page/#tools","text":"Tools/Compute stacks/Docker images that are added in your PERSONAL repository or available to your group. To run a Tool - please use a RUN button that appears when hovering an item with a mouse. Use a search bar to find tools that are shared by other users and groups. To get a full list of available stacks - please use the Tools menu item in the left toolbar. Note : group-level Tools will be shown on the top of the Tools list.","title":"Tools"},{"location":"manual/18_Home_page/18._Home_page/#pipelines","text":"Pipelines that are available to you for READ/WRITE operations. This is the same list, as available in the Library hierarchy. Pipeline can be run right from this widget using a RUN button that appears when hovering an item with a mouse. Press the HISTORY control to view runs history of a chosen pipeline.","title":"Pipelines"},{"location":"manual/18_Home_page/18._Home_page/#projects","text":"Projects that are available to you. This is the same list, as available in the Library hierarchy. Projects could be opened from this widget. Press the HISTORY control to view runs history of a chosen Project. Press the DATA STORAGE control to open a data storage included into a chosen Project.","title":"Projects"},{"location":"manual/18_Home_page/18._Home_page/#recently-completed-runs","text":"This widget lists your runs that were recently completed. Click a corresponding entry in this widget to navigate to the run details/logs page. Use the Rerun control to rerun selected item. Note : completed runs are tagged with the Cloud Region flag to visually distinguish compute instance locations. Note : if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - corresponding auxiliary Cloud Provider icons are additionally displayed, e.g.: Help tooltips are provided when hovering a run state icon, e.g.:","title":"Recently completed runs"},{"location":"manual/18_Home_page/18._Home_page/#active-runs","text":"List of the jobs/tools that are currently in RUNNING or PAUSED state with some information about instance, elapsed time and estimated price, that is calculated based on the run duration and instance type. If this list is empty - start a new run from the Tools or Pipelines widgets. Hover a run item to view a list of available action. Use STOP / PAUSE / RESUME / TERMINATE actions to change the state of the run, use OPEN to navigate to the GUI of the interactive job or use SSH to open SSH session over the running container. Select the LINKS button to view/navigate the run input/output parameters. Click a run item to navigate to the details and logs page. Note : active runs are tagged with the Cloud Region flag to visually distinguish compute instance locations. Only top 50 active runs will be shown, if more than 50 jobs/tools are running - use Explore all active runs link. Note : if a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - corresponding auxiliary Cloud Provider icons are additionally displayed, e.g.: Note : if specific run CPU/Memory/Disk consumption is lower or higher specified in the configurations values, the IDLE or PRESSURE labels will be displayed respectively: Help tooltips are provided when hovering a run state icon, e.g.:","title":"Active runs"},{"location":"manual/18_Home_page/18._Home_page/#services","text":"This widget lists direct links to the Interactive services, that are exposed by the launched Tools. Compared to the ACTIVE RUNS widget - this one does NOT show all the active jobs/tools, only links to the web/desktop GUI. If this list is empty - start a new run of an interactive compute stack from the Tools widget. Click an item within this widget to navigate to the corresponding service.","title":"Services"},{"location":"manual/18_Home_page/18._Home_page/#adjust-home-page-view","text":"Dashboard view can be adjusted by clicking the Configure button. In the opened window a user can adjust view of the Home tab by selecting widgets. Selected items will be displayed after you click OK : Show only favourites checkbox allows to restrict displaying items in selected widgets and show only ones ticked by favourite mark (\"star\" icon in the left side of the items - ). This feature works only for DATA , PIPELINES , PROJECTS and TOOLS widgets. Restore default layout control is used to restore default tab configuration. Also, user can remove widgets by clicking the \"Delete\" icon on them.","title":"Adjust Home page view"},{"location":"manual/18_Home_page/18._Home_page/#start-a-run-from-the-home-tab","text":"User shall have EXECUTE rights to run selected Tool/pipeline. Although Runs can be started from the Tools or Library tabs respectively (see 10.5. Launch a Tool and 6.2. Launch a pipeline ), we can start them from the Home tab as well. In the example below we will start a Run from the Tools widget of the Home tab: Navigate to the Home tab. Select a Tool in the Tools widget. Click Run . Run a Tool with custom settings with Run custom control or run them with default settings with Run button. Note : Active runs with endpoints are highlighted in yellow. They expose the Open button that shows endpoints. Clicking them will navigate you to the endpoint URL: Note : all widgets that display Runs show input/output links via LINKS control. Clicking a link will navigate you to the appropriate Data Storage.","title":"Start a Run from the Home tab"},{"location":"manual/19_Search/19._Global_search/","text":"Global Search Simple search Advanced search Results output view List view Table view When the certain Cloud Pipeline deployment grows in terms of data and pipelines being added/implemented - it is crucial to be able to search for specific datasets or tools. To find other data and objects, there are two types of the search - Simple and Advanced . Simple search The following object types are being indexed and can be searched via Simple search : \" FOLDERS \" - the folders and metadata entities (in \"Library\") \" PIPELINES \" - the pipelines metadata, pipelines files (documents) and configurations \" RUNS \" - the job runs \" TOOLS \" - the docker registries, groups and tools \" DATA \" - the storages and their files (names only) \" ISSUES \" - the issues (discussions) User can open the Simple search form by pressing \"Ctrl+F\" being at any page of the Cloud Pipeline Web GUI ( excluding cases when the Run logs page or Advanced search page is open ): Note : Simple search form will not be opened if any pop-up window is shown To start searching, a \"google-like\" query string shall be entered (search can be triggered by pressing \"Enter\" button or automatically if no new input is provided for 2 seconds): Special expressions in the query string are available as well (the rules for their assignment are described in pop-up window that appears when hovering over the icon): By default search will occur across all the available object types. If user would to limit search scope - appropriate section can be selected above the query input: To get a brief information for the object found, you can hover an item with a mouse and a \"Preview\" pane will be shown to the right, e.g.: tool preview: pipeline preview: In the \"Preview\" window user can see: name of the found object block with indication and highlighting of the object's concrete part, where inputted word was found path to the object in library ( optionally ) description ( optionally ) preview of the found object ( if it's available ) or the link to download ( for files in storages ) other useful info like settings, OWNER name, attributes (tags), etc. To open found object, click it to navigate to its location within the Cloud Pipeline . Advanced search Currently, advanced search is available only for admins To open the Advanced search form, open the Simple search form (by pressing \"Ctrl+F\") and click the Advanced search button: OR Click the Search icon in the main menu: Advanced search repeats the functionality of the Simple search but has some advanced capabilities. The structure of the Advanced search form: This form contains: main search bar - to specify query string (search can be triggered by pressing \"Enter\" button or by the corresponding Search button near the search bar) a - similar to Simple search - panel to restrict object types among which the search should being performed. By default search will occur across all the available object types. If user would to restrict search scope - appropriate section can be selected, e.g.: b - control that allows to select the view type of the search results - list (by default) or table . See details below c - faceted filters panel that allows to search objects by their attributes (tags). Operating principle of this search part is similar to the E-Commerce sites. Details about this panel and working with it see here . Example of the search using filters from this panel: d - similar to Simple search - search results. User can use mouse scroll for auto-paging similar to Simple search - to get a brief information for the object found, you can hover an item with a mouse and click the Info to view its \"Preview\" pane, e.g.: To clear all filters and remove restrictions click the corresponding button at the top of the \"Faceted filters\" panel: Results output view Search results contain an array of objects (different Cloud Pipeline entities - file , storage , run , etc.). Results output in the Simple search has the view as simple list of the object names only. In the Advanced search , that output contains the additional info - according to the entity type of the certain result. Info displayed in the search results: Entity Required info Additional info Folder NAME OWNER Pipeline NAME OWNER / DESCRIPTION Document in the Pipeline NAME Configuration NAME OWNER / DESCRIPTION Run RUN_ID OWNER / DOCKER IMAGE or PIPELINE NAME / DATE_STARTED / DATE_COMPLETED Storage ALIAS_NAME OWNER / PATH / DESCRIPTION Tool NAME OWNER / DESCRIPTION File NAME OWNER / PATH / SIZE / DATE_CHANGED Issue TITLE OWNER / DESCRIPTION Objects in results can be shown in two formats: list view - each result is presented by the \"row\" in the list, additional info is placed in line table view - all results are presented by the single table, additional info is placed in columns User shall have the ability to switch between formats by the special control under the main search bar: List view This view type is default. Example of the list view in search results: Here: required info (object name and its type icon) is placed in the header of the result row additional info is placed in bottom line of the tile and colored in grey object owners are specified at the right side in each row Table view This view presents the table where all search results info (required and additional) is placed into corresponding columns. The set of columns is defined by the object types of the search results according to the table above. Example of the table view in search results (with the same results as in example above ): Also the table view can be customized by the admin user. To the existing columns, user can add ones for the object attributes (tags) values, where the attribute key becomes the column header. If the object in results has the value for that attribute - it will be displayed in the corresponding column. To specify which attributes shall be added to the table view, there is the special system preference - search.elastic.index.metadata.fields . In this preference, the additional attributes (tags) keys should be specified as the plain JSON string array: [\"AttributeKey1\", \"AttributeKey2\", ...] . Example: specified additional attribute keys: when these keys are specified and saved, they will automatically appear as additional columns in the table view of the Advanced search results: for the objects that have values for added attribute keys, they will be shown in the corresponding columns, e.g.:","title":"19. Global search"},{"location":"manual/19_Search/19._Global_search/#global-search","text":"Simple search Advanced search Results output view List view Table view When the certain Cloud Pipeline deployment grows in terms of data and pipelines being added/implemented - it is crucial to be able to search for specific datasets or tools. To find other data and objects, there are two types of the search - Simple and Advanced .","title":"Global Search"},{"location":"manual/19_Search/19._Global_search/#simple-search","text":"The following object types are being indexed and can be searched via Simple search : \" FOLDERS \" - the folders and metadata entities (in \"Library\") \" PIPELINES \" - the pipelines metadata, pipelines files (documents) and configurations \" RUNS \" - the job runs \" TOOLS \" - the docker registries, groups and tools \" DATA \" - the storages and their files (names only) \" ISSUES \" - the issues (discussions) User can open the Simple search form by pressing \"Ctrl+F\" being at any page of the Cloud Pipeline Web GUI ( excluding cases when the Run logs page or Advanced search page is open ): Note : Simple search form will not be opened if any pop-up window is shown To start searching, a \"google-like\" query string shall be entered (search can be triggered by pressing \"Enter\" button or automatically if no new input is provided for 2 seconds): Special expressions in the query string are available as well (the rules for their assignment are described in pop-up window that appears when hovering over the icon): By default search will occur across all the available object types. If user would to limit search scope - appropriate section can be selected above the query input: To get a brief information for the object found, you can hover an item with a mouse and a \"Preview\" pane will be shown to the right, e.g.: tool preview: pipeline preview: In the \"Preview\" window user can see: name of the found object block with indication and highlighting of the object's concrete part, where inputted word was found path to the object in library ( optionally ) description ( optionally ) preview of the found object ( if it's available ) or the link to download ( for files in storages ) other useful info like settings, OWNER name, attributes (tags), etc. To open found object, click it to navigate to its location within the Cloud Pipeline .","title":"Simple search"},{"location":"manual/19_Search/19._Global_search/#advanced-search","text":"Currently, advanced search is available only for admins To open the Advanced search form, open the Simple search form (by pressing \"Ctrl+F\") and click the Advanced search button: OR Click the Search icon in the main menu: Advanced search repeats the functionality of the Simple search but has some advanced capabilities. The structure of the Advanced search form: This form contains: main search bar - to specify query string (search can be triggered by pressing \"Enter\" button or by the corresponding Search button near the search bar) a - similar to Simple search - panel to restrict object types among which the search should being performed. By default search will occur across all the available object types. If user would to restrict search scope - appropriate section can be selected, e.g.: b - control that allows to select the view type of the search results - list (by default) or table . See details below c - faceted filters panel that allows to search objects by their attributes (tags). Operating principle of this search part is similar to the E-Commerce sites. Details about this panel and working with it see here . Example of the search using filters from this panel: d - similar to Simple search - search results. User can use mouse scroll for auto-paging similar to Simple search - to get a brief information for the object found, you can hover an item with a mouse and click the Info to view its \"Preview\" pane, e.g.: To clear all filters and remove restrictions click the corresponding button at the top of the \"Faceted filters\" panel:","title":"Advanced search"},{"location":"manual/19_Search/19._Global_search/#results-output-view","text":"Search results contain an array of objects (different Cloud Pipeline entities - file , storage , run , etc.). Results output in the Simple search has the view as simple list of the object names only. In the Advanced search , that output contains the additional info - according to the entity type of the certain result. Info displayed in the search results: Entity Required info Additional info Folder NAME OWNER Pipeline NAME OWNER / DESCRIPTION Document in the Pipeline NAME Configuration NAME OWNER / DESCRIPTION Run RUN_ID OWNER / DOCKER IMAGE or PIPELINE NAME / DATE_STARTED / DATE_COMPLETED Storage ALIAS_NAME OWNER / PATH / DESCRIPTION Tool NAME OWNER / DESCRIPTION File NAME OWNER / PATH / SIZE / DATE_CHANGED Issue TITLE OWNER / DESCRIPTION Objects in results can be shown in two formats: list view - each result is presented by the \"row\" in the list, additional info is placed in line table view - all results are presented by the single table, additional info is placed in columns User shall have the ability to switch between formats by the special control under the main search bar:","title":"Results output view"},{"location":"manual/19_Search/19._Global_search/#list-view","text":"This view type is default. Example of the list view in search results: Here: required info (object name and its type icon) is placed in the header of the result row additional info is placed in bottom line of the tile and colored in grey object owners are specified at the right side in each row","title":"List view"},{"location":"manual/19_Search/19._Global_search/#table-view","text":"This view presents the table where all search results info (required and additional) is placed into corresponding columns. The set of columns is defined by the object types of the search results according to the table above. Example of the table view in search results (with the same results as in example above ): Also the table view can be customized by the admin user. To the existing columns, user can add ones for the object attributes (tags) values, where the attribute key becomes the column header. If the object in results has the value for that attribute - it will be displayed in the corresponding column. To specify which attributes shall be added to the table view, there is the special system preference - search.elastic.index.metadata.fields . In this preference, the additional attributes (tags) keys should be specified as the plain JSON string array: [\"AttributeKey1\", \"AttributeKey2\", ...] . Example: specified additional attribute keys: when these keys are specified and saved, they will automatically appear as additional columns in the table view of the Advanced search results: for the objects that have values for added attribute keys, they will be shown in the corresponding columns, e.g.:","title":"Table view"},{"location":"manual/Appendix_A/Appendix_A._Instance_and_Docker_container_lifecycles/","text":"Instance and Docker container lifecycles Overview Instance lifecycle stages Docker container lifecycle stages Overview In the context of Cloud Pipeline , both life cycles are tied together. A user is charged for instance usage. Note : to run a container you need a launched instance. In the Cloud Pipeline , instances are bought for hourly rates with a minimum of one hour. This is due to the following reasons: It helps to decrease the time for node relaunch. It helps to decrease the time for Docker image and data (\"type\": \"common\") download. Most pipelines will take much longer than 1 hour to complete. Note : same instance configuration must be used in order to reuse currently active nodes. If the node has no running jobs 10 minutes before the new hour of payment begins, it will be terminated. Instance lifecycle stages Note : instance lifecycle stages are presented on the example of one of the supported instances - EC2 of AWS Cloud Provider. The general overview of the EC2 instance lifecycle - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-lifecycle.html . In the Cloud Pipeline there are 3 stages: Pending state - no billing. Running state - you're billed for each second, with a one-minute minimum, that you keep the instance running. Terminated state - no billing. Docker container lifecycle stages A general overview of the Docker container lifecycle - https://medium.com/@nagarwal/lifecycle-of-docker-container-d2da9f85959 . In the Cloud Pipeline there are 2 stages: Running state - possible to execute some commands inside the container. Terminated state - the container is not accessible.","title":"Appendix A. Instance and Docker container lifecycles"},{"location":"manual/Appendix_A/Appendix_A._Instance_and_Docker_container_lifecycles/#instance-and-docker-container-lifecycles","text":"Overview Instance lifecycle stages Docker container lifecycle stages","title":"Instance and Docker container lifecycles"},{"location":"manual/Appendix_A/Appendix_A._Instance_and_Docker_container_lifecycles/#overview","text":"In the context of Cloud Pipeline , both life cycles are tied together. A user is charged for instance usage. Note : to run a container you need a launched instance. In the Cloud Pipeline , instances are bought for hourly rates with a minimum of one hour. This is due to the following reasons: It helps to decrease the time for node relaunch. It helps to decrease the time for Docker image and data (\"type\": \"common\") download. Most pipelines will take much longer than 1 hour to complete. Note : same instance configuration must be used in order to reuse currently active nodes. If the node has no running jobs 10 minutes before the new hour of payment begins, it will be terminated.","title":"Overview"},{"location":"manual/Appendix_A/Appendix_A._Instance_and_Docker_container_lifecycles/#instance-lifecycle-stages","text":"Note : instance lifecycle stages are presented on the example of one of the supported instances - EC2 of AWS Cloud Provider. The general overview of the EC2 instance lifecycle - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-lifecycle.html . In the Cloud Pipeline there are 3 stages: Pending state - no billing. Running state - you're billed for each second, with a one-minute minimum, that you keep the instance running. Terminated state - no billing.","title":"Instance lifecycle stages"},{"location":"manual/Appendix_A/Appendix_A._Instance_and_Docker_container_lifecycles/#docker-container-lifecycle-stages","text":"A general overview of the Docker container lifecycle - https://medium.com/@nagarwal/lifecycle-of-docker-container-d2da9f85959 . In the Cloud Pipeline there are 2 stages: Running state - possible to execute some commands inside the container. Terminated state - the container is not accessible.","title":"Docker container lifecycle stages"},{"location":"manual/Appendix_B/Appendix_B._Working_with_a_Project/","text":"Working with a Project Project is a special type of Folder . Projects might be used to organize data and metadata and simplify analysis runs for a large data set. Also, you can set a project attributes as parameters of analysis method configuration. Note : learn more about metadata here . Create a project To create a project you need WRITE permissions for the parent folder and the ROLE_FOLDER_MANAGER role. For more information see 13. Permissions . To create a project in the system, the following steps shall be performed: Navigate to a folder of a future-project destination. Click + Create \u2192 PROJECT . Note PROJECT is an oncology project template. It supports the default structure of an oncology project. The system suggests that you name a new project. Enter a name. Click OK button. You'll be fetched into the new project's parent folder page automatically. The folder has a default attribute: type = project (see picture above, 2 ). Note : It's an essential attribute for a project. Based on this attribute, the system recognizes a folder as a project. If the attribute is removed, the folder is no longer a project. The new project contains (see picture above, 1 ): Method-Configuration folder. It will be a container for all your methods to run. Storage . The storage will be empty. The name of the storage will be set as a default. Here you can see default settings of new storage. History . This is a table that contains all at any time scheduled runs of a project's methods. For now, it's empty. The picture below illustrated how the table looks with an existing run's history. The History GUI repeats the Run space. For more details see 11. Manage Runs . How to add metadata in the project, see here .","title":"Appendix B. Working with a Project"},{"location":"manual/Appendix_B/Appendix_B._Working_with_a_Project/#working-with-a-project","text":"Project is a special type of Folder . Projects might be used to organize data and metadata and simplify analysis runs for a large data set. Also, you can set a project attributes as parameters of analysis method configuration. Note : learn more about metadata here .","title":"Working with a Project"},{"location":"manual/Appendix_B/Appendix_B._Working_with_a_Project/#create-a-project","text":"To create a project you need WRITE permissions for the parent folder and the ROLE_FOLDER_MANAGER role. For more information see 13. Permissions . To create a project in the system, the following steps shall be performed: Navigate to a folder of a future-project destination. Click + Create \u2192 PROJECT . Note PROJECT is an oncology project template. It supports the default structure of an oncology project. The system suggests that you name a new project. Enter a name. Click OK button. You'll be fetched into the new project's parent folder page automatically. The folder has a default attribute: type = project (see picture above, 2 ). Note : It's an essential attribute for a project. Based on this attribute, the system recognizes a folder as a project. If the attribute is removed, the folder is no longer a project. The new project contains (see picture above, 1 ): Method-Configuration folder. It will be a container for all your methods to run. Storage . The storage will be empty. The name of the storage will be set as a default. Here you can see default settings of new storage. History . This is a table that contains all at any time scheduled runs of a project's methods. For now, it's empty. The picture below illustrated how the table looks with an existing run's history. The History GUI repeats the Run space. For more details see 11. Manage Runs . How to add metadata in the project, see here .","title":"Create a project"},{"location":"manual/Appendix_C/Appendix_C._Working_with_autoscaled_cluster_runs/","text":"Working with autoscaled cluster runs Overview Limitations How it works Finding expired jobs Launching workers Determing worker instance types Killing excessive workers Preventing deadlocks Configurations Examples Homogeneous cluster Hybrid cluster Overview Cloud Pipeline has support for launching runs using a single machine or a cluster of machines. Moreover Cloud Pipeline allows to launch so-called autoscaled clusters which are basically clusters of machines with dynamic size. It means that during the run execution additional worker machines can be attached to the cluster as well as removed from it. By the way an autoscaled cluster can have so-called persistent workers which cannot be affected by the autoscaler. Limitations Currently only the SGE-based runs can be used efficiently with an autoscaled cluster capability. And only the CPU requirements of the SGE jobs are considered while calculating cluster unsatisfied resources. How it works The cluster autoscaling tries to be intuitive and is pretty straightforward in most cases. Nevertheless it can get cumbersome with all the allowed customization parameters. The overall autoscaling approach is briefly described below. Please notice that all the described steps are executed repeatedly for all the cluster lifetime depending on the current situation. Finding expired jobs Autoscaler is a background process which constantly watches the SGE queues in order to find expired jobs . Basically expired jobs are the jobs that wait in any queue for more than some predefined time. Launching workers Once expired jobs are found the autoscaler tries to launch an additional worker. The only case additional worker won't be launched is if all allowed additional workers are already set up. Determing worker instance types Additional worker instance types can vary from only a master instance type up to some instance types family. Therefore there are homogeneous clusters that launches only the master instance type machines and hybrid clusters which launches instance types from some instance type family. By default all autoscaled clusters are homogeneous. If an autoscaled cluster is hybrid and it is launching an additional worker then its instance type will be resolved based on the amount of unsatisfied CPU requirements of all pending jobs. The autoscaler will try to launch the smallest allowed instance from a specific instance type family that can process all the pending jobs simultaneously. For instance if there are two pending jobs with the CPU requirements of 4 and 8 then the autoscaler will try to launch the instance which has at least 12 CPUs. Killing excessive workers Once required additional workers are set up the cluster gets bigger and more jobs can be executed simultaneously. At some point while jobs finish their execution some additional workers may become excessive. In this case the autoscaler will check if all the queues were empty for at least some predefined time and try to remove excessive additional workers from the cluster. Preventing deadlocks In some specific cases autoscaled clusters may enter the deadlock situation. In terms of autoscaled clusters a deadlock is a situation when submitted jobs cannot be executed in the best allowed cluster configuration. For example if a hybrid autoscaled cluster which can have at most a machine with 64 CPUs is used to submit a job with a requirement of 100 CPUs then the job won't be ever executed and will stuck in queue forever. Fortunately the autoscaler can detect such deadlocks and prevent them simply killing jobs that cannot be executed anyway. It most likely will fail the run execution but it is reasonable since the cluster usage is not properly configured in the run. Besides an actual deadlock situation where are some similar cases when the cluster has reached its full capacity but some of the jobs are still pending. It is not the deadlock situation since the jobs can be still executed but in the different cluster configuration. The autoscaler will detect such situations and replace weak addition workers with a better ones. Configurations System parameter Description CP_CAP_AUTOSCALE_HYBRID Enables hybrid cluster mode. It means that additional worker type can vary within either master instance type family or CP_CAP_AUTOSCALE_HYBRID_FAMILY if specified. CP_CAP_AUTOSCALE_HYBRID_FAMILY Hybrid cluster additional worker instance type family. For example c5 or r5 instance families can be used for AWS. CP_CAP_AUTOSCALE_HYBRID_MAX_CORE_PER_NODE The maximum number of cores that hybrid cluster additional worker instances can have. CP_CAP_AUTOSCALE_VERBOSE Enables verbose logging. CP_CAP_AUTOSCALE_PRICE_TYPE Cluster additional worker instance price type. Defaults to master instance price type. CP_CAP_SGE_MASTER_CORES The number of cores that master run can use for job submissions. If set to 0 then no jobs will be executed on the master. Defaults to all the master instance cores. CP_CAP_SGE_WORKER_FREE_CORES The number of cores that all worker and master runs have to reserve from job submissions. Defaults to 0 which means that all instance cores will be used for job submissions. Examples Homogeneous cluster In this example we will see how an autoscaled cluster behaves on different conditions. Let's say we have launch an autoscaled cluster of m5.large instances (2 CPUs per instance) with no persistent workers which can scale up to 2 workers. You can find information on how to do that in the corresponding page . On the Runs page we can see that the launched run doesn't have any workers. On the Run logs page click the SSH button when it will become available. In the opened terminal submit 10 single-core jobs to SGE queue using the following command qsub -b y -t 1:10 sleep 10m . Check that the jobs have been successfully submitted using qstat command: Within several minutes an additional worker will be automatically added to out cluster: Once the additional worker is fully initialized we can see that it takes several jobs from the queue: A few moments later if there are still pending jobs in the queue a second additional worker will be created: And once it is initialized the worker will grab remaining jobs from the queue as well: As long as the most of the jobs are finishing and the workers become excessive they will be removed from the cluster: At first one of the workers becomes stopped: And then the last one becomes stopped too: From this point the cluster can scale up and down again and again depending on the workload. Hybrid cluster In this example we will see how a hybrid cluster behaves on different workloads. To start a hybrid cluster you have to configure a regular autoscaling cluster as described here . Only the additional system parameter CP_CAP_AUTOSCALE_HYBRID have to be specified. Open the Advanced tab on the launch page and click Add system parameter button. Then type down HYBRID into the search field of the opened popup and select CP_CAP_AUTOSCALE_HYBRID system parameter. Once the parameter is selected click OK (1) button. Configure an autoscaled cluster with the same configuration as in the example above and launch the run. On the Runs page click on the launched run. Wait for the SSH button to appear and click on it. In the opened terminal submit 10 single-core jobs to SGE queue using the following command qsub -b y -t 1:10 sleep 10m . Check that the jobs have been successfully submitted using qstat command: If you go back to the run page and wait for a few minutes then you will see that an additional worker is launched. Click on the additional worker run link to find out what instance type it uses. The additional worker instance type in this case is m5.2xlarge which has 8 CPUs and 32GB of RAM. Please notice that the instance type selecting mechanism is highly situational and the resulting instance type can be different between launches. Once the additional worker is initialized it will grab all the remaining jobs from the queue. And after about 10 minutes an additional worker will be scaled down since there are no pending jobs anymore. From this point the cluster can scale up and down again and again depending on the workload.","title":"Appendix C. Working with autoscaled cluster runs"},{"location":"manual/Appendix_C/Appendix_C._Working_with_autoscaled_cluster_runs/#working-with-autoscaled-cluster-runs","text":"Overview Limitations How it works Finding expired jobs Launching workers Determing worker instance types Killing excessive workers Preventing deadlocks Configurations Examples Homogeneous cluster Hybrid cluster","title":"Working with autoscaled cluster runs"},{"location":"manual/Appendix_C/Appendix_C._Working_with_autoscaled_cluster_runs/#overview","text":"Cloud Pipeline has support for launching runs using a single machine or a cluster of machines. Moreover Cloud Pipeline allows to launch so-called autoscaled clusters which are basically clusters of machines with dynamic size. It means that during the run execution additional worker machines can be attached to the cluster as well as removed from it. By the way an autoscaled cluster can have so-called persistent workers which cannot be affected by the autoscaler.","title":"Overview"},{"location":"manual/Appendix_C/Appendix_C._Working_with_autoscaled_cluster_runs/#limitations","text":"Currently only the SGE-based runs can be used efficiently with an autoscaled cluster capability. And only the CPU requirements of the SGE jobs are considered while calculating cluster unsatisfied resources.","title":"Limitations"},{"location":"manual/Appendix_C/Appendix_C._Working_with_autoscaled_cluster_runs/#how-it-works","text":"The cluster autoscaling tries to be intuitive and is pretty straightforward in most cases. Nevertheless it can get cumbersome with all the allowed customization parameters. The overall autoscaling approach is briefly described below. Please notice that all the described steps are executed repeatedly for all the cluster lifetime depending on the current situation.","title":"How it works"},{"location":"manual/Appendix_C/Appendix_C._Working_with_autoscaled_cluster_runs/#finding-expired-jobs","text":"Autoscaler is a background process which constantly watches the SGE queues in order to find expired jobs . Basically expired jobs are the jobs that wait in any queue for more than some predefined time.","title":"Finding expired jobs"},{"location":"manual/Appendix_C/Appendix_C._Working_with_autoscaled_cluster_runs/#launching-workers","text":"Once expired jobs are found the autoscaler tries to launch an additional worker. The only case additional worker won't be launched is if all allowed additional workers are already set up.","title":"Launching workers"},{"location":"manual/Appendix_C/Appendix_C._Working_with_autoscaled_cluster_runs/#determing-worker-instance-types","text":"Additional worker instance types can vary from only a master instance type up to some instance types family. Therefore there are homogeneous clusters that launches only the master instance type machines and hybrid clusters which launches instance types from some instance type family. By default all autoscaled clusters are homogeneous. If an autoscaled cluster is hybrid and it is launching an additional worker then its instance type will be resolved based on the amount of unsatisfied CPU requirements of all pending jobs. The autoscaler will try to launch the smallest allowed instance from a specific instance type family that can process all the pending jobs simultaneously. For instance if there are two pending jobs with the CPU requirements of 4 and 8 then the autoscaler will try to launch the instance which has at least 12 CPUs.","title":"Determing worker instance types"},{"location":"manual/Appendix_C/Appendix_C._Working_with_autoscaled_cluster_runs/#killing-excessive-workers","text":"Once required additional workers are set up the cluster gets bigger and more jobs can be executed simultaneously. At some point while jobs finish their execution some additional workers may become excessive. In this case the autoscaler will check if all the queues were empty for at least some predefined time and try to remove excessive additional workers from the cluster.","title":"Killing excessive workers"},{"location":"manual/Appendix_C/Appendix_C._Working_with_autoscaled_cluster_runs/#preventing-deadlocks","text":"In some specific cases autoscaled clusters may enter the deadlock situation. In terms of autoscaled clusters a deadlock is a situation when submitted jobs cannot be executed in the best allowed cluster configuration. For example if a hybrid autoscaled cluster which can have at most a machine with 64 CPUs is used to submit a job with a requirement of 100 CPUs then the job won't be ever executed and will stuck in queue forever. Fortunately the autoscaler can detect such deadlocks and prevent them simply killing jobs that cannot be executed anyway. It most likely will fail the run execution but it is reasonable since the cluster usage is not properly configured in the run. Besides an actual deadlock situation where are some similar cases when the cluster has reached its full capacity but some of the jobs are still pending. It is not the deadlock situation since the jobs can be still executed but in the different cluster configuration. The autoscaler will detect such situations and replace weak addition workers with a better ones.","title":"Preventing deadlocks"},{"location":"manual/Appendix_C/Appendix_C._Working_with_autoscaled_cluster_runs/#configurations","text":"System parameter Description CP_CAP_AUTOSCALE_HYBRID Enables hybrid cluster mode. It means that additional worker type can vary within either master instance type family or CP_CAP_AUTOSCALE_HYBRID_FAMILY if specified. CP_CAP_AUTOSCALE_HYBRID_FAMILY Hybrid cluster additional worker instance type family. For example c5 or r5 instance families can be used for AWS. CP_CAP_AUTOSCALE_HYBRID_MAX_CORE_PER_NODE The maximum number of cores that hybrid cluster additional worker instances can have. CP_CAP_AUTOSCALE_VERBOSE Enables verbose logging. CP_CAP_AUTOSCALE_PRICE_TYPE Cluster additional worker instance price type. Defaults to master instance price type. CP_CAP_SGE_MASTER_CORES The number of cores that master run can use for job submissions. If set to 0 then no jobs will be executed on the master. Defaults to all the master instance cores. CP_CAP_SGE_WORKER_FREE_CORES The number of cores that all worker and master runs have to reserve from job submissions. Defaults to 0 which means that all instance cores will be used for job submissions.","title":"Configurations"},{"location":"manual/Appendix_C/Appendix_C._Working_with_autoscaled_cluster_runs/#examples","text":"","title":"Examples"},{"location":"manual/Appendix_C/Appendix_C._Working_with_autoscaled_cluster_runs/#homogeneous-cluster","text":"In this example we will see how an autoscaled cluster behaves on different conditions. Let's say we have launch an autoscaled cluster of m5.large instances (2 CPUs per instance) with no persistent workers which can scale up to 2 workers. You can find information on how to do that in the corresponding page . On the Runs page we can see that the launched run doesn't have any workers. On the Run logs page click the SSH button when it will become available. In the opened terminal submit 10 single-core jobs to SGE queue using the following command qsub -b y -t 1:10 sleep 10m . Check that the jobs have been successfully submitted using qstat command: Within several minutes an additional worker will be automatically added to out cluster: Once the additional worker is fully initialized we can see that it takes several jobs from the queue: A few moments later if there are still pending jobs in the queue a second additional worker will be created: And once it is initialized the worker will grab remaining jobs from the queue as well: As long as the most of the jobs are finishing and the workers become excessive they will be removed from the cluster: At first one of the workers becomes stopped: And then the last one becomes stopped too: From this point the cluster can scale up and down again and again depending on the workload.","title":"Homogeneous cluster"},{"location":"manual/Appendix_C/Appendix_C._Working_with_autoscaled_cluster_runs/#hybrid-cluster","text":"In this example we will see how a hybrid cluster behaves on different workloads. To start a hybrid cluster you have to configure a regular autoscaling cluster as described here . Only the additional system parameter CP_CAP_AUTOSCALE_HYBRID have to be specified. Open the Advanced tab on the launch page and click Add system parameter button. Then type down HYBRID into the search field of the opened popup and select CP_CAP_AUTOSCALE_HYBRID system parameter. Once the parameter is selected click OK (1) button. Configure an autoscaled cluster with the same configuration as in the example above and launch the run. On the Runs page click on the launched run. Wait for the SSH button to appear and click on it. In the opened terminal submit 10 single-core jobs to SGE queue using the following command qsub -b y -t 1:10 sleep 10m . Check that the jobs have been successfully submitted using qstat command: If you go back to the run page and wait for a few minutes then you will see that an additional worker is launched. Click on the additional worker run link to find out what instance type it uses. The additional worker instance type in this case is m5.2xlarge which has 8 CPUs and 32GB of RAM. Please notice that the instance type selecting mechanism is highly situational and the resulting instance type can be different between launches. Once the additional worker is initialized it will grab all the remaining jobs from the queue. And after about 10 minutes an additional worker will be scaled down since there are no pending jobs anymore. From this point the cluster can scale up and down again and again depending on the workload.","title":"Hybrid cluster"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/","text":"Costs management rationale and concepts Restricting the size of the compute cluster Compute nodes size Number of compute nodes Spot/Preemptible compute instances Instance PAUSE/RESUME IDLE instances Scheduled instances PAUSE/RESUME Spending quotas Billing reports General costs report User report Billing center/group report Resources reports Storages report File storages report Object storages report Compute instances report CPU/GPU division Export reports Report aggregation level according to the permissions Any job, run in the Cloud Pipeline, or any file placed into the Cloud Storage - cost money. These costs are incurred by the underlying provider: AWS/GCP/Azure/etc. At a scale, when hundreds of the platform users submit the jobs and utilize the storage capacity, the cloud bill may grow to a quite high level. Even more, some of the users may not be aware of the underlying billing and keep the instances up and running for months, but not performing any productive work. To address these issues, Cloud Pipeline provides a number of features, that allow to reduce and control the costs. This machinery can be split into separate sections: Features that allow to minimize the costs , e.g. by limiting the size of the compute nodes or automatically stopping IDLE jobs Reporting features , that make users aware of the workload costs, e.g. by notifying the users on the IDLE jobs or providing interactive spending dashboard This document lists the concepts and corresponding features (either Ready and WIP as well). Restricting the size of the compute cluster From the overall Cloud usage experience - the largest bills are generated by the compute resources, not the storages. And the key driver of the compute cost is the size/shape of the compute nodes used. Compute nodes size Each of the Cloud Providers offers lots of different compute instances families and types and most likely the users won't need that variety. Users may even choose huge GPU-enabled nodes by mistake, which are the most expensive. Cloud Pipeline allows to restrict which sizes are available to the users for a specific platform deployment, thus reducing a chance of spending money unused resources of a huge node. These restrictions can be applied to the overall platform and than fine-tuned for a specific users' group, specific user or a docker image. Number of compute nodes While we can restrict the size of each single compute node - it is still possible to spin up an uncontrolled number of smaller instances, which still costs a lot. Cloud Pipeline offers a way to restrict the number of simultaneously running Cloud instances in the following manner: Administrator may configure the overall number of compute nodes, that may be created for this particular platform deployment. This restriction will be in effect across all the users/groups/tools. E.g. if the administrators sets this parameter to 50 nodes and we already have 50 nodes running jobs - the 51st node won't be created. The corresponding job will sit in the queue until the previous runs finish and free the space for the new instance. Users may also spin-up on-demand clusters , i.e. a single job run which require more than a single compute node (e.g. a molecular dynamics job, which needs a 200 of CPU cores interconnected with MPI). In this case - administrator may limit the size of such on-demand clusters . E.g. limiting this parameter to 5 will allow user to launch several jobs in a cluster mode, but each cluster will be limited to max 5 hosts. Spot/Preemptible compute instances One of the very first cost reduction options to consider is the usage of Spot/Preemptible compute nodes. The behavior and savings are a bit different across the underlying Cloud providers, but in general all of them follow the strategy: allow to use the compute resources for a limited time at a greatly reduced costs. It's quite hard to predict the savings in general, but typically this will be ~twice cheaper than general on-demand instance type. For more details on the Spot/Preemptible compute instances details, please review the corresponding provider's documentation: AWS Spot instances GCP Preemptible VMs Azure Preemptible VMs While being quite cost-effective, such type of compute instances are not reliable for the long-running or stateful jobs. Cloud Pipeline encourages users to leverage the Spots/Preemptibles for: The Batch jobs, e.g. NGS pipelines which can be easily restarted Testing/Proofing tasks, when some script shall be debugged or a new package tested For the interactive tasks, e.g. Jupyter notebooks , which require online access from the users - such type of instance does not fit well. Another limitation of the Spots/Preemptibles is that such jobs cannot be Paused (i.e. stop consuming money, but keep the job's environment). Such kind of compute nodes can be fully terminated only. From the Cloud Pipeline's point of view this option is considered a Price Type and can be enabled at different levels: Globally, at a platform level (if the Spot/Preemptible can be used by any job in the platform) User group or a specific user levels (if can be used by the specific users) Pipeline level (if a particular pipeline can tolerate Spots/Preemptibles restarts) Docker image/version (if a particular image can be launched using a reduced cost instances) Instance PAUSE/RESUME From the existing usage scenarios the best compute costs reduction was observed, when the interactive tools were stopped while not used. The typical use case here is: User launches some IDE (e.g. RStudio/Jupyter) Works during the day Keeps the instance running over the night or weekends This introduces really high spendings, but without any actual outcome, as the instance is doing nothing. To address this, Cloud Pipeline allows to PAUSE and RESUME any instance/job, which is created using on-demand price type. While the instance is paused - compute is not charged, but the job's environment and filesystem is persisted. Once required - the instance can be resumed . Under the hood, the \"real\" compute instances are stopped/deallocated and than restarted. Cloud Pipeline takes care of the software state persistance and restore. In general, this feature is available via the Web GUI and API and the users are in charge of performing this pause/resume operation. API allows to automate this procedure (e.g. based on the schedule or resources usage) and the subsequent sections describe this approach. See Manage runs lifecycles for more details. IDLE instances This section extends the \"plain\" PAUSE/RESUME by managing the IDLE instances. Besides the describe above night-time/weekends cases, here we also consider under-utilization of the compute resource. E.g. if the user selects 96-cored instance (maybe by mistake), but runs a single-threaded application. In this case lots of CPU resources are just wasted. Cloud Pipeline mixes together the PAUSE/RESUME and instances workload monitoring and offers a set of policies, which can be applied to the compute instances to take care of such IDLE instances. These policies can be used in the following manner: Platform administrators can define thresholds for the hardware utilization (CPU/GPU usage) and overall run duration. If some job is considered as IDLE (the hardware utilization is below the threshold for the configured period of time) \u2013 a number of actions can be performed (a single or a mixture of them): Job is marked as IDLE in the GUI Email Notification is sent (to the Adminstrators and the Owner of the instance) to make user aware of the event Job can be automatically paused (if it\u2019s price type and cluster mode are compatible with the PAUSE operation) Job can be terminated Scheduled instances PAUSE/RESUME For certain use cases (e.g. when then user leverages Cloud Pipeline as a development/research environment) users launch runs and keep them running all the time, including weekends and holidays. As shown in the examples above. One can use the IDLE policy to PAUSE such jobs, but the IDLE status is set only when the threshold is exceeded. This time (while the platform will decide to treat a job as IDLE ) also costs some money. So to manage the jobs that shall be stopped for the non-working hours - a PAUSE/RESUME schedule is used. User (who have permissions to pause/resume a run) is able to set a schedule for an active run or a run being launched Schedule is defined as a list of rules (user shall be able to specify any number of them): Action: PAUSE or RESUME Recurrence: Daily: every N days, time Weekly: every N weeks, weekday, time User is able to create/view/modify/delete schedule rules anytime run is active (i.e. running or paused) This is applied only to the \"Pauseable\" runs (i.e. On-demand/Non-cluster) Spending quotas While the described options are mostly focused on the soft cost reduction (e.g. help the user to decide on the instance type), the platform shall be also capable of enforcing certain policies if the soft restrictions didn't work. This kind of restriction is controlled by the spending quotas . This functionality allows to apply policies to the platform's entities: User - quotas can be applied to a specific user Users group - group of users can be restricted separately as well Billing/Cost center - this is a different dimension of the users grouping. Typically this is a meta-group , which is not used to apply security permissions. Such groups describe, e.g. the departments, which have separate budget and can manage it Global - administrator can define what is the overall budget for the platform Each of those user groupings can be managed by the platform administrator or an authorized manager (who is assigned a corresponding platform role). For each of the quotas configured \u2013 there is an option to specify the thresholds (e.g. 50% / 75% / 100%), when a specific action shall be performed by the platform: \"Notify\" (default) - this action will only notify corresponding users that a limited is exceeded. The access to the platform shall not be restricted \"Read-only & keep jobs\" - corresponding users that a limited is exceeded and then will have the \"read-only\" access to the platform, they won't be able to launch new jobs. All active runs will be kept \"Read-only & stop jobs\" \u2013 same as previous, but all the active runs will be stopped \"Block\" - corresponding users will be blocked and won\u2019t have any access to the platform. After a time period for which the limit is set, the access to the platform shall be restored to users according to their permissions. Billing reports To make the users aware of the current bills and quota policy attached - platform logs the information on the compute and storage costs. This reporting feature is available in two flavors: When a compute job is launched - user is notified on the hourly cost of the chosen hardware configuration Compute and storage costs are aggregated into the ElasticSearch index on a daily basis and can be queried to build the historical reports General costs report The latter one offers graphical/tabular visualization options to get the insights on a current or a previous period bills: By these forms users can view the whole system spendings, or spendings divided by the specific resources. Presented metrics (resources): costs of launching compute instances, used for tools/pipelines runs. There could be: CPU instances GPU instances costs of storing user data in storages. There could be costs of storing: in Object storages in File storages info about auxiliary costs isn't supported yet All costs are aggregated and displayed for the specific period (by default - the current calendar month). Selected specific period, for which the costs are being calculated and displayed, is called \" current \". Also, for comparison, the costs for the analogical previous period are displayed in diagrams/charts (where this data is available). The user can select the desired ( current ) period to view the costs incurred: from one of predefined periods. For each of them there will be a specific \" previous \" period: Month . If the month is selected as a current period - costs will be shown for the period from the first day of the month till previous calendar day (if the month hasn't ended yet) or the whole month if it has already ended. The \" previous \" period will be the same days interval but in the previous calendar month Quarter . If the quarter is selected as a current period - costs will be shown for the period from the first day of the quarter till previous calendar day (if the quarter hasn't ended yet) or the whole quarter if it has already ended. The \" previous \" period will be the same interval in a quarter but in the previous calendar year Year . If the year is selected as a current period - costs will be shown for the period from the first day of the year till previous calendar day (if the year hasn't ended yet) or the whole year if it has already ended. The \" previous \" period will be the same interval in the previous calendar year custom period. That period is configured by the user manually and can have any duration. The \" previous \" period isn't displayed in this case By default, the \"Billing Visualization\" form looks like on the picture above. It contains: General report table with the following data: current and previous periods, for which the costs are calculated summary spendings according to selected configurations in the toolbar for the current and previous periods the difference between the costs of current period and the previous one in percentage with a certain mark to determine whether spendings grow or reduce The main summary costs diagram. On this diagram, the user can see summary spendings over the current and the previous time periods according to selected configurations in the toolbar. This data could be displayed with the accumulation (as line chart ) or as fact (as bar chart with actual spending values in each time point of the period without accumulation) if the selected \" current \" period coincides with the current corresponding calendar period - there a slice line is displayed on the current calendar date: on its intersection with the \" current \" chart line, the amount of money which was spent from the beginning of the period to the current date is displayed and on its intersection with the \" previous \" chart line - the amount of money which was spent for the same previous time period is displayed the user can hover any point of the \" current \"/\" previous \" line of the diagram - the summary spendings at the corresponding datetime will appear in the tooltip the aggregation value of timeline division for selected periods less than 4 months is 1 day, in other cases - 1 month Note : costs of the current calendar day is being aggregated in the following day, so they aren't displayed in diagrams With accumulation: And actual values (without accumulation): The spendings bar chart with the Resources division. On this chart, the user can view the division of summary spendings in a selected time period over the resource groups - Storages (with division into Object and File storages) and Compute Instances (with division into CPU and GPU ). Also, for each resource, the summary spendings for the same previous period are presented as well: The spendings bar chart with the Billing centers division. On this chart, the user can view the division of summary spendings (all resource types) in a selected time period over the billing centers (the system displays only top N most costly billing centers). For each displayed center/group, the summary spendings for the same previous period is presented as well: Data can be shown as a bar chart (default, see the picture above) or as a pie chart The report toolbar with the following controls: calendar control - to select another \" current \" period that doesn't coincide with the current calendar period (for predefined periods) or manually select desired period duration (for custom period) period selector - to select a current time period for the report - from one of predefined periods or the manual custom period filter to restrict shown spendings to a specific billing center (user group) or a user. By default, all available centers/users are selected (no restrictions) filter to restrict shown spendings to a specific Cloud region or Cloud provider. By default, all available regions/providers are selected (no restrictions) options menu. Includes items: to configure discounts for the current Cloud Pipeline deployment (if the are applicable). Discounts can be configured separately - for compute instances and storages to restore origin layout. As all forms at the billing page can be displaced or resized, this item may be helpful to restore the origin forms view to show/hide spendings quotas on the billing costs charts to export raw spendings data of a displayed report to the local workstation - in *.csv or image format The billing report form described above is general. To get info/charts of summary spendings in any desired period (from available) you shall select the corresponding period via the Period selector and Calendar control . To get info/charts of summary spendings for specific Cloud Provider(s) ( for multi-Provider deployments ) or for specific Region(s) of the Cloud Provider you shall apply the corresponding filter, e.g.: To get info/charts of summary spendings in the selected period only for the specific resource group you can click that resource in the Resources chart or use the corresponding item of the menu in the left side of the page (for more details see sections below ). User report To get info/charts of summary spendings in the selected period only for the specific user you can select the desired user from the dropdown list in the main toolbar. In this case: the costs data will be calculated and displayed only for the selected user (in the general report table, in the main diagram and in the resources chart) the spendings bar chart with the Billing centers division will disappear, e.g.: Filter supports multiselect, i.e. several users can be selected simultaneously. Shown spendings will be summarized for them. Note : although field is single for the filter by users and billing centers (groups), you can not select some user(s) and some group(s) in the same moment - filter only by one metric may be applied. Billing center/group report To get info/charts of summary spendings in the selected period only for the specific Billing center you can: select the desired one from the dropdown list in the main toolbar, e.g.: OR click the Billing Center's bar in the Billing centers division chart, e.g.: In this case: the costs data will be calculated and displayed only for the selected Billing center (summary for all its users - in the general report table, in the main diagram and in the resources chart) the bar chart with the top N of users' spendings in the selected period (among all users in that Billing center) will appear the table with short info about summary spendings of each user of that Billing center in the selected period (also info contains summary duration and count of the runs launched by the user, used storages volume) will appear, e.g.: You can click any user from the bar chart or from the details table with the top N of users' spendings - the corresponding user report will be opened. Notes : filter by billing centers supports multiselect, i.e. several billing centers can be selected simultaneously. Shown spendings will be summarized for them, but the view of the page will be the same as general report , i.e. the bar chart and table with the top N of users' spendings will not be displayed. Also although field is single for the filter by users and billing centers (groups), you can not select some user(s) and some group(s) in the same moment - filter only by one metric may be applied. Resources reports If you wish - you may get info/charts of summary spendings not for all resources but for the specific resource group. Also, for these charts you may select the desired time period and specific user/Billing center (group) to view costs in the way analogical as described above. Storages report To get info/charts of summary spendings in the selected period only for the Storages resource group (costs of the storing data in storages) you can click the corresponding item of the menu in the left side of the page: In this case, the summary costs for all used storages during the selected period by selected users/Billing center will be calculated and displayed (both types - Object/File storages). In the appeared page, you can see: General report table with summary costs for all used storages during the selected and previous ( if it's available ) periods The summary Storages spendings diagram over the current and the previous time periods according to selected configurations in the toolbar. This data could be displayed with the accumulation (as line chart ) or as fact (as bar chart with actual spending values in each time point of the period without accumulation) The spendings bar chart with top N most costly storages used during the selected period compared with the previous one ( if it's available ) The detailed spendings table under the chart with the full list of storages used during the selected period by selected user/Billing center Example: Spendings bar chart with top N most costly storages can show the information as storages costs - in $ (default, see the picture above) or as average storages volumes - in Gb : Switching this view mode does not affect the summary chart or details spendings table. The detailed spendings table contains the following info for each storage: Storage - storage name Owner - storage OWNER username Billing Center - billing center (group) of the storage OWNER , if it's defined Type - storage type ( S3 / GS / NFS / LustreFS /etc.) Cost - total spendings for the storage in the selected period Avg.Vol. (GB) - the average storage volume, in GB. It means that the exact volumes for each day of the selected report period were brought and then the average value was calculated Cur.Vol. (GB) - the current storage volume, in GB. This volume is a real volume for a current moment/last day of the selected period Region - Cloud Region Provider - Cloud provider Created date - storage creation date In the Storages report, there is no division by storage type ( Object / File ). All data is calculated and displayed summary for both types. If you want to view costs only for storages with the specific type - you can select the corresponding one in the menu at the left side of the page (see sections below for details). File storages report File storages report has fully the same view (set of forms) as common storages report , but with information for file storages only e.g.: Object storages report Object storages report has the view (set of forms) distinct from common storages report , e.g.: As object storages supports versioning and archiving data into different archive tiers, the Object storages report supports the displaying of the corresponding related information. In this section, the following terms will be used: current version - last (current) version of the versioning object storage or current version of the non-versioning object storage old versions - all non-last (previous) versions of the versioning object storage Report includes: General report table with summary costs for all used object storages during the selected and previous ( if it's available ) periods Object storages usages - the summary spendings diagram over the current and the previous time periods according to selected configurations in the toolbar. This data can be displayed: with the accumulation (as line chart ). Solid lines show summary spendings on the data usage for current version of object storages (for current and previous periods). Additionally, for each solid line, there is a dashed line of the same color - that line shows summary spendings on the data usage for all old versions of object storages Spendings on different versions are shown in a tooltip as well: the summary value is shown and separately - spendings on current version and on old versions , e.g.: or as fact (as bar chart with actual spending values in each time point of the period without accumulation). Each bar is displayed as a stack of current version spendings / old versions spendings in the specific time point. Current version spendings are shown with solid filling, old versions spendings are shown without filling. For current and previous periods charts are shown in different colors. Object storages - the bar chart with top N most costly object storages used during the selected period compared with the previous one ( if it's available ). Each bar is displayed as a stack of current version spendings / old versions spendings of the specific object storage. Current version spendings are shown with solid filling, old versions spendings are shown without filling. For previous period, charts are shown in different colors and only as dashed lines. If data in the storage is storing in different tiers (archive types), this can be viewed in a tooltip - there will be a division of spendings by the used tiers. Versions division for each tier will be shown as well, e.g.: Object storages layers - the bar chart with cost division to different tiers (archive types). This chart does not contain any information for previous period. Only layers used for data storing in the current period according to selected filters are shown. Up to 4 layers can be here: Standard , Glacier , Glacier IR , Deep Archive . If any layer has old versions - this will be shown as a stack of current version spendings / old versions spendings for that layer. Current version spendings are shown with solid filling, old versions spendings are shown without filling, e.g.: Object storage layers chart can show the information as storages usage costs - in $ (default, see the picture above) or as average storages volumes - in Gb : Note : switching this view mode also affects the chart with top N most costly object storages: Details of the versions division can be viewed in a tooltip of each layer bar, e.g.: The detailed spendings table under the chart with the full list of object storages used during the selected period by selected user/Billing center. This table contains the following info for each object storage: Storage - storage name Owner - storage OWNER username Billing Center - billing center (group) of the storage OWNER , if it's defined Type - object storage type ( S3 / GS / etc.) Cost - spendings for the storage in the selected period. There are two values are shown in format total spendings for all versions / spendings for old versions only Avg.Vol. (GB) - the average storage volume, in GB. It means that the exact volumes for each day of the selected report period were brought and then the average value was calculated. There are two values shown in the format average volume of all versions / average volume of old versions only Cur.Vol. (GB) - the current storage volume, in GB. This volume is a real volume for a current moment/last day of the selected period. There are two values shown in the format current volume of all versions / current volume of old versions only Region - Cloud Region Provider - Cloud provider Created date - storage creation date User can select one of the object storage layers - by click it on the corresponding chart. In this case, all charts and tables will be updated - only storages, that contain files in the selected layer type, will be shown in forms. Also, shown spendings/data volume will be related only to files in the selected layer, not for the whole storage(s) or other layers. For example, Glasier IR was selected in the Object storage layers chart for some user: To clear selection of the object storage layer - click it again or click the cross-button near the layer label in the top of the chart: Compute instances report To get info/charts of summary spendings in the selected period only for the Compute instances resource group (costs of the launching instances for running tools/pipelines) you can click the corresponding item of the menu in the left side of the page: In this case, the summary costs for all launched runs during the selected period by selected users/Billing center will be calculated and displayed - uniting launched tools/pipelines of both processing unit types (CPU/GPU): At the appeared page, you can see: General report table with summary costs for all runs launched in the selected and previous ( if it's available ) periods Compute instances runs - the summary spendings chart over the current and the previous time periods according to selected configurations in the toolbar. This data can be displayed: with the accumulation (as line chart ) or as fact (as bar chart with actual spending values in each time point of the period without accumulation) Cost details - the bar chart that displays the sharing of the cost of runs launched during the selected period into layers - Compute (cost of compute instances used in runs) and Disk (cost of EBS drives connected to runs during their performing): Instance types - the bar chart with top N most costly instance types launched in the selected period compared to the previous one ( if it's available ), e.g.: Under the chart, there is a detailed spendings table with the full list of launched instance types (not only top N ) during the selected period. This table contains the following info for each instance type: Instance - instance type name Usage (hours) - summary number of hours that instances of this type were run Runs count - summary number of this instance type's runs Cost - summary cost of this instance type's runs Compute cost - separately the compute cost as part of summary runs cost Disk cost - separately the disk (EBS drives) cost as part of summary runs cost Pipelines - the bar chart with top N most costly pipelines launched in the selected period compared to the previous one ( if it's available ), e.g.: Under the chart, there is a detailed spendings table with the full list of launched pipelines (not only top N ) during the selected period. This table contains the following info for each pipeline: Pipeline - pipeline name Owner - pipeline owner's username Usage (hours) - summary number of hours this pipeline was run Runs count - summary number of this pipeline's runs Cost - summary cost of this pipeline's runs Compute cost - separately the compute cost as part of summary runs cost of this pipeline Disk cost - separately the disk (EBS drives) cost as part of summary runs cost of this pipeline Tools - the bar chart with top N most costly tools launched in the selected period compared to the previous one ( if it's available ), e.g.: Under the chart, there is a detailed spendings table with the full list of launched tools (not only top N ) during the selected period. This table contains the following info for each tool: Tool - tool name Owner - tool owner's username Usage (hours) - summary number of hours this tool was run Runs count - summary number of this tool's runs Cost - summary cost of this tool's runs Compute cost - separately the compute cost as part of summary runs cost of this tool Disk cost - separately the disk (EBS drives) cost as part of summary runs cost of this tool Additional control that allows to change displaying of spendings bar charts ( Instance types , Pipelines , Tools ) with the following possible values: Cost ( default ) - at 3 above described bar charts, top N most costly objects (instance types, pipelines, tools) are displayed. Data Unit - currency Usage (hours) - at 3 above described bar charts, top N most involved objects (instance types, pipelines, tools) are displayed. Data Unit - usage hours Runs - at 3 above described bar charts, top N objects (instance types, pipelines, tools) with the largest runs number are displayed. Data Unit - runs count For example, the Instance types chart in case when \" Usage (hours) \" displaying type is selected: Please note, this control changes only the chart view, but the data in tables under charts is not changed - only column is being changed by which tables are sorted. User can select one of the runs cost layers ( Compute or Disk ) - by click it in the Cost details chart. In this case: summary runs cost chart will be updated (for both periods - current and previous ) - only summary spendings, that correspond to the selected layer ( Compute or Disk ), will be shown charts Instance types , Pipelines , Tools will be updated - only spendings, that correspond to the selected layer ( Compute or Disk ), will be shown data in tables under charts will not be changed, but the sorting column will be set the same as the selected layer For example, if the Compute layer of the runs cost is selected: To clear selection of the runs cost layer - click it again or click the cross-button near the layer label in the top of the Cost details chart. CPU/GPU division By default, in Compute instances report form there is no division by processing unit type (CPU/GPU). All data is calculated and displayed summary for both types. To view costs only for instances with the specific processing unit's type - select the corresponding one in the menu at the left side of the Billing page. E.g., for CPU only: Export billling reports User can export a shown billing report. Export is available from the Options menu in the right-upper corner of each report section. Export options can vary in different report sections. Possible formats - CSV (for datatables) and PNG (for images). General section From the General section, the following exports are available: CSV by Billing Centers - in this case, the general billing report will be split by billing centers (groups) CSV by Users - in this case, the general billing report will be split by users Image - in this case, export will be performed as image that contains pictures of charts as they were on the screen Storages section From the Storages section and its sub-sections, the following exports are available: CSV - in this case, the billing report will be split by storages. The view of the report is similar to the detailed spendings table Image - in this case, export will be performed as image that contains pictures of charts as they were on the screen (without detailed spendings table) Export of the Object storages reports is different from other storages' reports. This export additionally supports breaking of data by versions and archive tiers: if there are any spendings in the archive layer - for that layer, separate columns of Cost , Average Volume (GB) and Current Volume (GB) will be in the exported report for versioning storages, there are separate columns for the current version and old versions Example of the Object storages report export (please note, there is only a part of the report is shown): Compute instances From the Compute instances section and its sub-sections, the following exports are available: CSV - in this case, the billing report will contain 3 parts - for most costly instances types, pipelines and tools - as on the report page Image - in this case, export will be performed as image that contains pictures of charts as they were on the screen (without detailed spendings tables) Raw data - in this case, the export will contain the full raw list of runs that were used for the report shown on the screen Report aggregation level according to the permissions Available reports are calculated and displayed according to the user permissions: General users can have base access to the Billing reports that allows to view some information - about users' own spendings: this behavior is enabled by the system preference billing.reports.enabled . If this preference is set, all general users can access personal billing information - runs/storages where the user is an owner. Also general users can use filters, change chart types, make reports export. the following restrictions are set for general users when \"base\" billing access is enabled: all showing charts are being displayed only spendings of the current user there isn't an ability to configure discounts, the button \" Configure discounts \" is disabled \"Billing centers (TOP 10)\" chart isn't displayed Billing center (group) leader can have extended access to the Billing reports : a special role exists for that - ROLE_BILLING_MANAGER . The Billing reports Dashboard becomes available for user in full, if this role is assigned to the user. In this case, all possible filters, charts and their types, discounts configuration, export feature and etc. become available too. So, users who are granted this role are able to view the whole Billing reports info of the platform (as if they were admins). Platform admin can view all available reports Note : there is a special system preference billing.reports.enabled.admins . It allows to configure Billing reports visibility for admins and billing managers. Default value is true .","title":"Appendix D. Costs management"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/#costs-management-rationale-and-concepts","text":"Restricting the size of the compute cluster Compute nodes size Number of compute nodes Spot/Preemptible compute instances Instance PAUSE/RESUME IDLE instances Scheduled instances PAUSE/RESUME Spending quotas Billing reports General costs report User report Billing center/group report Resources reports Storages report File storages report Object storages report Compute instances report CPU/GPU division Export reports Report aggregation level according to the permissions Any job, run in the Cloud Pipeline, or any file placed into the Cloud Storage - cost money. These costs are incurred by the underlying provider: AWS/GCP/Azure/etc. At a scale, when hundreds of the platform users submit the jobs and utilize the storage capacity, the cloud bill may grow to a quite high level. Even more, some of the users may not be aware of the underlying billing and keep the instances up and running for months, but not performing any productive work. To address these issues, Cloud Pipeline provides a number of features, that allow to reduce and control the costs. This machinery can be split into separate sections: Features that allow to minimize the costs , e.g. by limiting the size of the compute nodes or automatically stopping IDLE jobs Reporting features , that make users aware of the workload costs, e.g. by notifying the users on the IDLE jobs or providing interactive spending dashboard This document lists the concepts and corresponding features (either Ready and WIP as well).","title":"Costs management rationale and concepts"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/#restricting-the-size-of-the-compute-cluster","text":"From the overall Cloud usage experience - the largest bills are generated by the compute resources, not the storages. And the key driver of the compute cost is the size/shape of the compute nodes used.","title":"Restricting the size of the compute cluster"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/#compute-nodes-size","text":"Each of the Cloud Providers offers lots of different compute instances families and types and most likely the users won't need that variety. Users may even choose huge GPU-enabled nodes by mistake, which are the most expensive. Cloud Pipeline allows to restrict which sizes are available to the users for a specific platform deployment, thus reducing a chance of spending money unused resources of a huge node. These restrictions can be applied to the overall platform and than fine-tuned for a specific users' group, specific user or a docker image.","title":"Compute nodes size"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/#number-of-compute-nodes","text":"While we can restrict the size of each single compute node - it is still possible to spin up an uncontrolled number of smaller instances, which still costs a lot. Cloud Pipeline offers a way to restrict the number of simultaneously running Cloud instances in the following manner: Administrator may configure the overall number of compute nodes, that may be created for this particular platform deployment. This restriction will be in effect across all the users/groups/tools. E.g. if the administrators sets this parameter to 50 nodes and we already have 50 nodes running jobs - the 51st node won't be created. The corresponding job will sit in the queue until the previous runs finish and free the space for the new instance. Users may also spin-up on-demand clusters , i.e. a single job run which require more than a single compute node (e.g. a molecular dynamics job, which needs a 200 of CPU cores interconnected with MPI). In this case - administrator may limit the size of such on-demand clusters . E.g. limiting this parameter to 5 will allow user to launch several jobs in a cluster mode, but each cluster will be limited to max 5 hosts.","title":"Number of compute nodes"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/#spotpreemptible-compute-instances","text":"One of the very first cost reduction options to consider is the usage of Spot/Preemptible compute nodes. The behavior and savings are a bit different across the underlying Cloud providers, but in general all of them follow the strategy: allow to use the compute resources for a limited time at a greatly reduced costs. It's quite hard to predict the savings in general, but typically this will be ~twice cheaper than general on-demand instance type. For more details on the Spot/Preemptible compute instances details, please review the corresponding provider's documentation: AWS Spot instances GCP Preemptible VMs Azure Preemptible VMs While being quite cost-effective, such type of compute instances are not reliable for the long-running or stateful jobs. Cloud Pipeline encourages users to leverage the Spots/Preemptibles for: The Batch jobs, e.g. NGS pipelines which can be easily restarted Testing/Proofing tasks, when some script shall be debugged or a new package tested For the interactive tasks, e.g. Jupyter notebooks , which require online access from the users - such type of instance does not fit well. Another limitation of the Spots/Preemptibles is that such jobs cannot be Paused (i.e. stop consuming money, but keep the job's environment). Such kind of compute nodes can be fully terminated only. From the Cloud Pipeline's point of view this option is considered a Price Type and can be enabled at different levels: Globally, at a platform level (if the Spot/Preemptible can be used by any job in the platform) User group or a specific user levels (if can be used by the specific users) Pipeline level (if a particular pipeline can tolerate Spots/Preemptibles restarts) Docker image/version (if a particular image can be launched using a reduced cost instances)","title":"Spot/Preemptible compute instances"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/#instance-pauseresume","text":"From the existing usage scenarios the best compute costs reduction was observed, when the interactive tools were stopped while not used. The typical use case here is: User launches some IDE (e.g. RStudio/Jupyter) Works during the day Keeps the instance running over the night or weekends This introduces really high spendings, but without any actual outcome, as the instance is doing nothing. To address this, Cloud Pipeline allows to PAUSE and RESUME any instance/job, which is created using on-demand price type. While the instance is paused - compute is not charged, but the job's environment and filesystem is persisted. Once required - the instance can be resumed . Under the hood, the \"real\" compute instances are stopped/deallocated and than restarted. Cloud Pipeline takes care of the software state persistance and restore. In general, this feature is available via the Web GUI and API and the users are in charge of performing this pause/resume operation. API allows to automate this procedure (e.g. based on the schedule or resources usage) and the subsequent sections describe this approach. See Manage runs lifecycles for more details.","title":"Instance PAUSE/RESUME"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/#idle-instances","text":"This section extends the \"plain\" PAUSE/RESUME by managing the IDLE instances. Besides the describe above night-time/weekends cases, here we also consider under-utilization of the compute resource. E.g. if the user selects 96-cored instance (maybe by mistake), but runs a single-threaded application. In this case lots of CPU resources are just wasted. Cloud Pipeline mixes together the PAUSE/RESUME and instances workload monitoring and offers a set of policies, which can be applied to the compute instances to take care of such IDLE instances. These policies can be used in the following manner: Platform administrators can define thresholds for the hardware utilization (CPU/GPU usage) and overall run duration. If some job is considered as IDLE (the hardware utilization is below the threshold for the configured period of time) \u2013 a number of actions can be performed (a single or a mixture of them): Job is marked as IDLE in the GUI Email Notification is sent (to the Adminstrators and the Owner of the instance) to make user aware of the event Job can be automatically paused (if it\u2019s price type and cluster mode are compatible with the PAUSE operation) Job can be terminated","title":"IDLE instances"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/#scheduled-instances-pauseresume","text":"For certain use cases (e.g. when then user leverages Cloud Pipeline as a development/research environment) users launch runs and keep them running all the time, including weekends and holidays. As shown in the examples above. One can use the IDLE policy to PAUSE such jobs, but the IDLE status is set only when the threshold is exceeded. This time (while the platform will decide to treat a job as IDLE ) also costs some money. So to manage the jobs that shall be stopped for the non-working hours - a PAUSE/RESUME schedule is used. User (who have permissions to pause/resume a run) is able to set a schedule for an active run or a run being launched Schedule is defined as a list of rules (user shall be able to specify any number of them): Action: PAUSE or RESUME Recurrence: Daily: every N days, time Weekly: every N weeks, weekday, time User is able to create/view/modify/delete schedule rules anytime run is active (i.e. running or paused) This is applied only to the \"Pauseable\" runs (i.e. On-demand/Non-cluster)","title":"Scheduled instances PAUSE/RESUME"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/#spending-quotas","text":"While the described options are mostly focused on the soft cost reduction (e.g. help the user to decide on the instance type), the platform shall be also capable of enforcing certain policies if the soft restrictions didn't work. This kind of restriction is controlled by the spending quotas . This functionality allows to apply policies to the platform's entities: User - quotas can be applied to a specific user Users group - group of users can be restricted separately as well Billing/Cost center - this is a different dimension of the users grouping. Typically this is a meta-group , which is not used to apply security permissions. Such groups describe, e.g. the departments, which have separate budget and can manage it Global - administrator can define what is the overall budget for the platform Each of those user groupings can be managed by the platform administrator or an authorized manager (who is assigned a corresponding platform role). For each of the quotas configured \u2013 there is an option to specify the thresholds (e.g. 50% / 75% / 100%), when a specific action shall be performed by the platform: \"Notify\" (default) - this action will only notify corresponding users that a limited is exceeded. The access to the platform shall not be restricted \"Read-only & keep jobs\" - corresponding users that a limited is exceeded and then will have the \"read-only\" access to the platform, they won't be able to launch new jobs. All active runs will be kept \"Read-only & stop jobs\" \u2013 same as previous, but all the active runs will be stopped \"Block\" - corresponding users will be blocked and won\u2019t have any access to the platform. After a time period for which the limit is set, the access to the platform shall be restored to users according to their permissions.","title":"Spending quotas"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/#billing-reports","text":"To make the users aware of the current bills and quota policy attached - platform logs the information on the compute and storage costs. This reporting feature is available in two flavors: When a compute job is launched - user is notified on the hourly cost of the chosen hardware configuration Compute and storage costs are aggregated into the ElasticSearch index on a daily basis and can be queried to build the historical reports","title":"Billing reports"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/#general-costs-report","text":"The latter one offers graphical/tabular visualization options to get the insights on a current or a previous period bills: By these forms users can view the whole system spendings, or spendings divided by the specific resources. Presented metrics (resources): costs of launching compute instances, used for tools/pipelines runs. There could be: CPU instances GPU instances costs of storing user data in storages. There could be costs of storing: in Object storages in File storages info about auxiliary costs isn't supported yet All costs are aggregated and displayed for the specific period (by default - the current calendar month). Selected specific period, for which the costs are being calculated and displayed, is called \" current \". Also, for comparison, the costs for the analogical previous period are displayed in diagrams/charts (where this data is available). The user can select the desired ( current ) period to view the costs incurred: from one of predefined periods. For each of them there will be a specific \" previous \" period: Month . If the month is selected as a current period - costs will be shown for the period from the first day of the month till previous calendar day (if the month hasn't ended yet) or the whole month if it has already ended. The \" previous \" period will be the same days interval but in the previous calendar month Quarter . If the quarter is selected as a current period - costs will be shown for the period from the first day of the quarter till previous calendar day (if the quarter hasn't ended yet) or the whole quarter if it has already ended. The \" previous \" period will be the same interval in a quarter but in the previous calendar year Year . If the year is selected as a current period - costs will be shown for the period from the first day of the year till previous calendar day (if the year hasn't ended yet) or the whole year if it has already ended. The \" previous \" period will be the same interval in the previous calendar year custom period. That period is configured by the user manually and can have any duration. The \" previous \" period isn't displayed in this case By default, the \"Billing Visualization\" form looks like on the picture above. It contains: General report table with the following data: current and previous periods, for which the costs are calculated summary spendings according to selected configurations in the toolbar for the current and previous periods the difference between the costs of current period and the previous one in percentage with a certain mark to determine whether spendings grow or reduce The main summary costs diagram. On this diagram, the user can see summary spendings over the current and the previous time periods according to selected configurations in the toolbar. This data could be displayed with the accumulation (as line chart ) or as fact (as bar chart with actual spending values in each time point of the period without accumulation) if the selected \" current \" period coincides with the current corresponding calendar period - there a slice line is displayed on the current calendar date: on its intersection with the \" current \" chart line, the amount of money which was spent from the beginning of the period to the current date is displayed and on its intersection with the \" previous \" chart line - the amount of money which was spent for the same previous time period is displayed the user can hover any point of the \" current \"/\" previous \" line of the diagram - the summary spendings at the corresponding datetime will appear in the tooltip the aggregation value of timeline division for selected periods less than 4 months is 1 day, in other cases - 1 month Note : costs of the current calendar day is being aggregated in the following day, so they aren't displayed in diagrams With accumulation: And actual values (without accumulation): The spendings bar chart with the Resources division. On this chart, the user can view the division of summary spendings in a selected time period over the resource groups - Storages (with division into Object and File storages) and Compute Instances (with division into CPU and GPU ). Also, for each resource, the summary spendings for the same previous period are presented as well: The spendings bar chart with the Billing centers division. On this chart, the user can view the division of summary spendings (all resource types) in a selected time period over the billing centers (the system displays only top N most costly billing centers). For each displayed center/group, the summary spendings for the same previous period is presented as well: Data can be shown as a bar chart (default, see the picture above) or as a pie chart The report toolbar with the following controls: calendar control - to select another \" current \" period that doesn't coincide with the current calendar period (for predefined periods) or manually select desired period duration (for custom period) period selector - to select a current time period for the report - from one of predefined periods or the manual custom period filter to restrict shown spendings to a specific billing center (user group) or a user. By default, all available centers/users are selected (no restrictions) filter to restrict shown spendings to a specific Cloud region or Cloud provider. By default, all available regions/providers are selected (no restrictions) options menu. Includes items: to configure discounts for the current Cloud Pipeline deployment (if the are applicable). Discounts can be configured separately - for compute instances and storages to restore origin layout. As all forms at the billing page can be displaced or resized, this item may be helpful to restore the origin forms view to show/hide spendings quotas on the billing costs charts to export raw spendings data of a displayed report to the local workstation - in *.csv or image format The billing report form described above is general. To get info/charts of summary spendings in any desired period (from available) you shall select the corresponding period via the Period selector and Calendar control . To get info/charts of summary spendings for specific Cloud Provider(s) ( for multi-Provider deployments ) or for specific Region(s) of the Cloud Provider you shall apply the corresponding filter, e.g.: To get info/charts of summary spendings in the selected period only for the specific resource group you can click that resource in the Resources chart or use the corresponding item of the menu in the left side of the page (for more details see sections below ).","title":"General costs report"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/#user-report","text":"To get info/charts of summary spendings in the selected period only for the specific user you can select the desired user from the dropdown list in the main toolbar. In this case: the costs data will be calculated and displayed only for the selected user (in the general report table, in the main diagram and in the resources chart) the spendings bar chart with the Billing centers division will disappear, e.g.: Filter supports multiselect, i.e. several users can be selected simultaneously. Shown spendings will be summarized for them. Note : although field is single for the filter by users and billing centers (groups), you can not select some user(s) and some group(s) in the same moment - filter only by one metric may be applied.","title":"User report"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/#billing-centergroup-report","text":"To get info/charts of summary spendings in the selected period only for the specific Billing center you can: select the desired one from the dropdown list in the main toolbar, e.g.: OR click the Billing Center's bar in the Billing centers division chart, e.g.: In this case: the costs data will be calculated and displayed only for the selected Billing center (summary for all its users - in the general report table, in the main diagram and in the resources chart) the bar chart with the top N of users' spendings in the selected period (among all users in that Billing center) will appear the table with short info about summary spendings of each user of that Billing center in the selected period (also info contains summary duration and count of the runs launched by the user, used storages volume) will appear, e.g.: You can click any user from the bar chart or from the details table with the top N of users' spendings - the corresponding user report will be opened. Notes : filter by billing centers supports multiselect, i.e. several billing centers can be selected simultaneously. Shown spendings will be summarized for them, but the view of the page will be the same as general report , i.e. the bar chart and table with the top N of users' spendings will not be displayed. Also although field is single for the filter by users and billing centers (groups), you can not select some user(s) and some group(s) in the same moment - filter only by one metric may be applied.","title":"Billing center/group report"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/#resources-reports","text":"If you wish - you may get info/charts of summary spendings not for all resources but for the specific resource group. Also, for these charts you may select the desired time period and specific user/Billing center (group) to view costs in the way analogical as described above.","title":"Resources reports"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/#storages-report","text":"To get info/charts of summary spendings in the selected period only for the Storages resource group (costs of the storing data in storages) you can click the corresponding item of the menu in the left side of the page: In this case, the summary costs for all used storages during the selected period by selected users/Billing center will be calculated and displayed (both types - Object/File storages). In the appeared page, you can see: General report table with summary costs for all used storages during the selected and previous ( if it's available ) periods The summary Storages spendings diagram over the current and the previous time periods according to selected configurations in the toolbar. This data could be displayed with the accumulation (as line chart ) or as fact (as bar chart with actual spending values in each time point of the period without accumulation) The spendings bar chart with top N most costly storages used during the selected period compared with the previous one ( if it's available ) The detailed spendings table under the chart with the full list of storages used during the selected period by selected user/Billing center Example: Spendings bar chart with top N most costly storages can show the information as storages costs - in $ (default, see the picture above) or as average storages volumes - in Gb : Switching this view mode does not affect the summary chart or details spendings table. The detailed spendings table contains the following info for each storage: Storage - storage name Owner - storage OWNER username Billing Center - billing center (group) of the storage OWNER , if it's defined Type - storage type ( S3 / GS / NFS / LustreFS /etc.) Cost - total spendings for the storage in the selected period Avg.Vol. (GB) - the average storage volume, in GB. It means that the exact volumes for each day of the selected report period were brought and then the average value was calculated Cur.Vol. (GB) - the current storage volume, in GB. This volume is a real volume for a current moment/last day of the selected period Region - Cloud Region Provider - Cloud provider Created date - storage creation date In the Storages report, there is no division by storage type ( Object / File ). All data is calculated and displayed summary for both types. If you want to view costs only for storages with the specific type - you can select the corresponding one in the menu at the left side of the page (see sections below for details).","title":"Storages report"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/#file-storages-report","text":"File storages report has fully the same view (set of forms) as common storages report , but with information for file storages only e.g.:","title":"File storages report"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/#object-storages-report","text":"Object storages report has the view (set of forms) distinct from common storages report , e.g.: As object storages supports versioning and archiving data into different archive tiers, the Object storages report supports the displaying of the corresponding related information. In this section, the following terms will be used: current version - last (current) version of the versioning object storage or current version of the non-versioning object storage old versions - all non-last (previous) versions of the versioning object storage Report includes: General report table with summary costs for all used object storages during the selected and previous ( if it's available ) periods Object storages usages - the summary spendings diagram over the current and the previous time periods according to selected configurations in the toolbar. This data can be displayed: with the accumulation (as line chart ). Solid lines show summary spendings on the data usage for current version of object storages (for current and previous periods). Additionally, for each solid line, there is a dashed line of the same color - that line shows summary spendings on the data usage for all old versions of object storages Spendings on different versions are shown in a tooltip as well: the summary value is shown and separately - spendings on current version and on old versions , e.g.: or as fact (as bar chart with actual spending values in each time point of the period without accumulation). Each bar is displayed as a stack of current version spendings / old versions spendings in the specific time point. Current version spendings are shown with solid filling, old versions spendings are shown without filling. For current and previous periods charts are shown in different colors. Object storages - the bar chart with top N most costly object storages used during the selected period compared with the previous one ( if it's available ). Each bar is displayed as a stack of current version spendings / old versions spendings of the specific object storage. Current version spendings are shown with solid filling, old versions spendings are shown without filling. For previous period, charts are shown in different colors and only as dashed lines. If data in the storage is storing in different tiers (archive types), this can be viewed in a tooltip - there will be a division of spendings by the used tiers. Versions division for each tier will be shown as well, e.g.: Object storages layers - the bar chart with cost division to different tiers (archive types). This chart does not contain any information for previous period. Only layers used for data storing in the current period according to selected filters are shown. Up to 4 layers can be here: Standard , Glacier , Glacier IR , Deep Archive . If any layer has old versions - this will be shown as a stack of current version spendings / old versions spendings for that layer. Current version spendings are shown with solid filling, old versions spendings are shown without filling, e.g.: Object storage layers chart can show the information as storages usage costs - in $ (default, see the picture above) or as average storages volumes - in Gb : Note : switching this view mode also affects the chart with top N most costly object storages: Details of the versions division can be viewed in a tooltip of each layer bar, e.g.: The detailed spendings table under the chart with the full list of object storages used during the selected period by selected user/Billing center. This table contains the following info for each object storage: Storage - storage name Owner - storage OWNER username Billing Center - billing center (group) of the storage OWNER , if it's defined Type - object storage type ( S3 / GS / etc.) Cost - spendings for the storage in the selected period. There are two values are shown in format total spendings for all versions / spendings for old versions only Avg.Vol. (GB) - the average storage volume, in GB. It means that the exact volumes for each day of the selected report period were brought and then the average value was calculated. There are two values shown in the format average volume of all versions / average volume of old versions only Cur.Vol. (GB) - the current storage volume, in GB. This volume is a real volume for a current moment/last day of the selected period. There are two values shown in the format current volume of all versions / current volume of old versions only Region - Cloud Region Provider - Cloud provider Created date - storage creation date User can select one of the object storage layers - by click it on the corresponding chart. In this case, all charts and tables will be updated - only storages, that contain files in the selected layer type, will be shown in forms. Also, shown spendings/data volume will be related only to files in the selected layer, not for the whole storage(s) or other layers. For example, Glasier IR was selected in the Object storage layers chart for some user: To clear selection of the object storage layer - click it again or click the cross-button near the layer label in the top of the chart:","title":"Object storages report"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/#compute-instances-report","text":"To get info/charts of summary spendings in the selected period only for the Compute instances resource group (costs of the launching instances for running tools/pipelines) you can click the corresponding item of the menu in the left side of the page: In this case, the summary costs for all launched runs during the selected period by selected users/Billing center will be calculated and displayed - uniting launched tools/pipelines of both processing unit types (CPU/GPU): At the appeared page, you can see: General report table with summary costs for all runs launched in the selected and previous ( if it's available ) periods Compute instances runs - the summary spendings chart over the current and the previous time periods according to selected configurations in the toolbar. This data can be displayed: with the accumulation (as line chart ) or as fact (as bar chart with actual spending values in each time point of the period without accumulation) Cost details - the bar chart that displays the sharing of the cost of runs launched during the selected period into layers - Compute (cost of compute instances used in runs) and Disk (cost of EBS drives connected to runs during their performing): Instance types - the bar chart with top N most costly instance types launched in the selected period compared to the previous one ( if it's available ), e.g.: Under the chart, there is a detailed spendings table with the full list of launched instance types (not only top N ) during the selected period. This table contains the following info for each instance type: Instance - instance type name Usage (hours) - summary number of hours that instances of this type were run Runs count - summary number of this instance type's runs Cost - summary cost of this instance type's runs Compute cost - separately the compute cost as part of summary runs cost Disk cost - separately the disk (EBS drives) cost as part of summary runs cost Pipelines - the bar chart with top N most costly pipelines launched in the selected period compared to the previous one ( if it's available ), e.g.: Under the chart, there is a detailed spendings table with the full list of launched pipelines (not only top N ) during the selected period. This table contains the following info for each pipeline: Pipeline - pipeline name Owner - pipeline owner's username Usage (hours) - summary number of hours this pipeline was run Runs count - summary number of this pipeline's runs Cost - summary cost of this pipeline's runs Compute cost - separately the compute cost as part of summary runs cost of this pipeline Disk cost - separately the disk (EBS drives) cost as part of summary runs cost of this pipeline Tools - the bar chart with top N most costly tools launched in the selected period compared to the previous one ( if it's available ), e.g.: Under the chart, there is a detailed spendings table with the full list of launched tools (not only top N ) during the selected period. This table contains the following info for each tool: Tool - tool name Owner - tool owner's username Usage (hours) - summary number of hours this tool was run Runs count - summary number of this tool's runs Cost - summary cost of this tool's runs Compute cost - separately the compute cost as part of summary runs cost of this tool Disk cost - separately the disk (EBS drives) cost as part of summary runs cost of this tool Additional control that allows to change displaying of spendings bar charts ( Instance types , Pipelines , Tools ) with the following possible values: Cost ( default ) - at 3 above described bar charts, top N most costly objects (instance types, pipelines, tools) are displayed. Data Unit - currency Usage (hours) - at 3 above described bar charts, top N most involved objects (instance types, pipelines, tools) are displayed. Data Unit - usage hours Runs - at 3 above described bar charts, top N objects (instance types, pipelines, tools) with the largest runs number are displayed. Data Unit - runs count For example, the Instance types chart in case when \" Usage (hours) \" displaying type is selected: Please note, this control changes only the chart view, but the data in tables under charts is not changed - only column is being changed by which tables are sorted. User can select one of the runs cost layers ( Compute or Disk ) - by click it in the Cost details chart. In this case: summary runs cost chart will be updated (for both periods - current and previous ) - only summary spendings, that correspond to the selected layer ( Compute or Disk ), will be shown charts Instance types , Pipelines , Tools will be updated - only spendings, that correspond to the selected layer ( Compute or Disk ), will be shown data in tables under charts will not be changed, but the sorting column will be set the same as the selected layer For example, if the Compute layer of the runs cost is selected: To clear selection of the runs cost layer - click it again or click the cross-button near the layer label in the top of the Cost details chart.","title":"Compute instances report"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/#cpugpu-division","text":"By default, in Compute instances report form there is no division by processing unit type (CPU/GPU). All data is calculated and displayed summary for both types. To view costs only for instances with the specific processing unit's type - select the corresponding one in the menu at the left side of the Billing page. E.g., for CPU only:","title":"CPU/GPU division"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/#export-billling-reports","text":"User can export a shown billing report. Export is available from the Options menu in the right-upper corner of each report section. Export options can vary in different report sections. Possible formats - CSV (for datatables) and PNG (for images). General section From the General section, the following exports are available: CSV by Billing Centers - in this case, the general billing report will be split by billing centers (groups) CSV by Users - in this case, the general billing report will be split by users Image - in this case, export will be performed as image that contains pictures of charts as they were on the screen Storages section From the Storages section and its sub-sections, the following exports are available: CSV - in this case, the billing report will be split by storages. The view of the report is similar to the detailed spendings table Image - in this case, export will be performed as image that contains pictures of charts as they were on the screen (without detailed spendings table) Export of the Object storages reports is different from other storages' reports. This export additionally supports breaking of data by versions and archive tiers: if there are any spendings in the archive layer - for that layer, separate columns of Cost , Average Volume (GB) and Current Volume (GB) will be in the exported report for versioning storages, there are separate columns for the current version and old versions Example of the Object storages report export (please note, there is only a part of the report is shown): Compute instances From the Compute instances section and its sub-sections, the following exports are available: CSV - in this case, the billing report will contain 3 parts - for most costly instances types, pipelines and tools - as on the report page Image - in this case, export will be performed as image that contains pictures of charts as they were on the screen (without detailed spendings tables) Raw data - in this case, the export will contain the full raw list of runs that were used for the report shown on the screen","title":"Export billling reports"},{"location":"manual/Appendix_D/Appendix_D._Costs_management/#report-aggregation-level-according-to-the-permissions","text":"Available reports are calculated and displayed according to the user permissions: General users can have base access to the Billing reports that allows to view some information - about users' own spendings: this behavior is enabled by the system preference billing.reports.enabled . If this preference is set, all general users can access personal billing information - runs/storages where the user is an owner. Also general users can use filters, change chart types, make reports export. the following restrictions are set for general users when \"base\" billing access is enabled: all showing charts are being displayed only spendings of the current user there isn't an ability to configure discounts, the button \" Configure discounts \" is disabled \"Billing centers (TOP 10)\" chart isn't displayed Billing center (group) leader can have extended access to the Billing reports : a special role exists for that - ROLE_BILLING_MANAGER . The Billing reports Dashboard becomes available for user in full, if this role is assigned to the user. In this case, all possible filters, charts and their types, discounts configuration, export feature and etc. become available too. So, users who are granted this role are able to view the whole Billing reports info of the platform (as if they were admins). Platform admin can view all available reports Note : there is a special system preference billing.reports.enabled.admins . It allows to configure Billing reports visibility for admins and billing managers. Default value is true .","title":"Report aggregation level according to the permissions"},{"location":"manual/Appendix_E/Appendix_E._Pipeline_objects_concept/","text":"Pipeline objects concept In general, pipelines represent a sequence of tasks that are executed along with each other in order to process some data. They help to automate complex tasks that consist of many sub-tasks. Pipelines allow users to solve a wide range of analytical tasks. The Pipeline object in the Cloud Pipeline environment represents a workflow script with versioned files of source code, documentation, and configuration: Source code represents the code that directly shall be performed to solve tasks according to the specific pipeline needs Configuration represents the special config that contains all settings for the environment in which the pipeline shall be performed Documentation represents the set of documents that describe purposes, main solving tasks, features of the specific pipeline, etc. This is an optional part of the Pipeline and could be omitted Under the hood, each Pipeline is a GitLab repository. So, each of the constituent Pipeline parts represents one or more versioned files in that repository. The data processing via pipeline runs, in a nutshell, can be described in these several steps: The user prepares a specific calculation script that shall be registered in the Cloud Pipeline environment as the source code file of the pipeline. The user defines the environment for the pipeline execution - first of all, it is a specific package of software that is defined by a docker image. Also, the user defines the characteristics of the Cloud compute instance on which the pipeline will be run. All described definitions shall be specified in the pipeline configuration file. To store pipeline's inputs and outputs datasets the Cloud data storages shall be used. The user defines paths to these datasets as different parameters of the pipeline. Pipeline parameters shall be specified in the pipeline's configuration file. The user launches the pipeline execution in the Cloud Pipeline environment. This includes the following steps: the Cloud compute node (according to the specified characteristics in the pipeline configuration file) is being initialized on the initialized node a docker container (from the docker image specified in the pipeline's configuration file) is being launched. The subsequent execution is being continued in the docker container when the environment is set, the Git repository (corresponding to the pipeline) is being cloned to the node disk pipeline calculation script (from the cloned source code files) launches pipeline's input datasets are used for calculations (from the data storages according to the specified paths in the pipeline configuration file) output datasets are uploaded after calculations (into the data storages according to specified paths in the pipeline configuration file) After the completion of the calculation script execution, the pipeline run is being completed. The Cloud compute instance enters the pending state (no billing). The user can monitor outputs in the data storages according to specified paths in the pipeline configuration file before the pipeline run Pipeline components For more details about Pipeline components representation see here . Source code By default, when a new Pipeline object is being created from any available template - the Cloud Pipeline creates a default code file on the template's programming language. The user can manually edit this code file, add new ones or upload any count of existing code files from the local workstation. By default, all source code files are written into the /src directory of the repository. During the Pipeline execution, its corresponding repository is cloned to the compute node - so, all source code scripts will be downloaded to the node disk (more details about pipeline execution see above). Also, the user shall specify the order of scripts execution. The execution of the main (first) running script shall be specified in the Pipeline config. The execution order of other scripts should be specified in the main script or in any other, which execution is specified in the main one, otherwise, unspecified code files will be ignored. Configuration By default, when a new Pipeline object is being created from any available template - the Cloud Pipeline creates a default configuration json file. It contains the list of all general execution settings that are necessary for the pipeline run: the characteristics of the Cloud compute instance that would be initialized: type of instance (count of CPU, GPU, RAM) in terms of the Cloud Provider size of the instance disk size based docker image from which the docker container would be launched at the initialized Cloud instance the command template which would be performed at the launched docker container - here the command of the main (first) script execution shall be necessarily specified the name of the main script from source code the list of the pipeline parameters - each parameter defines the parameter for the calculation pipeline script (it could be a simple string/boolean parameter or the path to the input/output dataset) and others The user can manually change these settings via the GUI elements or via the editing of the config.json file. Documentation By default, when a new Pipeline object is being created from any available template - the Cloud Pipeline creates a default README file. The user can edit/remove this file or upload any count of existing documents from the local workstation. By default, all documents are written into the /docs directory of the pipeline repository. Schematic composition of the Pipeline object: If the user makes any changes in the Pipeline object and saves them - it causes the new commit into the corresponding Git repository of that pipeline. All commits perform automatically by the Cloud Pipeline, the user only should specify commit message. Main working scenarios Below the main scenarios of working with the Pipeline object in the Cloud Pipeline environment are described. Creation of a new Pipeline object For more details how to create and configure a Pipeline object see here . New pipeline When the user creates a new pipeline in the Cloud Pipeline environment - it automatically leads to the creation of the GitLab repository with the same name as the pipeline. On the Pipeline page, users can see general info about the repository that corresponds to the specific pipeline: the last commit checksum hash and commit message (on the picture below, it is pipeline initial commit and its automatically naming message) the info about the last repository update and the user name which did that update Just created pipeline contains: the template of the main README file the template of the main file with the calculation script the config.json file with execution environments for the pipeline run Edit and configure a pipeline After the pipeline creation, the user should configure it for his needs. Users can change the following: Source code the user can manually edit the initial template code file, add new empty files and create scripts manually or upload any count of existing code files from his local workstation Configuration the user can manually edit the full set of execution settings via the config.json file or via the GUI elements. Both variants are identical and supported by the Cloud Pipeline general settings that the user should pay attention to: the characteristics of the Cloud compute instance that would be initialized for the pipeline run: type of instance (count of CPU, GPU, RAM) in terms of the Cloud Provider size of the instance disk size the cluster configuration - if the user wants to launch pipeline tasks on several nodes he can configure the cluster based docker image from which the docker container would be launched at the initialized Cloud instance. Each docker image contains a specific software package. The user can leverage one of the existing and available docker images or also can create his own docker image with the specific software using the Cloud Pipeline features (see below), and then use it for launch pipeline command template which would be performed at the launched docker container during the pipeline run - here the command of the main (first) script execution shall be necessarily specified. If in the command template no execution is specified - no scripts from the source code will be performed if the user wants to input/output any data to/from the pipeline script during its execution - he shall configure parameters . Parameters allow varying of the set of initial data that is necessary for the pipeline run: each parameter has a name (required), a type (required) and a value (it could be optionally predefined or specified before the pipeline run) each parameter can be one of the several types (\"string\", \"boolean\", \"input path\", \"output path\" and others) the \" string \" parameter allows to transfer into the pipeline execution some string value the \" boolean \" parameter allows to transfer into the pipeline execution the boolean value the \" input path \" parameter allows to specify the path to the input dataset. The path specified as \"input path\" will be automatically mounted to the running docker container. The path where the file or directory will be mounted in the container is defined by the system Cloud Pipeline parameter - $INPUT_DIR . It has default value, but also could be redefined by the user the \" output path \" parameter allows to specify the path for data output. After the finish of the pipeline execution, all data from working directory of the docker container will be automatically uploaded to the output path. The path to the working directory in the container is defined by the system Cloud Pipeline parameter - $ANALYSIS_DIR . It has default value, but also could be redefined by the user the user can add several parameters of each type so, the user can add some necessary parameters to the pipeline, use them in the pipeline code and define parameter values directly before a specific pipeline launch Documentation the user can manually edit/delete the initial template README file or upload any count of existing documents from his local workstation Any saved changes in the Pipeline object automatically lead to the new commit into the corresponding Git repository. Short info about the last commit is always available to the user. In some cases, if users want - they can work directly with Git. Note that the repository name is the same as a pipeline name: Create a specific docker image For some specific tasks, it might be convenient to create a new docker image that will be used for the Pipeline execution. Such docker image can contain special packages that are necessary for the specific Pipeline. A new image can be based on the existing one and simply created in the Cloud Pipeline environment. For doing that: user shall launch one of the existing Docker images in the Cloud Pipeline environment (named \" Tools \"), allow just to initialize cloud compute instance and launch a Docker container on it. After that, the user can connect into the launched container via the SSH session ( SSH Web GUI terminal ), then install the necessary software and then commit the resulting container as a new Docker image or a new version of the existing one - by the Cloud Pipeline capabilities. See more details here . or user can create a Docker image without the Cloud Pipeline and then push the prepared image into the Cloud Pipeline registry. See more details here . After that, the user can leverage such image as the based docker image for his own Pipeline object. Create a multi-configuration For some reasons, the user may need to launch the Pipeline on different compute instances or, for example, with different sets of parameters, etc. For such cases, the Cloud Pipeline allows to create several configurations for one Pipeline object that all are saved in one config.json file but with the ability to specify which one directly should be used before the pipeline run. Testing an existing Pipeline object For more details how to launch a Pipeline object see here . Launch the Pipeline After the pipeline is created, configured and saved - it can be launched. The Cloud Pipeline allows to launch the pipeline with the saved execution settings and parameters (if they have default values) or manually configure execution settings and parameters before a specific run (such changes will not be saved in the pipeline configuration and will use only for one pipeline run). After the pipeline was launched all described procedures are performed - the cloud instance is initialized, the docker container is launched, input paths are mounted (if they were specified before the run via the parameters) - then the main script will start execution. As the main pipeline script is prepared manually by the user, it can be incorrect during the first runnings and contain errors. To easily debug the script the user should not launch the created pipeline and wait for the full execution, but connect to the compute node via the SSH and work with the script execution directly in the terminal. For that, the user should set the command template , e.g. sleep infinity , to start the Pipeline in \"idle\" mode (not launching any script). It allows just to initialize cloud instance, launch a Docker container on it, clone the pipeline repository and perform other initialize tasks. After that, the user can connect into the launched container via the SSH session ( SSH Web GUI terminal ), launch and debug the script manually (the script is available from the cloned repository): All tasks performed during the run and logs are available to the user, e.g.: So, the user can observe for the pipeline execution in real-time. Review the results After completing all tasks the pipeline run ends. Output data is downloaded into the output path (if it was specified before the run via the parameters). Docker container stops. Run logs page is still available, the pipeline run gets the \"Completed\" state. The user can view output data on the output path after the pipeline execution. These files can be downloaded to the local workstation or, for example, used in further calculations. Also, each output file is marked by special tags that contain general info about the pipeline run, in which the file was received: user name who launched the run run ID in the Cloud Pipeline environment link to the Pipeline object link to the Docker image used in the run main characteristics of the compute cloud instance used in the run and others E.g.: For convenience, the user can view all run history for the Pipeline object with short information about the run. Each record is the hyperlink to the specific run logs page (see above ), where user can review full information about that pipeline run's settings: Deployment of an existing Pipeline object Release the Pipeline After the pipeline was created, configured, saved and tested - the user can release it. In the Cloud Pipeline Web GUI this action looks like creating the named version of the Pipeline object. User can't change such named version, only create a new \" draft \" version over it and then work with it. Then, this \" draft \" version also can be released as another \" stable \" version, etc. Under the hood, version release is the tagging in the corresponding Git repository. When the user releases some version - he just creates the annotated Git tag. Share to other users The Cloud Pipeline has a useful RBAC model. The user can share his prepared pipeline with other users or user groups for further usage/common work. There are 3 permission settings over the Pipeline objects in the Cloud Pipeline: Read - allows to view a pipeline, its code, configuration, settings, history of runs Write - allows to edit/delete a pipeline, its code, configuration, settings Execute - allows to run a pipeline So, the user-owner of the pipeline (or system admin) can set combinations of described permissions to other users/user groups to rule their access to the Pipeline , e.g.: See a simple example of the complete Pipeline running procedure here .","title":"Appendix E. Pipeline objects concept"},{"location":"manual/Appendix_E/Appendix_E._Pipeline_objects_concept/#pipeline-objects-concept","text":"In general, pipelines represent a sequence of tasks that are executed along with each other in order to process some data. They help to automate complex tasks that consist of many sub-tasks. Pipelines allow users to solve a wide range of analytical tasks. The Pipeline object in the Cloud Pipeline environment represents a workflow script with versioned files of source code, documentation, and configuration: Source code represents the code that directly shall be performed to solve tasks according to the specific pipeline needs Configuration represents the special config that contains all settings for the environment in which the pipeline shall be performed Documentation represents the set of documents that describe purposes, main solving tasks, features of the specific pipeline, etc. This is an optional part of the Pipeline and could be omitted Under the hood, each Pipeline is a GitLab repository. So, each of the constituent Pipeline parts represents one or more versioned files in that repository. The data processing via pipeline runs, in a nutshell, can be described in these several steps: The user prepares a specific calculation script that shall be registered in the Cloud Pipeline environment as the source code file of the pipeline. The user defines the environment for the pipeline execution - first of all, it is a specific package of software that is defined by a docker image. Also, the user defines the characteristics of the Cloud compute instance on which the pipeline will be run. All described definitions shall be specified in the pipeline configuration file. To store pipeline's inputs and outputs datasets the Cloud data storages shall be used. The user defines paths to these datasets as different parameters of the pipeline. Pipeline parameters shall be specified in the pipeline's configuration file. The user launches the pipeline execution in the Cloud Pipeline environment. This includes the following steps: the Cloud compute node (according to the specified characteristics in the pipeline configuration file) is being initialized on the initialized node a docker container (from the docker image specified in the pipeline's configuration file) is being launched. The subsequent execution is being continued in the docker container when the environment is set, the Git repository (corresponding to the pipeline) is being cloned to the node disk pipeline calculation script (from the cloned source code files) launches pipeline's input datasets are used for calculations (from the data storages according to the specified paths in the pipeline configuration file) output datasets are uploaded after calculations (into the data storages according to specified paths in the pipeline configuration file) After the completion of the calculation script execution, the pipeline run is being completed. The Cloud compute instance enters the pending state (no billing). The user can monitor outputs in the data storages according to specified paths in the pipeline configuration file before the pipeline run","title":"Pipeline objects concept"},{"location":"manual/Appendix_E/Appendix_E._Pipeline_objects_concept/#pipeline-components","text":"For more details about Pipeline components representation see here .","title":"Pipeline components"},{"location":"manual/Appendix_E/Appendix_E._Pipeline_objects_concept/#source-code","text":"By default, when a new Pipeline object is being created from any available template - the Cloud Pipeline creates a default code file on the template's programming language. The user can manually edit this code file, add new ones or upload any count of existing code files from the local workstation. By default, all source code files are written into the /src directory of the repository. During the Pipeline execution, its corresponding repository is cloned to the compute node - so, all source code scripts will be downloaded to the node disk (more details about pipeline execution see above). Also, the user shall specify the order of scripts execution. The execution of the main (first) running script shall be specified in the Pipeline config. The execution order of other scripts should be specified in the main script or in any other, which execution is specified in the main one, otherwise, unspecified code files will be ignored.","title":"Source code"},{"location":"manual/Appendix_E/Appendix_E._Pipeline_objects_concept/#configuration","text":"By default, when a new Pipeline object is being created from any available template - the Cloud Pipeline creates a default configuration json file. It contains the list of all general execution settings that are necessary for the pipeline run: the characteristics of the Cloud compute instance that would be initialized: type of instance (count of CPU, GPU, RAM) in terms of the Cloud Provider size of the instance disk size based docker image from which the docker container would be launched at the initialized Cloud instance the command template which would be performed at the launched docker container - here the command of the main (first) script execution shall be necessarily specified the name of the main script from source code the list of the pipeline parameters - each parameter defines the parameter for the calculation pipeline script (it could be a simple string/boolean parameter or the path to the input/output dataset) and others The user can manually change these settings via the GUI elements or via the editing of the config.json file.","title":"Configuration"},{"location":"manual/Appendix_E/Appendix_E._Pipeline_objects_concept/#documentation","text":"By default, when a new Pipeline object is being created from any available template - the Cloud Pipeline creates a default README file. The user can edit/remove this file or upload any count of existing documents from the local workstation. By default, all documents are written into the /docs directory of the pipeline repository. Schematic composition of the Pipeline object: If the user makes any changes in the Pipeline object and saves them - it causes the new commit into the corresponding Git repository of that pipeline. All commits perform automatically by the Cloud Pipeline, the user only should specify commit message.","title":"Documentation"},{"location":"manual/Appendix_E/Appendix_E._Pipeline_objects_concept/#main-working-scenarios","text":"Below the main scenarios of working with the Pipeline object in the Cloud Pipeline environment are described.","title":"Main working scenarios"},{"location":"manual/Appendix_E/Appendix_E._Pipeline_objects_concept/#creation-of-a-new-pipeline-object","text":"For more details how to create and configure a Pipeline object see here .","title":"Creation of a new Pipeline object"},{"location":"manual/Appendix_E/Appendix_E._Pipeline_objects_concept/#new-pipeline","text":"When the user creates a new pipeline in the Cloud Pipeline environment - it automatically leads to the creation of the GitLab repository with the same name as the pipeline. On the Pipeline page, users can see general info about the repository that corresponds to the specific pipeline: the last commit checksum hash and commit message (on the picture below, it is pipeline initial commit and its automatically naming message) the info about the last repository update and the user name which did that update Just created pipeline contains: the template of the main README file the template of the main file with the calculation script the config.json file with execution environments for the pipeline run","title":"New pipeline"},{"location":"manual/Appendix_E/Appendix_E._Pipeline_objects_concept/#edit-and-configure-a-pipeline","text":"After the pipeline creation, the user should configure it for his needs. Users can change the following: Source code the user can manually edit the initial template code file, add new empty files and create scripts manually or upload any count of existing code files from his local workstation Configuration the user can manually edit the full set of execution settings via the config.json file or via the GUI elements. Both variants are identical and supported by the Cloud Pipeline general settings that the user should pay attention to: the characteristics of the Cloud compute instance that would be initialized for the pipeline run: type of instance (count of CPU, GPU, RAM) in terms of the Cloud Provider size of the instance disk size the cluster configuration - if the user wants to launch pipeline tasks on several nodes he can configure the cluster based docker image from which the docker container would be launched at the initialized Cloud instance. Each docker image contains a specific software package. The user can leverage one of the existing and available docker images or also can create his own docker image with the specific software using the Cloud Pipeline features (see below), and then use it for launch pipeline command template which would be performed at the launched docker container during the pipeline run - here the command of the main (first) script execution shall be necessarily specified. If in the command template no execution is specified - no scripts from the source code will be performed if the user wants to input/output any data to/from the pipeline script during its execution - he shall configure parameters . Parameters allow varying of the set of initial data that is necessary for the pipeline run: each parameter has a name (required), a type (required) and a value (it could be optionally predefined or specified before the pipeline run) each parameter can be one of the several types (\"string\", \"boolean\", \"input path\", \"output path\" and others) the \" string \" parameter allows to transfer into the pipeline execution some string value the \" boolean \" parameter allows to transfer into the pipeline execution the boolean value the \" input path \" parameter allows to specify the path to the input dataset. The path specified as \"input path\" will be automatically mounted to the running docker container. The path where the file or directory will be mounted in the container is defined by the system Cloud Pipeline parameter - $INPUT_DIR . It has default value, but also could be redefined by the user the \" output path \" parameter allows to specify the path for data output. After the finish of the pipeline execution, all data from working directory of the docker container will be automatically uploaded to the output path. The path to the working directory in the container is defined by the system Cloud Pipeline parameter - $ANALYSIS_DIR . It has default value, but also could be redefined by the user the user can add several parameters of each type so, the user can add some necessary parameters to the pipeline, use them in the pipeline code and define parameter values directly before a specific pipeline launch Documentation the user can manually edit/delete the initial template README file or upload any count of existing documents from his local workstation Any saved changes in the Pipeline object automatically lead to the new commit into the corresponding Git repository. Short info about the last commit is always available to the user. In some cases, if users want - they can work directly with Git. Note that the repository name is the same as a pipeline name:","title":"Edit and configure a pipeline"},{"location":"manual/Appendix_E/Appendix_E._Pipeline_objects_concept/#create-a-specific-docker-image","text":"For some specific tasks, it might be convenient to create a new docker image that will be used for the Pipeline execution. Such docker image can contain special packages that are necessary for the specific Pipeline. A new image can be based on the existing one and simply created in the Cloud Pipeline environment. For doing that: user shall launch one of the existing Docker images in the Cloud Pipeline environment (named \" Tools \"), allow just to initialize cloud compute instance and launch a Docker container on it. After that, the user can connect into the launched container via the SSH session ( SSH Web GUI terminal ), then install the necessary software and then commit the resulting container as a new Docker image or a new version of the existing one - by the Cloud Pipeline capabilities. See more details here . or user can create a Docker image without the Cloud Pipeline and then push the prepared image into the Cloud Pipeline registry. See more details here . After that, the user can leverage such image as the based docker image for his own Pipeline object.","title":"Create a specific docker image"},{"location":"manual/Appendix_E/Appendix_E._Pipeline_objects_concept/#create-a-multi-configuration","text":"For some reasons, the user may need to launch the Pipeline on different compute instances or, for example, with different sets of parameters, etc. For such cases, the Cloud Pipeline allows to create several configurations for one Pipeline object that all are saved in one config.json file but with the ability to specify which one directly should be used before the pipeline run.","title":"Create a multi-configuration"},{"location":"manual/Appendix_E/Appendix_E._Pipeline_objects_concept/#testing-an-existing-pipeline-object","text":"For more details how to launch a Pipeline object see here .","title":"Testing an existing Pipeline object"},{"location":"manual/Appendix_E/Appendix_E._Pipeline_objects_concept/#launch-the-pipeline","text":"After the pipeline is created, configured and saved - it can be launched. The Cloud Pipeline allows to launch the pipeline with the saved execution settings and parameters (if they have default values) or manually configure execution settings and parameters before a specific run (such changes will not be saved in the pipeline configuration and will use only for one pipeline run). After the pipeline was launched all described procedures are performed - the cloud instance is initialized, the docker container is launched, input paths are mounted (if they were specified before the run via the parameters) - then the main script will start execution. As the main pipeline script is prepared manually by the user, it can be incorrect during the first runnings and contain errors. To easily debug the script the user should not launch the created pipeline and wait for the full execution, but connect to the compute node via the SSH and work with the script execution directly in the terminal. For that, the user should set the command template , e.g. sleep infinity , to start the Pipeline in \"idle\" mode (not launching any script). It allows just to initialize cloud instance, launch a Docker container on it, clone the pipeline repository and perform other initialize tasks. After that, the user can connect into the launched container via the SSH session ( SSH Web GUI terminal ), launch and debug the script manually (the script is available from the cloned repository): All tasks performed during the run and logs are available to the user, e.g.: So, the user can observe for the pipeline execution in real-time.","title":"Launch the Pipeline"},{"location":"manual/Appendix_E/Appendix_E._Pipeline_objects_concept/#review-the-results","text":"After completing all tasks the pipeline run ends. Output data is downloaded into the output path (if it was specified before the run via the parameters). Docker container stops. Run logs page is still available, the pipeline run gets the \"Completed\" state. The user can view output data on the output path after the pipeline execution. These files can be downloaded to the local workstation or, for example, used in further calculations. Also, each output file is marked by special tags that contain general info about the pipeline run, in which the file was received: user name who launched the run run ID in the Cloud Pipeline environment link to the Pipeline object link to the Docker image used in the run main characteristics of the compute cloud instance used in the run and others E.g.: For convenience, the user can view all run history for the Pipeline object with short information about the run. Each record is the hyperlink to the specific run logs page (see above ), where user can review full information about that pipeline run's settings:","title":"Review the results"},{"location":"manual/Appendix_E/Appendix_E._Pipeline_objects_concept/#deployment-of-an-existing-pipeline-object","text":"","title":"Deployment of an existing Pipeline object"},{"location":"manual/Appendix_E/Appendix_E._Pipeline_objects_concept/#release-the-pipeline","text":"After the pipeline was created, configured, saved and tested - the user can release it. In the Cloud Pipeline Web GUI this action looks like creating the named version of the Pipeline object. User can't change such named version, only create a new \" draft \" version over it and then work with it. Then, this \" draft \" version also can be released as another \" stable \" version, etc. Under the hood, version release is the tagging in the corresponding Git repository. When the user releases some version - he just creates the annotated Git tag.","title":"Release the Pipeline"},{"location":"manual/Appendix_E/Appendix_E._Pipeline_objects_concept/#share-to-other-users","text":"The Cloud Pipeline has a useful RBAC model. The user can share his prepared pipeline with other users or user groups for further usage/common work. There are 3 permission settings over the Pipeline objects in the Cloud Pipeline: Read - allows to view a pipeline, its code, configuration, settings, history of runs Write - allows to edit/delete a pipeline, its code, configuration, settings Execute - allows to run a pipeline So, the user-owner of the pipeline (or system admin) can set combinations of described permissions to other users/user groups to rule their access to the Pipeline , e.g.: See a simple example of the complete Pipeline running procedure here .","title":"Share to other users"},{"location":"manual/Appendix_F/Appendix_F._%D0%A1omparison_of_using_different_FS_storages_%28FSx_for_Lustre_vs_EFS_in_AWS%29/","text":"\u0421omparison of using different FS storage types in Cloud Pipeline environment Performance Synthetic data experiment Real data experiment Costs Performance comparison The performance was measured for different AWS file systems are using in Cloud Pipeline: filesystems are managed by S3 EFS in AWS FSx for Lustre local filesystems BTRFS on EBS LizardFS on EBS A performance comparison of Cloud Pipeline storages was conducted against synthetic and real data. All experiments were carried out on c5.2xlarge ( 8 CPU, 16 RAM) AWS instance. Synthetic data experiment In this experiment we have generated two types of data: 100Gb large file 100 000 small files by 500Kb With this data, we have measured the creation and read times using the time command of Unix and Unix-like systems. The command that was used to create a 100Gb large file by 100Mb chunks: dd if=/dev/urandom of=/path-to-storage/large_100gb.txt iflag=fullblock bs=100M count=1024 The command that was used to create a 100 000 small files by 500Kb: for j in {1..100000}; do head -c 500kB </dev/urandom > /path-to-storage/small/randfile$j.txt done The command that was used to read a large file: dd if=/path-to-storage/large_100gb.txt of=/dev/null conv=fdatasync The command that was used to read small files: for file in /path-to-storage/small/* ; do dd if=$file of=/dev/null done The synthetic data experiment was initially carried out in one thread and then 4 and 8 threads. The experimental results on synthetic data are presented in the following tables: 1 thread Storage Create the large file Read the large file Create many small files Read many small files EFS real 17m24.812s user 0m0.004s sys 10m34.675s real 17m18.216s user 0m48.904s sys 3m9.517s real 53m3.498s user 0m58.209s sys 5m46.386s real 27m11.831s user 2m14.295s sys 1m35.923s LUSTRE real 10m55.494s user 0m0.004s sys 9m47.326s real 13m28.536s user 0m51.675s sys 12m36.813s real 12m37.380s user 1m54.744s sys 5m13.759s real 20m40.877s user 0m44.095s sys 9m4.877s BTRFS on EBS real 11m9.866s user 0m0.032s sys 11m7.813s real 7m0.032s user 0m53.303s sys 3m47.549s real 6m48.540s user 0m47.254s sys 6m6.036s real 6m26.190s user 2m1.610s sys 1m25.726s LizardFS on EBS real 13m4.352s user 0m0.008s sys 10m57.101s real 7m54.142s user 0m53.089s sys 3m51.089s real 16m39.980s user 2m11.618s sys 6m35.383s real 10m3.791s user 2m18.924s sys 1m28.035s 4 threads Storage Create the large file Read the large file Create many small files Read many small files EFS real 69m25.583s user 0m0.015s sys 11m30.451s real 64m20.614s user 0m48.233s sys 3m15.074s real 59m18.137s user 0m37.185s sys 8m29.882s real 33m19.459s user 2m23.134s sys 2m18.345s LUSTRE real 38m32.383s user 0m0.014s sys 36m40.821s real 20m45.156s user 0m59.189s sys 19m26.054s real 25m38.531s user 0m21.820s sys 16m59.318s real 24m58.620s user 2m11.449s sys 11m57.240s BTRFS on EBS real 38m50.438s user 0m0.028s sys 38m45.451s real 27m55.173s user 0m52.903s sys 4m26.061s real 20m54.831s user 0m20.394s sys 20m34.926s real 12m50.153s user 2m5.149s sys 1m18.555s LizardFS on EBS real 48m47.367s user 0m0.020s sys 40m17.341s real 32m21.257s user 0m57.588s sys 5m32.215s real 28m12.707s user 1m30.504s sys 12m40.881s real 15m31.591s user 2m21.772s sys 2m31.211s 8 threads Storage Create the large file Read the large file Create many small files Read many small files EFS real 127m49.718s user 0m0.010s sys 14m44.358s real 122m46.188s user 1m12.786s sys 21m14.236s real 72m43.596s user 0m26.727s sys 15m0.582s real 62m46.118s user 2m31.595s sys 2m31.577s LUSTRE real 93m56.846s user 0m0.018s sys 90m30.5s real 94m1.258s user 0m48.908s sys 3m18.557s real 50m42.845s user 0m23.462s sys 35m12.511s real 30m53.199s user 0m54.020s sys 14m42.712s BTRFS on EBS real 87m48.066s user 0m0.016s sys 86m15.610s real 39m59.167s user 0m50.900s sys 4m25.582s real 44m36.847s user 0m24.491s sys 43m11.719s real 17m12.744s user 2m14.324s sys 1m50.667s LizardFS on EBS real 97m25.045s user 0m0.007s sys 73m32.042s real 39m59.167s user 0m50.900s sys 4m25.582s real 50m25.868s user 0m58.079s sys 19m12.443s real 27m50.506s user 2m26.704s sys 3m9.926s As we can see from the presented results, Amazon FSx for Lustre is several times faster (from 1.3 to 3.2 times for read mode and from 1.3 to 4 times for write mode) than Amazon EFS . As expected, the local systems performed better than FSx for Lustre or EFS generally. But it should be noted, that the FSx for Lustre was comparable to local systems in some cases. Real data experiment The cellranger count pipeline was used to conduct an experiment with real data. The input data: 15Gb transcriptome reference 50Gb fastqs The command that was used to run cellranger count pipeline: /path-to-cellranger/3.0.2/bin/cellranger count --localcores=8 --id={id} --transcriptome=/path-to-transcriptome-reference/refdata-cellranger-mm10-3.0.0 --chemistry=SC5P-R2 --fastqs=/path-to-fastqs/fastqs_test --sample={sample_name} The experimental run result is presented in the following table: Storage Execution time EFS real 207m15.566s user 413m32.168s sys 13m40.581s LUSTRE real 189m23.586s user 434m6.950s sys 13m2.902s BTRFS on EBS real 187m23.048s user 413m32.666s sys 12m30.285s LizardFS on EBS real 189m8.210s user 412m6.558s sys 14m18.429s The best result was shown by the BTRFS on EBS local system. However, it can be said that BTRFS on EBS , LizardFS on EBS , and FSx for Lustre are showed the comparable time. Amazon FSx for Lustre was faster Amazon EFS just to ~ 9%. Costs Cost calculations have been performed in according to Amazon pricing at the time of this document. For the experiments were used the storages that were created in the US East (N.Virginia) region with similar features: Storage Storage size Throughput mode EFS Size in EFS Standard: 1 TiB (100%) Bursting: 50 MB/s/TiB LUSTRE SSD: 1.2 TiB Capacity 50 MB/s/TiB baseline, up to 1.3 GB/s/TiB burst BTRFS on EBS SSD: 1.2 TiB Capacity Max Throughput/Instance - 4,750 MB/s LizardFS on EBS SSD: 1.2 TiB Capacity Max Throughput/Instance - 4,750 MB/s The total charge for the month of usage for various storages is calculated in different ways: EFS 1 TB per month x 1024 GB in a TB = 1024 GB per month (data stored in Standard Storage ) 1 024 GB per month x $0,30 GB-month = $307,20 ( Standard Storage monthly cost) FSx for Lustre $0.14 GB-month / 30 / 24 = $0.000194 GB-hour 1228 GB x $0.000194 GB-hour x 720 hours = $171,5 ( FSx for Lustre monthly cost) BTRFS on EBS and LizardFS on EBS (1228 GB x $0.10 GB-month * 86400 seconds (for 24 hours)) / (86,400 seconds/day * 30 day-month) = $4 (for the volume) As seen from the calculation, the most beneficial is to use BTRFS on EBS and LizardFS on EBS local systems. However, this is not suitable for long term storage. In this case, EFS monthly cost is more expensive than Amazon FSx for Lustre monthly cost in 1.8 times for similar features storage at the time of this document.","title":"Appendix F. \u0421omparison of using different FS storage types"},{"location":"manual/Appendix_F/Appendix_F._%D0%A1omparison_of_using_different_FS_storages_%28FSx_for_Lustre_vs_EFS_in_AWS%29/#omparison-of-using-different-fs-storage-types-in-cloud-pipeline-environment","text":"Performance Synthetic data experiment Real data experiment Costs","title":"\u0421omparison of using different FS storage types in Cloud Pipeline environment"},{"location":"manual/Appendix_F/Appendix_F._%D0%A1omparison_of_using_different_FS_storages_%28FSx_for_Lustre_vs_EFS_in_AWS%29/#performance-comparison","text":"The performance was measured for different AWS file systems are using in Cloud Pipeline: filesystems are managed by S3 EFS in AWS FSx for Lustre local filesystems BTRFS on EBS LizardFS on EBS A performance comparison of Cloud Pipeline storages was conducted against synthetic and real data. All experiments were carried out on c5.2xlarge ( 8 CPU, 16 RAM) AWS instance.","title":"Performance comparison"},{"location":"manual/Appendix_F/Appendix_F._%D0%A1omparison_of_using_different_FS_storages_%28FSx_for_Lustre_vs_EFS_in_AWS%29/#synthetic-data-experiment","text":"In this experiment we have generated two types of data: 100Gb large file 100 000 small files by 500Kb With this data, we have measured the creation and read times using the time command of Unix and Unix-like systems. The command that was used to create a 100Gb large file by 100Mb chunks: dd if=/dev/urandom of=/path-to-storage/large_100gb.txt iflag=fullblock bs=100M count=1024 The command that was used to create a 100 000 small files by 500Kb: for j in {1..100000}; do head -c 500kB </dev/urandom > /path-to-storage/small/randfile$j.txt done The command that was used to read a large file: dd if=/path-to-storage/large_100gb.txt of=/dev/null conv=fdatasync The command that was used to read small files: for file in /path-to-storage/small/* ; do dd if=$file of=/dev/null done The synthetic data experiment was initially carried out in one thread and then 4 and 8 threads. The experimental results on synthetic data are presented in the following tables:","title":"Synthetic data experiment"},{"location":"manual/Appendix_F/Appendix_F._%D0%A1omparison_of_using_different_FS_storages_%28FSx_for_Lustre_vs_EFS_in_AWS%29/#1-thread","text":"Storage Create the large file Read the large file Create many small files Read many small files EFS real 17m24.812s user 0m0.004s sys 10m34.675s real 17m18.216s user 0m48.904s sys 3m9.517s real 53m3.498s user 0m58.209s sys 5m46.386s real 27m11.831s user 2m14.295s sys 1m35.923s LUSTRE real 10m55.494s user 0m0.004s sys 9m47.326s real 13m28.536s user 0m51.675s sys 12m36.813s real 12m37.380s user 1m54.744s sys 5m13.759s real 20m40.877s user 0m44.095s sys 9m4.877s BTRFS on EBS real 11m9.866s user 0m0.032s sys 11m7.813s real 7m0.032s user 0m53.303s sys 3m47.549s real 6m48.540s user 0m47.254s sys 6m6.036s real 6m26.190s user 2m1.610s sys 1m25.726s LizardFS on EBS real 13m4.352s user 0m0.008s sys 10m57.101s real 7m54.142s user 0m53.089s sys 3m51.089s real 16m39.980s user 2m11.618s sys 6m35.383s real 10m3.791s user 2m18.924s sys 1m28.035s","title":"1 thread"},{"location":"manual/Appendix_F/Appendix_F._%D0%A1omparison_of_using_different_FS_storages_%28FSx_for_Lustre_vs_EFS_in_AWS%29/#4-threads","text":"Storage Create the large file Read the large file Create many small files Read many small files EFS real 69m25.583s user 0m0.015s sys 11m30.451s real 64m20.614s user 0m48.233s sys 3m15.074s real 59m18.137s user 0m37.185s sys 8m29.882s real 33m19.459s user 2m23.134s sys 2m18.345s LUSTRE real 38m32.383s user 0m0.014s sys 36m40.821s real 20m45.156s user 0m59.189s sys 19m26.054s real 25m38.531s user 0m21.820s sys 16m59.318s real 24m58.620s user 2m11.449s sys 11m57.240s BTRFS on EBS real 38m50.438s user 0m0.028s sys 38m45.451s real 27m55.173s user 0m52.903s sys 4m26.061s real 20m54.831s user 0m20.394s sys 20m34.926s real 12m50.153s user 2m5.149s sys 1m18.555s LizardFS on EBS real 48m47.367s user 0m0.020s sys 40m17.341s real 32m21.257s user 0m57.588s sys 5m32.215s real 28m12.707s user 1m30.504s sys 12m40.881s real 15m31.591s user 2m21.772s sys 2m31.211s","title":"4 threads"},{"location":"manual/Appendix_F/Appendix_F._%D0%A1omparison_of_using_different_FS_storages_%28FSx_for_Lustre_vs_EFS_in_AWS%29/#8-threads","text":"Storage Create the large file Read the large file Create many small files Read many small files EFS real 127m49.718s user 0m0.010s sys 14m44.358s real 122m46.188s user 1m12.786s sys 21m14.236s real 72m43.596s user 0m26.727s sys 15m0.582s real 62m46.118s user 2m31.595s sys 2m31.577s LUSTRE real 93m56.846s user 0m0.018s sys 90m30.5s real 94m1.258s user 0m48.908s sys 3m18.557s real 50m42.845s user 0m23.462s sys 35m12.511s real 30m53.199s user 0m54.020s sys 14m42.712s BTRFS on EBS real 87m48.066s user 0m0.016s sys 86m15.610s real 39m59.167s user 0m50.900s sys 4m25.582s real 44m36.847s user 0m24.491s sys 43m11.719s real 17m12.744s user 2m14.324s sys 1m50.667s LizardFS on EBS real 97m25.045s user 0m0.007s sys 73m32.042s real 39m59.167s user 0m50.900s sys 4m25.582s real 50m25.868s user 0m58.079s sys 19m12.443s real 27m50.506s user 2m26.704s sys 3m9.926s As we can see from the presented results, Amazon FSx for Lustre is several times faster (from 1.3 to 3.2 times for read mode and from 1.3 to 4 times for write mode) than Amazon EFS . As expected, the local systems performed better than FSx for Lustre or EFS generally. But it should be noted, that the FSx for Lustre was comparable to local systems in some cases.","title":"8 threads"},{"location":"manual/Appendix_F/Appendix_F._%D0%A1omparison_of_using_different_FS_storages_%28FSx_for_Lustre_vs_EFS_in_AWS%29/#real-data-experiment","text":"The cellranger count pipeline was used to conduct an experiment with real data. The input data: 15Gb transcriptome reference 50Gb fastqs The command that was used to run cellranger count pipeline: /path-to-cellranger/3.0.2/bin/cellranger count --localcores=8 --id={id} --transcriptome=/path-to-transcriptome-reference/refdata-cellranger-mm10-3.0.0 --chemistry=SC5P-R2 --fastqs=/path-to-fastqs/fastqs_test --sample={sample_name} The experimental run result is presented in the following table: Storage Execution time EFS real 207m15.566s user 413m32.168s sys 13m40.581s LUSTRE real 189m23.586s user 434m6.950s sys 13m2.902s BTRFS on EBS real 187m23.048s user 413m32.666s sys 12m30.285s LizardFS on EBS real 189m8.210s user 412m6.558s sys 14m18.429s The best result was shown by the BTRFS on EBS local system. However, it can be said that BTRFS on EBS , LizardFS on EBS , and FSx for Lustre are showed the comparable time. Amazon FSx for Lustre was faster Amazon EFS just to ~ 9%.","title":"Real data experiment"},{"location":"manual/Appendix_F/Appendix_F._%D0%A1omparison_of_using_different_FS_storages_%28FSx_for_Lustre_vs_EFS_in_AWS%29/#costs","text":"Cost calculations have been performed in according to Amazon pricing at the time of this document. For the experiments were used the storages that were created in the US East (N.Virginia) region with similar features: Storage Storage size Throughput mode EFS Size in EFS Standard: 1 TiB (100%) Bursting: 50 MB/s/TiB LUSTRE SSD: 1.2 TiB Capacity 50 MB/s/TiB baseline, up to 1.3 GB/s/TiB burst BTRFS on EBS SSD: 1.2 TiB Capacity Max Throughput/Instance - 4,750 MB/s LizardFS on EBS SSD: 1.2 TiB Capacity Max Throughput/Instance - 4,750 MB/s The total charge for the month of usage for various storages is calculated in different ways: EFS 1 TB per month x 1024 GB in a TB = 1024 GB per month (data stored in Standard Storage ) 1 024 GB per month x $0,30 GB-month = $307,20 ( Standard Storage monthly cost) FSx for Lustre $0.14 GB-month / 30 / 24 = $0.000194 GB-hour 1228 GB x $0.000194 GB-hour x 720 hours = $171,5 ( FSx for Lustre monthly cost) BTRFS on EBS and LizardFS on EBS (1228 GB x $0.10 GB-month * 86400 seconds (for 24 hours)) / (86,400 seconds/day * 30 day-month) = $4 (for the volume) As seen from the calculation, the most beneficial is to use BTRFS on EBS and LizardFS on EBS local systems. However, this is not suitable for long term storage. In this case, EFS monthly cost is more expensive than Amazon FSx for Lustre monthly cost in 1.8 times for similar features storage at the time of this document.","title":"Costs"},{"location":"release_notes/v.0.13/v.0.13_-_Release_notes/","text":"Cloud Pipeline v.0.13 - Release notes Data sharing with external collaborators Batch processing in EU Running instances sharing with other user(s) or group(s) of users Automated instances pause/stop according to the resource usage Tools versions details Data download from external http/ftp resources to the cloud data storage Automatically rerun a batch job if a spot instance is terminated Displaying estimated price of a job run Displaying details of the user profile Breadcrumbs for the Library view Data sharing with external collaborators NGS users often get the raw datasets from the external partners for processing. Typically external collaborator sends such datasets using hard drives. To enable such type of collaboration - S3 buckets within a Cloud Platform can now be \"Shared\". When a bucket is created - owner can set \"Enable sharing\" option. Bucket will be created and can be managed and consumed as any other bucket: But such types of buckets also display a \"Share\" button, which can be used to generate URL, that can be shared with the external collaborator Click \"Share\" Get the URL and send it to the external partner Once external users loads this URL: Authentication is performed using SAML Access is granted according to the user's permissions S3 bucket browser is displayed This collaboration space can be used to exchange large data files (up to 5Tb per one file) Compared to the \" Data download from external http/ftp resources to the cloud data storage \" feature (see below) - this use case considers that external colleague cannot provide a URL for direct download . For more information about data sharing with external collaborators see 8.8. Data sharing . Batch processing in EU Previously all computing nodes and storages were located in the US region. For EU NGS use cases, which operate on huge data volumes, data movement to US took too much time. To workaround this issue - CP Platform was improved to support batch processing and compute nodes management within other Cloud regions. Bucket creation form now allows to set - where to create a data storage: All the buckets, that are shown in the \"Library tree view\", \"Library details form\" and \"Home dashboard\" are now tagged with region flag to visually distinguish storage locations: Library tree Library details view Home dashboard When a pipeline configuration is created within a project or a new run is launched - user can specify to use a specific region for a compute node placement: Project method configuration Launch form configuration Examples of using Cloud regions see in sections 6. Manage Pipeline , 7. Manage Detached configuration , 8. Manage Data Storage . Running instances sharing with other user(s) or group(s) of users For certain use cases it is beneficial to be able to share applications with other users/groups. Thus, providing a \"persistent\" service that can be shared and run in the Cloud platform. Current version introduces a feature that allows to: Specify a \"Friendly URL\" for persistent services. This produces endpoint URL in a more friendly/descriptive format: {cloud-pipeline_url}/ friendly_url instead of {cloud-pipeline_url}/ pipeline-XXXX-XXXX . This can be configured at a service launch time in the \"Advanced\" section of the Launch form (name shall be unique) URL will be generated using the specified name For more information about \"Friendly URL\" for persistent services see here . User can now share a run with others: \"Share with: ...\" parameter, within a run log form, can be used for this Specific users or whole groups can be set for sharing Once this is set - other users will be able to access run's endpoints (not SSH) \"Services\" widget within a Home dashboard page is now capable of listing such \"shared\" services. It is intended to display a \"catalog\" of services, that can be accessed by a current user, without running own jobs. For more information about runs sharing see 11.3. Sharing with other users or groups of users . Automated instances pause/stop according to the resource usage Version v0.12 introduced a PAUSE/RESUME option for the users, which allowed to persist whole state of the environment and resume it. This feature required AWS On-Demand instances to be used, which are more expensive compared to Spots. Current version provides a way to control spendings by automatically pausing on-demand instances if they are not used. Administrators can now control this behaviour using a set of parameters: system.idle.cpu.threshold - specify %% of the average CPU, below which action shall be taken system.resource.monitoring.period - specify period (in seconds) between the users' instances scanning to collect the monitoring metrics system.max.idle.timeout.minutes - specify a duration in minutes. If CPU utilization is below system.idle.cpu.threshold for this duration - notification will be sent to the user. system.idle.action.timeout.minutes - specify a duration in minutes. If CPU utilization is below system.idle.cpu.threshold for this duration - an action, specified in system.idle.action will be performed system.idle.action - which action to perform on the instance, that showed low CPU utilization: NOTIFY - only send notification PAUSE - pause an instance if possible (e.g. instance is On-Demand, Spot instances are skipped) PAUSE_OR_STOP - pause an instance if it is On-Demand, stop an instance if it is Spot STOP - Stop an instance, disregarding price-type Tools versions details Now more information on the docker image version is available: Image size Modified date Unique identifier (Digest) Corresponding aliases (e.g. if some digest has two aliases) For more information see here . Data download from external http/ftp resources to the cloud data storage Users often get the raw datasets from the external partners for processing. Previously, users had to get the data to the local cluster storage and them upload data to the cloud using clommand-line interface. Now users can provide CSV/TSV files with the external links and submit a data transfer job, so that the files will be moved to the cloud storage in the background. Upload CSV/TSV file and view list of samples with the external links: Select the transfer options: Which S3 bucket to use as a destination Which CSV/TSV columns shall be used to get external URLs (if several columns contain URLs - both can be used) ( optionally ) Select whether to rename resulting path to some other value (can be specified as another column cell, e.g. sample name) ( optionally ) Create new folders within destination if several columns are selected for \"Path fields\" option. E.g. if two columns contain URLs and both are selected - then folders will be created for the corresponding column name and used for appropriate files storage. Whether to update external URL within a table to the new location of the files. If set - http/ftp URLs will be changed to s3 path. Such data structure can be then used for a processing by a pipeline (See below - URLs are changed to the S3-clickable hyperlinks): Once transfer job is finished - files will be located in the selected S3 storage: For more information about data downloading from external http/ftp resources to the CP see 5.5. Download data from external resources to the cloud data storage . Automatically rerun a batch job if a spot instance is terminated In certain cases - AWS may terminate a node, that is used to run a job or an interactive tool: Spot prices changed AWS experienced a hardware issue These cases shall not be treated as a Cloud Platform bug. To make it more explicit, the following features are implemented: If a job fails due to server-related issue - a more friendly message is displayed, describing a reason for the hardware failure: If a batch job fails due to server-related issue and AWS reports one of the following EC2 status codes - batch job is restarted from scratch: Server.SpotInstanceShutdown - AWS stopped a spot instance due to price changes Server.SpotInstanceTermination - AWS terminated a spot instance due to price changes Server.InternalError - AWS hardware issue Administrator can configure whether to apply this behavior and how much retries shall be performed: For more information about automatically reruns batch jobs in cases when spot instances are terminated see here and in section 12.10. Manage system-level settings . Displaying estimated price of a job run Now a list of active runs (both \"ACTIVE RUNS\" menu and \"Dashboard\") shows \"Estimated price\", which is calculated based on the run duration and selected instance type. This field is updated interactively (i.e. each 5 - 10 seconds) ACTIVE RUNS menu Dashboard widget For more information and examples of using see in sections 11. Manage Runs , 18. Home page . Displaying details of the user profile Previously only user id was shown within all GUI forms, that displayed object/run OWNER. Now one can get information on the user name/email when hovering user id, which is shown in the GUI. It is shown in the tooltip with all information available from the IdP. Breadcrumbs for the Library view Previously user was able to collapse a \"Library\" tree view using button. This allows to work with the plain objects lists, which is more comfortable for certain users, compared to the hierarchy. But navigation to the upper level of the hierarchy was not convenient in a collapsed mode. Now breadcrumbs are shown in the header of the plain objects list, which allow to view current path and navigate to the upper level by clicking a path item. Expanded mode (default) Collapsed mode","title":"v.0.13"},{"location":"release_notes/v.0.13/v.0.13_-_Release_notes/#cloud-pipeline-v013-release-notes","text":"Data sharing with external collaborators Batch processing in EU Running instances sharing with other user(s) or group(s) of users Automated instances pause/stop according to the resource usage Tools versions details Data download from external http/ftp resources to the cloud data storage Automatically rerun a batch job if a spot instance is terminated Displaying estimated price of a job run Displaying details of the user profile Breadcrumbs for the Library view","title":"Cloud Pipeline v.0.13 - Release notes"},{"location":"release_notes/v.0.13/v.0.13_-_Release_notes/#data-sharing-with-external-collaborators","text":"NGS users often get the raw datasets from the external partners for processing. Typically external collaborator sends such datasets using hard drives. To enable such type of collaboration - S3 buckets within a Cloud Platform can now be \"Shared\". When a bucket is created - owner can set \"Enable sharing\" option. Bucket will be created and can be managed and consumed as any other bucket: But such types of buckets also display a \"Share\" button, which can be used to generate URL, that can be shared with the external collaborator Click \"Share\" Get the URL and send it to the external partner Once external users loads this URL: Authentication is performed using SAML Access is granted according to the user's permissions S3 bucket browser is displayed This collaboration space can be used to exchange large data files (up to 5Tb per one file) Compared to the \" Data download from external http/ftp resources to the cloud data storage \" feature (see below) - this use case considers that external colleague cannot provide a URL for direct download . For more information about data sharing with external collaborators see 8.8. Data sharing .","title":"Data sharing with external collaborators"},{"location":"release_notes/v.0.13/v.0.13_-_Release_notes/#batch-processing-in-eu","text":"Previously all computing nodes and storages were located in the US region. For EU NGS use cases, which operate on huge data volumes, data movement to US took too much time. To workaround this issue - CP Platform was improved to support batch processing and compute nodes management within other Cloud regions. Bucket creation form now allows to set - where to create a data storage: All the buckets, that are shown in the \"Library tree view\", \"Library details form\" and \"Home dashboard\" are now tagged with region flag to visually distinguish storage locations: Library tree Library details view Home dashboard When a pipeline configuration is created within a project or a new run is launched - user can specify to use a specific region for a compute node placement: Project method configuration Launch form configuration Examples of using Cloud regions see in sections 6. Manage Pipeline , 7. Manage Detached configuration , 8. Manage Data Storage .","title":"Batch processing in EU"},{"location":"release_notes/v.0.13/v.0.13_-_Release_notes/#running-instances-sharing-with-other-users-or-groups-of-users","text":"For certain use cases it is beneficial to be able to share applications with other users/groups. Thus, providing a \"persistent\" service that can be shared and run in the Cloud platform. Current version introduces a feature that allows to: Specify a \"Friendly URL\" for persistent services. This produces endpoint URL in a more friendly/descriptive format: {cloud-pipeline_url}/ friendly_url instead of {cloud-pipeline_url}/ pipeline-XXXX-XXXX . This can be configured at a service launch time in the \"Advanced\" section of the Launch form (name shall be unique) URL will be generated using the specified name For more information about \"Friendly URL\" for persistent services see here . User can now share a run with others: \"Share with: ...\" parameter, within a run log form, can be used for this Specific users or whole groups can be set for sharing Once this is set - other users will be able to access run's endpoints (not SSH) \"Services\" widget within a Home dashboard page is now capable of listing such \"shared\" services. It is intended to display a \"catalog\" of services, that can be accessed by a current user, without running own jobs. For more information about runs sharing see 11.3. Sharing with other users or groups of users .","title":"Running instances sharing with other user(s) or group(s) of users"},{"location":"release_notes/v.0.13/v.0.13_-_Release_notes/#automated-instances-pausestop-according-to-the-resource-usage","text":"Version v0.12 introduced a PAUSE/RESUME option for the users, which allowed to persist whole state of the environment and resume it. This feature required AWS On-Demand instances to be used, which are more expensive compared to Spots. Current version provides a way to control spendings by automatically pausing on-demand instances if they are not used. Administrators can now control this behaviour using a set of parameters: system.idle.cpu.threshold - specify %% of the average CPU, below which action shall be taken system.resource.monitoring.period - specify period (in seconds) between the users' instances scanning to collect the monitoring metrics system.max.idle.timeout.minutes - specify a duration in minutes. If CPU utilization is below system.idle.cpu.threshold for this duration - notification will be sent to the user. system.idle.action.timeout.minutes - specify a duration in minutes. If CPU utilization is below system.idle.cpu.threshold for this duration - an action, specified in system.idle.action will be performed system.idle.action - which action to perform on the instance, that showed low CPU utilization: NOTIFY - only send notification PAUSE - pause an instance if possible (e.g. instance is On-Demand, Spot instances are skipped) PAUSE_OR_STOP - pause an instance if it is On-Demand, stop an instance if it is Spot STOP - Stop an instance, disregarding price-type","title":"Automated instances pause/stop according to the resource usage"},{"location":"release_notes/v.0.13/v.0.13_-_Release_notes/#tools-versions-details","text":"Now more information on the docker image version is available: Image size Modified date Unique identifier (Digest) Corresponding aliases (e.g. if some digest has two aliases) For more information see here .","title":"Tools versions details"},{"location":"release_notes/v.0.13/v.0.13_-_Release_notes/#data-download-from-external-httpftp-resources-to-the-cloud-data-storage","text":"Users often get the raw datasets from the external partners for processing. Previously, users had to get the data to the local cluster storage and them upload data to the cloud using clommand-line interface. Now users can provide CSV/TSV files with the external links and submit a data transfer job, so that the files will be moved to the cloud storage in the background. Upload CSV/TSV file and view list of samples with the external links: Select the transfer options: Which S3 bucket to use as a destination Which CSV/TSV columns shall be used to get external URLs (if several columns contain URLs - both can be used) ( optionally ) Select whether to rename resulting path to some other value (can be specified as another column cell, e.g. sample name) ( optionally ) Create new folders within destination if several columns are selected for \"Path fields\" option. E.g. if two columns contain URLs and both are selected - then folders will be created for the corresponding column name and used for appropriate files storage. Whether to update external URL within a table to the new location of the files. If set - http/ftp URLs will be changed to s3 path. Such data structure can be then used for a processing by a pipeline (See below - URLs are changed to the S3-clickable hyperlinks): Once transfer job is finished - files will be located in the selected S3 storage: For more information about data downloading from external http/ftp resources to the CP see 5.5. Download data from external resources to the cloud data storage .","title":"Data download from external http/ftp resources to the cloud data storage"},{"location":"release_notes/v.0.13/v.0.13_-_Release_notes/#automatically-rerun-a-batch-job-if-a-spot-instance-is-terminated","text":"In certain cases - AWS may terminate a node, that is used to run a job or an interactive tool: Spot prices changed AWS experienced a hardware issue These cases shall not be treated as a Cloud Platform bug. To make it more explicit, the following features are implemented: If a job fails due to server-related issue - a more friendly message is displayed, describing a reason for the hardware failure: If a batch job fails due to server-related issue and AWS reports one of the following EC2 status codes - batch job is restarted from scratch: Server.SpotInstanceShutdown - AWS stopped a spot instance due to price changes Server.SpotInstanceTermination - AWS terminated a spot instance due to price changes Server.InternalError - AWS hardware issue Administrator can configure whether to apply this behavior and how much retries shall be performed: For more information about automatically reruns batch jobs in cases when spot instances are terminated see here and in section 12.10. Manage system-level settings .","title":"Automatically rerun a batch job if a spot instance is terminated"},{"location":"release_notes/v.0.13/v.0.13_-_Release_notes/#displaying-estimated-price-of-a-job-run","text":"Now a list of active runs (both \"ACTIVE RUNS\" menu and \"Dashboard\") shows \"Estimated price\", which is calculated based on the run duration and selected instance type. This field is updated interactively (i.e. each 5 - 10 seconds) ACTIVE RUNS menu Dashboard widget For more information and examples of using see in sections 11. Manage Runs , 18. Home page .","title":"Displaying estimated price of a job run"},{"location":"release_notes/v.0.13/v.0.13_-_Release_notes/#displaying-details-of-the-user-profile","text":"Previously only user id was shown within all GUI forms, that displayed object/run OWNER. Now one can get information on the user name/email when hovering user id, which is shown in the GUI. It is shown in the tooltip with all information available from the IdP.","title":"Displaying details of the user profile"},{"location":"release_notes/v.0.13/v.0.13_-_Release_notes/#breadcrumbs-for-the-library-view","text":"Previously user was able to collapse a \"Library\" tree view using button. This allows to work with the plain objects lists, which is more comfortable for certain users, compared to the hierarchy. But navigation to the upper level of the hierarchy was not convenient in a collapsed mode. Now breadcrumbs are shown in the header of the plain objects list, which allow to view current path and navigate to the upper level by clicking a path item. Expanded mode (default) Collapsed mode","title":"Breadcrumbs for the Library view"},{"location":"release_notes/v.0.14/v.0.14_-_Release_notes/","text":"Cloud Pipeline v.0.14 - Release notes Docker image installed packages list Docker image version settings Global Search Default values (restrictions) on instance types for the users (groups) and tools Auto-scaled cluster A mandatory prefix for the new creating S3-buckets \"White list\" for docker images \"Grace\" period for unscanned or vulnerable docker images Docker image installed packages list Often users would like to know the full list software packages installed into a specific Docker images. E.g. to decide which one to run. Now this information is available from the docker version menu. User can click a specific version of the tool \" Packages \" tab will shown in the list of tabs in menu of tool's version. List of packages is generated from the docker version together with vulnerabilities scanning (introduced in v0.12). This occurs nightly (all dockers are scanned) or if admin explicitly requests scanning by clicking SCAN button for a specific version. Currently the following types of software packages can be scanned: System package manager's database (i.e. yum , apt ) R packages Python packages Software packages are combined into groups named \" Ecosystems \". User can select one of the ecosystems in the dropdown list and view its contents: Information about each package contains the package name and its short description ( if available ). User can filter packages by name (search will be done across all ecosystems ) For more details see here . Docker image version settings Previously docker image settings (instance type, disk, default command) were set using the following approaches: Global defaults Docker image settings Specific run launch parameters This introduces a number of limitations, e.g. if a docker image contains two version: one with CPU-only and another with GPU support, user was not able to define which instance type to use for what version. Now settings can be applied to the specific docker version. These settings are defined in the \" Settings \" tab of the tool's version menu: If these (version-level) settings are specified - they will be applied to each run of the docker image: All other settings levels are still remaining in place. If version-specific settings are not defined: docker-level settings will be applied. For more details see here . Global Search While Cloud Pipeline grows in terms of data and pipelines being add/implemented - it is crucial to be able to search for specific datasets or tools. Previously user was able to search only for the jobs runs. In v0.14 of the Cloud Pipeline Platform - a capability to search over all existing objects types is implemented. The following object types are being indexed and can be searched: \" FOLDERS \" - in the folders (\"Library hierarchy\") and metadata entities \" PIPELINES \" - in the pipelines metadata, pipelines files (documents) and configurations \" RUNS \" - in the runs information \" TOOLS \" - in the docker registries, groups and tools \" STORAGES \" - in S3/NFS storages metadata, S3/NFS files (names) \" ISSUES \" - in the issues (discussions) User can open a search form by pressing \"Ctrl+F\" being at any page of the Cloud Pipeline Web GUI ( excluding case when run's logs page is open ) or by clicking on in left menu bar ( global searching form will not be opened if any pop-up window is shown ): To start searching a \"google-like\" query string shall be entered (search can be triggered by pressing \"Enter\" button or automatically if no new input is provided for 2 seconds): Special expressions in query string are available as well (the rules for their assignment are described in pop-up window that appears when hovering over the icon): By default search will occur across all the available object types. If user would to limit search scope - appropriate section can be selected above the query input: To get a brief information on the object found, user can hover an item with a mouse and a \"Preview\" pane will be shown to the right or click an entry to navigate to it's location within the Cloud Pipeline. Tool preview: Pipeline preview: In the \"Preview\" window user can see: name of the found object path to the object in library description ( optionally ) block with indication and highlighting of the object's concrete part, where inputted word was found preview of the found object ( if it's available ) More details see here . Default values (restrictions) on instance types for the users (groups) and tools Users may make a mistake while selecting instance types and storages when launching a run. This may be: Too large instance type for the job Spot instance for GPU job Not valid data storage path etc. Now admin can restrict certain options for the specific users/groups or tools to minimize a number of invalid configurations runs. Restrictions could be set by different ways on several forms: within \"User management\" tab (for more information see 12.4. Edit/delete a user and 12.6. Edit a group/role ) admin can specify for a user or a group of users (role) allowed price types and allowed instance types for the pipelines, configurations and tool runs: within \"Instance management\" panel in tool settings (for more information see 10.5. Launch a Tool ) admin can specify allowed price types and allowed instance types for Tool runs: within \"Cluster\" tab in \"Preferences\" section of the system-level settings (see here ) admin also can specify allowed price types (with setting cluster.allowed.price.types ) and allowed instance types for the pipelines, configurations and tool runs (with settings cluster.allowed.instance.types and cluster.allowed.instance.types.docker ) as global defaults: Next hierarchy is set for applying of inputted allowed instance types (sorted by priority): User level (specified for a user on \"User management\" tab) User group level (specified for a group (role) on \"User management\" tab. If a user is a member of several groups - list of allowed instances will be summarized across all the groups) Tool level (specified for a tool on \"Instance management\" panel ) (global) cluster.allowed.instance.types.docker (specified on \"Cluster\" tab in \"Preferences\" section of the system-level settings) (global) cluster.allowed.instance.types (specified on \"Cluster\" tab in \"Preferences\" section of the system-level settings) After specifying allowed instance types, all GUI forms that allow to select the list of instance types (configurations/launch forms) - will display only valid instance type, according to hierarchy above. For price type specifying - if it is set for the user/group/tool - GUI will allow to select only that price type. Auto-scaled cluster Previously, when you needed several nodes running at one task, you could configure and launch cluster: master machine and one or several worker machines. Their count was predefined before launching the task and could not changing during the run. In current version, the auto-scaled cluster is implemented. This one differs from \"usual\" cluster in that it can attach or drop additional nodes during the run depending on the queue load. Using such type of cluster will minimize total cost. If during the run there are jobs in waiting state longer than a specific time ( this time threshold is set by admin in \"Preferences\" section of the system-level settings ) - auto-scaled cluster will attach new computation nodes (\"scale-up\"). If during the run the queue is empty or all jobs are running longer than a specific time ( this time threshold is set by admin tab in \"Preferences\" section of the system-level settings ) - auto-scaled cluster will drop existing auto-scaled nodes (\"scale-down\"). For this cluster user may specify total count of child nodes - max count of auto-scaled nodes, and the count of \"persistent\" child nodes - count of nodes that will never be \"scaled-down\". To set auto-scaled cluster click \"Configure cluster\" in \"Exec environment\" panel before launch a run: In pop-up window select \"Auto-scaled cluster\" and specify total count of \"auto-scaled\" nodes: If you want to set a count of \"persistent\" child nodes, click \"Setup default child nodes count\" and specify the value: For more details see here and here . A mandatory prefix for the new creating S3-buckets Now admin can set storage.object.prefix on \"Data storage\" tab in \"Preferences\" section of the system-level settings: If it is set, all new storages will be created with this prefix (e.g. \" ds \"): For more information see 8.1. Create and edit storage . \"White list\" for docker images Previously user is not able to run a new image if it is not scanned yet or has a lot of vulnerabilities. In current version a \"white list\" option for docker images is implemented. It's meaning that admin may allow users to run certain docker versions even if they are vulnerable/unscanned. For do that admin has to check a docker version in tool-versions list by click on special flag \"Add to white list\" near the \"SCAN\" button: In this case user would be able to run such version of tool and none errors will be displayed during launch time and viewing. Only admin can \"add\"/\"remove\" tool version to the \"white list\". For more details see 10.6. Tool security check . \"Grace\" period for unscanned or vulnerable docker images Previously user is not able to run a new image if it is not scanned yet or has a lot of vulnerabilities. In current version a \"grace\" period option for such docker images is implemented. During this period user will be able to run a tool, but an appropriate message will be displayed when viewing a tool or running it. The duration of \"grace\" period (in hours ) is set on \"Docker security\" tab in \"Preferences\" section of the system-level settings: If this time value is not elapsed from the date/time since the docker version became vulnerable or since the push time (if this version was not scanned yet) - user would be able to run such version of tool, but warning message will be displaying during version launch. Since \"grace\" period is elapsed for this tool's version - behavior will became as for \"usual\" vulnerable/unscanned docker image. For more details see 10.6. Tool security check and 12.10. Manage the system-level settings .","title":"v.0.14"},{"location":"release_notes/v.0.14/v.0.14_-_Release_notes/#cloud-pipeline-v014-release-notes","text":"Docker image installed packages list Docker image version settings Global Search Default values (restrictions) on instance types for the users (groups) and tools Auto-scaled cluster A mandatory prefix for the new creating S3-buckets \"White list\" for docker images \"Grace\" period for unscanned or vulnerable docker images","title":"Cloud Pipeline v.0.14 - Release notes"},{"location":"release_notes/v.0.14/v.0.14_-_Release_notes/#docker-image-installed-packages-list","text":"Often users would like to know the full list software packages installed into a specific Docker images. E.g. to decide which one to run. Now this information is available from the docker version menu. User can click a specific version of the tool \" Packages \" tab will shown in the list of tabs in menu of tool's version. List of packages is generated from the docker version together with vulnerabilities scanning (introduced in v0.12). This occurs nightly (all dockers are scanned) or if admin explicitly requests scanning by clicking SCAN button for a specific version. Currently the following types of software packages can be scanned: System package manager's database (i.e. yum , apt ) R packages Python packages Software packages are combined into groups named \" Ecosystems \". User can select one of the ecosystems in the dropdown list and view its contents: Information about each package contains the package name and its short description ( if available ). User can filter packages by name (search will be done across all ecosystems ) For more details see here .","title":"Docker image installed packages list"},{"location":"release_notes/v.0.14/v.0.14_-_Release_notes/#docker-image-version-settings","text":"Previously docker image settings (instance type, disk, default command) were set using the following approaches: Global defaults Docker image settings Specific run launch parameters This introduces a number of limitations, e.g. if a docker image contains two version: one with CPU-only and another with GPU support, user was not able to define which instance type to use for what version. Now settings can be applied to the specific docker version. These settings are defined in the \" Settings \" tab of the tool's version menu: If these (version-level) settings are specified - they will be applied to each run of the docker image: All other settings levels are still remaining in place. If version-specific settings are not defined: docker-level settings will be applied. For more details see here .","title":"Docker image version settings"},{"location":"release_notes/v.0.14/v.0.14_-_Release_notes/#global-search","text":"While Cloud Pipeline grows in terms of data and pipelines being add/implemented - it is crucial to be able to search for specific datasets or tools. Previously user was able to search only for the jobs runs. In v0.14 of the Cloud Pipeline Platform - a capability to search over all existing objects types is implemented. The following object types are being indexed and can be searched: \" FOLDERS \" - in the folders (\"Library hierarchy\") and metadata entities \" PIPELINES \" - in the pipelines metadata, pipelines files (documents) and configurations \" RUNS \" - in the runs information \" TOOLS \" - in the docker registries, groups and tools \" STORAGES \" - in S3/NFS storages metadata, S3/NFS files (names) \" ISSUES \" - in the issues (discussions) User can open a search form by pressing \"Ctrl+F\" being at any page of the Cloud Pipeline Web GUI ( excluding case when run's logs page is open ) or by clicking on in left menu bar ( global searching form will not be opened if any pop-up window is shown ): To start searching a \"google-like\" query string shall be entered (search can be triggered by pressing \"Enter\" button or automatically if no new input is provided for 2 seconds): Special expressions in query string are available as well (the rules for their assignment are described in pop-up window that appears when hovering over the icon): By default search will occur across all the available object types. If user would to limit search scope - appropriate section can be selected above the query input: To get a brief information on the object found, user can hover an item with a mouse and a \"Preview\" pane will be shown to the right or click an entry to navigate to it's location within the Cloud Pipeline. Tool preview: Pipeline preview: In the \"Preview\" window user can see: name of the found object path to the object in library description ( optionally ) block with indication and highlighting of the object's concrete part, where inputted word was found preview of the found object ( if it's available ) More details see here .","title":"Global Search"},{"location":"release_notes/v.0.14/v.0.14_-_Release_notes/#default-values-restrictions-on-instance-types-for-the-users-groups-and-tools","text":"Users may make a mistake while selecting instance types and storages when launching a run. This may be: Too large instance type for the job Spot instance for GPU job Not valid data storage path etc. Now admin can restrict certain options for the specific users/groups or tools to minimize a number of invalid configurations runs. Restrictions could be set by different ways on several forms: within \"User management\" tab (for more information see 12.4. Edit/delete a user and 12.6. Edit a group/role ) admin can specify for a user or a group of users (role) allowed price types and allowed instance types for the pipelines, configurations and tool runs: within \"Instance management\" panel in tool settings (for more information see 10.5. Launch a Tool ) admin can specify allowed price types and allowed instance types for Tool runs: within \"Cluster\" tab in \"Preferences\" section of the system-level settings (see here ) admin also can specify allowed price types (with setting cluster.allowed.price.types ) and allowed instance types for the pipelines, configurations and tool runs (with settings cluster.allowed.instance.types and cluster.allowed.instance.types.docker ) as global defaults: Next hierarchy is set for applying of inputted allowed instance types (sorted by priority): User level (specified for a user on \"User management\" tab) User group level (specified for a group (role) on \"User management\" tab. If a user is a member of several groups - list of allowed instances will be summarized across all the groups) Tool level (specified for a tool on \"Instance management\" panel ) (global) cluster.allowed.instance.types.docker (specified on \"Cluster\" tab in \"Preferences\" section of the system-level settings) (global) cluster.allowed.instance.types (specified on \"Cluster\" tab in \"Preferences\" section of the system-level settings) After specifying allowed instance types, all GUI forms that allow to select the list of instance types (configurations/launch forms) - will display only valid instance type, according to hierarchy above. For price type specifying - if it is set for the user/group/tool - GUI will allow to select only that price type.","title":"Default values (restrictions) on instance types for the users (groups) and tools"},{"location":"release_notes/v.0.14/v.0.14_-_Release_notes/#auto-scaled-cluster","text":"Previously, when you needed several nodes running at one task, you could configure and launch cluster: master machine and one or several worker machines. Their count was predefined before launching the task and could not changing during the run. In current version, the auto-scaled cluster is implemented. This one differs from \"usual\" cluster in that it can attach or drop additional nodes during the run depending on the queue load. Using such type of cluster will minimize total cost. If during the run there are jobs in waiting state longer than a specific time ( this time threshold is set by admin in \"Preferences\" section of the system-level settings ) - auto-scaled cluster will attach new computation nodes (\"scale-up\"). If during the run the queue is empty or all jobs are running longer than a specific time ( this time threshold is set by admin tab in \"Preferences\" section of the system-level settings ) - auto-scaled cluster will drop existing auto-scaled nodes (\"scale-down\"). For this cluster user may specify total count of child nodes - max count of auto-scaled nodes, and the count of \"persistent\" child nodes - count of nodes that will never be \"scaled-down\". To set auto-scaled cluster click \"Configure cluster\" in \"Exec environment\" panel before launch a run: In pop-up window select \"Auto-scaled cluster\" and specify total count of \"auto-scaled\" nodes: If you want to set a count of \"persistent\" child nodes, click \"Setup default child nodes count\" and specify the value: For more details see here and here .","title":"Auto-scaled cluster"},{"location":"release_notes/v.0.14/v.0.14_-_Release_notes/#a-mandatory-prefix-for-the-new-creating-s3-buckets","text":"Now admin can set storage.object.prefix on \"Data storage\" tab in \"Preferences\" section of the system-level settings: If it is set, all new storages will be created with this prefix (e.g. \" ds \"): For more information see 8.1. Create and edit storage .","title":"A mandatory prefix for the new creating S3-buckets"},{"location":"release_notes/v.0.14/v.0.14_-_Release_notes/#white-list-for-docker-images","text":"Previously user is not able to run a new image if it is not scanned yet or has a lot of vulnerabilities. In current version a \"white list\" option for docker images is implemented. It's meaning that admin may allow users to run certain docker versions even if they are vulnerable/unscanned. For do that admin has to check a docker version in tool-versions list by click on special flag \"Add to white list\" near the \"SCAN\" button: In this case user would be able to run such version of tool and none errors will be displayed during launch time and viewing. Only admin can \"add\"/\"remove\" tool version to the \"white list\". For more details see 10.6. Tool security check .","title":"\"White list\" for docker images"},{"location":"release_notes/v.0.14/v.0.14_-_Release_notes/#grace-period-for-unscanned-or-vulnerable-docker-images","text":"Previously user is not able to run a new image if it is not scanned yet or has a lot of vulnerabilities. In current version a \"grace\" period option for such docker images is implemented. During this period user will be able to run a tool, but an appropriate message will be displayed when viewing a tool or running it. The duration of \"grace\" period (in hours ) is set on \"Docker security\" tab in \"Preferences\" section of the system-level settings: If this time value is not elapsed from the date/time since the docker version became vulnerable or since the push time (if this version was not scanned yet) - user would be able to run such version of tool, but warning message will be displaying during version launch. Since \"grace\" period is elapsed for this tool's version - behavior will became as for \"usual\" vulnerable/unscanned docker image. For more details see 10.6. Tool security check and 12.10. Manage the system-level settings .","title":"\"Grace\" period for unscanned or vulnerable docker images"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/","text":"Cloud Pipeline v.0.15 - Release notes Microsoft Azure Support SAML claims based access Notifications on the RAM/Disk pressure Limit mounted storages Personal SSH keys configuration Allow to set the Grid Engine capability for the \"fixed\" cluster Enable Apache Spark for the Cloud Pipeline's clusters Consider Cloud Providers' resource limitations when scheduling a job Allow to terminate paused runs Pre/Post-commit hooks implementation Restricting manual installation of the nvidia tools Setup swap files for the Cloud VMs Run's system paths shall be available for the general user account Renewed WDL visualization \"QUEUED\" state of the run Help tooltips for the run state icons VM monitor service Web GUI caching Installation via pipectl Add more logging to troubleshoot unexpected pods failures Displaying information on the nested runs Environment Modules support Sharing SSH access to running instances with other user(s)/group(s) Allow to limit the number of concurrent SSH sessions Verification of docker/storage permissions when launching a run Ability to override the queue/PE configuration in the GE configuration Estimation run's disk size according to the input/common parameters Disabling of the Global Search form if a corresponding service is not installed Disabling of the FS mounts creation if no FS mount points are registered Displaying resource limit errors during run resuming Object storage creation in despite of that the CORS/Policies could not be applied Track the confirmation of the \"Blocking\" notifications pipe CLI warnings on the JWT expiration pipe configuration for using NTLM Authentication Proxy Files uploading via pipe in case of restrictions Run a single command or an interactive session over the SSH protocol via pipe Perform objects restore in a batch mode via pipe Mounting data storages to Linux and Mac workstations Allow to run pipe commands on behalf of the other user Ability to restrict the visibility of the jobs Ability to perform scheduled runs from detached configurations Using custom domain names as a \"friendly URL\" for the interactive services Displaying of the additional support icon/info Pass proxy settings to the DIND containers Interactive endpoints can be (optionally) available to the anonymous users Notable Bug fixes Incorrect behavior of the global search filter \"COMMITING...\" status hangs Instances of Metadata entity aren't correctly sorted Tool group cannot be deleted until all child tools are removed Missing region while estimating a run price Cannot specify region when an existing object storage is added ACL control for PIPELINE_USER and ROLE entities for metadata API Getting logs from Kubernetes may cause OutOfMemory error AWS: Incorrect nodeup handling of spot request status Not handling clusters in autopause daemon Incorrect pipe CLI version displaying JWT token shall be updated for the jobs being resumed Trying to rename file in the data storage, while the \"Attributes\" panel is opened, throws an error pipe : incorrect behavior of the -nc option for the run command Cluster run cannot be launched with a Pretty URL Cloning of large repositories might fail System events HTML overflow AWS: Pipeline run InitializeNode task fails git-sync shall not fail the whole object synchronization if a single entry errors endDate isn't set when node of a paused run was terminated AWS: Nodeup retry process may stuck when first attempt to create a spot instance failed Resume job timeout throws strange error message GE autoscaler doesn't remove dead additional workers from cluster Broken layouts Microsoft Azure Support One of the major v0.15 features is a support for the Microsoft Azure Cloud . All the features, that were previously used for AWS , are now available in all the same manner, from all the same GUI/CLI, for Azure . Another cool thing, is that now it is possible to have a single installation of the Cloud Pipeline , which will manage both Cloud Providers ( AWS and Azure ). This provides a flexibility to launch jobs in the locations, closer to the data, with cheaper prices or better compute hardware for a specific task. SAML claims based access Previously, there were two options for users to be generally authenticated in Cloud Pipeline Platform, i.e. to pass SAML validation: Auto-register - any valid SAML authentication automatically registers user (if he isn't registered yet) and grant ROLE_USER access Explicit-register - after a valid SAML authentication, it is checked whether this user is already registered in the Cloud Pipeline catalog, and if no - request denies, authentication fails To automate things a bit more, in v0.15 additional way to grant first-time access to the users was implemented: Explicit-group register. If such registration type is set - once a valid SAML authentication is received, it is checked, whether SAML response contains any of the domain groups, that are already granted any access to the Cloud Pipeline objects (registered in Cloud Pipeline catalog). If so - proceeds as with Auto-register - user with all his domain groups and granted ROLE_USER access is being registered. If user's SAML domain groups aren't intersected with pre-registered groups the authentication fails. This Platform's behavior is set via application property saml.user.auto.create that could accept one of the corresponding values: AUTO , EXPLICIT , EXPLICIT_GROUP . Notifications on the RAM/Disk pressure When a compute-intensive job is run - compute node may start starving for the resources. CPU high load is typically a normal situation - it could result just to the SSH/metrics slowdown. But running low on memory and disk could crash the node, in such cases autoscaler will eventually terminate the cloud instance. In v0.15 version, the Cloud Pipeline platform could notify user on the fact of Memory/Disk high load. When memory or disk consuming will be higher than a threshold value for a specified period of time (in average) - a notification will be sent (and resent after a delay, if the problem is still in place). Such notifications could be configured at HIGH_CONSUMED_RESOURCES section of the Email notifications : The following items at System section of the Preferences define behavior of such notifications: system.memory.consume.threshold - memory threshold (in %) above which the notification would be sent system.disk.consume.threshold - disk threshold (in %) above which the notification would be sent system.monitoring.time.range - time delay (in sec) after which the notification would be sent again, if the problem is still in place. See more information about Email notifications and System Preferences . Limit mounted storages Previously, all available storages were mounted to the container during the run initialization. User could have access to them via /cloud-data or ~/cloud-data folder using the interactive sessions (SSH/Web endpoints/Desktop) or pipeline runs. For certain reasons (e.g. takes too much time to mount all or a run is going to be shared with others), user may want to limit the number of data storages being mounted to a specific job run. Now, user can configure the list of the storages that will be mounted. This can be accomplished using the Limit mount field of the Launch form: By default, All available storages are mounted (i.e. the ones, that user has ro or rw permissions) To change the default behavior - click the drop-down list next to \"Limit mounts\" label: Select storages that shall be mounted: Review that only a limited number of data storages is mounted: Mounted storage is available for the interactive/batch jobs using the path /cloud-data/{storage_name} : See an example here . Personal SSH keys configuration Previously, there were two options to communicate with the embedded gitlab repositories, that host pipelines code: From the local (non-cloud) machine: use the https protocol and the repository URI, provided by the GUI (i.e. GIT REPOSITORY button in the pipeline view) From the cloud compute node: git command line interface, that is preconfigured to authenticate using https protocol and the auto-generated credentials Both options consider that https is used. It worked fine for 99% of the use cases. But for some of the applications - ssh protocol is required as this is the only way to achieve a passwordless authentication against the gitlab instance To address this issue, SSH keys management was introduced: Users' ssh keys are generated by the Git Sync service SSH key-pair is created and assigned to the Cloud Pipeline user and a public key is registered in the GitLab Those SSH keys are now also configured for all the runs, launched by the user. So it is possible to perform a passwordless authentication, when working with the gitlab or other services, that will be implemented in the near future HTTPS/SSH selector is added to the GIT REPOSITORY popup of the Cloud Pipeline Web GUI: Default selection is HTTP, which displays the same URI as previously (repository), but user is able to switch it to the SSH and get the reformatted address: For more details see here . Allow to set the Grid Engine capability for the \"fixed\" cluster Version 0.14 introduced an ability to launch autoscaled clusters. Besides the autoscaling itself - such cluster were configured to use GridEngine server by default, which is pretty handy. On the other hand - fixed cluster (i.e. those which contain a predefined number of compute nodes), required user to set the CP_CAP_SGE explicitly. Which is no a big deal, but may be tedious. In v0.15 Grid Engine can be configured within the Cluster (fixed size cluster) tab. This is accomplished by using the Enable GridEngine checkbox. By default, this checkbox is unticked. If the user sets this ON - CP_CAP_SGE parameter is added automatically. Also a number of help icons is added to the Cluster configuration dialog to clarify the controls purpose: Popup header (E.g. next to the tabs line) - displays information on different cluster modes (Cluster) Enable GridEngine checkbox - displays information on the GridEngine usage (Cluster) Enable Apache Spark checkbox - displays information on the Apache Spark usage (see below ) (Auto-scaled cluster) Auto-scaled up - displays information on the autoscaling logic (Auto-scaled cluster) Default child nodes - displays information on the initial node pool size See more information about cluster launch here . Enable Apache Spark for the Cloud Pipeline's clusters Another one feature for the Cloud Pipeline's clusters was implemented in v0.15 . Now, Apache Spark with the access to File/Object Storages from the Spark Applications can be configured within the Cluster tab. It is available only for the fixed size clusters. To enable this feature - tick the Enable Apache Spark checkbox and set the child nodes count at cluster settings. By default, this checkbox is unticked. Also users can manually enable Spark functionality by the CP_CAP_SPARK system parameter: This feature, for example, allows you to run Apache Spark cluster with RStudio where you may code in R using sparklyr to run the workload over the cluster: Open the RStudio tool Select the node type, set the Apache Spark cluster as shown above, and launch the tool: Open main Dashboard and wait until the OPEN hyperlink for the launched tool will appear. Hover over it: Two endpoints will appear: RStudio (as the main endpoint it is in bold) - it exposes RStudio's Web IDE SparkUI - it exposes Web GUI of the Spark. It allows to monitor Spark master/workers/application via the web-browser. Details are available in the Spark UI manual Click the RStudio endpoint: Here you can start create scripts in R using the pre-installed sparklyr package to distribute the workload over the cluster. Click the SparkUI endpoint: Here you can view the details of the jobs being executed in Spark, how the memory is used and get other useful information. For more information about using Apache Spark via the Cloud Pipeline see here . Consider Cloud Providers' resource limitations when scheduling a job Cloud Pipeline supports queueing of the jobs, that cannot be scheduled to the existing or new compute nodes immediately. Queueing occurs if: cluster.max.size limit is reached (configurable via Preferences ) Cloud Provider limitations are reached (e.g. AWS EC2 Limits ) If ( 1 ) happens - job will sit in the QUEUED state, until a spare will be available or stopped manually. This is the correct behavior. But if ( 2 ) occurs - an error will be raised by the Cloud Provider, Cloud Pipeline treats this as an issue with the node creation. Autoscaler will then resubmit the node creation task for cluster.nodeup.retry.count times (default: 5) and then fail the job run. This behavior confuses users, as ( 2 ) shall behave almost in the same manner as ( 1 ) - job shall be kept in the queue until there will be free space for a new node. In v0.15 ( 2 ) scenario works as described: If a certain limit is reached (e.g. number of m4.large instances exceeds the configured limit) - run will not fail, but will await for a spare node or limit increase A warning will be highlighted in the job initialization log: Allow to terminate paused runs Some of the jobs, that were paused (either manually, or by the automated service), may be not needed anymore. But when a run is paused - the user cannot terminate/stop it before resuming. I.e. one have to run RESUME operation, wait for it's completion and then STOP the run. While this is the expected behavior (at least designed in this manner) - it requires some extra steps to be performed, which may look like meaningless (why one shall resume a run, that is going to be stopped?). Another issue with such a behavior is that in certain \"bad\" conditions - paused runs are not able to resume and just fail, e.g.: An underlying instance is terminated outside of the Cloud Pipeline Docker image was removed from the registry And other cases that are not yet uncovered This introduces a number of stale runs, that just sit there in the PAUSED state and nobody can remove them. To address those concerns - current version of Cloud Pipeline allows to terminate PAUSED run, without a prior RESUME . This operation can be performed by the OWNER of the run and the ADMIN users. Termination of the PAUSED run drops the underlying cloud instance and marks the run as STOPPED . From the GUI perspective - TERMINATE button is shown (instead of STOP ), when a run is in the PAUSED state: on the \"Active runs\" page on the \"Run logs\" page on the \"Dashboard\" page Clicking it - performs the run termination, as described above. See more information here . Pre/Post-commit hooks implementation In certain use-cases, extra steps shall be executed before/after running the commit command in the container. E.g. imagine the following scenario: User launches RStudio User installs packages and commits it as a new image User launches the new image The following error will be displayed in the R Console: 16 Jan 2019 21:17:15 [rsession-GRODE01] ERROR session hadabend; LOGGED FROM: rstudio::core::Error {anonymous}::rInit(const rstudio::r::session::RInitInfo&) /home/ubuntu/rstudio/src/cpp/session/SessionMain.cpp:563 There is nothing bad about this message and states that previous RSession was terminated in a non-graceful manner. RStudio will work correctly, but it may confuse the users. While this is only one example - there are other applications, that require extra cleanup to be performed before the termination. To workaround such issues (RStudio and others) an approach of pre/post-commit hooks is implemented. That allows to perform some graceful cleanup/restore before/after performing the commit itself. Those hooks are valid only for the specific images and therefore shall be contained within those images. Cloud Pipeline itself performs the calls to the hooks if they exist. Two preferences are introduced: commit.pre.command.path : specified a path to a script within a docker image, that will be executed in a currently running container, BEFORE docker commit occurs (default: /root/pre_commit.sh ). This option is useful if any operations shall be performed with the running processes (e.g. send a signal), because in the subsequent post operation - only filesystem operations will be available. Note that any changes done at this stage will affect the running container. commit.post.command.path : specified a path to a script within a docker image, that will be executed in a committed image, AFTER docker commit occurs (default: /root/post_commit.sh ). This hook can be used to perform any filesystem cleanup or other operations, that shall not affect the currently running processes. If a corresponding pre/post script is not found in the docker image - it will not be executed. For more details see here . Restricting manual installation of the nvidia tools It was uncovered that some of the GPU-enabled runs are not able to initialize due to an issue describe at NVIDIA/nvidia-docker#825 . To limit a possibility of producing such docker images (which will not be able to start using GPU instance types) - a set of restrictions/notifications was implemented: A notification is now displayed (in the Web GUI), that warns a user about the risks of installing any of the nvidia packages manually. And that all cuda-based dockers shall be built using nvidia/cuda base images instead: Restrict users (to the reasonable extent) from installing those packages while running SSH/terminal session in the container. If user will try to install a restricted package - a warning will be shown (with an option to bypass it - for the advanced users): Setup swap files for the Cloud VMs This feature is addresses the same issues as the previous Notifications about high resources pressure by making the compute-intensive jobs runs more reliable. In certain cases jobs may fail with unexpected errors if the compute node runs Out Of Memory . v0.15 provides an ability for admin users to configure a default swap volume to the compute node being created. This allows to avoid runs failures due to memory limits. The size of the swap volume can be configured via cluster.networks.config item of the Preferences . It is accomplished by adding the similar json object to the platform's global or a region/cloud specific configuration: Options that can be used to configure swap : swap_ratio - defines a swap file size. It is equal the node RAM multiplied by that ratio. If ratio is 0, a swap file will not be created (default: 0) swap_location - defines a location of the swap file. If that option is not set - default location will be used (default: AWS will use SSD/gp2 EBS, Azure will be Temporary Storage ) See an example here . Run's system paths shall be available for the general user account Previously, all the system-level directories (e.g. pipeline code location - $SCRIPTS_DIR , input/common data folders - $INPUT_DATA , etc.) were owned by the root user with read-only access to the general users. This was working fine for the pipeline runs, as they are executed on behalf of root . But for the interactive sessions (SSH/Web endpoints/Desktop) - such location were not writable. From now on - all the system-level location will be granted rwx access for the OWNER of the job (the user, who launched that run). Renewed WDL visualization v0.15 offers an updated Web GUI viewer/editor for the WDL scripts. These improvements allow to focus on the WDL diagram and make it more readable and clear. Which very useful for large WDL scripts. Auxiliary controls (\"Save\", \"Revert changes\", \"Layout\", \"Fit to screen\", \"Show links\", \"Zoom out\", \"Zoom in\", \"Fullscreen\") are moved to the left side of the WDL GRAPH into single menu: WDL search capabilities are added. This feature allows to search for any task/variable/input/output within the script and focus/zoom to the found element. Search box is on the auxiliary controls menu and supports entry navigation (for cases when more than one item was found in the WDL): Workflow/Task editor is moved from the modal popup to the right floating menu PROPERTIES : See more details here . \"QUEUED\" state of the run Previously, user was not able to distinguish runs that are waiting in the queue and the runs that are being initialized (both were reporting the same state using the same icons). Now, a more clear run's state is provided - \"QUEUED\" state is introduced: During this phase of the lifecycle - a job is waiting in the queue for the available compute node. Typically this shall last for a couple of second and proceed to the initialization phase. But if this state lasts for a long time - it may mean that a cluster capacity is reached (limited by the administrator). This feature allows users to make a decision - whether to wait for run in a queue or stop it and resubmit. See more details here . Help tooltips for the run state icons With the runs' QUEUED state introduction - we now have a good number of possible job phases. To make the phases meaning more clear - tooltips are provided when hovering a run state icon within all relevant forms, i.e.: Dashboard , Runs ( Active Runs / History /etc.), Run Log . Tooltips contain a state name in bold (e.g. Queued ) and a short description of the state and info on the next stage: See more details - Active runs states , Completed runs states and Home page . VM monitor service For various reasons cloud VM instances may \"hang\" and become invisible to Cloud Pipeline services. E.g. VM was created but some error occurred during joining k8s cluster or a communication to the Cloud Providers API is interrupted. In this case Autoscaler service will not be able to find such instance and it won't be shut down. This problem may lead to unmonitored useless resource consumption and billing. To address this issue - a separate VM-Monitor service was implemented: Tracks all VM instances in the registered Cloud Regions Determines whether instances is in \"hang\" state Notifies a configurable set of users about a possible problem Notification recipients (administrators) may check the actual state of VM in Cloud Provider console and shut down VM manually. Additionally, VM-Monitor : Checks states of Cloud Pipeline's services (Kubernetes deployments). If any service changes it's state (i.e. goes down or up) - administrators will get the corresponding notification. List of such Kubernetes deployments to check includes all Cloud Pipeline's services by default, but also could be configed manually Checks all the PKI assets, available for the Platform, for the expiration - traverses over the list of directories that should contains certificate files, searches that certificates and verifies their expiration date. If some certificate expires less than in a certain amount of days - administrators also will get the corresponding notification. List of certificate directories to scan, certificate's mask and amount of days before expiration after which the notification will be sent are configurable Web GUI caching Previously, Cloud Pipeline Web GUI was not using HTTP caching to optimize the page load time. Each time application was loaded - ~2Mb of the app bundle was downloaded. This caused \"non-optimal\" experience for the end-users. Now the application bundle is split into chunks, which are identified by the content-hash in names: If nothing is changed - no data will be downloaded If some part of the app is changed - only certain chunks will be downloaded, not the whole bundle Administrator may control cache period using the static.resources.cache.sec.period parameter in the application.properties of the Core API service. Installation via pipectl Previous versions of the Cloud Pipeline did not offer any automated approach for deploying its components/services. All the deployment tasks were handed manually or by custom scripts. To simplify the deployment procedure and improve stability of the deployment - pipectl utility was introduced. pipectl offers an automated approach to deploy and configure the Cloud Pipeline platform, as well as publish some demo pipelines and docker images for NGS/MD/MnS tasks. Brief description and example installation commands are available in the pipectl's home directory . More sophisticated documentation on the installation procedure and resulting deployment architecture will be created further. Add more logging to troubleshoot unexpected pods failures When a Cloud Pipeline is being for a long time (e.g. years), it is common to observe rare \"strange\" problems with the jobs execution. I.e. the following behavior was observed couple of times over the last year: Scenario 1 Run is launched and initialized fine During processing execution - run fails Console logs print nothing, compute node is fine and is attached to the cluster Scenario 2 Run is launched, compute node is up Run fails during initialization Console logs print the similar error message: failed to open log file \"/var/log/pods/**.log\": open /var/log/pods/**.log: no such file or directory Both scenarios are flaky and almost impossible to reproduce. To provide more insights into the situation - an extended node-level logging was implemented: kubelet logs (from all compute nodes) are now written to the files (via DaemonSet ) Log files are streamed to the storage, identified by storage.system.storage.name preference Administrators can find the corresponding node logs (e.g. by the hostname or ip that are attached to the run information) in that storage under logs/node/{hostname} See an example here . Displaying information on the nested runs within a parent log form Previously, if user launched a run, that has a number of children (e.g. a cluster run or any other case with the parent-id specified), he could view the children list only from \"Runs\" page. In v.0.15 a convenient opportunity to view the list of children directly in the parent's run logs form is implemented: For each child-run in the list the following information is displayed: State icons with help tooltips when hovering over them Pipeline name and version/docker image and version Run time duration Similar as a parent-run state, states for nested runs are automatically updated without page refreshing. So, you can watch for them in real time. If you click any of the children-runs, you will navigate to its log page. That feature is implemented for the comleted runs too: More information about nested runs displaying see here and here . Environment Modules support for the Cloud Pipeline runs The Environment Modules package provides for the dynamic modification of a user's environment via modulefiles . In current version, an ability to configure the Modules support for the compute jobs is introduced, if this is required by any use case. For using facilities of the Environment Modules package, a new system parameter was added to the Cloud Pipeline: CP_CAP_MODULES (boolean) - enables installation and using the Modules for the current run (for all supported Linux distributions) If CP_CAP_MODULES system parameter is set - the Modules will be installed and made available. While installing, Modules will be configured to the source modulefiles path from the CP_CAP_MODULES_FILES_DIR launch environment variable (value of this variable could be set only by admins via system-level settings). If that variable is not set - default modulefiles location will be used. See an example here . Sharing SSH access to running instances with other user(s)/group(s) As was introduced in Release Notes v.0.13 , for certain use cases it is beneficial to be able to share applications with other users/groups. v0.15 introduces a feature that allows to share the SSH-session of any active run (regardless of whether the job type is interactive or not): The user can share an interactive run with others: \"Share with: ...\" parameter, within a run log form, can be used for this Specific users or whole groups can be set for sharing Once this is set - other users will be able to access run's endpoints Also you can share SSH access to the running instance via setting \" Enable SSH connection \" checkbox Also, the user can share a non-interactive run: \"Share with: ...\" parameter, within a run log form, can be used for this Specific users or whole groups can be set for sharing Once this is set - specified users/groups will be able to access the running instance via the SSH SERVICES widget within a Home dashboard page lists such \"shared\" services. It displays a \"catalog\" of services, that can be accessed by a current user, without running own jobs. To open shared instance application user should click the service name. To get SSH-access to the shared instance (regardless of whether the job type is interactive or not), the user should hover over the service \"card\" and click the SSH hyperlink For more information about runs sharing see 11.3. Sharing with other users or groups of users . Allow to limit the number of concurrent SSH sessions Previously, some users could try to start a real big number of Web SSH sessions. If 1000+ SSH sessions are established via EDGE service, the performance will degrade. It is not common, but it could be critical as it affects all the users of the platform deploment. To avoid such cases, in v0.15 the pipectl parameter CP_EDGE_MAX_SSH_CONNECTIONS (with default value 25 ) for the EDGE server is introduced, that allows to control a number of simultaneous SSH connections to a single job. Now, if this max number will be reached, the next attemp to open another one Web SSH session to the same job will return a notification to the user and a new session will not be opened until the any one previous is closed: Verification of docker/storage permissions when launching a run Users are allowed to launch pipeline, detached configuration or tool if they have a corresponding permission for that executable. But in some cases this verification is not enough, e.g. when user has no read permission for input parameter - in this case, run execution could cause an error. In v0.15 additional verification implemented that checks if: execution is allowed for specified docker image; read operations are allowed for input and common path parameters; write operations are allowed for output path parameters. If there are such permission issues, run won't be launched and special warning notifications will be shown to a user, e.g.: For more details see sections 6.2. Launch a pipeline , 7.2. Launch Detached Configuration and 10.5. Launch a Tool . Ability to override the queue / PE configuration in the GE configuration Previously, if the Grid Engine was enabled, the following was configured: a single queue with all the hosts was creating, named \" main.q \" a single PE (Parallel Environment) was creating, named \" local \" In v0.15 , the overriding of the names of the queue / PE is implemented to be compatible with any existing scripts, that rely on a specific GE configuration (e.g. hardcoded). You can do it using two new System Parameters at the Launch or the Configuration forms: CP_CAP_SGE_QUEUE_NAME (string) - allows to override the GE's queue name (default: \" main.q \") CP_CAP_SGE_PE_NAME (string) - allows to override the GE's PE name (default: \" local \") More information how to use System Parameters when a job is launched see here . Estimation run's disk size according to the input/common parameters Previously, if a job was run with the disk size, which was not enough to handle the job's inputs - it failed (e.g. 10Gb disk was set for a run, which processed data using STAR aligner, where the genome index file is 20Gb). In v0.15 , an attempt to handle some of such cases is implemented. Now, the Cloud Pipeline try to estimate the required disk size using the input/common parameters and warn the user if the requested disk is not enough. When a job is launching, the system try to get the size of all input/common parameters. The time of the size getting for all files is limited, as this may take too much for lots of small files. Limit for this time is set by the storage.listing.time.limit system preference (in milliseconds). Default: 3 sec (3000 milliseconds). If computation doesn't end in this timeout, accumulated size will return as is. If the resulting size of all input/common parameters is greater than requested disk size (considering cluster configuration) - the user will be warned: User can set suggested disk size or launch a job at user's own risk with the requested size. If calculated suggested disk size exceeds 16Tb (hard limit) a different warning message will be shown: The requested disk size for this run is <N> Gb, but the data that is going to be processed exceeds 16 Tb (which is a hard limit). Please use the cluster run configuration to scale the disks horizontally or reduce the input data volume. Do you want to use the maximum disk size 16 Tb anyway? Disabling of the Global Search form if a corresponding service is not installed Version 0.14 introduced the Global Search feature over all Cloud Pipeline objects. In current version, a small enhancement for the Global Search is implemented. Now, if the search.elastic.host system preference is not set by admin - other users will not be able to try search performing: the \"Search\" button will be hidden from the left menu keyboard search shortcut will be disabled Disabling of the FS mounts creation if no FS mount points are registered In the Cloud Pipeline , along with the regular data storages user can also create FS mounts - data storages based on the network file system: For the correct FS mount creation, at least one mount point shall be registered in the Cloud Pipeline Preferences. Now, if no FS mount points are registered for any Cloud Region in the System Preferences - user can not create a new FS mount, the corresponding button becomes invisible: Displaying resource limit errors during run resuming User may hit a situation of resource limits while trying to resume previously paused run. E.g. instance type was available when run was initially launched, but at the moment of resume operation provider has no sufficient capacity for this type. Previously, in this case run could be failed with an error of insufficient resources. In v0.15 the following approach is implemented for such cases: resuming run doesn't fail if resource limits are hit. That run returns to the Paused state log message that contains a reason for resume failure and returning back to the Paused state is being added to the ResumeRun task user is notified about such event. The corresponding warning messages are displayed: at the Run logs page at the ACTIVE RUNS page (hint message while hovering the RESUME button) at the ACTIVE RUNS panel of the Dashboard (hint message while hovering the RESUME button) Object storage creation in despite of that the CORS/Policies could not be applied Previously, if the Cloud service account/role had permissions to create object storages, but lacked permissions to apply CORS or other policies - object storage was created, but the Cloud Pipeline API threw an exception and storage was not being registered. This led to the creation of a \"zombie\" storage, which was not available via GUI, but existed in the Cloud. Currently, the Cloud Pipeline API doesn't fail such requests and storage is being registered normally. But the corresponding warning will be displayed to the user like this: The storage {storage_name} was created, but certain policies were not applied. This can be caused by insufficient permissions. Track the confirmation of the \"Blocking\" notifications System events allow to create popup notifications for users. One of the notification types - the \"Blocking\" notification. Such event emerges in the middle of the window and requires confirmation from the user to disappear for proceeding with the GUI operations. In certain cases (e.g. for some important messages), it is handy to be able to check which users confirmed the notification. For that, in the current version the ability to view, which \"blocking\" notifications confirmed by specific user, was implemented for admins. Information about confirmed notifications can be viewed at the \" Attributes \" section of the specific user's profile page: Confirmed notifications are displayed as user attribute with the KEY confirmed_notifications (that name could be changed via the system-level preference system.events.confirmation.metadata.key ) and the VALUE link that shows summary count of confirmed notifications for the user. Click the VALUE link with the notification count to open the detailed table with confirmed notifications: For more details see \"blocking\" notifications track . pipe CLI warnings on the JWT expiration By default, when pipe CLI is being configured JWT token is given for one month, if user didn't select another expiration date. In v.0.15 extra pipe CLI warnings are introduced to provide users an information on the JWT token expiration: When pipe configure command is executed - the warning about the expiration date of the provided token is printed, if it is less than 7 days left: When --version option is specified - pipe prints dates of issue and expiration for the currently used token: When any other command is running - the warning about the expiration date of the provided JWT token is printed, if it is less than 7 days left: See more information about pipe CLI installation here . pipe configuration for using NTLM Authentication Proxy For some special customer needs, pipe configuration for using NTLM Authentication Proxy, when running in Linux, could be required. For that, several new options were added to pipe configure command: -nt or --proxy-ntlm - flag that enable NTLM proxy support -nu or --proxy-ntlm-user - username for NTLM proxy authorization -np or --proxy-ntlm-pass - password for NTLM proxy authorization -nd or --proxy-ntlm-domain - domain for NTLM proxy authorization If --proxy-ntlm is set, pipe will try to get the proxy value from the environment variables or --proxy option ( --proxy option has a higher priority). If --proxy-ntlm-user and --proxy-ntlm-pass options are not set - user will be prompted for username/password in an interactive manner. Valid configuration examples: User will be prompted for NTLM Proxy Username, Password and Domain: pipe configure --proxy-ntlm ... Username for the proxy NTLM authentication: user1 Domain of the user1 user: '' Password of the user1 user: Use http://myproxy:3128 as the \"original\" proxy address. User will not be prompted for NTLM credentials: pipe configure --proxy-ntlm --proxy-ntlm-user $MY_NAME --proxy-ntlm-pass $MY_PASS --proxy \"http://myproxy:3128\" See more information about pipe CLI installation and configure here . Execution of files uploading via pipe without failures in case of lacks read permissions Previously, pipe storage cp / mv commands could fail if a \"local\" source file/dir lacked read permissions. For example, when user tried to upload to the \"remote\" storage several files and when the pipe process had reached one of files that was not readable for the pipe process, then the whole command was being failed, remaining files did not upload. In current version, the pipe process checks read permission for the \"local\" source (directories and files) and skip those that are not readable: Run a single command or an interactive session over the SSH protocol via pipe For the certain purposes, it could be conveniently to start an interactive session over the SSH protocol for the job run via the pipe CLI. For such cases, in v0.15 the pipe ssh command was implemented. It allows you, if you are the ADMIN or the run OWNER , to perform a single command or launch an interactive session for the specified job run. Launching of an interactive session: This session is similar to the terminal access that user can get via the GUI. Performing the same single command without launching an interactive session: Perform objects restore in a batch mode via pipe Users can restore files that were removed from the data storages with enabled versioning. For these purposes, the Cloud Pipeline's CLI has the restore command which is capable of restoring a single object at a time. In v0.15 the ability to recursively restore the whole folder, deleted from the storage, was implemented. Now, if the source path is a directory, the pipe storage restore command gets the top-level deleted files from the source directory and restore them to the latest version. Also, to the restore command some options were added: -r or --recursive - flag allows to restore the whole directory hierarchy -i or --include [TEXT] - flag allows to restore only files which names match the [TEXT] pattern and skip all others -e or --exclude [TEXT] - flag allows to skip restoring of files which names match the [TEXT] pattern and restore all others Note : this feature is yet supported for AWS only. For more details about file restoring via the pipe see here . Mounting data storages to Linux and Mac workstations Previously, when users had to copy/move datasets to/from Cloud data storages via CLI, they could use only special pipe storage commands. That was not always comfortable or could lead to some functionality restrictions. In v0.15 the ability to mount Cloud data storages (both - File Storages and Object Storages) to Linux and Mac workstations (requires FUSE installed) was added. For the mounted storages, regular listing/read/write commands are supported, users can manage files/folders as with any general hard drive. Note : this feature is yet supported for AWS only. To mount a data storage into the local mountpoint the pipe storage mount command was implemented. It has two main options that are mutually exclusive: -f or --file specifies that all available file systems should be mounted into a mountpoint -b or --bucket [STORAGE_NAME] specifies a storage name to mount into a mountpoint Users can: leverage mount options, supported by underlying FUSE implementation, via -o option enable multithreading for simultaneously interaction of several processes with the mount point, via -t option trace all information about mount operations into a log-file, via -l option To unmount a mountpoint the pipe storage umount command was implemented. For more details about mounting data storages via the pipe see here . Allow to run pipe commands on behalf of the other user In the current version, the ability to run pipe commands on behalf of the other user was implemented. It could be convenient when administrators need to perform some operations on behalf of the other user (e.g. check permissions/act as a service account/etc.). This feature is implemented via the common option that was added to all pipe commands: --user|-u <USER_ID> (where <USER_ID> is the name of the user account). Note : the option isn't available for the following pipe commands: configure , --version , --help . If this option is specified - operation (command execution) will be performed using the corresponding user account, e.g.: In the example above, active runs were outputted from the admin account (firstly) and then on behalf of the user without ROLE_ADMIN role. Additionally, a new command pipe token <USER_ID> was implemented. It prints the JWT token for a specified user. This command also can be used with non-required option -d ( --duration ), that specified the number of days the token will be valid. If it's not set - the default value will be used, same as in the GUI. Example of using: Then, the generated JWT token could be used manually with the pipe configure command - to configure pipe CLI on behalf of the desired user. Note : both - the command ( pipe token ) and the option ( --user ) - are available only for admins. For more details see here . Ability to restrict the visibility of the jobs Previously, Cloud Pipeline inherited the pipeline jobs' permissions from the parent pipeline object. So, if two users had permissions on the same pipeline, then when the first user had launched that pipeline - the second user also could view (not manage) launched run in the Active Runs tab. Now admin can restrict the visibility of the jobs for non-owner users. The setting of such visibility can get one of the following values: Inherit - the behavior is the same as described above (for the previous approach), when the runs visibility is controlled by the pipeline permissions. It is set as a default for the Cloud Pipeline environment Only owner - when only the person who launch a run can see it Jobs visibility could be set by different ways on several forms: within User management tab in the system-level settings admin can specify runs visibility for a user/group/role: within Launch section of Preferences tab in the system-level settings admin can specify runs visibility for a whole platform as global defaults - by the setting launch.run.visibility : Next hierarchy is set for applying of specified jobs visibility: User level - highest priority (specified for a user) Group level (specified for a group/role) Platform level launch.run.visibility (specified as global defaults via system-level settings) Note : admins can see all runs despite of settings Ability to perform scheduled runs from detached configurations Previously, Cloud Pipeline allowed starting compute jobs only manually (API/GUI/CLI). But in certain use cases, it is beneficial to launch runs on a scheduled basis. In v0.15 the ability to configure a schedule for detached configuration was implemented: User is able to set a schedule for launch a run from the detached configuration: Schedule is defined as a list of rules - user is able to specify any number of them: For each rule in the list user is able to set the recurrence: If any schedule rule is configured for the detached configuration - a corresponding job (plain container or a pipeline) will be started accordingly in the scheduled day and time. See more details here . The ability to use custom domain names as a \"friendly URL\" for the interactive services In v0.13 the ability to set a \"friendly URL\" for the interactive services endpoint was implemented. It allows to configure the view of the interactive service endpoint: Default view: https://<host>/pipeline-<run-id>-<port> \"Friendly\" view: https://<host>/<friendly-url> In the current version this feature is expanded: users allow to specify a custom host. So the endpoint url now can look like: https://<custom-host> or https://<custom-host>/<friendly-url> . Note : custom host should exist, be valid and configured. The custom host is being specified into the same field as a \"friendly URL\" previously, e.g.: Final URL for the service endpoint will be generated using the specified host and friendly URL: For more details see here . Displaying of the additional support icon/info In certain cases, users shall have a quick access to the help/support information (e.g. links to docs/faq/support request/etc.) In the current version, the ability to display additional \"support\" icon with the corresponding info in the bottom of the main menu was implemented: The displaying of this icon and the info content can be configured by admins via the system-level preference ui.support.template : this preference is empty by default - in this case the support icon is invisible if this preference contains any text ( Markdown -formatted): the support icon is visible specified text is displayed in the support icon tooltip (support info) For more details see UI system settings . Pass proxy settings to the DIND containers Previously, DIND containers configuration included only registry credentials and a couple of driver settings. In certain environments, it is not possible to access external networks (e.g. for the packages installation) without the proxy settings. So the users had to pass this manually every time when using the docker run command. In the current version, a new system preference launch.dind.container.vars is introduced. It allows to specify all the additions variables, which will be passed to the DIND containers (if they are set for the host environment). By default, the following variables are set for the launch.dind.container.vars preference (and so will be passed to DIND container): http_proxy , https_proxy , no_proxy , API , API_TOKEN . Variables are being specified as a comma-separated list. Example of using: At the same time, a new system parameter (per run) was added - CP_CAP_DIND_CONTAINER_NO_VARS , which disables described behavior. You can set it before any run if you don't want to pass any additional variations to the DIND container. Interactive endpoints can be (optionally) available to the anonymous users Cloud Pipeline allows sharing the interactive and SSH endpoints with the other users/groups. Previously, this necessary required the end-user to be registered in the Cloud Pipeline users database. For certain use-cases, it is required to allow such type of access for any user, who has successfully passed the IdP authentication but is not registered in the Cloud Pipeline and also such users shall not be automatically registered at all and remain Anonymous . In the current version, such ability is implemented. It's enabled by the following application properties: saml.user.auto.create=EXPLICIT_GROUP saml.user.allow.anonymous=true After that, to share any interactive run with the Anonymous - it's simple enough to share endpoints with the following user group - ROLE_ANONYMOUS_USER : At the Run logs page: The user should select the ROLE_ANONYMOUS_USER role to share: Sharing with the Anonymous will be displayed at the Run logs page: That's all. Now, the endpoint-link of the run could be sent to the Anonymous user. If that Anonymous user passes SAML authentication, he will get access to the endpoint. Attempts to open any other Platform pages will fail. For more details see here . Notable Bug fixes Incorrect behavior of the global search filter #221 When user was searching for an entry, that may belong to different classes (e.g. issues and folders ) - user was not able to filter the results by the class. \"COMMITTING...\" status hangs #152 In certain cases, while committing pipeline with the stop flag enabled - the run's status hangs in Committing... state. Run state does not change even after the commit operation succeeds and a job is stopped. Instances of Metadata entity aren't correctly sorted #150 Metadata entities (i.e. project-related metadata) sorting was faulty: Sort direction indicator (Web GUI) was displaying an inverted direction Entities were not sorted correctly Tool group cannot be deleted until all child tools are removed #144 If there is a tool group in the registry, which is not empty (i.e. contains 1+ tools) - an attempt to delete it throws SQL error. It works fine if the child tools are dropped beforehand. Now, it is possible to delete such a group if a force flag is set in the confirmation dialog. Missing region while estimating a run price #93 On the launch page, while calculating a price of the run, Cloud Provider's region was ignored. This way a calculation used a price of the specified instance type in any of the available regions. In practice, requested price may vary from region to region. Cannot specify region when an existing object storage is added #45 Web GUI interface was not providing an option to select a region when adding an existing object storage. And it was impossible to add a bucket from the non-default region. ACL control for PIPELINE_USER and ROLE entities for metadata API #265 All authorized users were permitted to browse the metadata of users and roles entities. But those entries may contain a sensitive data, that shall not be shared across users. Now, a general user may list only personal user-level metadata. Administrators may list both users and roles metadata across all entries. Getting logs from Kubernetes may cause OutOfMemory error #468 For some workloads, container logs may become very large: up to several gigabytes. When we tried to fetch such logs it is likely to cause OutOfMemory error, since Kubernetes library tries to load it into a single String object. In current version, a new system preference was introduced: system.logs.line.limit . That preference sets allowable log size in lines. If actual pod logs exceeds the specified limit only log tail lines will be loaded, the rest will be truncated. AWS: Incorrect nodeup handling of spot request status #556 Previously, in a situation when an AWS spot instance created after some timeout - spot status wasn't updated correctly in the handling of spot request status . It might cause errors while getting spot instance info. Not handling clusters in autopause daemon #557 Previously, if cluster run was launched with enabled \"Auto pause\" option, parent-run or its child-runs could be paused (when autopause conditions were satisfied, of course). It was incorrect behavior because in that case, user couldn't resume such paused runs and go on his work (only \"Terminate\" buttons were available). In current version, autopause daemon doesn't handle any clusters (\"Static\" or \"Autoscaled\"). Also now, if the cluster is configured - Auto pause checkbox doesn't display in the Launch Form for the On-Demand node types. Incorrect pipe CLI version displaying #561 Previously, pipe CLI version displayed incorrectly for the pipe CLI installations performed via hints from the Cloud Pipeline System Settings menu. JWT token shall be updated for the jobs being resumed #579 In cases when users launched on-demand jobs, paused them and then, after a long time period (2+ months), tried to resume such jobs - expired JWT tokens were set for them that led to different problems when any of the initialization routines tried to communicate with the API. Now, the JWT token and other variables as well are being updated when a job is being resumed. Trying to rename file in the data storage, while the \"Attributes\" panel is opened, throws an error #520 Renaming file in the datastorage with opened \"Attributes\" panel caused an unexpected error. pipe : incorrect behavior of the -nc option for the run command #609 Previously, trying to launch a pipeline via the pipe run command with the single -nc option threw an error. Cluster run cannot be launched with a Pretty URL #620 Previously, if user tried to launch any interactive tool with Pretty URL and configured cluster - an error appeared URL {Pretty URL} is already used for run {Run ID} . Now, pretty URL could be set only for the parent runs, for the child runs regular URLs are set. Cloning of large repositories might fail #626 When large repository (> 1Gb) was cloned (e.g. when a pipeline was being run) - git clone could fail with the OOM error happened at the GitLab server if it is not powerful enough. OOM was produced by the git pack-objects process, which tries to pack all the data in-memory. Now, git pack-objects memory usage is limited to avoid errors in cases described above. System events HTML overflow #630 If admin set a quite long text (without separators) into the message body of the system event notifications - the resulting notification text \"overflowed\" the browser window. Now, text wrapping is considered for such cases. Also, support of Markdown was added for the system notification messages: AWS: Pipeline run InitializeNode task fails #635 Previously, if AWS spot instance could not be created after the specific number of attempts during the run initialization - such run was failed with the error, e.g.: Exceeded retry count (100) for spot instance. Spot instance request status code: capacity-not-available . Now, in these cases, if spot instance isn't created after specific attempts number - the price type is switched to on-demand and run initialization continues. git-sync shall not fail the whole object synchronization if a single entry errors #648 , #691 When the git-sync script processed a repository and failed to sync permissions of a specific user (e.g. git exception was thrown) - the subsequent users were not being processed for that repository. Now, the repository sync routine does not fail if a single user cannot be synced. Also, the issues with the synchronization of users with duplicate email addresses and users with empty email were resolved. endDate isn't set when node of a paused run was terminated #743 Previously, when user terminated the node of a paused run - endDate for that run wasn't being set. This was leading to wrong record of running time for such run. AWS: Nodeup retry process may stuck when first attempt to create a spot instance failed #744 Previously, if first nodeup attempt failed due to unavailablity to connect on 8888 port (in expected amounts of attempts) after getting instance running state, the second nodeup attempt might stuck because it waited for the same instance (associated with existed SpotRequest for the first attempt) to be up. But it couldn't happen - this instance was already in terminating state after the first attempt. Now, checks that instance associated with SpotRequest (created for the first attempt) is in appropriate status, if not - a new SpotRequest is being created and the nodeup process is being started from scratch. Resume job timeout throws strange error message #832 Previously, the non-informative error message was shown if the paused run could't be resumed in a reasonable amount of time - the count of attempts to resume was displaying incorrectly. GE autoscaler doesn't remove dead additional workers from cluster #946 Previously, the Grid Engine Autoscaler didn't properly handle dead workers downscaling. For example, if some spot worker instance was preempted during the run then the autoscaler could not remove such worker from GE. Moreover, such cluster was blocked from accepting new jobs. Broken layouts #553 , #619 , #643 , #644 , #915 Previously, pipeline versions page had broken layout if there were pipeline versions with long description. Global search page was not rendered correctly when the search results table had too many records. When a list of items in the docker groups selection dialog was long - it was almost impossible to use a search feature, as the list hid immediately. Some of the other page layouts also were broken.","title":"v.0.15"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#cloud-pipeline-v015-release-notes","text":"Microsoft Azure Support SAML claims based access Notifications on the RAM/Disk pressure Limit mounted storages Personal SSH keys configuration Allow to set the Grid Engine capability for the \"fixed\" cluster Enable Apache Spark for the Cloud Pipeline's clusters Consider Cloud Providers' resource limitations when scheduling a job Allow to terminate paused runs Pre/Post-commit hooks implementation Restricting manual installation of the nvidia tools Setup swap files for the Cloud VMs Run's system paths shall be available for the general user account Renewed WDL visualization \"QUEUED\" state of the run Help tooltips for the run state icons VM monitor service Web GUI caching Installation via pipectl Add more logging to troubleshoot unexpected pods failures Displaying information on the nested runs Environment Modules support Sharing SSH access to running instances with other user(s)/group(s) Allow to limit the number of concurrent SSH sessions Verification of docker/storage permissions when launching a run Ability to override the queue/PE configuration in the GE configuration Estimation run's disk size according to the input/common parameters Disabling of the Global Search form if a corresponding service is not installed Disabling of the FS mounts creation if no FS mount points are registered Displaying resource limit errors during run resuming Object storage creation in despite of that the CORS/Policies could not be applied Track the confirmation of the \"Blocking\" notifications pipe CLI warnings on the JWT expiration pipe configuration for using NTLM Authentication Proxy Files uploading via pipe in case of restrictions Run a single command or an interactive session over the SSH protocol via pipe Perform objects restore in a batch mode via pipe Mounting data storages to Linux and Mac workstations Allow to run pipe commands on behalf of the other user Ability to restrict the visibility of the jobs Ability to perform scheduled runs from detached configurations Using custom domain names as a \"friendly URL\" for the interactive services Displaying of the additional support icon/info Pass proxy settings to the DIND containers Interactive endpoints can be (optionally) available to the anonymous users Notable Bug fixes Incorrect behavior of the global search filter \"COMMITING...\" status hangs Instances of Metadata entity aren't correctly sorted Tool group cannot be deleted until all child tools are removed Missing region while estimating a run price Cannot specify region when an existing object storage is added ACL control for PIPELINE_USER and ROLE entities for metadata API Getting logs from Kubernetes may cause OutOfMemory error AWS: Incorrect nodeup handling of spot request status Not handling clusters in autopause daemon Incorrect pipe CLI version displaying JWT token shall be updated for the jobs being resumed Trying to rename file in the data storage, while the \"Attributes\" panel is opened, throws an error pipe : incorrect behavior of the -nc option for the run command Cluster run cannot be launched with a Pretty URL Cloning of large repositories might fail System events HTML overflow AWS: Pipeline run InitializeNode task fails git-sync shall not fail the whole object synchronization if a single entry errors endDate isn't set when node of a paused run was terminated AWS: Nodeup retry process may stuck when first attempt to create a spot instance failed Resume job timeout throws strange error message GE autoscaler doesn't remove dead additional workers from cluster Broken layouts","title":"Cloud Pipeline v.0.15 - Release notes"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#microsoft-azure-support","text":"One of the major v0.15 features is a support for the Microsoft Azure Cloud . All the features, that were previously used for AWS , are now available in all the same manner, from all the same GUI/CLI, for Azure . Another cool thing, is that now it is possible to have a single installation of the Cloud Pipeline , which will manage both Cloud Providers ( AWS and Azure ). This provides a flexibility to launch jobs in the locations, closer to the data, with cheaper prices or better compute hardware for a specific task.","title":"Microsoft Azure Support"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#saml-claims-based-access","text":"Previously, there were two options for users to be generally authenticated in Cloud Pipeline Platform, i.e. to pass SAML validation: Auto-register - any valid SAML authentication automatically registers user (if he isn't registered yet) and grant ROLE_USER access Explicit-register - after a valid SAML authentication, it is checked whether this user is already registered in the Cloud Pipeline catalog, and if no - request denies, authentication fails To automate things a bit more, in v0.15 additional way to grant first-time access to the users was implemented: Explicit-group register. If such registration type is set - once a valid SAML authentication is received, it is checked, whether SAML response contains any of the domain groups, that are already granted any access to the Cloud Pipeline objects (registered in Cloud Pipeline catalog). If so - proceeds as with Auto-register - user with all his domain groups and granted ROLE_USER access is being registered. If user's SAML domain groups aren't intersected with pre-registered groups the authentication fails. This Platform's behavior is set via application property saml.user.auto.create that could accept one of the corresponding values: AUTO , EXPLICIT , EXPLICIT_GROUP .","title":"SAML claims based access"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#notifications-on-the-ramdisk-pressure","text":"When a compute-intensive job is run - compute node may start starving for the resources. CPU high load is typically a normal situation - it could result just to the SSH/metrics slowdown. But running low on memory and disk could crash the node, in such cases autoscaler will eventually terminate the cloud instance. In v0.15 version, the Cloud Pipeline platform could notify user on the fact of Memory/Disk high load. When memory or disk consuming will be higher than a threshold value for a specified period of time (in average) - a notification will be sent (and resent after a delay, if the problem is still in place). Such notifications could be configured at HIGH_CONSUMED_RESOURCES section of the Email notifications : The following items at System section of the Preferences define behavior of such notifications: system.memory.consume.threshold - memory threshold (in %) above which the notification would be sent system.disk.consume.threshold - disk threshold (in %) above which the notification would be sent system.monitoring.time.range - time delay (in sec) after which the notification would be sent again, if the problem is still in place. See more information about Email notifications and System Preferences .","title":"Notifications on the RAM/Disk pressure"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#limit-mounted-storages","text":"Previously, all available storages were mounted to the container during the run initialization. User could have access to them via /cloud-data or ~/cloud-data folder using the interactive sessions (SSH/Web endpoints/Desktop) or pipeline runs. For certain reasons (e.g. takes too much time to mount all or a run is going to be shared with others), user may want to limit the number of data storages being mounted to a specific job run. Now, user can configure the list of the storages that will be mounted. This can be accomplished using the Limit mount field of the Launch form: By default, All available storages are mounted (i.e. the ones, that user has ro or rw permissions) To change the default behavior - click the drop-down list next to \"Limit mounts\" label: Select storages that shall be mounted: Review that only a limited number of data storages is mounted: Mounted storage is available for the interactive/batch jobs using the path /cloud-data/{storage_name} : See an example here .","title":"Limit mounted storages"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#personal-ssh-keys-configuration","text":"Previously, there were two options to communicate with the embedded gitlab repositories, that host pipelines code: From the local (non-cloud) machine: use the https protocol and the repository URI, provided by the GUI (i.e. GIT REPOSITORY button in the pipeline view) From the cloud compute node: git command line interface, that is preconfigured to authenticate using https protocol and the auto-generated credentials Both options consider that https is used. It worked fine for 99% of the use cases. But for some of the applications - ssh protocol is required as this is the only way to achieve a passwordless authentication against the gitlab instance To address this issue, SSH keys management was introduced: Users' ssh keys are generated by the Git Sync service SSH key-pair is created and assigned to the Cloud Pipeline user and a public key is registered in the GitLab Those SSH keys are now also configured for all the runs, launched by the user. So it is possible to perform a passwordless authentication, when working with the gitlab or other services, that will be implemented in the near future HTTPS/SSH selector is added to the GIT REPOSITORY popup of the Cloud Pipeline Web GUI: Default selection is HTTP, which displays the same URI as previously (repository), but user is able to switch it to the SSH and get the reformatted address: For more details see here .","title":"Personal SSH keys configuration"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#allow-to-set-the-grid-engine-capability-for-the-fixed-cluster","text":"Version 0.14 introduced an ability to launch autoscaled clusters. Besides the autoscaling itself - such cluster were configured to use GridEngine server by default, which is pretty handy. On the other hand - fixed cluster (i.e. those which contain a predefined number of compute nodes), required user to set the CP_CAP_SGE explicitly. Which is no a big deal, but may be tedious. In v0.15 Grid Engine can be configured within the Cluster (fixed size cluster) tab. This is accomplished by using the Enable GridEngine checkbox. By default, this checkbox is unticked. If the user sets this ON - CP_CAP_SGE parameter is added automatically. Also a number of help icons is added to the Cluster configuration dialog to clarify the controls purpose: Popup header (E.g. next to the tabs line) - displays information on different cluster modes (Cluster) Enable GridEngine checkbox - displays information on the GridEngine usage (Cluster) Enable Apache Spark checkbox - displays information on the Apache Spark usage (see below ) (Auto-scaled cluster) Auto-scaled up - displays information on the autoscaling logic (Auto-scaled cluster) Default child nodes - displays information on the initial node pool size See more information about cluster launch here .","title":"Allow to set the Grid Engine capability for the \"fixed\" cluster"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#enable-apache-spark-for-the-cloud-pipelines-clusters","text":"Another one feature for the Cloud Pipeline's clusters was implemented in v0.15 . Now, Apache Spark with the access to File/Object Storages from the Spark Applications can be configured within the Cluster tab. It is available only for the fixed size clusters. To enable this feature - tick the Enable Apache Spark checkbox and set the child nodes count at cluster settings. By default, this checkbox is unticked. Also users can manually enable Spark functionality by the CP_CAP_SPARK system parameter: This feature, for example, allows you to run Apache Spark cluster with RStudio where you may code in R using sparklyr to run the workload over the cluster: Open the RStudio tool Select the node type, set the Apache Spark cluster as shown above, and launch the tool: Open main Dashboard and wait until the OPEN hyperlink for the launched tool will appear. Hover over it: Two endpoints will appear: RStudio (as the main endpoint it is in bold) - it exposes RStudio's Web IDE SparkUI - it exposes Web GUI of the Spark. It allows to monitor Spark master/workers/application via the web-browser. Details are available in the Spark UI manual Click the RStudio endpoint: Here you can start create scripts in R using the pre-installed sparklyr package to distribute the workload over the cluster. Click the SparkUI endpoint: Here you can view the details of the jobs being executed in Spark, how the memory is used and get other useful information. For more information about using Apache Spark via the Cloud Pipeline see here .","title":"Enable Apache Spark for the Cloud Pipeline's clusters"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#consider-cloud-providers-resource-limitations-when-scheduling-a-job","text":"Cloud Pipeline supports queueing of the jobs, that cannot be scheduled to the existing or new compute nodes immediately. Queueing occurs if: cluster.max.size limit is reached (configurable via Preferences ) Cloud Provider limitations are reached (e.g. AWS EC2 Limits ) If ( 1 ) happens - job will sit in the QUEUED state, until a spare will be available or stopped manually. This is the correct behavior. But if ( 2 ) occurs - an error will be raised by the Cloud Provider, Cloud Pipeline treats this as an issue with the node creation. Autoscaler will then resubmit the node creation task for cluster.nodeup.retry.count times (default: 5) and then fail the job run. This behavior confuses users, as ( 2 ) shall behave almost in the same manner as ( 1 ) - job shall be kept in the queue until there will be free space for a new node. In v0.15 ( 2 ) scenario works as described: If a certain limit is reached (e.g. number of m4.large instances exceeds the configured limit) - run will not fail, but will await for a spare node or limit increase A warning will be highlighted in the job initialization log:","title":"Consider Cloud Providers' resource limitations when scheduling a job"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#allow-to-terminate-paused-runs","text":"Some of the jobs, that were paused (either manually, or by the automated service), may be not needed anymore. But when a run is paused - the user cannot terminate/stop it before resuming. I.e. one have to run RESUME operation, wait for it's completion and then STOP the run. While this is the expected behavior (at least designed in this manner) - it requires some extra steps to be performed, which may look like meaningless (why one shall resume a run, that is going to be stopped?). Another issue with such a behavior is that in certain \"bad\" conditions - paused runs are not able to resume and just fail, e.g.: An underlying instance is terminated outside of the Cloud Pipeline Docker image was removed from the registry And other cases that are not yet uncovered This introduces a number of stale runs, that just sit there in the PAUSED state and nobody can remove them. To address those concerns - current version of Cloud Pipeline allows to terminate PAUSED run, without a prior RESUME . This operation can be performed by the OWNER of the run and the ADMIN users. Termination of the PAUSED run drops the underlying cloud instance and marks the run as STOPPED . From the GUI perspective - TERMINATE button is shown (instead of STOP ), when a run is in the PAUSED state: on the \"Active runs\" page on the \"Run logs\" page on the \"Dashboard\" page Clicking it - performs the run termination, as described above. See more information here .","title":"Allow to terminate paused runs"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#prepost-commit-hooks-implementation","text":"In certain use-cases, extra steps shall be executed before/after running the commit command in the container. E.g. imagine the following scenario: User launches RStudio User installs packages and commits it as a new image User launches the new image The following error will be displayed in the R Console: 16 Jan 2019 21:17:15 [rsession-GRODE01] ERROR session hadabend; LOGGED FROM: rstudio::core::Error {anonymous}::rInit(const rstudio::r::session::RInitInfo&) /home/ubuntu/rstudio/src/cpp/session/SessionMain.cpp:563 There is nothing bad about this message and states that previous RSession was terminated in a non-graceful manner. RStudio will work correctly, but it may confuse the users. While this is only one example - there are other applications, that require extra cleanup to be performed before the termination. To workaround such issues (RStudio and others) an approach of pre/post-commit hooks is implemented. That allows to perform some graceful cleanup/restore before/after performing the commit itself. Those hooks are valid only for the specific images and therefore shall be contained within those images. Cloud Pipeline itself performs the calls to the hooks if they exist. Two preferences are introduced: commit.pre.command.path : specified a path to a script within a docker image, that will be executed in a currently running container, BEFORE docker commit occurs (default: /root/pre_commit.sh ). This option is useful if any operations shall be performed with the running processes (e.g. send a signal), because in the subsequent post operation - only filesystem operations will be available. Note that any changes done at this stage will affect the running container. commit.post.command.path : specified a path to a script within a docker image, that will be executed in a committed image, AFTER docker commit occurs (default: /root/post_commit.sh ). This hook can be used to perform any filesystem cleanup or other operations, that shall not affect the currently running processes. If a corresponding pre/post script is not found in the docker image - it will not be executed. For more details see here .","title":"Pre/Post-commit hooks implementation"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#restricting-manual-installation-of-the-nvidia-tools","text":"It was uncovered that some of the GPU-enabled runs are not able to initialize due to an issue describe at NVIDIA/nvidia-docker#825 . To limit a possibility of producing such docker images (which will not be able to start using GPU instance types) - a set of restrictions/notifications was implemented: A notification is now displayed (in the Web GUI), that warns a user about the risks of installing any of the nvidia packages manually. And that all cuda-based dockers shall be built using nvidia/cuda base images instead: Restrict users (to the reasonable extent) from installing those packages while running SSH/terminal session in the container. If user will try to install a restricted package - a warning will be shown (with an option to bypass it - for the advanced users):","title":"Restricting manual installation of the nvidia tools"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#setup-swap-files-for-the-cloud-vms","text":"This feature is addresses the same issues as the previous Notifications about high resources pressure by making the compute-intensive jobs runs more reliable. In certain cases jobs may fail with unexpected errors if the compute node runs Out Of Memory . v0.15 provides an ability for admin users to configure a default swap volume to the compute node being created. This allows to avoid runs failures due to memory limits. The size of the swap volume can be configured via cluster.networks.config item of the Preferences . It is accomplished by adding the similar json object to the platform's global or a region/cloud specific configuration: Options that can be used to configure swap : swap_ratio - defines a swap file size. It is equal the node RAM multiplied by that ratio. If ratio is 0, a swap file will not be created (default: 0) swap_location - defines a location of the swap file. If that option is not set - default location will be used (default: AWS will use SSD/gp2 EBS, Azure will be Temporary Storage ) See an example here .","title":"Setup swap files for the Cloud VMs"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#runs-system-paths-shall-be-available-for-the-general-user-account","text":"Previously, all the system-level directories (e.g. pipeline code location - $SCRIPTS_DIR , input/common data folders - $INPUT_DATA , etc.) were owned by the root user with read-only access to the general users. This was working fine for the pipeline runs, as they are executed on behalf of root . But for the interactive sessions (SSH/Web endpoints/Desktop) - such location were not writable. From now on - all the system-level location will be granted rwx access for the OWNER of the job (the user, who launched that run).","title":"Run's system paths shall be available for the general user account"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#renewed-wdl-visualization","text":"v0.15 offers an updated Web GUI viewer/editor for the WDL scripts. These improvements allow to focus on the WDL diagram and make it more readable and clear. Which very useful for large WDL scripts. Auxiliary controls (\"Save\", \"Revert changes\", \"Layout\", \"Fit to screen\", \"Show links\", \"Zoom out\", \"Zoom in\", \"Fullscreen\") are moved to the left side of the WDL GRAPH into single menu: WDL search capabilities are added. This feature allows to search for any task/variable/input/output within the script and focus/zoom to the found element. Search box is on the auxiliary controls menu and supports entry navigation (for cases when more than one item was found in the WDL): Workflow/Task editor is moved from the modal popup to the right floating menu PROPERTIES : See more details here .","title":"Renewed WDL visualization"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#queued-state-of-the-run","text":"Previously, user was not able to distinguish runs that are waiting in the queue and the runs that are being initialized (both were reporting the same state using the same icons). Now, a more clear run's state is provided - \"QUEUED\" state is introduced: During this phase of the lifecycle - a job is waiting in the queue for the available compute node. Typically this shall last for a couple of second and proceed to the initialization phase. But if this state lasts for a long time - it may mean that a cluster capacity is reached (limited by the administrator). This feature allows users to make a decision - whether to wait for run in a queue or stop it and resubmit. See more details here .","title":"\"QUEUED\" state of the run"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#help-tooltips-for-the-run-state-icons","text":"With the runs' QUEUED state introduction - we now have a good number of possible job phases. To make the phases meaning more clear - tooltips are provided when hovering a run state icon within all relevant forms, i.e.: Dashboard , Runs ( Active Runs / History /etc.), Run Log . Tooltips contain a state name in bold (e.g. Queued ) and a short description of the state and info on the next stage: See more details - Active runs states , Completed runs states and Home page .","title":"Help tooltips for the run state icons"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#vm-monitor-service","text":"For various reasons cloud VM instances may \"hang\" and become invisible to Cloud Pipeline services. E.g. VM was created but some error occurred during joining k8s cluster or a communication to the Cloud Providers API is interrupted. In this case Autoscaler service will not be able to find such instance and it won't be shut down. This problem may lead to unmonitored useless resource consumption and billing. To address this issue - a separate VM-Monitor service was implemented: Tracks all VM instances in the registered Cloud Regions Determines whether instances is in \"hang\" state Notifies a configurable set of users about a possible problem Notification recipients (administrators) may check the actual state of VM in Cloud Provider console and shut down VM manually. Additionally, VM-Monitor : Checks states of Cloud Pipeline's services (Kubernetes deployments). If any service changes it's state (i.e. goes down or up) - administrators will get the corresponding notification. List of such Kubernetes deployments to check includes all Cloud Pipeline's services by default, but also could be configed manually Checks all the PKI assets, available for the Platform, for the expiration - traverses over the list of directories that should contains certificate files, searches that certificates and verifies their expiration date. If some certificate expires less than in a certain amount of days - administrators also will get the corresponding notification. List of certificate directories to scan, certificate's mask and amount of days before expiration after which the notification will be sent are configurable","title":"VM monitor service"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#web-gui-caching","text":"Previously, Cloud Pipeline Web GUI was not using HTTP caching to optimize the page load time. Each time application was loaded - ~2Mb of the app bundle was downloaded. This caused \"non-optimal\" experience for the end-users. Now the application bundle is split into chunks, which are identified by the content-hash in names: If nothing is changed - no data will be downloaded If some part of the app is changed - only certain chunks will be downloaded, not the whole bundle Administrator may control cache period using the static.resources.cache.sec.period parameter in the application.properties of the Core API service.","title":"Web GUI caching"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#installation-via-pipectl","text":"Previous versions of the Cloud Pipeline did not offer any automated approach for deploying its components/services. All the deployment tasks were handed manually or by custom scripts. To simplify the deployment procedure and improve stability of the deployment - pipectl utility was introduced. pipectl offers an automated approach to deploy and configure the Cloud Pipeline platform, as well as publish some demo pipelines and docker images for NGS/MD/MnS tasks. Brief description and example installation commands are available in the pipectl's home directory . More sophisticated documentation on the installation procedure and resulting deployment architecture will be created further.","title":"Installation via pipectl"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#add-more-logging-to-troubleshoot-unexpected-pods-failures","text":"When a Cloud Pipeline is being for a long time (e.g. years), it is common to observe rare \"strange\" problems with the jobs execution. I.e. the following behavior was observed couple of times over the last year:","title":"Add more logging to troubleshoot unexpected pods failures"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#scenario-1","text":"Run is launched and initialized fine During processing execution - run fails Console logs print nothing, compute node is fine and is attached to the cluster","title":"Scenario 1"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#scenario-2","text":"Run is launched, compute node is up Run fails during initialization Console logs print the similar error message: failed to open log file \"/var/log/pods/**.log\": open /var/log/pods/**.log: no such file or directory Both scenarios are flaky and almost impossible to reproduce. To provide more insights into the situation - an extended node-level logging was implemented: kubelet logs (from all compute nodes) are now written to the files (via DaemonSet ) Log files are streamed to the storage, identified by storage.system.storage.name preference Administrators can find the corresponding node logs (e.g. by the hostname or ip that are attached to the run information) in that storage under logs/node/{hostname} See an example here .","title":"Scenario 2"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#displaying-information-on-the-nested-runs-within-a-parent-log-form","text":"Previously, if user launched a run, that has a number of children (e.g. a cluster run or any other case with the parent-id specified), he could view the children list only from \"Runs\" page. In v.0.15 a convenient opportunity to view the list of children directly in the parent's run logs form is implemented: For each child-run in the list the following information is displayed: State icons with help tooltips when hovering over them Pipeline name and version/docker image and version Run time duration Similar as a parent-run state, states for nested runs are automatically updated without page refreshing. So, you can watch for them in real time. If you click any of the children-runs, you will navigate to its log page. That feature is implemented for the comleted runs too: More information about nested runs displaying see here and here .","title":"Displaying information on the nested runs within a parent log form"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#environment-modules-support-for-the-cloud-pipeline-runs","text":"The Environment Modules package provides for the dynamic modification of a user's environment via modulefiles . In current version, an ability to configure the Modules support for the compute jobs is introduced, if this is required by any use case. For using facilities of the Environment Modules package, a new system parameter was added to the Cloud Pipeline: CP_CAP_MODULES (boolean) - enables installation and using the Modules for the current run (for all supported Linux distributions) If CP_CAP_MODULES system parameter is set - the Modules will be installed and made available. While installing, Modules will be configured to the source modulefiles path from the CP_CAP_MODULES_FILES_DIR launch environment variable (value of this variable could be set only by admins via system-level settings). If that variable is not set - default modulefiles location will be used. See an example here .","title":"Environment Modules support for the Cloud Pipeline runs"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#sharing-ssh-access-to-running-instances-with-other-usersgroups","text":"As was introduced in Release Notes v.0.13 , for certain use cases it is beneficial to be able to share applications with other users/groups. v0.15 introduces a feature that allows to share the SSH-session of any active run (regardless of whether the job type is interactive or not): The user can share an interactive run with others: \"Share with: ...\" parameter, within a run log form, can be used for this Specific users or whole groups can be set for sharing Once this is set - other users will be able to access run's endpoints Also you can share SSH access to the running instance via setting \" Enable SSH connection \" checkbox Also, the user can share a non-interactive run: \"Share with: ...\" parameter, within a run log form, can be used for this Specific users or whole groups can be set for sharing Once this is set - specified users/groups will be able to access the running instance via the SSH SERVICES widget within a Home dashboard page lists such \"shared\" services. It displays a \"catalog\" of services, that can be accessed by a current user, without running own jobs. To open shared instance application user should click the service name. To get SSH-access to the shared instance (regardless of whether the job type is interactive or not), the user should hover over the service \"card\" and click the SSH hyperlink For more information about runs sharing see 11.3. Sharing with other users or groups of users .","title":"Sharing SSH access to running instances with other user(s)/group(s)"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#allow-to-limit-the-number-of-concurrent-ssh-sessions","text":"Previously, some users could try to start a real big number of Web SSH sessions. If 1000+ SSH sessions are established via EDGE service, the performance will degrade. It is not common, but it could be critical as it affects all the users of the platform deploment. To avoid such cases, in v0.15 the pipectl parameter CP_EDGE_MAX_SSH_CONNECTIONS (with default value 25 ) for the EDGE server is introduced, that allows to control a number of simultaneous SSH connections to a single job. Now, if this max number will be reached, the next attemp to open another one Web SSH session to the same job will return a notification to the user and a new session will not be opened until the any one previous is closed:","title":"Allow to limit the number of concurrent SSH sessions"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#verification-of-dockerstorage-permissions-when-launching-a-run","text":"Users are allowed to launch pipeline, detached configuration or tool if they have a corresponding permission for that executable. But in some cases this verification is not enough, e.g. when user has no read permission for input parameter - in this case, run execution could cause an error. In v0.15 additional verification implemented that checks if: execution is allowed for specified docker image; read operations are allowed for input and common path parameters; write operations are allowed for output path parameters. If there are such permission issues, run won't be launched and special warning notifications will be shown to a user, e.g.: For more details see sections 6.2. Launch a pipeline , 7.2. Launch Detached Configuration and 10.5. Launch a Tool .","title":"Verification of docker/storage permissions when launching a run"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#ability-to-override-the-queuepe-configuration-in-the-ge-configuration","text":"Previously, if the Grid Engine was enabled, the following was configured: a single queue with all the hosts was creating, named \" main.q \" a single PE (Parallel Environment) was creating, named \" local \" In v0.15 , the overriding of the names of the queue / PE is implemented to be compatible with any existing scripts, that rely on a specific GE configuration (e.g. hardcoded). You can do it using two new System Parameters at the Launch or the Configuration forms: CP_CAP_SGE_QUEUE_NAME (string) - allows to override the GE's queue name (default: \" main.q \") CP_CAP_SGE_PE_NAME (string) - allows to override the GE's PE name (default: \" local \") More information how to use System Parameters when a job is launched see here .","title":"Ability to override the queue/PE configuration in the GE configuration"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#estimation-runs-disk-size-according-to-the-inputcommon-parameters","text":"Previously, if a job was run with the disk size, which was not enough to handle the job's inputs - it failed (e.g. 10Gb disk was set for a run, which processed data using STAR aligner, where the genome index file is 20Gb). In v0.15 , an attempt to handle some of such cases is implemented. Now, the Cloud Pipeline try to estimate the required disk size using the input/common parameters and warn the user if the requested disk is not enough. When a job is launching, the system try to get the size of all input/common parameters. The time of the size getting for all files is limited, as this may take too much for lots of small files. Limit for this time is set by the storage.listing.time.limit system preference (in milliseconds). Default: 3 sec (3000 milliseconds). If computation doesn't end in this timeout, accumulated size will return as is. If the resulting size of all input/common parameters is greater than requested disk size (considering cluster configuration) - the user will be warned: User can set suggested disk size or launch a job at user's own risk with the requested size. If calculated suggested disk size exceeds 16Tb (hard limit) a different warning message will be shown: The requested disk size for this run is <N> Gb, but the data that is going to be processed exceeds 16 Tb (which is a hard limit). Please use the cluster run configuration to scale the disks horizontally or reduce the input data volume. Do you want to use the maximum disk size 16 Tb anyway?","title":"Estimation run's disk size according to the input/common parameters"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#disabling-of-the-global-search-form-if-a-corresponding-service-is-not-installed","text":"Version 0.14 introduced the Global Search feature over all Cloud Pipeline objects. In current version, a small enhancement for the Global Search is implemented. Now, if the search.elastic.host system preference is not set by admin - other users will not be able to try search performing: the \"Search\" button will be hidden from the left menu keyboard search shortcut will be disabled","title":"Disabling of the Global Search form if a corresponding service is not installed"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#disabling-of-the-fs-mounts-creation-if-no-fs-mount-points-are-registered","text":"In the Cloud Pipeline , along with the regular data storages user can also create FS mounts - data storages based on the network file system: For the correct FS mount creation, at least one mount point shall be registered in the Cloud Pipeline Preferences. Now, if no FS mount points are registered for any Cloud Region in the System Preferences - user can not create a new FS mount, the corresponding button becomes invisible:","title":"Disabling of the FS mounts creation if no FS mount points are registered"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#displaying-resource-limit-errors-during-run-resuming","text":"User may hit a situation of resource limits while trying to resume previously paused run. E.g. instance type was available when run was initially launched, but at the moment of resume operation provider has no sufficient capacity for this type. Previously, in this case run could be failed with an error of insufficient resources. In v0.15 the following approach is implemented for such cases: resuming run doesn't fail if resource limits are hit. That run returns to the Paused state log message that contains a reason for resume failure and returning back to the Paused state is being added to the ResumeRun task user is notified about such event. The corresponding warning messages are displayed: at the Run logs page at the ACTIVE RUNS page (hint message while hovering the RESUME button) at the ACTIVE RUNS panel of the Dashboard (hint message while hovering the RESUME button)","title":"Displaying resource limit errors during run resuming"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#object-storage-creation-in-despite-of-that-the-corspolicies-could-not-be-applied","text":"Previously, if the Cloud service account/role had permissions to create object storages, but lacked permissions to apply CORS or other policies - object storage was created, but the Cloud Pipeline API threw an exception and storage was not being registered. This led to the creation of a \"zombie\" storage, which was not available via GUI, but existed in the Cloud. Currently, the Cloud Pipeline API doesn't fail such requests and storage is being registered normally. But the corresponding warning will be displayed to the user like this: The storage {storage_name} was created, but certain policies were not applied. This can be caused by insufficient permissions.","title":"Object storage creation in despite of that the CORS/Policies could not be applied"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#track-the-confirmation-of-the-blocking-notifications","text":"System events allow to create popup notifications for users. One of the notification types - the \"Blocking\" notification. Such event emerges in the middle of the window and requires confirmation from the user to disappear for proceeding with the GUI operations. In certain cases (e.g. for some important messages), it is handy to be able to check which users confirmed the notification. For that, in the current version the ability to view, which \"blocking\" notifications confirmed by specific user, was implemented for admins. Information about confirmed notifications can be viewed at the \" Attributes \" section of the specific user's profile page: Confirmed notifications are displayed as user attribute with the KEY confirmed_notifications (that name could be changed via the system-level preference system.events.confirmation.metadata.key ) and the VALUE link that shows summary count of confirmed notifications for the user. Click the VALUE link with the notification count to open the detailed table with confirmed notifications: For more details see \"blocking\" notifications track .","title":"Track the confirmation of the \"Blocking\" notifications"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#pipe-cli-warnings-on-the-jwt-expiration","text":"By default, when pipe CLI is being configured JWT token is given for one month, if user didn't select another expiration date. In v.0.15 extra pipe CLI warnings are introduced to provide users an information on the JWT token expiration: When pipe configure command is executed - the warning about the expiration date of the provided token is printed, if it is less than 7 days left: When --version option is specified - pipe prints dates of issue and expiration for the currently used token: When any other command is running - the warning about the expiration date of the provided JWT token is printed, if it is less than 7 days left: See more information about pipe CLI installation here .","title":"pipe CLI warnings on the JWT expiration"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#pipe-configuration-for-using-ntlm-authentication-proxy","text":"For some special customer needs, pipe configuration for using NTLM Authentication Proxy, when running in Linux, could be required. For that, several new options were added to pipe configure command: -nt or --proxy-ntlm - flag that enable NTLM proxy support -nu or --proxy-ntlm-user - username for NTLM proxy authorization -np or --proxy-ntlm-pass - password for NTLM proxy authorization -nd or --proxy-ntlm-domain - domain for NTLM proxy authorization If --proxy-ntlm is set, pipe will try to get the proxy value from the environment variables or --proxy option ( --proxy option has a higher priority). If --proxy-ntlm-user and --proxy-ntlm-pass options are not set - user will be prompted for username/password in an interactive manner. Valid configuration examples: User will be prompted for NTLM Proxy Username, Password and Domain: pipe configure --proxy-ntlm ... Username for the proxy NTLM authentication: user1 Domain of the user1 user: '' Password of the user1 user: Use http://myproxy:3128 as the \"original\" proxy address. User will not be prompted for NTLM credentials: pipe configure --proxy-ntlm --proxy-ntlm-user $MY_NAME --proxy-ntlm-pass $MY_PASS --proxy \"http://myproxy:3128\" See more information about pipe CLI installation and configure here .","title":"pipe configuration for using NTLM Authentication Proxy"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#execution-of-files-uploading-via-pipe-without-failures-in-case-of-lacks-read-permissions","text":"Previously, pipe storage cp / mv commands could fail if a \"local\" source file/dir lacked read permissions. For example, when user tried to upload to the \"remote\" storage several files and when the pipe process had reached one of files that was not readable for the pipe process, then the whole command was being failed, remaining files did not upload. In current version, the pipe process checks read permission for the \"local\" source (directories and files) and skip those that are not readable:","title":"Execution of files uploading via pipe without failures in case of lacks read permissions"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#run-a-single-command-or-an-interactive-session-over-the-ssh-protocol-via-pipe","text":"For the certain purposes, it could be conveniently to start an interactive session over the SSH protocol for the job run via the pipe CLI. For such cases, in v0.15 the pipe ssh command was implemented. It allows you, if you are the ADMIN or the run OWNER , to perform a single command or launch an interactive session for the specified job run. Launching of an interactive session: This session is similar to the terminal access that user can get via the GUI. Performing the same single command without launching an interactive session:","title":"Run a single command or an interactive session over the SSH protocol via pipe"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#perform-objects-restore-in-a-batch-mode-via-pipe","text":"Users can restore files that were removed from the data storages with enabled versioning. For these purposes, the Cloud Pipeline's CLI has the restore command which is capable of restoring a single object at a time. In v0.15 the ability to recursively restore the whole folder, deleted from the storage, was implemented. Now, if the source path is a directory, the pipe storage restore command gets the top-level deleted files from the source directory and restore them to the latest version. Also, to the restore command some options were added: -r or --recursive - flag allows to restore the whole directory hierarchy -i or --include [TEXT] - flag allows to restore only files which names match the [TEXT] pattern and skip all others -e or --exclude [TEXT] - flag allows to skip restoring of files which names match the [TEXT] pattern and restore all others Note : this feature is yet supported for AWS only. For more details about file restoring via the pipe see here .","title":"Perform objects restore in a batch mode via pipe"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#mounting-data-storages-to-linux-and-mac-workstations","text":"Previously, when users had to copy/move datasets to/from Cloud data storages via CLI, they could use only special pipe storage commands. That was not always comfortable or could lead to some functionality restrictions. In v0.15 the ability to mount Cloud data storages (both - File Storages and Object Storages) to Linux and Mac workstations (requires FUSE installed) was added. For the mounted storages, regular listing/read/write commands are supported, users can manage files/folders as with any general hard drive. Note : this feature is yet supported for AWS only. To mount a data storage into the local mountpoint the pipe storage mount command was implemented. It has two main options that are mutually exclusive: -f or --file specifies that all available file systems should be mounted into a mountpoint -b or --bucket [STORAGE_NAME] specifies a storage name to mount into a mountpoint Users can: leverage mount options, supported by underlying FUSE implementation, via -o option enable multithreading for simultaneously interaction of several processes with the mount point, via -t option trace all information about mount operations into a log-file, via -l option To unmount a mountpoint the pipe storage umount command was implemented. For more details about mounting data storages via the pipe see here .","title":"Mounting data storages to Linux and Mac workstations"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#allow-to-run-pipe-commands-on-behalf-of-the-other-user","text":"In the current version, the ability to run pipe commands on behalf of the other user was implemented. It could be convenient when administrators need to perform some operations on behalf of the other user (e.g. check permissions/act as a service account/etc.). This feature is implemented via the common option that was added to all pipe commands: --user|-u <USER_ID> (where <USER_ID> is the name of the user account). Note : the option isn't available for the following pipe commands: configure , --version , --help . If this option is specified - operation (command execution) will be performed using the corresponding user account, e.g.: In the example above, active runs were outputted from the admin account (firstly) and then on behalf of the user without ROLE_ADMIN role. Additionally, a new command pipe token <USER_ID> was implemented. It prints the JWT token for a specified user. This command also can be used with non-required option -d ( --duration ), that specified the number of days the token will be valid. If it's not set - the default value will be used, same as in the GUI. Example of using: Then, the generated JWT token could be used manually with the pipe configure command - to configure pipe CLI on behalf of the desired user. Note : both - the command ( pipe token ) and the option ( --user ) - are available only for admins. For more details see here .","title":"Allow to run pipe commands on behalf of the other user"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#ability-to-restrict-the-visibility-of-the-jobs","text":"Previously, Cloud Pipeline inherited the pipeline jobs' permissions from the parent pipeline object. So, if two users had permissions on the same pipeline, then when the first user had launched that pipeline - the second user also could view (not manage) launched run in the Active Runs tab. Now admin can restrict the visibility of the jobs for non-owner users. The setting of such visibility can get one of the following values: Inherit - the behavior is the same as described above (for the previous approach), when the runs visibility is controlled by the pipeline permissions. It is set as a default for the Cloud Pipeline environment Only owner - when only the person who launch a run can see it Jobs visibility could be set by different ways on several forms: within User management tab in the system-level settings admin can specify runs visibility for a user/group/role: within Launch section of Preferences tab in the system-level settings admin can specify runs visibility for a whole platform as global defaults - by the setting launch.run.visibility : Next hierarchy is set for applying of specified jobs visibility: User level - highest priority (specified for a user) Group level (specified for a group/role) Platform level launch.run.visibility (specified as global defaults via system-level settings) Note : admins can see all runs despite of settings","title":"Ability to restrict the visibility of the jobs"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#ability-to-perform-scheduled-runs-from-detached-configurations","text":"Previously, Cloud Pipeline allowed starting compute jobs only manually (API/GUI/CLI). But in certain use cases, it is beneficial to launch runs on a scheduled basis. In v0.15 the ability to configure a schedule for detached configuration was implemented: User is able to set a schedule for launch a run from the detached configuration: Schedule is defined as a list of rules - user is able to specify any number of them: For each rule in the list user is able to set the recurrence: If any schedule rule is configured for the detached configuration - a corresponding job (plain container or a pipeline) will be started accordingly in the scheduled day and time. See more details here .","title":"Ability to perform scheduled runs from detached configurations"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#the-ability-to-use-custom-domain-names-as-a-friendly-url-for-the-interactive-services","text":"In v0.13 the ability to set a \"friendly URL\" for the interactive services endpoint was implemented. It allows to configure the view of the interactive service endpoint: Default view: https://<host>/pipeline-<run-id>-<port> \"Friendly\" view: https://<host>/<friendly-url> In the current version this feature is expanded: users allow to specify a custom host. So the endpoint url now can look like: https://<custom-host> or https://<custom-host>/<friendly-url> . Note : custom host should exist, be valid and configured. The custom host is being specified into the same field as a \"friendly URL\" previously, e.g.: Final URL for the service endpoint will be generated using the specified host and friendly URL: For more details see here .","title":"The ability to use custom domain names as a \"friendly URL\" for the interactive services"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#displaying-of-the-additional-support-iconinfo","text":"In certain cases, users shall have a quick access to the help/support information (e.g. links to docs/faq/support request/etc.) In the current version, the ability to display additional \"support\" icon with the corresponding info in the bottom of the main menu was implemented: The displaying of this icon and the info content can be configured by admins via the system-level preference ui.support.template : this preference is empty by default - in this case the support icon is invisible if this preference contains any text ( Markdown -formatted): the support icon is visible specified text is displayed in the support icon tooltip (support info) For more details see UI system settings .","title":"Displaying of the additional support icon/info"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#pass-proxy-settings-to-the-dind-containers","text":"Previously, DIND containers configuration included only registry credentials and a couple of driver settings. In certain environments, it is not possible to access external networks (e.g. for the packages installation) without the proxy settings. So the users had to pass this manually every time when using the docker run command. In the current version, a new system preference launch.dind.container.vars is introduced. It allows to specify all the additions variables, which will be passed to the DIND containers (if they are set for the host environment). By default, the following variables are set for the launch.dind.container.vars preference (and so will be passed to DIND container): http_proxy , https_proxy , no_proxy , API , API_TOKEN . Variables are being specified as a comma-separated list. Example of using: At the same time, a new system parameter (per run) was added - CP_CAP_DIND_CONTAINER_NO_VARS , which disables described behavior. You can set it before any run if you don't want to pass any additional variations to the DIND container.","title":"Pass proxy settings to the DIND containers"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#interactive-endpoints-can-be-optionally-available-to-the-anonymous-users","text":"Cloud Pipeline allows sharing the interactive and SSH endpoints with the other users/groups. Previously, this necessary required the end-user to be registered in the Cloud Pipeline users database. For certain use-cases, it is required to allow such type of access for any user, who has successfully passed the IdP authentication but is not registered in the Cloud Pipeline and also such users shall not be automatically registered at all and remain Anonymous . In the current version, such ability is implemented. It's enabled by the following application properties: saml.user.auto.create=EXPLICIT_GROUP saml.user.allow.anonymous=true After that, to share any interactive run with the Anonymous - it's simple enough to share endpoints with the following user group - ROLE_ANONYMOUS_USER : At the Run logs page: The user should select the ROLE_ANONYMOUS_USER role to share: Sharing with the Anonymous will be displayed at the Run logs page: That's all. Now, the endpoint-link of the run could be sent to the Anonymous user. If that Anonymous user passes SAML authentication, he will get access to the endpoint. Attempts to open any other Platform pages will fail. For more details see here .","title":"Interactive endpoints can be (optionally) available to the anonymous users"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#notable-bug-fixes","text":"","title":"Notable Bug fixes"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#incorrect-behavior-of-the-global-search-filter","text":"#221 When user was searching for an entry, that may belong to different classes (e.g. issues and folders ) - user was not able to filter the results by the class.","title":"Incorrect behavior of the global search filter"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#committing-status-hangs","text":"#152 In certain cases, while committing pipeline with the stop flag enabled - the run's status hangs in Committing... state. Run state does not change even after the commit operation succeeds and a job is stopped.","title":"\"COMMITTING...\" status hangs"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#instances-of-metadata-entity-arent-correctly-sorted","text":"#150 Metadata entities (i.e. project-related metadata) sorting was faulty: Sort direction indicator (Web GUI) was displaying an inverted direction Entities were not sorted correctly","title":"Instances of Metadata entity aren't correctly sorted"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#tool-group-cannot-be-deleted-until-all-child-tools-are-removed","text":"#144 If there is a tool group in the registry, which is not empty (i.e. contains 1+ tools) - an attempt to delete it throws SQL error. It works fine if the child tools are dropped beforehand. Now, it is possible to delete such a group if a force flag is set in the confirmation dialog.","title":"Tool group cannot be deleted until all child tools are removed"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#missing-region-while-estimating-a-run-price","text":"#93 On the launch page, while calculating a price of the run, Cloud Provider's region was ignored. This way a calculation used a price of the specified instance type in any of the available regions. In practice, requested price may vary from region to region.","title":"Missing region while estimating a run price"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#cannot-specify-region-when-an-existing-object-storage-is-added","text":"#45 Web GUI interface was not providing an option to select a region when adding an existing object storage. And it was impossible to add a bucket from the non-default region.","title":"Cannot specify region when an existing object storage is added"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#acl-control-for-pipeline_user-and-role-entities-for-metadata-api","text":"#265 All authorized users were permitted to browse the metadata of users and roles entities. But those entries may contain a sensitive data, that shall not be shared across users. Now, a general user may list only personal user-level metadata. Administrators may list both users and roles metadata across all entries.","title":"ACL control for PIPELINE_USER and ROLE entities for metadata API"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#getting-logs-from-kubernetes-may-cause-outofmemory-error","text":"#468 For some workloads, container logs may become very large: up to several gigabytes. When we tried to fetch such logs it is likely to cause OutOfMemory error, since Kubernetes library tries to load it into a single String object. In current version, a new system preference was introduced: system.logs.line.limit . That preference sets allowable log size in lines. If actual pod logs exceeds the specified limit only log tail lines will be loaded, the rest will be truncated.","title":"Getting logs from Kubernetes may cause OutOfMemory error"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#aws-incorrect-nodeup-handling-of-spot-request-status","text":"#556 Previously, in a situation when an AWS spot instance created after some timeout - spot status wasn't updated correctly in the handling of spot request status . It might cause errors while getting spot instance info.","title":"AWS: Incorrect nodeup handling of spot request status"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#not-handling-clusters-in-autopause-daemon","text":"#557 Previously, if cluster run was launched with enabled \"Auto pause\" option, parent-run or its child-runs could be paused (when autopause conditions were satisfied, of course). It was incorrect behavior because in that case, user couldn't resume such paused runs and go on his work (only \"Terminate\" buttons were available). In current version, autopause daemon doesn't handle any clusters (\"Static\" or \"Autoscaled\"). Also now, if the cluster is configured - Auto pause checkbox doesn't display in the Launch Form for the On-Demand node types.","title":"Not handling clusters in autopause daemon"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#incorrect-pipe-cli-version-displaying","text":"#561 Previously, pipe CLI version displayed incorrectly for the pipe CLI installations performed via hints from the Cloud Pipeline System Settings menu.","title":"Incorrect pipe CLI version displaying"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#jwt-token-shall-be-updated-for-the-jobs-being-resumed","text":"#579 In cases when users launched on-demand jobs, paused them and then, after a long time period (2+ months), tried to resume such jobs - expired JWT tokens were set for them that led to different problems when any of the initialization routines tried to communicate with the API. Now, the JWT token and other variables as well are being updated when a job is being resumed.","title":"JWT token shall be updated for the jobs being resumed"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#trying-to-rename-file-in-the-data-storage-while-the-attributes-panel-is-opened-throws-an-error","text":"#520 Renaming file in the datastorage with opened \"Attributes\" panel caused an unexpected error.","title":"Trying to rename file in the data storage, while the \"Attributes\" panel is opened, throws an error"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#pipe-incorrect-behavior-of-the-nc-option-for-the-run-command","text":"#609 Previously, trying to launch a pipeline via the pipe run command with the single -nc option threw an error.","title":"pipe: incorrect behavior of the -nc option for the run command"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#cluster-run-cannot-be-launched-with-a-pretty-url","text":"#620 Previously, if user tried to launch any interactive tool with Pretty URL and configured cluster - an error appeared URL {Pretty URL} is already used for run {Run ID} . Now, pretty URL could be set only for the parent runs, for the child runs regular URLs are set.","title":"Cluster run cannot be launched with a Pretty URL"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#cloning-of-large-repositories-might-fail","text":"#626 When large repository (> 1Gb) was cloned (e.g. when a pipeline was being run) - git clone could fail with the OOM error happened at the GitLab server if it is not powerful enough. OOM was produced by the git pack-objects process, which tries to pack all the data in-memory. Now, git pack-objects memory usage is limited to avoid errors in cases described above.","title":"Cloning of large repositories might fail"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#system-events-html-overflow","text":"#630 If admin set a quite long text (without separators) into the message body of the system event notifications - the resulting notification text \"overflowed\" the browser window. Now, text wrapping is considered for such cases. Also, support of Markdown was added for the system notification messages:","title":"System events HTML overflow"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#aws-pipeline-run-initializenode-task-fails","text":"#635 Previously, if AWS spot instance could not be created after the specific number of attempts during the run initialization - such run was failed with the error, e.g.: Exceeded retry count (100) for spot instance. Spot instance request status code: capacity-not-available . Now, in these cases, if spot instance isn't created after specific attempts number - the price type is switched to on-demand and run initialization continues.","title":"AWS: Pipeline run InitializeNode task fails"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#git-sync-shall-not-fail-the-whole-object-synchronization-if-a-single-entry-errors","text":"#648 , #691 When the git-sync script processed a repository and failed to sync permissions of a specific user (e.g. git exception was thrown) - the subsequent users were not being processed for that repository. Now, the repository sync routine does not fail if a single user cannot be synced. Also, the issues with the synchronization of users with duplicate email addresses and users with empty email were resolved.","title":"git-sync shall not fail the whole object synchronization if a single entry errors"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#enddate-isnt-set-when-node-of-a-paused-run-was-terminated","text":"#743 Previously, when user terminated the node of a paused run - endDate for that run wasn't being set. This was leading to wrong record of running time for such run.","title":"endDate isn't set when node of a paused run was terminated"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#aws-nodeup-retry-process-may-stuck-when-first-attempt-to-create-a-spot-instance-failed","text":"#744 Previously, if first nodeup attempt failed due to unavailablity to connect on 8888 port (in expected amounts of attempts) after getting instance running state, the second nodeup attempt might stuck because it waited for the same instance (associated with existed SpotRequest for the first attempt) to be up. But it couldn't happen - this instance was already in terminating state after the first attempt. Now, checks that instance associated with SpotRequest (created for the first attempt) is in appropriate status, if not - a new SpotRequest is being created and the nodeup process is being started from scratch.","title":"AWS: Nodeup retry process may stuck when first attempt to create a spot instance failed"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#resume-job-timeout-throws-strange-error-message","text":"#832 Previously, the non-informative error message was shown if the paused run could't be resumed in a reasonable amount of time - the count of attempts to resume was displaying incorrectly.","title":"Resume job timeout throws strange error message"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#ge-autoscaler-doesnt-remove-dead-additional-workers-from-cluster","text":"#946 Previously, the Grid Engine Autoscaler didn't properly handle dead workers downscaling. For example, if some spot worker instance was preempted during the run then the autoscaler could not remove such worker from GE. Moreover, such cluster was blocked from accepting new jobs.","title":"GE autoscaler doesn't remove dead additional workers from cluster"},{"location":"release_notes/v.0.15/v.0.15_-_Release_notes/#broken-layouts","text":"#553 , #619 , #643 , #644 , #915 Previously, pipeline versions page had broken layout if there were pipeline versions with long description. Global search page was not rendered correctly when the search results table had too many records. When a list of items in the docker groups selection dialog was long - it was almost impossible to use a search feature, as the list hid immediately. Some of the other page layouts also were broken.","title":"Broken layouts"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/","text":"Cloud Pipeline v.0.16 - Release notes Google Cloud Platform Support System logs Displaying Cloud Provider's icon Configurable timeout of GE Autoscale waiting Storage mounts data transfer restrictor Extended recursive symlinks handling Displaying of the latest commit date/time Renaming of the GitLab repository in case of Pipeline renaming Pushing pipeline changes to the GitLab on behalf of the user Allowing to expose compute node FS to upload and download files Resource usage form improvement View the historical resources utilization Ability to schedule automatic pause/restart of the running jobs Update pipe CLI version Blocking/unblocking users and groups Displaying additional node metrics Export user list Displaying SSH link for the active runs in the Dashboard view Enable Slurm for the Cloud Pipeline's clusters The ability to generate the pipe run command from the GUI pipe CLI: view tools definitions List the users/groups/objects permissions globally via pipe CLI Storage usage statistics retrieval via pipe GE Autoscaler respects CPU requirements of the job in the queue Restrictions of \"other\" users permissions for the mounted storage Search the tool by its version/package name The ability to restrict which run statuses trigger the email notification The ability to force the specific Cloud Provider for an image Restrict mounting of data storages for a given Cloud Provider Ability to \"symlink\" the tools between the tools groups Notable Bug fixes Parameter values changes cannot be saved for a tool Packages are duplicated in the tool's version details Autoscaling cluster can be autopaused pipe : not-handled error while trying to execute commands with invalid config Setting of the tool icon size NPE while building cloud-specific environment variables for run Worker nodes fail due to mismatch of the regions with the parent run Worker nodes shall not be restarted automatically Uploaded storage file content is downloaded back to client GUI improperly works with detached configurations in a non-default region Detached configuration doesn't respect region setting Incorrect behavior of the \"Transfer to the cloud\" form in case when a subfolder has own metadata Incorrect displaying of the \"Start idle\" checkbox Limit check of the maximum cluster size is incorrect Fixed cluster with SGE and DIND capabilities fails to start Azure: Server shall check Azure Blob existence when a new storage is created Azure: pipe CLI cannot transfer empty files between storages Azure: runs with enabled GE autoscaling doesn't stop Incorrect behavior while download files from external resources into several folders Detach configuration doesn't setup SGE for a single master run Broken layouts Google Cloud Platform Support One of the major v0.16 features is a support for the Google Cloud Platform . All the features, that were previously used for AWS and Azure , are now available in all the same manner, from all the same GUI/CLI, for GCP . This provides an even greater level of a flexibility to launch different jobs in the locations, closer to the data, with cheaper prices or better compute hardware in depend on a specific task. System logs In the current version, the \"Security Logging\" was implemented. Now, the system records audit trail events: users' authentication attempts users' profiles modifications platform objects' permissions management access to interactive applications from pipeline runs other platform functionality features Logs are collected/managed at the Elasticsearch node and backed up to the object storage (that could be configured during the platform deployment). The administrator can view/filter these logs via the GUI - in the System-level settings, e.g.: Each record in the logs list contains: Field Description Date The date and time of the log event Log status The status of the log message ( INFO , ERROR , etc.) Log message Description of the log event User User name who performed the event Service Service name that registered the event ( api-srv , edge ) Type Log message type (currently, only security type is available) For more details see here . Displaying Cloud Provider's icon for the storage/compute resources As presented in v0.15 , Cloud Pipeline can manage multi Cloud Providers in a single installation. In the current version, useful icon-hints with the information about using Cloud Provider are introduced. If a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - corresponding icons/text information are displaying next to the cloud resource. Such cloud resources are: Object/File Storages (icons in the Library , at the \"DATA\" panel of the Dashboard etc.) Regions (icons in the Cloud Regions configuration, at the Launch form etc.) Running jobs : text hints (at the RUNS page) icons (at the Run logs page, at the \"RUNS\" panels of the Dashboard ) Note : this feature is not available for deployments with a single Cloud Provider. Examples of displaying Cloud Region icons/info see in sections 6. Manage Pipeline , 7. Manage Detached configuration , 8. Manage Data Storage and 18. Home page . Configurable timeout of GE Autoscale waiting for a worker node up Previously, GE Autoscaler waited for a worker node up for a fixed timeout. This could lead to incorrect behavior for specific CLoud Providers , because the timeout can be very different. Current version extracts GE Autoscaler polling timeout to a new system preference ge.autoscaling.scale.up.polling.timeout . That preference defines how many seconds GE Autoscaler should wait for pod initialization and run initialization . Default value is 600 seconds ( 10 minutes). Storage mounts data transfer restrictor Users may perform cp or mv operations of the large files (50+ Gb) to and from the fuse-mounted storages. It is uncovered that such operations are not handled properly within the fuse implementations. Commands may hang for a long timeout or produce zero-sized result. The suggested more graceful approach for the copying of the large files is to use pipe cp / pipe mv commands, which are behaving correctly for the huge volumes. To avoid users of performing usual cp or mv commands for operations with the large files - now, Cloud Pipeline warns them about possible errors and suggest to use corresponding pipe commands. Specified approach is implemented in the following manner: if a cp / mv command is called with the source/dest pointing to the storage (e.g. /cloud-data/<storage_path>/... ) - the overall size of the data being transferred is checked - if that size is greater than allowed, a warning message will be shown, e.g.: this warning doesn't abort the user's command execution, it is continued appearance of this warning is configured by the following launch environment variables (values of these variables could be set only by admins via system-level settings): CP_ALLOWED_MOUNT_TRANSFER_SIZE - sets number of gigabytes that is allowed to be transferred without warning. By default 50 Gb . CP_ALLOWED_MOUNT_TRANSFER_SIZE_TIMEOUT - sets number of seconds that the transfer size retrieving operation can take. By default 5 seconds . CP_ALLOWED_MOUNT_TRANSFER_FILES - sets number of files that is allowed to be transferred without warning. Supported only for Azure Cloud Provider. By default 100 files . Note : this feature is not available for NFS / SMB mounts, only for object storages. Extended recursive symlinks handling There could be specific cases when some services execute tasks using the on-prem storages, where \"recursive\" symlinks are presented. This causes the Cloud Pipeline Data transfer service to follow symlinks infinitely. In v0.16 , a new feature is introduced for Data transfer service to detect such issues and skip the upload for files/folders, that cause infinite loop over symlinks. A new option -sl ( --symlinks ) was added to the pipe storage cp / mv operations to handle symlinks (for local source) with the following possible values: follow - follow symlinks ( default ) skip - do not follow symlinks filter - follow symlinks but check for cyclic links and skip them Example for the folder with recursive and non-recursive symbolic links: Also options were added to the Data transfer service to set symlink policy for transfer operations. For more details about pipe storage cp / mv operations see here . Displaying of the latest commit date/time Users can modify existing tools and then commit them to save performed changes. It can be done by COMMIT button on the run's details page: Previously, if user committed some tool, only commit status was shown on the run's details page. In the current version, displaying of the date/time for the tool latest commit is added: For more details about tool commit see here . Renaming of the GitLab repository in case of Pipeline renaming Pipeline in the Cloud Pipeline environment is a workflow script with versioned source code, documentation, and configuration. Under the hood, it is a git repository. Previously, if the Pipeline object was renamed - the underlying GitLab repository was keeping the previous name. In the current version, if user renames a Pipeline the corresponding GitLab repository will be also automatically renamed: Need to consider in such case that the clone/pull/push URL changes too. Make sure to change the remote address, if you use the Pipeline somewhere else. For more details see here . Pushing pipeline changes to the GitLab on behalf of the user If the user saves the changed Pipeline object - it actually means that a new commit is created and pushed to the corresponding GitLab repo. Previously, all the commits pushed to the GitLab via the Cloud Pipeline GUI were made on behalf of the service account . This could break traceability of the changes. In current version, the author of the commit is displayed in the Web GUI (for all Pipeline versions - the draft and released), the commits are performed on behalf of the real user: Allowing to expose compute node FS to upload and download files For the interactive runs users are processing data in ad-hoc manner, which requires upload data from the local storage to the cloud and download results from the cloud to the local storage. Cloud Pipeline supports a number of options for the user to perform that data transfers for the interactive runs: via the Object Storage (using Web GUI or CLI) via the File Storage (using Web GUI, CLI, WebDav-mounted-drive) Previously, view, download, upload and delete operations required an intermediate location (bucket or fs) to be used. It might confuse user when a small dataset shall be loaded to the specific location within a run's filesystem. In the current version, direct exposure of the run's filesystem is supported. The BROWSE hyperlink is displayed on the Run logs page after a job had been initialized: User can click the link and a Storage browser Web GUI will be loaded: User is able to: view files and directories download and delete files and directories upload files and directories search files and directories For more details see here . Resource usage form improvement In v0.16 , the number of filters were added to the Monitor cluster nodes feature: Common range for all charts User can synchronize the time period for all plots. To do so user should mark the \"Common range for all charts\" filter. If this filter is unmarked, user can zoom any plot without any change for others. Live update The plots data will be updated every 5 seconds in a real-time manner. The fields with dates will be updated as well. Set range User can select the predefined time range for all plots from the list: Whole range Last week Last day Last hour Date filter User can specify the Start and the End dates for plots. The system will substitute the node creating date as the Start date and current date for the End date, if user doesn't select anything. All filters are working for all plots simultaneously: data for all plots will be dynamically updated as soon as the user changes filter value. For more details see here . Allow to view the historical resources utilization Another convenient feature that was implemented in v0.16 linked to the Cluster nodes monitor is viewing of the detailed history of the resource utilization for any jobs. Previously, it was available only for active jobs. Now, users can view the utilization data even for completed (succeed/stopped/failed) jobs for debugging/optimization purposes. The utilization data for all runs is stored for a preconfigured period of time that is set by the system preference system.resource.monitoring.stats.retention.period (defaults to 5 days). I.e. if the job has been stopped and the specified time period isn't over - the user can access to the resources utilization data of that job: Open the Run logs page for the completed job: Click the node name hyperlink The Monitor page of the node resources utilization will be opened: Also now, users have the ability to export the utilization information into a .csv file. This is required, if the user wants to keep locally the information for a longer period of time than defined by system.resource.monitoring.stats.retention.period : The user can select the interval for the utilization statistics output and export the corresponding file. For more details see here . Ability to schedule automatic pause/restart of the running jobs For certain use cases (e.g. when Cloud Pipeline is used as a development/research environment) users can launch jobs and keep them running all the time, including weekends and holidays. To reduce costs, in the current version, the ability to set a Run schedule was implemented. This feature allows to automatically pause/resume runs, based on the configuration specified. This feature is applied only to the \"Pausable\" runs (i.e. \"On-demand\" and non-cluster): The user (who has permissions to pause/resume a run) is able to set a schedule for a run being launched: Schedule is defined as a list of rules - user is able to specify any number of them: For each rule in the list user is able to set the action ( PAUSE / RESUME ) and the recurrence: If any schedule rule is configured for the launched active run - that run will be paused/restarted accordingly in the scheduled day and time. Also, users (who have permissions to pause/resume a run) can create/view/modify/delete schedule rules anytime run is active via the Run logs page: See more details here (item 5). Update pipe CLI version Previously, if the user installed pipe CLI to his local workstation, used it some time - and the Cloud Pipeline API version could be updated during this period - so the user had to manually perform complete re-installing of pipe CLI every time to have an actual version. Currently, the pipe update command to update the Cloud Pipeline CLI version was implemented. This command compare the CLI and API versions. If the CLI version is less than the API one, it will update the CLI - the latest Cloud Pipeline CLI version will be installed. Otherwise no actions will be performed: For more details see here . Blocking/unblocking users and groups In current version, the ability for administrators to set the \"blocked\" flag for the specific user or a whole group is implemented. This flag prevents user(s) to access Cloud Pipeline platform by the GUI. Administrators can block or unblock users/groups via the User management tab of the System Settings dashboard. For example, for a user: For the blocked user the special label will be displayed aside to the user's name: The blocked user won't be able to access Cloud Pipeline via the GUI. The error message will be displayed when trying to login: Note : any content generated by the blocked users is kept in the Cloud Pipeline, all the log trails on the users activities are kept as well. Only access to the platform is being restricted. To unblock click the UNBLOCK button that appeared instead of the BLOCK button: All these actions could be performed also to a group. Users from a blocked group will not have access to the platform. For more details about users blocking/unblocking see here . For more details about groups blocking/unblocking see here . Displaying additional node metrics at the Runs page In v0.16 , the displaying of high-level metrics information for the Active Runs is implemented: - \"Idle\" - this auxiliary label is shown when node's CPU consumption is lower than a certain level, defined by the admin. This label should attract the users attention cause such run may produce extra costs. - \"Pressure\" - this auxiliary label is shown when node's Memory/Disk consumption is higher than a certain level, defined by the admin. This label should attract the users attention cause such run may accidentally fail. These labels are displayed: at the Runs page at the Run logs page at the main dashboard (the ACTIVE RUNS panel) Note : if a user clicks this label from the Runs or the Run logs page the Cluster node Monitor will be opened to view the current node consumption. Admins can configure the emergence of these labels by the set of previous implemented system-level parameters. For the IDLE label: system.max.idle.timeout.minutes - specifies the duration in minutes how often the system should check node's activity system.idle.cpu.threshold - specifies the percentage of the CPU utilization, below which label will be displayed For the PRESSURE label: system.disk.consume.threshold - specifies the percentage of the node disk threshold above which the label will be displayed system.memory.consume.threshold - specifies the percentage of the node memory threshold above which the label will be displayed For more details see here . Export user list In the current version, the ability to export list of the platform users is implemented. This feature is available only for admins via User management tab of the system-level settings: Admin can download list with all users and their properties (user name, email, roles, attributes, state, etc.) by default or custom configure which properties should be exported: The downloaded file will have the .csv format. For more details see here . Displaying SSH link for the active runs in the Dashboard view Users can run SSH session over launched container. Previously, SSH link is only available from the Run logs page. In v0.16 , a new helpful capability was implemented that allows to open the SSH connection right from the Active Runs panel of the main Dashboard: That SSH link is available for all the non-paused active jobs (interactive and non-interactive) after all run's checks and initializations have passed (same as at the Run logs page). Enable Slurm workload manager for the Cloud Pipeline's clusters A new feature for the Cloud Pipeline's clusters was implemented in v0.16 . Now, Slurm can be configured within the Cluster tab. It is available only for the fixed size clusters. To enable this feature - tick the Enable Slurm checkbox and set the child nodes count at cluster settings. By default, this checkbox is unticked. Also users can manually enable Slurm functionality by the CP_CAP_SLURM system parameter: This feature allows you to use the full stack of Slurm cluster's commands to allocate the workload over the cluster, for example: For more information about using Slurm via the Cloud Pipeline see here . The ability to generate the pipe run command from the GUI A user has a couple of options to launch a new job in the Cloud Pipeline: API CLI GUI The easiest way to perform it is the GUI . But for automation purposes - the CLI is much more handy. Previously, users had to construct the commands manually, which made it hard to use. Now, the ability to automatically generate the CLI commands for job runs appeared in the GUI . Now, users can get a generated CLI command (that assembled all the run information): at the Launch form: at the Run logs form: Note : this button is available for completed runs too Once click these buttons - the popup with the corresponding pipe run command will appear: User can copy such command and paste it to the CLI for further launch. Also user can select the API tab in that popup and get the POST request for a job launch: See an example here . pipe CLI: view tools definitions In v0.16 the ability to view details of a tool/tool version or tools group via the CLI was implemented. The general command to perform these operations: pipe view-tools [OPTIONS] Via the options users can specify a Docker registry ( -r option), a tools group ( -g option), a tool ( -t option), a tool version ( -v option) and view the corresponding information. For example, to show a full tools list of a group: To show a tool definition with versions list: Also the specifying of \"path\" to the object (registry/group/tool) is supported. The \"full path\" format is: <registry_name>:<port>/<group_name>/<tool_name>:<verion_name> : For more details and usage examples see here . List the users/groups/objects permissions globally via pipe CLI Administrators may need to receive the following information - in a quick and convenient way: Which objects are accessible by a user? Which objects are accessible by a user group? Which user(s)/group(s) have access to the object? The lattest case was implemented early - see the command pipe view-acl . For other cases, new commands were implemented: pipe view-user-objects <Username> [OPTIONS] and pipe view-group-objects <Groupname> [OPTIONS] - to get a list of objects accessible by a user and by a user group/role respectively: Each of these commands has the non-required option -t ( --object-type ) <OBJECT_TYPE> - to restrict the output list of accessible objects only for the specific type (e.g., \"pipeline\" or \"tool\", etc.): For more details see: pipe view-user-objects and pipe view-group-objects . Storage usage statistics retrieval via pipe In some cases, it may be necessary to obtain an information about storage usage or some inner folder(s). In the current version, the command pipe storage du is implemented that provides \"disk usage\" information on the supported data storages/path: number of files in the storage/path summary size of the files in the storage/path In general, the command has the following format: pipe storage du [OPTIONS] [STORAGE] Without specifying any options and storage this command prints the full list of the available storages (both types - object and FS) with the \"usage\" information for each of them. With specifying the storage name this command prints the \"usage\" information only by that storage, e.g.: With -p ( --relative-path ) option the command prints the \"usage\" information for the specified path in the required storage, e.g.: With -d ( --depth ) option the command prints the \"usage\" information in the required storage (and path) for the specified folders nesting depth, e.g.: For more details about that command and full list of its options see here . GE Autoscaler respects CPU requirements of the job in the queue At the moment, GE Autoscaler treats each job in the queue as a single-core job. Previously, autoscale workers could have only fixed instance type (the same as the master) - that could lead to unschedulable jobs in the queue - for examle, if one of the jobs requested 4 slots in the local parallel environment within a 2 -cored machine. Described job waited in a queue forever (as autoscaler could setup only new 2 -cored nodes). In the current version the hybrid behavior for the GE Autoscaler was implemented, that allows processing the data even if the initial node type is not enough. That behavior allows to scale-up the cluster (attach a worker node) with the instance type distinct of the master - worker is being picked up based on the amount of unsatisfied CPU requirements of all pending jobs (according to required slots and parallel environment types). To enable hybrid mode for auto-scaled cluster set the corresponding checkbox in the cluster settings before the run: On the other hand, there are several System parameters to configure hybrid behavior in details: CP_CAP_AUTOSCALE_HYBRID ( boolean ) - enables the hybrid mode ( the same as the \"Enable Hybrid cluster\" checkbox setting ). In that mode the additional worker type can vary within either master instance type family (or CP_CAP_AUTOSCALE_HYBRID_FAMILY if specified). If disabled or not specified - the GE Autoscaler will work in a general regimen (when scaled-up workers have the same instance type as the master node) CP_CAP_AUTOSCALE_HYBRID_FAMILY ( string ) - defines the instance \"family\", from which the GE Autoscaler should pick up the worker node in case of hybrid behavior. If not specified (by default) - the GE Autoscaler will pick up worker instance from the same \"family\" as the master node CP_CAP_AUTOSCALE_HYBRID_MAX_CORE_PER_NODE ( string ) - determines the maximum number of instance cores for the node to be scaled up by the GE Autoscaler in case of hybrid behavior Also now, if no matching instance is present for the job (no matter - in hybrid or general regimen), GE Autoscaler logs error message and rejects such job: for example, when try to request 32 slots for the autoscaled cluster launched in hybrid mode with the parameter CP_CAP_AUTOSCALE_HYBRID_MAX_CORE_PER_NODE set to 20 : and in the logs console at the same time: For more details about GE Autoscaler see here . Restrictions of \"other\" users permissions for the storages mounted via the pipe storage mount command As was introduced in Release Notes v.0.15 , the ability to mount Cloud data storages (both - File Storages and Object Storages) to Linux and Mac workstations (requires FUSE installed) was added. For that, the pipe storage mount command was implemented. Previously, Cloud Pipeline allowed read access to the mounted cloud storages for the other users, by default. This might introduce a security issue when dealing with the sensitive data. In the current version, for the pipe storage mount command the new option is added: -m ( --mode ), that allows to set the permissions on the mountpoint at a mount time. Permissions are being configured by the numerical mask - similarly to chmod Linux command. E.g. to mount the storage with RW access to the OWNER , R access to the GROUP and no access to the OTHERS : If the option -m isn't specified - the default permission mask will be set - 700 (full access to the OWNER ( RWX ), no access to the GROUP and OTHERS ). For more details about mounting data storages via the pipe see here . The ability to find the tool by its version/package name Cloud Pipeline allows searching for the tools in the registry by its name or description. But in some cases, it is more convenient and useful to find which tool contains a specific software package and then use it. In the current version, this ability - to find a tool by its content - is implemented based on the global search capabilities. Now, via the Global Search, you may find a tool by its version name, e.g.: And by the package name (from any available ecosystem), e.g.: The ability to restrict which run statuses trigger the email notification Cloud Pipeline can be configured to send the email to the owner of the job (or specific users) if its status is changed. But in certain cases - it is not desired to get a notification about all changes of the run state. To reduce a number of the emails in these cases, the ability to configure, which statuses are triggering the notifications, is implemented in v0.16 . Now, when the administrator configures the emails sending linked to the run status changes - he can select specific run states that will trigger the notifications. It is done through the PIPELINE_RUN_STATUS section at the Email notifications tab of the System Settings (the \" Statuses to inform \" field): The email notifications will be sent only if the run enters one of the selected states. Note : if no statuses are selected in the \" Statuses to inform \" field - email notifications will be sent as previously - for all status changes. For more information how to configure the email notifications see here . The ability to force the usage of the specific Cloud Provider/Region for a given image Previously, the platform allowed to select a Cloud Provider (and Cloud Region ) for a particular job execution via the Launch Form, but a tool/version itself didn't not have any link with a region. In certain cases, it's necessary to enforce users to run some tools in a specific Cloud Provider / Region . In the current version, such ability was implemented. The Tool / Version Settings forms contain the field for specifying a Cloud Region , e.g.: By default, this parameter has Not configured value. This means, that a tool will be launched in a Default region (configured by the Administrator in the global settings). Or a user can set any allowed Cloud Region / Provider manually. This behavior will be the same as previously. Admin or a tool owner can forcibly set a specific Cloud Region / Provider where the run shall be launched, e.g.: Then, if a specific Cloud Region / Provider is configured - users will have to use it, when launching a tool (regardless of how the launch was started - with default or custom settings): And if a user does not have access to that Cloud Region / Provider - tool won't launch: Note : if a specific Cloud Region / Provider is being specified for the Tool, in general - this action enforce the Region / Provider only for the latest version of that tool. For other versions the settings will remain previous. See for more details about tool execution settings here . See for more details about tool version execution settings here . Restrict mounting of data storages for a given Cloud Provider Previously, Cloud Pipeline attempted to mount all data storages available for the user despite the Cloud Providers / Regions of these storages. E.g. if a job was launched in the GCP , but the user has access to AWS S3 buckets - they were also mounted to the GCP instance. In the current version, the ability to restrict storage mount availability for a run, based on its Cloud Provider / Region , was implemented. Cloud Regions system configuration now has a separate parameter \" Mount storages across other regions \": This parameter has 3 possible values: None - if set, storages from this region will be unavailable for a mount to any jobs. Such storages will not be available even to the same regions (e.g. storage from AWS us-east-1 will be unavailable for a mount to instances launched in AWS eu-central-1 or any GCP region and even in AWS us-east-1 ) Same Cloud - if set, storages from this region will be available only to different Cloud Regions of the same Cloud Provider (e.g. storage from AWS us-east-1 will be available to instances launched in AWS eu-central-1 too, but not in any GCP region) All - if set, storages from this region will be available to all other Cloud Regions / Providers Ability to \"symlink\" the tools between the tools groups The majority of the tools are managed by the administrators and are available via the library tool group. But for some of the users it would be convenient to have separate tool groups, which are going to contain a mix of the custom tools (managed by the users themselves) and the library tools (managed by the admins). For the latter ones the ability to create \" symlinks \" into the other tool groups was implemented. \"Symlinked\" tools are displayed in that users' tool groups as the original tools but can't be edited/updated. When a run is started with \"symlinked\" tool as docker image it is being replaced with original image for Kubernetes pod spec. Example of the \"symlinked\" ubuntu tool: The following behavior is implemented: to create a \"symlink\" to the tool, the user shall have READ access to the source tool and WRITE access to the destination tool group for the \"symlinked\" tool all the same description, icon, settings as in the source image are displayed. It isn't possible to make any changes to the \"symlink\" data (description, icon, settings. attributes, issues, etc.), even for the admins admins and image OWNERs are able to manage the permissions for the \"symlinks\". Permissions on the \"symlinked\" tools are configured separately from the original tool two levels of \"symlinks\" is not possible (\"symlink\" to the \"symlinked\" tool can't be created) it isn't possible to \"push\" into the \"symlinked\" tool For more details see here . Notable Bug fixes Parameter values changes cannot be saved for a tool #871 Previously, if a user changed only the parameter value within a tool's settings form - the SAVE button stayed still unavailable. One had to modify some other option (e.g. disk size) to save the overall changes. Packages are duplicated in the tool's version details #843 Previously, certain tools packages were duplicated in the PACKAGES details page of the tools version. Autoscaling cluster can be autopaused #819 Previously, when users launched auto-scaled clusters without default child-nodes and the PAUSE action was specified as action for the \"idle\" run (via system-levels settings), such cluster runs could be paused. Any cluster runs shall not have the ability to be paused, only stopped. pipe : not-handled error while trying to execute commands with invalid config #750 Previously, if pipe config contained some invalid data (e.g. outdated or invalid access token), then trying to execute any pipe command had been causing an not-handled error. Setting of the tool icon size #493 Previously, setting of any value for the maximum tool's icon size via the sytem-level preference misc.max.tool.icon.size.kb didn't lead to anything - restriction for the size while trying to change an icon was remaining the same - 50 Kb. NPE while building cloud-specific environment variables for run #486 For each run a set of cloud-specific environment variables (including account names, credentials, etc.) is build. This functionality resulted to fails with NPE when some of these variables are null . Now, such null variables are filtered out with warn logs. Worker nodes fail due to mismatch of the regions with the parent run #485 In certain cases, when a new child run was launching in cluster, cloud region was not specified directly and it might be created in a region differing from the parent run, that could lead to fails. Now, worker runs inherit parent's run cloud region. Worker nodes shall not be restarted automatically #483 Cloud Pipeline has a functionality to restart so called batch job runs automatically when run is terminated due to some technical issues, e.g. spot instance termination. Previously, runs that were created as child nodes for some parent run were also restarted. Now, automatically child reruns for the described cases with the batch job runs are rejected. Uploaded storage file content is downloaded back to client #478 Cloud Pipeline clients use specific POST API method to upload local files to the cloud storages. Previously, this method not only uploaded files to the cloud storage but also mistakenly returned uploaded file content back to the client. It led to a significant upload time increase. GUI improperly works with detached configurations in a non-default region #476 Saved instance type of a non-default region in a detached configuration wasn't displayed in case when such configuration was reopened (instance type field was displayed as empty in that cases). Detached configuration doesn't respect region setting #458 Region setting was not applied when pipeline is launched using detached configuration. Now, cloud region ID is merged into the detached configuration settings. Incorrect behavior of the \"Transfer to the cloud\" form in case when a subfolder has own metadata #434 Previously, when you tried to download files from external resources using metadata (see here ) and in that metadata's folder there was any subfolder with its own metadata - on the \"Transfer to the Cloud\" form attributes (columns) of both metadata files were mistakenly displaying. Incorrect displaying of the \"Start idle\" checkbox #418 If for the configuration form with several tabs user was setting the Start idle checkbox on any tab and then switched between sub-configurations tabs - the \"checked\" state of the Start idle checkbox didn't change, even if Cmd template field was appearing with its value (these events are mutually exclusive). Limit check of the maximum cluster size is incorrect #412 Maximum allowed number of runs (size of the cluster) created at once is limited by system preference launch.max.scheduled.number . This check used strictly \"less\" check rather then \"less or equal\" to allow or deny cluster launch. Now, the \"less or equal\" check is used. Fixed cluster with SGE and DIND capabilities fails to start #392 Previously, fixed cluster with both CP_CAP_SGE and CP_CAP_DIND_CONTAINER options enabled with more than one worker failed to start. Some of the workers failed on either SGEWorkerSetup or SetupDind task with different errors. Scripts were executed in the same one shared analysis directory. So, some workers could delete files downloaded by other workers. Azure: Server shall check Azure Blob existence when a new storage is created #768 During the creation of AZ Storage, the validation whether Azure Blob exists or not didn't perform. In that case, if Azure Blob had already existed, the user was getting failed request with Azure exception. Azure: pipe CLI cannot transfer empty files between storages #386 Previously, empty files couldn't be transferred within a single Azure storage or between two Azure storages using pipe CLI, it throwed an error. So for example, a folder that contained empty files couldn't be copied correctly. Azure: runs with enabled GE autoscaling doesn't stop #377 All Azure runs with enabled GE autoscaling were stuck after the launch.sh script has finished its execution. Daemon GE autoscaler process kept container alive. It was caused by the run process stdout and stderr aren't handled the same way for different Cloud Provider. So background processes launched from launch.sh directly could prevent Azure run finalization. Incorrect behavior while download files from external resources into several folders #373 If user was tried to download files from external resources and at the Transfer settings form was set Create folders for each path field checkbox without setting any name field, all files downloaded into one folder without creating folders for each path field (column). Detach configuration doesn't setup SGE for a single master run #342 Grid Engine installation was mistakenly being skipped, if pipeline was launched with enabled system parameter CP_CAP_SGE via a detach configuration. Broken layouts #747 , #834 Previously, pipeline versions page had broken layout if there \"Attributes\" and \"Issues\" panels were simultaneously opened. If there were a lot of node labels at the Cluster nodes page, some of them were \"broken\" and spaced to different lines. Some of the other page layouts also were broken.","title":"v.0.16"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#cloud-pipeline-v016-release-notes","text":"Google Cloud Platform Support System logs Displaying Cloud Provider's icon Configurable timeout of GE Autoscale waiting Storage mounts data transfer restrictor Extended recursive symlinks handling Displaying of the latest commit date/time Renaming of the GitLab repository in case of Pipeline renaming Pushing pipeline changes to the GitLab on behalf of the user Allowing to expose compute node FS to upload and download files Resource usage form improvement View the historical resources utilization Ability to schedule automatic pause/restart of the running jobs Update pipe CLI version Blocking/unblocking users and groups Displaying additional node metrics Export user list Displaying SSH link for the active runs in the Dashboard view Enable Slurm for the Cloud Pipeline's clusters The ability to generate the pipe run command from the GUI pipe CLI: view tools definitions List the users/groups/objects permissions globally via pipe CLI Storage usage statistics retrieval via pipe GE Autoscaler respects CPU requirements of the job in the queue Restrictions of \"other\" users permissions for the mounted storage Search the tool by its version/package name The ability to restrict which run statuses trigger the email notification The ability to force the specific Cloud Provider for an image Restrict mounting of data storages for a given Cloud Provider Ability to \"symlink\" the tools between the tools groups Notable Bug fixes Parameter values changes cannot be saved for a tool Packages are duplicated in the tool's version details Autoscaling cluster can be autopaused pipe : not-handled error while trying to execute commands with invalid config Setting of the tool icon size NPE while building cloud-specific environment variables for run Worker nodes fail due to mismatch of the regions with the parent run Worker nodes shall not be restarted automatically Uploaded storage file content is downloaded back to client GUI improperly works with detached configurations in a non-default region Detached configuration doesn't respect region setting Incorrect behavior of the \"Transfer to the cloud\" form in case when a subfolder has own metadata Incorrect displaying of the \"Start idle\" checkbox Limit check of the maximum cluster size is incorrect Fixed cluster with SGE and DIND capabilities fails to start Azure: Server shall check Azure Blob existence when a new storage is created Azure: pipe CLI cannot transfer empty files between storages Azure: runs with enabled GE autoscaling doesn't stop Incorrect behavior while download files from external resources into several folders Detach configuration doesn't setup SGE for a single master run Broken layouts","title":"Cloud Pipeline v.0.16 - Release notes"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#google-cloud-platform-support","text":"One of the major v0.16 features is a support for the Google Cloud Platform . All the features, that were previously used for AWS and Azure , are now available in all the same manner, from all the same GUI/CLI, for GCP . This provides an even greater level of a flexibility to launch different jobs in the locations, closer to the data, with cheaper prices or better compute hardware in depend on a specific task.","title":"Google Cloud Platform Support"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#system-logs","text":"In the current version, the \"Security Logging\" was implemented. Now, the system records audit trail events: users' authentication attempts users' profiles modifications platform objects' permissions management access to interactive applications from pipeline runs other platform functionality features Logs are collected/managed at the Elasticsearch node and backed up to the object storage (that could be configured during the platform deployment). The administrator can view/filter these logs via the GUI - in the System-level settings, e.g.: Each record in the logs list contains: Field Description Date The date and time of the log event Log status The status of the log message ( INFO , ERROR , etc.) Log message Description of the log event User User name who performed the event Service Service name that registered the event ( api-srv , edge ) Type Log message type (currently, only security type is available) For more details see here .","title":"System logs"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#displaying-cloud-providers-icon-for-the-storagecompute-resources","text":"As presented in v0.15 , Cloud Pipeline can manage multi Cloud Providers in a single installation. In the current version, useful icon-hints with the information about using Cloud Provider are introduced. If a specific platform deployment has a number of Cloud Providers registered (e.g. AWS + Azure , GCP + Azure ) - corresponding icons/text information are displaying next to the cloud resource. Such cloud resources are: Object/File Storages (icons in the Library , at the \"DATA\" panel of the Dashboard etc.) Regions (icons in the Cloud Regions configuration, at the Launch form etc.) Running jobs : text hints (at the RUNS page) icons (at the Run logs page, at the \"RUNS\" panels of the Dashboard ) Note : this feature is not available for deployments with a single Cloud Provider. Examples of displaying Cloud Region icons/info see in sections 6. Manage Pipeline , 7. Manage Detached configuration , 8. Manage Data Storage and 18. Home page .","title":"Displaying Cloud Provider's icon for the storage/compute resources"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#configurable-timeout-of-ge-autoscale-waiting-for-a-worker-node-up","text":"Previously, GE Autoscaler waited for a worker node up for a fixed timeout. This could lead to incorrect behavior for specific CLoud Providers , because the timeout can be very different. Current version extracts GE Autoscaler polling timeout to a new system preference ge.autoscaling.scale.up.polling.timeout . That preference defines how many seconds GE Autoscaler should wait for pod initialization and run initialization . Default value is 600 seconds ( 10 minutes).","title":"Configurable timeout of GE Autoscale waiting for a worker node up"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#storage-mounts-data-transfer-restrictor","text":"Users may perform cp or mv operations of the large files (50+ Gb) to and from the fuse-mounted storages. It is uncovered that such operations are not handled properly within the fuse implementations. Commands may hang for a long timeout or produce zero-sized result. The suggested more graceful approach for the copying of the large files is to use pipe cp / pipe mv commands, which are behaving correctly for the huge volumes. To avoid users of performing usual cp or mv commands for operations with the large files - now, Cloud Pipeline warns them about possible errors and suggest to use corresponding pipe commands. Specified approach is implemented in the following manner: if a cp / mv command is called with the source/dest pointing to the storage (e.g. /cloud-data/<storage_path>/... ) - the overall size of the data being transferred is checked - if that size is greater than allowed, a warning message will be shown, e.g.: this warning doesn't abort the user's command execution, it is continued appearance of this warning is configured by the following launch environment variables (values of these variables could be set only by admins via system-level settings): CP_ALLOWED_MOUNT_TRANSFER_SIZE - sets number of gigabytes that is allowed to be transferred without warning. By default 50 Gb . CP_ALLOWED_MOUNT_TRANSFER_SIZE_TIMEOUT - sets number of seconds that the transfer size retrieving operation can take. By default 5 seconds . CP_ALLOWED_MOUNT_TRANSFER_FILES - sets number of files that is allowed to be transferred without warning. Supported only for Azure Cloud Provider. By default 100 files . Note : this feature is not available for NFS / SMB mounts, only for object storages.","title":"Storage mounts data transfer restrictor"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#extended-recursive-symlinks-handling","text":"There could be specific cases when some services execute tasks using the on-prem storages, where \"recursive\" symlinks are presented. This causes the Cloud Pipeline Data transfer service to follow symlinks infinitely. In v0.16 , a new feature is introduced for Data transfer service to detect such issues and skip the upload for files/folders, that cause infinite loop over symlinks. A new option -sl ( --symlinks ) was added to the pipe storage cp / mv operations to handle symlinks (for local source) with the following possible values: follow - follow symlinks ( default ) skip - do not follow symlinks filter - follow symlinks but check for cyclic links and skip them Example for the folder with recursive and non-recursive symbolic links: Also options were added to the Data transfer service to set symlink policy for transfer operations. For more details about pipe storage cp / mv operations see here .","title":"Extended recursive symlinks handling"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#displaying-of-the-latest-commit-datetime","text":"Users can modify existing tools and then commit them to save performed changes. It can be done by COMMIT button on the run's details page: Previously, if user committed some tool, only commit status was shown on the run's details page. In the current version, displaying of the date/time for the tool latest commit is added: For more details about tool commit see here .","title":"Displaying of the latest commit date/time"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#renaming-of-the-gitlab-repository-in-case-of-pipeline-renaming","text":"Pipeline in the Cloud Pipeline environment is a workflow script with versioned source code, documentation, and configuration. Under the hood, it is a git repository. Previously, if the Pipeline object was renamed - the underlying GitLab repository was keeping the previous name. In the current version, if user renames a Pipeline the corresponding GitLab repository will be also automatically renamed: Need to consider in such case that the clone/pull/push URL changes too. Make sure to change the remote address, if you use the Pipeline somewhere else. For more details see here .","title":"Renaming of the GitLab repository in case of Pipeline renaming"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#pushing-pipeline-changes-to-the-gitlab-on-behalf-of-the-user","text":"If the user saves the changed Pipeline object - it actually means that a new commit is created and pushed to the corresponding GitLab repo. Previously, all the commits pushed to the GitLab via the Cloud Pipeline GUI were made on behalf of the service account . This could break traceability of the changes. In current version, the author of the commit is displayed in the Web GUI (for all Pipeline versions - the draft and released), the commits are performed on behalf of the real user:","title":"Pushing pipeline changes to the GitLab on behalf of the user"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#allowing-to-expose-compute-node-fs-to-upload-and-download-files","text":"For the interactive runs users are processing data in ad-hoc manner, which requires upload data from the local storage to the cloud and download results from the cloud to the local storage. Cloud Pipeline supports a number of options for the user to perform that data transfers for the interactive runs: via the Object Storage (using Web GUI or CLI) via the File Storage (using Web GUI, CLI, WebDav-mounted-drive) Previously, view, download, upload and delete operations required an intermediate location (bucket or fs) to be used. It might confuse user when a small dataset shall be loaded to the specific location within a run's filesystem. In the current version, direct exposure of the run's filesystem is supported. The BROWSE hyperlink is displayed on the Run logs page after a job had been initialized: User can click the link and a Storage browser Web GUI will be loaded: User is able to: view files and directories download and delete files and directories upload files and directories search files and directories For more details see here .","title":"Allowing to expose compute node FS to upload and download files"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#resource-usage-form-improvement","text":"In v0.16 , the number of filters were added to the Monitor cluster nodes feature: Common range for all charts User can synchronize the time period for all plots. To do so user should mark the \"Common range for all charts\" filter. If this filter is unmarked, user can zoom any plot without any change for others. Live update The plots data will be updated every 5 seconds in a real-time manner. The fields with dates will be updated as well. Set range User can select the predefined time range for all plots from the list: Whole range Last week Last day Last hour Date filter User can specify the Start and the End dates for plots. The system will substitute the node creating date as the Start date and current date for the End date, if user doesn't select anything. All filters are working for all plots simultaneously: data for all plots will be dynamically updated as soon as the user changes filter value. For more details see here .","title":"Resource usage form improvement"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#allow-to-view-the-historical-resources-utilization","text":"Another convenient feature that was implemented in v0.16 linked to the Cluster nodes monitor is viewing of the detailed history of the resource utilization for any jobs. Previously, it was available only for active jobs. Now, users can view the utilization data even for completed (succeed/stopped/failed) jobs for debugging/optimization purposes. The utilization data for all runs is stored for a preconfigured period of time that is set by the system preference system.resource.monitoring.stats.retention.period (defaults to 5 days). I.e. if the job has been stopped and the specified time period isn't over - the user can access to the resources utilization data of that job: Open the Run logs page for the completed job: Click the node name hyperlink The Monitor page of the node resources utilization will be opened: Also now, users have the ability to export the utilization information into a .csv file. This is required, if the user wants to keep locally the information for a longer period of time than defined by system.resource.monitoring.stats.retention.period : The user can select the interval for the utilization statistics output and export the corresponding file. For more details see here .","title":"Allow to view the historical resources utilization"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#ability-to-schedule-automatic-pauserestart-of-the-running-jobs","text":"For certain use cases (e.g. when Cloud Pipeline is used as a development/research environment) users can launch jobs and keep them running all the time, including weekends and holidays. To reduce costs, in the current version, the ability to set a Run schedule was implemented. This feature allows to automatically pause/resume runs, based on the configuration specified. This feature is applied only to the \"Pausable\" runs (i.e. \"On-demand\" and non-cluster): The user (who has permissions to pause/resume a run) is able to set a schedule for a run being launched: Schedule is defined as a list of rules - user is able to specify any number of them: For each rule in the list user is able to set the action ( PAUSE / RESUME ) and the recurrence: If any schedule rule is configured for the launched active run - that run will be paused/restarted accordingly in the scheduled day and time. Also, users (who have permissions to pause/resume a run) can create/view/modify/delete schedule rules anytime run is active via the Run logs page: See more details here (item 5).","title":"Ability to schedule automatic pause/restart of the running jobs"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#update-pipe-cli-version","text":"Previously, if the user installed pipe CLI to his local workstation, used it some time - and the Cloud Pipeline API version could be updated during this period - so the user had to manually perform complete re-installing of pipe CLI every time to have an actual version. Currently, the pipe update command to update the Cloud Pipeline CLI version was implemented. This command compare the CLI and API versions. If the CLI version is less than the API one, it will update the CLI - the latest Cloud Pipeline CLI version will be installed. Otherwise no actions will be performed: For more details see here .","title":"Update pipe CLI version"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#blockingunblocking-users-and-groups","text":"In current version, the ability for administrators to set the \"blocked\" flag for the specific user or a whole group is implemented. This flag prevents user(s) to access Cloud Pipeline platform by the GUI. Administrators can block or unblock users/groups via the User management tab of the System Settings dashboard. For example, for a user: For the blocked user the special label will be displayed aside to the user's name: The blocked user won't be able to access Cloud Pipeline via the GUI. The error message will be displayed when trying to login: Note : any content generated by the blocked users is kept in the Cloud Pipeline, all the log trails on the users activities are kept as well. Only access to the platform is being restricted. To unblock click the UNBLOCK button that appeared instead of the BLOCK button: All these actions could be performed also to a group. Users from a blocked group will not have access to the platform. For more details about users blocking/unblocking see here . For more details about groups blocking/unblocking see here .","title":"Blocking/unblocking users and groups"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#displaying-additional-node-metrics-at-the-runs-page","text":"In v0.16 , the displaying of high-level metrics information for the Active Runs is implemented: - \"Idle\" - this auxiliary label is shown when node's CPU consumption is lower than a certain level, defined by the admin. This label should attract the users attention cause such run may produce extra costs. - \"Pressure\" - this auxiliary label is shown when node's Memory/Disk consumption is higher than a certain level, defined by the admin. This label should attract the users attention cause such run may accidentally fail. These labels are displayed: at the Runs page at the Run logs page at the main dashboard (the ACTIVE RUNS panel) Note : if a user clicks this label from the Runs or the Run logs page the Cluster node Monitor will be opened to view the current node consumption. Admins can configure the emergence of these labels by the set of previous implemented system-level parameters. For the IDLE label: system.max.idle.timeout.minutes - specifies the duration in minutes how often the system should check node's activity system.idle.cpu.threshold - specifies the percentage of the CPU utilization, below which label will be displayed For the PRESSURE label: system.disk.consume.threshold - specifies the percentage of the node disk threshold above which the label will be displayed system.memory.consume.threshold - specifies the percentage of the node memory threshold above which the label will be displayed For more details see here .","title":"Displaying additional node metrics at the Runs page"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#export-user-list","text":"In the current version, the ability to export list of the platform users is implemented. This feature is available only for admins via User management tab of the system-level settings: Admin can download list with all users and their properties (user name, email, roles, attributes, state, etc.) by default or custom configure which properties should be exported: The downloaded file will have the .csv format. For more details see here .","title":"Export user list"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#displaying-ssh-link-for-the-active-runs-in-the-dashboard-view","text":"Users can run SSH session over launched container. Previously, SSH link is only available from the Run logs page. In v0.16 , a new helpful capability was implemented that allows to open the SSH connection right from the Active Runs panel of the main Dashboard: That SSH link is available for all the non-paused active jobs (interactive and non-interactive) after all run's checks and initializations have passed (same as at the Run logs page).","title":"Displaying SSH link for the active runs in the Dashboard view"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#enable-slurm-workload-manager-for-the-cloud-pipelines-clusters","text":"A new feature for the Cloud Pipeline's clusters was implemented in v0.16 . Now, Slurm can be configured within the Cluster tab. It is available only for the fixed size clusters. To enable this feature - tick the Enable Slurm checkbox and set the child nodes count at cluster settings. By default, this checkbox is unticked. Also users can manually enable Slurm functionality by the CP_CAP_SLURM system parameter: This feature allows you to use the full stack of Slurm cluster's commands to allocate the workload over the cluster, for example: For more information about using Slurm via the Cloud Pipeline see here .","title":"Enable Slurm workload manager for the Cloud Pipeline's clusters"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#the-ability-to-generate-the-pipe-run-command-from-the-gui","text":"A user has a couple of options to launch a new job in the Cloud Pipeline: API CLI GUI The easiest way to perform it is the GUI . But for automation purposes - the CLI is much more handy. Previously, users had to construct the commands manually, which made it hard to use. Now, the ability to automatically generate the CLI commands for job runs appeared in the GUI . Now, users can get a generated CLI command (that assembled all the run information): at the Launch form: at the Run logs form: Note : this button is available for completed runs too Once click these buttons - the popup with the corresponding pipe run command will appear: User can copy such command and paste it to the CLI for further launch. Also user can select the API tab in that popup and get the POST request for a job launch: See an example here .","title":"The ability to generate the pipe run command from the GUI"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#pipe-cli-view-tools-definitions","text":"In v0.16 the ability to view details of a tool/tool version or tools group via the CLI was implemented. The general command to perform these operations: pipe view-tools [OPTIONS] Via the options users can specify a Docker registry ( -r option), a tools group ( -g option), a tool ( -t option), a tool version ( -v option) and view the corresponding information. For example, to show a full tools list of a group: To show a tool definition with versions list: Also the specifying of \"path\" to the object (registry/group/tool) is supported. The \"full path\" format is: <registry_name>:<port>/<group_name>/<tool_name>:<verion_name> : For more details and usage examples see here .","title":"pipe CLI: view tools definitions"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#list-the-usersgroupsobjects-permissions-globally-via-pipe-cli","text":"Administrators may need to receive the following information - in a quick and convenient way: Which objects are accessible by a user? Which objects are accessible by a user group? Which user(s)/group(s) have access to the object? The lattest case was implemented early - see the command pipe view-acl . For other cases, new commands were implemented: pipe view-user-objects <Username> [OPTIONS] and pipe view-group-objects <Groupname> [OPTIONS] - to get a list of objects accessible by a user and by a user group/role respectively: Each of these commands has the non-required option -t ( --object-type ) <OBJECT_TYPE> - to restrict the output list of accessible objects only for the specific type (e.g., \"pipeline\" or \"tool\", etc.): For more details see: pipe view-user-objects and pipe view-group-objects .","title":"List the users/groups/objects permissions globally via pipe CLI"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#storage-usage-statistics-retrieval-via-pipe","text":"In some cases, it may be necessary to obtain an information about storage usage or some inner folder(s). In the current version, the command pipe storage du is implemented that provides \"disk usage\" information on the supported data storages/path: number of files in the storage/path summary size of the files in the storage/path In general, the command has the following format: pipe storage du [OPTIONS] [STORAGE] Without specifying any options and storage this command prints the full list of the available storages (both types - object and FS) with the \"usage\" information for each of them. With specifying the storage name this command prints the \"usage\" information only by that storage, e.g.: With -p ( --relative-path ) option the command prints the \"usage\" information for the specified path in the required storage, e.g.: With -d ( --depth ) option the command prints the \"usage\" information in the required storage (and path) for the specified folders nesting depth, e.g.: For more details about that command and full list of its options see here .","title":"Storage usage statistics retrieval via pipe"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#ge-autoscaler-respects-cpu-requirements-of-the-job-in-the-queue","text":"At the moment, GE Autoscaler treats each job in the queue as a single-core job. Previously, autoscale workers could have only fixed instance type (the same as the master) - that could lead to unschedulable jobs in the queue - for examle, if one of the jobs requested 4 slots in the local parallel environment within a 2 -cored machine. Described job waited in a queue forever (as autoscaler could setup only new 2 -cored nodes). In the current version the hybrid behavior for the GE Autoscaler was implemented, that allows processing the data even if the initial node type is not enough. That behavior allows to scale-up the cluster (attach a worker node) with the instance type distinct of the master - worker is being picked up based on the amount of unsatisfied CPU requirements of all pending jobs (according to required slots and parallel environment types). To enable hybrid mode for auto-scaled cluster set the corresponding checkbox in the cluster settings before the run: On the other hand, there are several System parameters to configure hybrid behavior in details: CP_CAP_AUTOSCALE_HYBRID ( boolean ) - enables the hybrid mode ( the same as the \"Enable Hybrid cluster\" checkbox setting ). In that mode the additional worker type can vary within either master instance type family (or CP_CAP_AUTOSCALE_HYBRID_FAMILY if specified). If disabled or not specified - the GE Autoscaler will work in a general regimen (when scaled-up workers have the same instance type as the master node) CP_CAP_AUTOSCALE_HYBRID_FAMILY ( string ) - defines the instance \"family\", from which the GE Autoscaler should pick up the worker node in case of hybrid behavior. If not specified (by default) - the GE Autoscaler will pick up worker instance from the same \"family\" as the master node CP_CAP_AUTOSCALE_HYBRID_MAX_CORE_PER_NODE ( string ) - determines the maximum number of instance cores for the node to be scaled up by the GE Autoscaler in case of hybrid behavior Also now, if no matching instance is present for the job (no matter - in hybrid or general regimen), GE Autoscaler logs error message and rejects such job: for example, when try to request 32 slots for the autoscaled cluster launched in hybrid mode with the parameter CP_CAP_AUTOSCALE_HYBRID_MAX_CORE_PER_NODE set to 20 : and in the logs console at the same time: For more details about GE Autoscaler see here .","title":"GE Autoscaler respects CPU requirements of the job in the queue"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#restrictions-of-other-users-permissions-for-the-storages-mounted-via-the-pipe-storage-mount-command","text":"As was introduced in Release Notes v.0.15 , the ability to mount Cloud data storages (both - File Storages and Object Storages) to Linux and Mac workstations (requires FUSE installed) was added. For that, the pipe storage mount command was implemented. Previously, Cloud Pipeline allowed read access to the mounted cloud storages for the other users, by default. This might introduce a security issue when dealing with the sensitive data. In the current version, for the pipe storage mount command the new option is added: -m ( --mode ), that allows to set the permissions on the mountpoint at a mount time. Permissions are being configured by the numerical mask - similarly to chmod Linux command. E.g. to mount the storage with RW access to the OWNER , R access to the GROUP and no access to the OTHERS : If the option -m isn't specified - the default permission mask will be set - 700 (full access to the OWNER ( RWX ), no access to the GROUP and OTHERS ). For more details about mounting data storages via the pipe see here .","title":"Restrictions of \"other\" users permissions for the storages mounted via the pipe storage mount command"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#the-ability-to-find-the-tool-by-its-versionpackage-name","text":"Cloud Pipeline allows searching for the tools in the registry by its name or description. But in some cases, it is more convenient and useful to find which tool contains a specific software package and then use it. In the current version, this ability - to find a tool by its content - is implemented based on the global search capabilities. Now, via the Global Search, you may find a tool by its version name, e.g.: And by the package name (from any available ecosystem), e.g.:","title":"The ability to find the tool by its version/package name"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#the-ability-to-restrict-which-run-statuses-trigger-the-email-notification","text":"Cloud Pipeline can be configured to send the email to the owner of the job (or specific users) if its status is changed. But in certain cases - it is not desired to get a notification about all changes of the run state. To reduce a number of the emails in these cases, the ability to configure, which statuses are triggering the notifications, is implemented in v0.16 . Now, when the administrator configures the emails sending linked to the run status changes - he can select specific run states that will trigger the notifications. It is done through the PIPELINE_RUN_STATUS section at the Email notifications tab of the System Settings (the \" Statuses to inform \" field): The email notifications will be sent only if the run enters one of the selected states. Note : if no statuses are selected in the \" Statuses to inform \" field - email notifications will be sent as previously - for all status changes. For more information how to configure the email notifications see here .","title":"The ability to restrict which run statuses trigger the email notification"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#the-ability-to-force-the-usage-of-the-specific-cloud-providerregion-for-a-given-image","text":"Previously, the platform allowed to select a Cloud Provider (and Cloud Region ) for a particular job execution via the Launch Form, but a tool/version itself didn't not have any link with a region. In certain cases, it's necessary to enforce users to run some tools in a specific Cloud Provider / Region . In the current version, such ability was implemented. The Tool / Version Settings forms contain the field for specifying a Cloud Region , e.g.: By default, this parameter has Not configured value. This means, that a tool will be launched in a Default region (configured by the Administrator in the global settings). Or a user can set any allowed Cloud Region / Provider manually. This behavior will be the same as previously. Admin or a tool owner can forcibly set a specific Cloud Region / Provider where the run shall be launched, e.g.: Then, if a specific Cloud Region / Provider is configured - users will have to use it, when launching a tool (regardless of how the launch was started - with default or custom settings): And if a user does not have access to that Cloud Region / Provider - tool won't launch: Note : if a specific Cloud Region / Provider is being specified for the Tool, in general - this action enforce the Region / Provider only for the latest version of that tool. For other versions the settings will remain previous. See for more details about tool execution settings here . See for more details about tool version execution settings here .","title":"The ability to force the usage of the specific Cloud Provider/Region for a given image"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#restrict-mounting-of-data-storages-for-a-given-cloud-provider","text":"Previously, Cloud Pipeline attempted to mount all data storages available for the user despite the Cloud Providers / Regions of these storages. E.g. if a job was launched in the GCP , but the user has access to AWS S3 buckets - they were also mounted to the GCP instance. In the current version, the ability to restrict storage mount availability for a run, based on its Cloud Provider / Region , was implemented. Cloud Regions system configuration now has a separate parameter \" Mount storages across other regions \": This parameter has 3 possible values: None - if set, storages from this region will be unavailable for a mount to any jobs. Such storages will not be available even to the same regions (e.g. storage from AWS us-east-1 will be unavailable for a mount to instances launched in AWS eu-central-1 or any GCP region and even in AWS us-east-1 ) Same Cloud - if set, storages from this region will be available only to different Cloud Regions of the same Cloud Provider (e.g. storage from AWS us-east-1 will be available to instances launched in AWS eu-central-1 too, but not in any GCP region) All - if set, storages from this region will be available to all other Cloud Regions / Providers","title":"Restrict mounting of data storages for a given Cloud Provider"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#ability-to-symlink-the-tools-between-the-tools-groups","text":"The majority of the tools are managed by the administrators and are available via the library tool group. But for some of the users it would be convenient to have separate tool groups, which are going to contain a mix of the custom tools (managed by the users themselves) and the library tools (managed by the admins). For the latter ones the ability to create \" symlinks \" into the other tool groups was implemented. \"Symlinked\" tools are displayed in that users' tool groups as the original tools but can't be edited/updated. When a run is started with \"symlinked\" tool as docker image it is being replaced with original image for Kubernetes pod spec. Example of the \"symlinked\" ubuntu tool: The following behavior is implemented: to create a \"symlink\" to the tool, the user shall have READ access to the source tool and WRITE access to the destination tool group for the \"symlinked\" tool all the same description, icon, settings as in the source image are displayed. It isn't possible to make any changes to the \"symlink\" data (description, icon, settings. attributes, issues, etc.), even for the admins admins and image OWNERs are able to manage the permissions for the \"symlinks\". Permissions on the \"symlinked\" tools are configured separately from the original tool two levels of \"symlinks\" is not possible (\"symlink\" to the \"symlinked\" tool can't be created) it isn't possible to \"push\" into the \"symlinked\" tool For more details see here .","title":"Ability to \"symlink\" the tools between the tools groups"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#notable-bug-fixes","text":"","title":"Notable Bug fixes"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#parameter-values-changes-cannot-be-saved-for-a-tool","text":"#871 Previously, if a user changed only the parameter value within a tool's settings form - the SAVE button stayed still unavailable. One had to modify some other option (e.g. disk size) to save the overall changes.","title":"Parameter values changes cannot be saved for a tool"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#packages-are-duplicated-in-the-tools-version-details","text":"#843 Previously, certain tools packages were duplicated in the PACKAGES details page of the tools version.","title":"Packages are duplicated in the tool's version details"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#autoscaling-cluster-can-be-autopaused","text":"#819 Previously, when users launched auto-scaled clusters without default child-nodes and the PAUSE action was specified as action for the \"idle\" run (via system-levels settings), such cluster runs could be paused. Any cluster runs shall not have the ability to be paused, only stopped.","title":"Autoscaling cluster can be autopaused"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#pipe-not-handled-error-while-trying-to-execute-commands-with-invalid-config","text":"#750 Previously, if pipe config contained some invalid data (e.g. outdated or invalid access token), then trying to execute any pipe command had been causing an not-handled error.","title":"pipe: not-handled error while trying to execute commands with invalid config"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#setting-of-the-tool-icon-size","text":"#493 Previously, setting of any value for the maximum tool's icon size via the sytem-level preference misc.max.tool.icon.size.kb didn't lead to anything - restriction for the size while trying to change an icon was remaining the same - 50 Kb.","title":"Setting of the tool icon size"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#npe-while-building-cloud-specific-environment-variables-for-run","text":"#486 For each run a set of cloud-specific environment variables (including account names, credentials, etc.) is build. This functionality resulted to fails with NPE when some of these variables are null . Now, such null variables are filtered out with warn logs.","title":"NPE while building cloud-specific environment variables for run"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#worker-nodes-fail-due-to-mismatch-of-the-regions-with-the-parent-run","text":"#485 In certain cases, when a new child run was launching in cluster, cloud region was not specified directly and it might be created in a region differing from the parent run, that could lead to fails. Now, worker runs inherit parent's run cloud region.","title":"Worker nodes fail due to mismatch of the regions with the parent run"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#worker-nodes-shall-not-be-restarted-automatically","text":"#483 Cloud Pipeline has a functionality to restart so called batch job runs automatically when run is terminated due to some technical issues, e.g. spot instance termination. Previously, runs that were created as child nodes for some parent run were also restarted. Now, automatically child reruns for the described cases with the batch job runs are rejected.","title":"Worker nodes shall not be restarted automatically"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#uploaded-storage-file-content-is-downloaded-back-to-client","text":"#478 Cloud Pipeline clients use specific POST API method to upload local files to the cloud storages. Previously, this method not only uploaded files to the cloud storage but also mistakenly returned uploaded file content back to the client. It led to a significant upload time increase.","title":"Uploaded storage file content is downloaded back to client"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#gui-improperly-works-with-detached-configurations-in-a-non-default-region","text":"#476 Saved instance type of a non-default region in a detached configuration wasn't displayed in case when such configuration was reopened (instance type field was displayed as empty in that cases).","title":"GUI improperly works with detached configurations in a non-default region"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#detached-configuration-doesnt-respect-region-setting","text":"#458 Region setting was not applied when pipeline is launched using detached configuration. Now, cloud region ID is merged into the detached configuration settings.","title":"Detached configuration doesn't respect region setting"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#incorrect-behavior-of-the-transfer-to-the-cloud-form-in-case-when-a-subfolder-has-own-metadata","text":"#434 Previously, when you tried to download files from external resources using metadata (see here ) and in that metadata's folder there was any subfolder with its own metadata - on the \"Transfer to the Cloud\" form attributes (columns) of both metadata files were mistakenly displaying.","title":"Incorrect behavior of the \"Transfer to the cloud\" form in case when a subfolder has own metadata"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#incorrect-displaying-of-the-start-idle-checkbox","text":"#418 If for the configuration form with several tabs user was setting the Start idle checkbox on any tab and then switched between sub-configurations tabs - the \"checked\" state of the Start idle checkbox didn't change, even if Cmd template field was appearing with its value (these events are mutually exclusive).","title":"Incorrect displaying of the \"Start idle\" checkbox"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#limit-check-of-the-maximum-cluster-size-is-incorrect","text":"#412 Maximum allowed number of runs (size of the cluster) created at once is limited by system preference launch.max.scheduled.number . This check used strictly \"less\" check rather then \"less or equal\" to allow or deny cluster launch. Now, the \"less or equal\" check is used.","title":"Limit check of the maximum cluster size is incorrect"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#fixed-cluster-with-sge-and-dind-capabilities-fails-to-start","text":"#392 Previously, fixed cluster with both CP_CAP_SGE and CP_CAP_DIND_CONTAINER options enabled with more than one worker failed to start. Some of the workers failed on either SGEWorkerSetup or SetupDind task with different errors. Scripts were executed in the same one shared analysis directory. So, some workers could delete files downloaded by other workers.","title":"Fixed cluster with SGE and DIND capabilities fails to start"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#azure-server-shall-check-azure-blob-existence-when-a-new-storage-is-created","text":"#768 During the creation of AZ Storage, the validation whether Azure Blob exists or not didn't perform. In that case, if Azure Blob had already existed, the user was getting failed request with Azure exception.","title":"Azure: Server shall check Azure Blob existence when a new storage is created"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#azure-pipe-cli-cannot-transfer-empty-files-between-storages","text":"#386 Previously, empty files couldn't be transferred within a single Azure storage or between two Azure storages using pipe CLI, it throwed an error. So for example, a folder that contained empty files couldn't be copied correctly.","title":"Azure: pipe CLI cannot transfer empty files between storages"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#azure-runs-with-enabled-ge-autoscaling-doesnt-stop","text":"#377 All Azure runs with enabled GE autoscaling were stuck after the launch.sh script has finished its execution. Daemon GE autoscaler process kept container alive. It was caused by the run process stdout and stderr aren't handled the same way for different Cloud Provider. So background processes launched from launch.sh directly could prevent Azure run finalization.","title":"Azure: runs with enabled GE autoscaling doesn't stop"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#incorrect-behavior-while-download-files-from-external-resources-into-several-folders","text":"#373 If user was tried to download files from external resources and at the Transfer settings form was set Create folders for each path field checkbox without setting any name field, all files downloaded into one folder without creating folders for each path field (column).","title":"Incorrect behavior while download files from external resources into several folders"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#detach-configuration-doesnt-setup-sge-for-a-single-master-run","text":"#342 Grid Engine installation was mistakenly being skipped, if pipeline was launched with enabled system parameter CP_CAP_SGE via a detach configuration.","title":"Detach configuration doesn't setup SGE for a single master run"},{"location":"release_notes/v.0.16/v.0.16_-_Release_notes/#broken-layouts","text":"#747 , #834 Previously, pipeline versions page had broken layout if there \"Attributes\" and \"Issues\" panels were simultaneously opened. If there were a lot of node labels at the Cluster nodes page, some of them were \"broken\" and spaced to different lines. Some of the other page layouts also were broken.","title":"Broken layouts"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/","text":"Cloud Pipeline v.0.17 - Release notes Billing reports enhancements System dictionaries Cloud Data application Sending of email notifications enhancements Allowed price types for a cluster master node \"Max\" data series in the resources Monitoring User management enhancements Allowed instance count Export custom user's attributes User management and export in read-only mode Batch users import User states Usage report GUI impersonation \"All pipelines\" and \"All storages\" repositories Sensitive storages Versioned storages Updates of \"Limit mounts\" for object storages Hot node pools FS quotas Export cluster utilization in Excel format Export cluster utilization via pipe Pause/resume runs via pipe Home storage for each user SSH tunnel to the running compute instance Updates of Metadata object Custom node images Launch a tool with \"hosted\" applications Advanced global search with faceted filters Explicitly \"immutable\" pipeline parameters Disable Hyper-Threading Saving of interim data for jobs stopped by a timeout Resolve variables for a rerun NAT gateway Custom Run capabilities Storage lifecycle management Image history Environments synchronization via pipectl Data access audit System Jobs Cluster run usage Cluster run estimation price Terminal view AWS: seamless authentication AWS: transfer objects between AWS regions AWS: switching of regions for launched jobs in case of insufficient capacity Notable Bug fixes Unable to view pipeline sources for previous draft versions pipe storage ls works incorrectly with the option --page AWS deployment: unable to list more than 1000 files in the S3 bucket Size of tool version created from original tool without any changes is a lot larger than original one pipe storage cp fails in Windows for the GCS Shared endpoint for anonymous users is being opened from the second time Attempt to view permissions on a pipeline via the pipe view-pipes throws an error Scale down \"cold\" SGE autoscaling cluster \"Launch Command\" functionality issues Inner data storages navigation bar fails to navigate Region is being set incorrectly when trying to rerun pipeline PAUSE and COMMIT operations fail for the jobs with an autoscaled disk Billing reports enhancements In the previous version, the Billing reports functionality was introduced (see details here ). In v0.17 , several useful features for the Billing reports were implemented. Access to Billing reports for non-admin users Previously, only admins had access to the Billing reports Dashboard and can view Platform's spendings data. In some cases, it is convenient that non-admin users also have the access to specific cost reports info. In the current version, such ability was implemented - in two ways: a new role was added into the predefined roles list - ROLE_BILLING_MANAGER . If that role is assigned to the user - for him/her the Billing reports Dashboard becomes available. And all possible filters, charts and their types, discounts configuration, export feature and etc. become available too. So, users who are granted this role are able to view the whole Billing reports info of the platform (as if they were admins). Note : this behavior is enabled by the new system preference billing.reports.enabled.admins . It allows to configure Billing reports visibility for admins and billing managers. Default value is true . base access to the Billing reports for \"general\" users that allows to view some information - about users' own spendings: this behavior is enabled by the new system preference billing.reports.enabled . If this preference is set, all \"general\" users can access personal billing information - runs/storages where the user is an owner. Also \"general\" users can use filters, change chart types, make reports export. the following restrictions are set for \"general\" users when \"base\" billing access is enabled: all showing charts are being displayed only spendings of the current user there isn't an ability to configure discounts, the button \"Configure discounts\" is disabled \"Billing centers (TOP 10)\" chart isn't displayed For example, the view of the Billing reports Dashboard for the \"general\" user when the system preference billing.reports.enabled is enabled: Storage data consumption Previously, Cloud Pipeline allowed to show only costs for the data storages. But it would be convenient to understand what is the total consumption of the data usage (volume of storages usage in GB) across all operational groups or individual by specific user. Currently, this ability is implemented. In all \"Storages\" reports, for the TOP 10 Storages... chart, the Volume in GB for each storage is displayed in the table, e.g.: Additionally for each storage, its Billing Center is displayed (if it's defined) and the storage Type ( S3 / GS / NFS / LustreFS /etc.). There are two Volume values displaying for each storage: Avg. Vol. is the average storage volume, in GB. It means that the exact volumes for each day of the selected report period were brought and then the average value was calculated Cur. Vol. is the current storage volume, in GB. This volume is a real volume for a current moment/last day of the given period The user can switch the view of the TOP 10 Storages... charts by a new control - By default, Costs displaying is selected. When Volume displaying is being selected, \"average\" volumes of the corresponding storages (in GB) will be displayed in the chart: These new columns ( Average Volume , Current Volume , Billing Center , Type ) also are being exported in tables reports. Region/Provider filter Previously, Billing reports allowed displaying the different Cloud Providers' instance types and their usage. But there was no way to get the overall per-Cloud or per-Region information. In v0.17 , these abilities were implemented. Now the user can use the following filters: specific Cloud Provider(s) ( for multi-Provider deployments ) specific Region(s) of the specific Cloud Provider They all can be specified via the \" Regions/Providers \" dropdown list in the top of any Billing reports page, e.g.: Custom date range for the report Previously, Cloud Pipeline allowed to configure date range on the Billing reports dashboard for different periods ( year , quarter , month(s) ), but the minimum period for any report was only month . Sometimes, it is needed to view cost utilization for a specific period in days. In v0.17 , it was implemented - the user can view Billing reports with manually configured period accurate to the day: Select the \"Custom\" period and click the \"Calendar\" control: Select \"From\" and \"To\" dates, confirm the selection: Reports (charts and tables) will be rebuilt for the configured custom date range: Spendings for old versions of object storages As object storages supports versioning, it is convenient to view spendings for the old (previous) versions of the storage data. Old versions include all non-last (previous) versions of the versioning object storage. From the current version, the Object storages report supports the displaying of the corresponding related information. At the summary chart, new dashed lines of the same colors (as for current and previous periods) appeared - these lines show summary spendings on the data usage for all old versions of object storages: On all other object storage charts, bars are presented as stacks of current version spendings / old versions spendings. Current version spendings are shown with solid filling, old versions spendings are shown without filling, e.g.: Also now, the detailed spendings table for object storages shows the info for spendings/usage in the format total spendings/usage for all versions / spendings/usage for old versions only : Breakdown by versions is shown in the CSV report export as well. Spendings for object storages' archive layers As object storages supports archiving data into different archive tiers (layers), it is convenient to view spendings separately for each layer. From the current version, the Object storages report supports the displaying of the corresponding related information. This information is shown on the separate chart - bar chart with division to different tiers (archive types). This chart does not contain any information for previous period. Only layers used for data storing in the current period according to selected filters are shown. Up to 4 layers can be here: Standard , Glacier , Glacier IR , Deep Archive . Example: Object storage layers chart can show the information as storages usage costs - in $ or as average storages volumes - in Gb : If data in the storage is storing in different tiers (archive types), this can be viewed in a tooltip of other object storages charts - there will be a division of spendings by the used tiers, e.g.: Breakdown by archive layers is shown in the CSV report export as well. User can select one of the object storage layers - by click it on this new chart. In this case, all charts and tables will be updated - only storages, that contain files in the selected layer type, will be shown in forms. Also, shown spendings/data volume will be related only to files in the selected layer, not for the whole storage(s) or other layers. For example, Glasier IR was selected: Spendings in runs cost layers From the current version, the Compute instances report (and sub-reports - for CPU/GPU) supports the displaying of the runs cost division into layers: Compute - cost of compute instances used in runs Disk - cost of EBS drives connected to runs during their performing This information is shown on the new Cost details chart - bar chart with division to these layers. This chart does not contain any information for previous period - only cost of runs' layers in the current period according to selected filters are shown. Additionally, information about cost division are shown in details tables under charts Instance types , Pipelines , Tools - as separate columns, e.g.: User can select one of the runs cost layers - by click it in the Cost details chart. In this case: summary runs cost chart will be updated - only summary spendings, that correspond to the selected layer ( Compute or Disk ), will be shown charts Instance types , Pipelines , Tools will be updated - only spendings, that correspond to the selected layer ( Compute or Disk ), will be shown data in tables under charts will not be changed, but the sorting column will be set the same as the selected layer For example, if the Compute layer of the runs cost is selected: Displaying different user's attributes in the Billing reports Previously, in all the Billing reports , info about users was displayed as user ID only. In some cases, it would be more convenient to display user names or emails - to take a more readable form. In the current version, this ability is implemented. A new System Preference is introduced: billing.reports.user.name.attribute It defines which user's attribute shall be used to display the users in the Billing reports . If it is set, specified attribute will be used in all billing charts, tables, export reports. Possible values for described preference: userName , FirstName , LastName , etc. Export reports in CSV from any Billing page Previously, Cloud Pipeline allowed to export the Billing reports data into the CSV format via the \"General\" section only. But in separate sections - \"Storages\" and \"Compute Instances\" - the user could export data as PNG image format only. Currently, CSV export has been added to all the reports sections (\"Storages\"/\"Compute instances\" and all sub-sections): reports display the same structure as in the GUI - the top 10 records of the corresponding entities (e.g. storages or instances) for the reports, which contain more than one table - all the tables are exported one after another export in CSV from the \"General\" page remains the same Example of an export from the \"CPU\" page: Breakdown the billing reports by month Cloud Pipeline allows exporting billing reports in the CSV . Previously, the values were shown as aggregates for the whole selected period. In some cases, it is more convenient to change this view to a breakdown by month. In the current version, this ability is implemented. Now, if any period - longer than a month is selected (including a custom period), the CSV -report contains an aggregate for each month of that period. The whole period summary is being included as well (as previously). Example of the report for a custom period: \"Billing General\" export broken by the user Previously, Cloud Pipeline could export the \"General\" billing report split by the \"Cost Center\". For some use cases, needs to have this report broken by the user as well. Now, this ability is implemented. User can specify which dimension to use for the export: by Cost Center - in this case, the \"General\" billing report will be split by the \"Cost Center\" (as it was previosly) by User - in this case, export will be in the same format as for the \"Cost Center\", but split the values by the user (using billing.reports.user.name.attribute to display the username) Format of the report is being selected before the export: Example of the report broken by the user: System dictionaries Often admins have to set attributes (metadata) for \"general\" users manually. In case, when such metadata keys aren't different for each user and has certain amount of values, it is convenient to select these values from the predefined values list, not to specify them manually each time. In the current version, the ability to create System Dictionaries was implemented. Each dictionary is the categorical attribute. I.e. it is attribute which values are predefined. Each dictionary has its name and values, e.g.: If the dictionary exists in the system, then admins can use it when specifying attributes for any Platform object ( Pipeline , Folder , Storage , Project , Tool ), and also for User , Group or Role . In this case, it is enough to specify only the dictionary name as the attribute key, the list of dictionary values will appear automatically in the value field: Also, the different dictionaries may be connected (linked). I.e. admins can create two dictionaries, which values are mapped 1-1 or 1-many , e.g.: In the GUI, such connection is being handled in the following way: Admin specifies the links between the dictionaries items (e.g. for the example above ProjectID : BRCA1 -> DataStorage : <path> ). Links have the \"autofill\" attribute. If the admin selects the source key ( ProjectID : BRCA1 ) as attribute key for any object - the destination key will be specified automatically ( DataStorage will be added with the <path> selection): For more details see here . Cloud Data application Previously, there were several ways to manage data between local workstation and Cloud data storages, including CLI, GUI, mounting data storages as a network drives, and others. In the current version, a new Platform capability was implemented that provides a simple and convenient way to manage files, copy/move them between Cloud data storage and local workstation or even FTP-server. This introduces via the new separate application that can be downloaded from the Cloud Pipeline Platform and launched at the local machine - Cloud Data application. Cloud Data application allows you manage files/folders as in a file commander. Main application form contains two panels (left and right). In each panel, one of the following sources can be opened: local workstation / FTP server / Cloud data (datastorages). the local content shows files and folders of the local workstation (by default, home user's directory). Navigation between and inside folders is available: the Cloud data content includes: all FS mounts from the Cloud Pipeline environment - to which current user has permissions. They are shown as simple folders those object storages from the Cloud Pipeline environment - to which current user has permissions and \"File system access\" was requested. They are shown with storage icon Navigation between and inside folders/storages is available the ftp content shows files and folders of the FTP/SFTP server. Navigation between and inside folders is available: The base scenario of the application usage: User selects desired source and destination in panels, e.g. FTP server and object datastorage correspondingly: Users selects desired files/folders in the source and clicks the data management button in the source panel - according to the action user wants to perform, e.g. to copy a file: Action will be performed, content of the panels will be updated: For more details see here . Sending of email notifications enhancements Several additions and updates were implemented in the current version for the System Email notifications. You can view the general mechanism of the Cloud PIpeline email notifications sending described here . Additional options for IDLE / HIGH-CONSUMED runs notifications Previously, to customize a platform behavior with respect to idle or high-consumed runs, admin had to set a number of settings in two different system forms - Preferences and Email Notifications . It was inconvenient and could confused users. It would be nice to duplicate input fields for some preferences into the Email Notifications section - for faster and more convenient input of their values, and to avoid possible confusion and mistakes. In the current version, it was implemented. Now: For HIGH_CONSUMED_RESOURCES notification type settings, the following input fields were added: \" Threshold of disk consume (%) \" that duplicates system.disk.consume.threshold preference value \" Threshold of memory consume (%) \" that duplicates system.memory.consume.threshold preference value Saving of the listed values changes at the Email Notifications form will automatically change the corresponding values in the Preferences , and vice versa. For IDLE_RUN , IDLE_RUN_PAUSED , IDLE_RUN_STOPPED notification types settings, the following input fields were added: \" Max duration of idle (min) \" that duplicates system.max.idle.timeout.minutes preference value \" Action delay (min) \" that duplicates system.idle.action.timeout.minutes preference value \" CPU idle threshold (%) \" that duplicates system.idle.cpu.threshold preference value \" Action \" that should duplicates system.idle.action preference value These 4 fields are united into a single section for all idle notification types - you may configure these fields from any idle notification settings tab. Saving of the listed values changes at the Email Notifications form will automatically change the corresponding values in the Preferences , and vice versa. For all these fields, help tooltips were added to clarify their destination, e.g.: Notifications for long paused runs In v0.17 , new email notification types were added: LONG_PAUSED - the notification that is being sent when the run is in the PAUSED state for a long time. This new notification type has the following additional configurable parameters: Threshold (sec) - it is a time interval of the run PAUSED state after which the notification will be sent Resend delay (sec) - it is a delay after which the notification will be sent again, if the run is still in the PAUSED state LONG_PAUSED_STOPPED - the notification that is being sent when the run that has been in the PAUSED state for a long time, has been stopped by the system. This new notification type has the following additional configurable parameter: Threshold (sec) - it is a time interval of the run PAUSED state after which the notification will be sent and the run will be terminated There is a common setting for the both described notification types - Action . This setting could be only NOTIFY or STOP . It defines the system behavior with the long paused runs: if the Action is NOTIFY - for the appropriate run, the notification LONG_PAUSED will being sent according to its settings if the Action is STOP - for the appropriate run, the notification LONG_PAUSED_STOPPED will be sent once and the run will be terminated Action type also can be configured via the Systemp preference system.long.paused.action . Saving of the Action setting value changes at the Email Notifications form will automatically change the corresponding value in the Preferences , and vice versa. \"Resend\" setting for IDLE runs Previously, IDLE_RUN notifications were sent only once and then configured action had being performed. In the current version, the ability to resend this notifications was implemented. It could be configured via the corresponding field at the IDLE_RUN notification type form: If the Resend delay is specified and the Action for the idle runs is set as NOTIFY , then the IDLE_RUN notification will being resent every appropriate time interval. Allow to exclude certain node type from the specific notifications For quite small/cheap nodes, the users may not want to receive the following email notifications for the run: IDLE_RUN LONG_PAUSED LONG_RUNNING So, a new System preference system.notifications.exclude.instance.types was implemented to control that behavior. If the node type is specified in this preference, listed above notifications will not be submitted to the jobs, that use this node type. This preference allows a comma-separated list of the node types and wildcards, e.g.: Push notifications Previously, Cloud Pipeline platform sent notifications to users via email only. For many cases it would be useful to show such notifications in the GUI as well. In the current version, such ability was implemented. Now, all email notifications, that are sending by the platform, are also duplicated as push notifications. This allows to view notifications right in the Cloud Pipeline GUI. Push notifications do not require additional configuring - they are fully the same as corresponding email notifications, i.e. have the same header, content, recepients list, frequency and trigger of sending, etc. Once any system event is occurred and its trigger for sending email notification has fired, email will be sent to the configured recipients. Simultaneously, the push notification (with the same subject and body as in the email) will be \"sent\" to the same recipients, e.g.: Click it to view the whole notification - it will be opened in a pop-up: Additionally, a new section appeared in the main menu - Notifications . It allows to view all push notifications/emails sent to the current user, e.g.: User can switch notifications lists - to display only new \"unread\" notifications or only \"read\" ones. To view the notification full details, user can click it - notification will be opened in a pop-up: For more details see here . Allowed price types for a cluster master node Previously, Cloud Pipeline allowed the user to choose whether the cluster master node be a spot or on-demand instance. While spots are acceptable for the worker nodes, as they can be recreated in failure cases - master node failure will terminate the whole cluster. To make things easy for the end-users, an optional restriction on the specific price types usage for the master nodes was implemented. There is a new string system preference - cluster.allowed.price.types.master - that force the clusters' master node price type. Default value : \"spot,on_demand\" - so, both types are accessible for the user when he/she wants to launch a cluster. Possible values : \"spot\", \"on_demand\" or both together comma-separated. Specified value for that preference defines which price type(s) will be shown in the drop-down, when the cluster run is being configured. For example: set in the Preferences: once the user selects any cluster configuration in the \"Exec environment\" section - available price types becomes equal to the set value: Note : cluster.allowed.price.types.master preference doesn't apply on the price types for single-node jobs \"Max\" data series at the \"Resource Monitoring\" dashboard Previously, Cloud Pipeline displayed the resources utilization as an average value. This could hide some spikes (which resulted in job failure), when reviewing at a high zoom-level (e.g. several days). In the current version, to the \"CPU Usage\" and the \"Memory Usage\" charts additional data-series (\"lines\") were added, which are calculated as a max function in each moment. Existing lines are kept as well, but were renamed to average . For example: For more details see here . User management enhancements Allowed instance count Sometimes users' scripts may spawn hundreds of machines without a real need. This could lead to different bugs on the Platform. To prevent such situation, a new setting - Allowed instance max count - was added to the user's options. It allows to restrict the number of instances a user can run at the same time: Behavior is configured by the following way: for example, if this setting for the user is specified to 5 - they can launch only 5 jobs at a maximum. This includes worker nodes of the clusters. If the user tries to launch a job, but it exceeds a current limit (e.g. limit is 5 and user starts a new instance which is going to be a 6th job), GUI will warn the user before submitting a job: And if the user confirms a run operation - it will be rejected: Even if the user will try to start a new job via pipe CLI - it will be rejected as well, e.g.: Such restrictions could be set not only for a user, but on another levels too (in descending order of priority): User-level - i.e. specified for a user. This overrides any other limit for a particular user. See details here . User group level - i.e. specified for a group/role. Count of jobs of each member of the group/role is summed and compared to this parameter. If a number of jobs exceeds a limit - the job submission is rejected. This level is configured via the Allowed instance max count setting for a group/role. See details here . globally via the system preference launch.max.runs.user.global - it can be used to set a global default restriction for all the users. I.e. if it set to 5, each Platform user can launch 5 jobs at a maximum. Additionally, a new command was added to pipe CLI that allows to show the count of instances running by the user at the moment, and also all possible restrictions to the allowed count of instances to launch - pipe users instances : See details here . Export custom user's attributes Previously, user's metadata attributes couldn't be exported in an automatic way. In the current version, such feature is implemented. Now, before the users export, there is the ability to select which user's metadata attributes shall be exported. Previous export settings remain the same. Click the \" Export users \" button at the USER MANAGEMENT tab of the System Settings . Select the \"Custom configuration\": In the export pop-up, select additional metadata keys you wish to export with general user's info: Exported metadata will be included into the export file as separate columns, e.g. (part of the output): For more details about users export see here . User management and export in read-only mode Previously, only admins had access to the users info/metadata. In the current version, a new \"built-in\" role ROLE_USER_READER was added. This role allows: read-only access to the API endpoints, responsible for the users, groups, roles information in the GUI , users with this role can: get \"general\" user/groups information in read-only mode - name/email/etc. - without users' metadata get access to the user management tab in read-only mode - without users' metadata and launch options export users list - including users' metadata For more details about user roles see here . Batch users import Previously, Cloud Pipeline allowed creating users only one-by-one via the GUI. If a number of users shall be created - it could be quite complicated to perform those operation multiple times. To address this, a new feature was implemented in the current version - now, admins can import users from a CSV file using GUI and CLI. CSV format of the file for the batch import: UserName,Groups,<AttributeItem1>,<AttributeItem2>,<AttributeItemN> <user1>,<group1>,<Value1>,<Value2>,<ValueN> <user2>,<group2>|<group3>,<Value3>,<Value4>,<ValueN> <user3>,,<Value3>,<Value4>,<ValueN> <user4>,<group4>,,, Where: UserName - contains the user name Groups - contains the \"permission\" groups, which shall be assigned to the user <AttributeItem1> , <AttributeItem2> ... <AttributeItemN> - set of optional columns, which correspond to the user attributes (they could be existing or new) The import process takes a number of inputs: CSV file Users/Groups/Attributes creation options , which control if a corresponding object shall be created if not found in the database. If a creation option is not specified - the object creation won't happen: \" create-user \" \" create-group \" \" create-<ATTRIBUTE_ITEM_NAME> \" Import users via GUI Import users from a CSV file via GUI can be performed at the USER MANAGEMENT section of the System Settings . Click the \" Import users \" button: Select a CSV file for the import. The GUI will show the creation options selection, e.g.: After the options are selected, click the IMPORT button, e.g.: Once the import is done - you can review the import results: Users and groups have been created Users were assigned to the specified groups Attributes were assigned to the users as well For more details and examples see here . Import users via CLI Also in the current version, a new pipe command was implemented to import users from a CSV file via CLI: pipe users import [OPTIONS] FILE_PATH Where FILE_PATH - defines a path to the CSV file with users list Possible options: -cu / --create-user - allows the creation of new users -cg / --create-group - allows the creation of new groups -cm / --create-metadata <KEY> - allows the creation of a new metadata with specified key Results of the command execution are similar to the users import operation via GUI. For more details and examples see here . User states Previously, admins could monitor Platform usage, for example, by list of ACTIVE RUNS or via CLUSTER STATE pages. But for some cases, it can be useful to know which users do utilize the Platform in the current moment. In the current version, the displaying of user states in the \"User management\" system tab was implemented - now, that state is shown as an circle icon near the user name: Possible states: Online (green circle) - for users who are logged in and use the Platform in the moment Offline (blank white circle) - for users who are not logged in at the moment/do not use the Platform for some time By hover over the Offline icons - admin can know when the specific user has utilized the Platform the last time, e.g.: Usage report It is convenient to have the ability to view Platform statistics of users activity. E.g. when creating different schedulers or node pools and info about number of online users can be helpful. For that, the Usage report subtab, showing the Platform's statistics of users activity, was added to the \"User Management\" system tab. At this subtab, the summary info about total count of Platform users that were online at different time moments during the certain period is displayed in a chart form: User can configure the showing chart by the following ways: select the type of period of view - day ( by default ) or month select to display data for a specific day/month from the calendar restrict the displayed data for specific user(s) or user group(s)/role(s) only For more details see here . GUI impersonation While performing administrating, it is common to help users resolve issues, which can't be reproduced from the administrative accounts. This requires to perform operations on the users' behalf. To assist with such tasks, Cloud Pipeline offers \"Impersonation\" feature. It allows admins to login as a selected user into the Cloud Pipeline GUI and have the same permissions/level of access as the user. To start the impersonation, admin shall: Open the Users subtab of the \"User Management\" section of the system-level settings Load the user profile on whom behalf you are going to impersonate and click the Impersonate button in the top-right corner, e.g.: Platform GUI will be reloaded using the selected user: While in the \"Impersonation\" mode, the following changes happen to the GUI: Main menu turns orange, indicating that the impersonation mode is ON Logout button is being changed to the Stop impersonation button To stop the \"Impersonation\" mode, user shall click the Stop impersonation button. For more details see here . \"All pipelines\" and \"All storages\" repositories There are several ways for users to find the appropriate storage/pipeline object in the Cloud Pipeline Platform - manually via the Library , using the Search ability or via the corresponding panels of the main Dashboard. It would be convenient to get all lists of the storages/pipelines accessible to the user in one place with short info about each object and easy access to it. In the current version, such ability was implemented: there are two new controls displaying at the Library page, above the library-tree - separate \"repositories\" for storages and pipelines: each \"repository\" displays the full list of the corresponding objects accessible by the current user, e.g. for pipelines: for each object in the \"repository\" are displayed: object name object description (if it is available) OWNER user name additionally for pipelines, the Run button - if the pipeline is available for execute for the user additionally for storages, Cloud Region / Provider icons for multi-provider deployments if the user clicks any object in the list - its regular page is being opened for each \"repository\", there is a search field for the quick search over objects list Sensitive storages Previously, Cloud Pipeline platform allows performing upload/download operations for any authorized data storage. But certain storages may contain sensitive data, which shall not be copied anywhere outside that storage. For storing such data, special \"sensitive\" storages are implemented. Sensitive data from that storages can be used for calculations or different other jobs, but this data cannot be copy/download to another regular storage/local machine/via the Internet etc. Viewing of the sensitive data is also partially restricted. Sensitive storage is being created similar to general object storage, user only should tick the corresponding checkbox: Via the GUI, the sensitive storage looks similar to the regular object storage, but there are some differences (even for admin/storage OWNER ): I.e. files/folders in the sensitive storage can be created/renamed/removed but can't be downloaded/viewed or edited by any user. Sensitive storages can be mounted to the run. In this case, the run will become sensitive too. In sensitive runs, all storages selected for the mounting including sensitive are being mounted in readonly mode to exclude any copy/move operations between storages. Files from the sensitive storages can be viewed inside the sensitive run and also copied into the inner instance disk, but not to any other storage: Files from the sensitive storages can't be viewed outside the sensitive run or copied/moved anywhere (for example, when using not the web-terminal version of pipe SSH): For more details and restrictions that are imposed by using of sensitive storages see here . Versioned storages In some cases, users want to have a full-fledged system of the revision control of their stored data - to view revisions, history of changes, diffs between revisions. So far, for separate storages types (e.g. AWS s3 buckets), there is the ability to enable the versioning option. But it is not enough. Such versioning allows to manage the versions of the certain file, not the revisions of the full storage, which revision can contain changes of several files or folders. For the needs of full version control of the storing data, there was implemented a special storage type - Versioned storage . These storages are GitLab repositories under the hood, all changes performed in their data are versioned. Users can view the history of changes, diffs, etc. Versioned storages are created via the special menu: The view of the versioned storage is similar to regular data storage with some differences: For each file/folder in the storage, additional info is displayed: Revision - latest revision (SHA-1 hash of the latest commit) touched that file/folder Date changed - date and time of the latest commit touched that file/folder Author - user name who performed the latest commit touched that file/folder Message - message of the latest commit touched that file/folder Moreover, there are extra controls for this storage type: RUN button - allows to run the tool with cloning of the opened versioned storage into the instance Generate report button - allows to configure and then download the report of the storage usage (commit history, diffs, etc.) as the Microsoft Word document ( docx format) Show history button - allows to open the panel with commit history info of the current versioned storage or selected folder Each change in a such storage - is a commit by the fact, therefore each change has its related comment message - explicit or automatic created: One of the important advantages of versioned storages in condition with regular object storages - ability to view commit history and all changes that were performed with the data in details. Users can view the commit history of the file in the versioned storage - i.e. history of all commits that touched this file, e.g.: Using the commit history of the file, users can: revert the content of the file to the selected commit view/download revert version of the file corresponding to the specific commit view diffs between the content of the specific file in the selected commit and in the previous commit, e.g.: Besides that, users can view the commit history of the folder or the whole versioned storage - i.e. history of all changes touched files inside that folder or its subfolders, e.g.: Using the commit history of the folder, users can view diffs between the content of the specific folder in the selected commit and in the previous commit. Versioned storages can be also mounted during the runs, data can be used for the computations and results can be comitted back to such storages - with all the benefits of a version control system. For that, new management controls were added to the menu of the active runs: Via this controls, users can: clone the versioned storage(s) to the existing running instance check differences between cloned and current changed versions of the versioned storage save (commit) changes performed in the cloned version of the storage during the run checkout revision of the cloned storage in the run resolve conflicts appeared during the save or checkout operation The main scenario of using versioned storage during the run looks like: user clones selected versioned storage to the run: cloned versioned storages are available inside the run by the path /versioned-data/<storage_name>/ : user works with the data, performed changes can be viewed at any moment, e.g.: user saves performed changes (i.e. creates a new commit): saved changes become available in the origin versioned storage: For more details about versioned storages and operations with them see here . Updates of \"Limit mounts\" for object storages Displaying of the CP_CAP_LIMIT_MOUNTS in a user-friendly manner Previously, Cloud Pipeline displayed the run-enabled data storages (selected via \"Limit mounts\" feature before the launch) as a list of IDs at the Run logs page (as the CP_CAP_LIMIT_MOUNTS parameter). In the current version, this viewing was changed to more \"friendly\" for users: The data storage names are being displayed instead of the IDs Showing names are hyperlinks, pointing to the data storage in the Cloud Pipeline GUI \"Sensitive\" storages are being highlighted appropriately See details here . Allow to create run without mounts Previously, users could select all/several storages (from the available scope) to be mounted during the run. But in some cases, it might be needed to launch runs without mounts at all. In the current version, such ability was implemented. For that, the separate checkbox was added to the \"Limit mounts\" settings section: If this checkbox is set - there are no storages will be mounted during the run initialization: The ability to set \"Do not mount storages\" is added to all forms where limit mounts can be configured. Warning in case of a risk of OOM due to the number of the object storage mounts If the user has 100+ object storages available - they all are mounted to the jobs, by default. When using rather small nodes - this leads to the OOM errors, as the 100+ mount processes may oversubscribe the memory. Even if the memory consumption will be greatly optmized - the user may still face such issues, if the number of object storages grow. So in the current version, a sort of hard-limit was implemented to warn the user if there is risk of OOM . A new System preference is introduced - storage.mounts.per.gb.ratio ( int ). This preference allows to specify the \"safe\" number of storages per Gb of RAM (by default, it is 5 - i.e. \"5 storages per each Gb of RAM \"). When launching a job - the user's available object storages count is being calculated and checked that this count does not exceed the selected instance type RAM multiplied by the storage.mounts.per.gb.ratio . If it's exceeded - the user is being warned with the following wording and asked to reduce a number of mounts via the Limit mounts feature, e.g.: Note : Warning does not prohibit the run launching, user can start it at his own discretion changing nothing. If the storage.mounts.per.gb.ratio is not set - no checks are being performed, no warning appears. Before the launch, only the object storages count is being calculated, file mounts do not introduce this limitation. Hot node pools For some jobs, a waiting for a node launch can be too long. It is convenient to have some scope of the running nodes in the background that will be always or on schedule be available. In the current version, the mechanism of \" Hot node pools \" was implemented. It allows controlling the number of persistent compute nodes (of the certain configuration) in the cluster during the certain schedule. This is useful to speed up the compute instances creation process (as the nodes are already up and running). Admins can create node pools: each pool contains one or several identical nodes - admin specifies the node configuration (instance type, disk, Cloud Region , etc.) and a corresponding number of such nodes. This count can be fixed or flexible (\"autoscaled\") each pool has the schedule of these nodes creation/termination . E.g. the majority of the new compute jobs are started during the workday, so no need to keep these persistent instances over the weekends. For the pool, several schedules can be specified for each pool can be configured additional filters - to restrict its usage by the specific users/groups or for the specific pipelines/tools etc. When the pool is created, corresponding nodes are being up ( according to pool's schedule(s) ) and waiting in the background: If the user starts a job in this time ( pool's schedule(s) ) and the instance requested for a job matches to the pool's node - such running node from the pool is automatically being assigned to the job, e.g.: Note : pools management is available only for admins. Usage of pool nodes is available for any user. For more details and examples see here . FS quotas In some cases, users may store lots of extra files that are not needed more for them in FS storages. Such amount of extra files may lead to unnecessary storage costs. To prevent extra spending in this case, in the current version a new ability was implemented - FS quotas. There is a feature that allows admins to configure quota(s) to the FS storage volume that user can occupy. On exceeding such quota(s), different actions can be applied - e.g., just user notifying or fully read-only mode for the storage. This allows to minimize the shared filesystem costs by limiting the amount of data being stored in them and to notify the users/admins when FS storage is running out of the specific volume. To configure notifications/quota settings for the storage, admin shall: click the Configure notifications hyperlink in the Attributes panel of the storage: in the appeared pop-up, specify the username(s) or a groupname(s) in the Recipients input to choose who will get the FS quota notifications via emails and push notifications, e.g.: then click the Add notification to configure rules/thresholds: Put a threshold in Gb or % of the total volume and choose which action shall be performed when that threshold is reached. The following actions can be taken by the platform: Send email - just notify the recipients that a quota has been reached (notification will be resent each hour) Disable mount - used to let the users cleanup the data: GUI will still allow to perform the modification of this storage ( read-write mode ) In existing nodes (launched runs), FS storage mount will be switched to a read-only mode (if it was mounted previously) This FS storage will be mounted in a read-only mode to the new launched compute nodes Make read-only - used to stop any data activities from the users, only admins can cleanup the data per a request: GUI will show this FS storage in a read-only mode Existing nodes (launched runs) will turn this mounted FS storage in a read-only mode as well This FS storage will be mounted in a read-only mode to the new launched compute nodes The notification/quota rules can be combined in any form. E.g., the following example sets three levels of the thresholds. Each level notifies the users about the threshold exceeding and also introduces a new restriction: For example, if admin will configure notifications/quotas for the storage as described above: when user(s) will create/upload some files in the storage and summary FS size will exceed 5 Gb threshold - only notifications will be sent to recipients when user(s) will create/upload some more files in the storage and summary FS size will exceed 10 Gb threshold: for active jobs (that were already launched), filesystem mount becomes read-only and users will not be able to perform any modification for new jobs, filesystem will be mounted as read-only in GUI: permissions will not be changed. Write operations can be performed, according to the permissions \" Warning \" icon will be displayed in the storage page. It will show MOUNT DISABLED state: Storage size will be more than 10 Gb: when user(s) will create/upload some more files in the storage (e.g. via GUI) and summary FS size will exceed 20 Gb threshold: for active jobs (that were already launched), filesystem mount will remain read-only and users will not be able to perform any modification for new jobs, filesystem will be mounted as read-only in GUI: storage will become read-only . User will not be able to perform any modification to the filesystem \"Warning\" icon will be still displayed. It will show READ ONLY state Storage size will be more than 20 Gb: Please note, these restrictions will be applied to \"general\" users only. Admins will not be affected by the restrictions. Even if the storage is in read-only state - they can perform READ and WRITE operations. For more details about FS quotas, their settings and options see here . Export cluster utilization in Excel format Previously, users could export Cluster Node Monitor reports only in CSV format. From now, the ability to export these reports in XLSX format is implemented. Users can choose the format of the report before the download: Excel -reports contain not only raw monitoring data but the graphical info (diagrams) too as users can see on the GUI. Example of the Excel -report sheets: For more details how to configure Cluster Node Monitor reports see here . Export cluster utilization via pipe Also in the current version, the ability to export Cluster Node Monitor reports by pipe CLI is introduced. The command to download the node usage metrics: pipe cluster monitor [OPTIONS] The one of the below options should be specified: -i / --instance-id {ID} - allows to specify the cloud instance ID. This option cannot be used in conjunction with the --run-id option -r / --run-id {RUN_ID} - allows to specify the pipeline run ID. This option cannot be used in conjunction with the --instance-id option Using non-required options, user can specify desired format of the exported file, statistics intervals, report period, etc. For details and examples see here . Pause/resume runs via pipe Previously, users could automate the pause and resume operation for the pipeline execution only via the API calls. In the current version, pipe pause and pipe resume operations are exposed to the CLI. The command to pause a specific running pipeline: pipe pause [OPTIONS] RUN_ID Possible options: --check-size - to check firstly if free disk space is enough for the commit operation -s / --sync - to perform operation in a sync mode. This option blocks the terminal until the PAUSED status won't be returned for the pausing pipeline The command to resume a specific paused pipeline: pipe resume [OPTIONS] RUN_ID Possible option: -s / --sync - to perform operation in a sync mode. This option blocks the terminal until the RUNNING status won't be returned for the resuming pipeline For details and examples see here - pause command and resume command . Home storage for each user Typically each general user stores personal assets in the data storage, that is created for him/her by the Administrator. This is treated as a \"home\" storage and is used a lot. But the creation of multiple users becomes a tedious task (create the user/create storage/grant permissions for the user). To facilitate this task, in the current version the ability (optionally) to create home storages for the newly created users in automatic mode was implemented. This behavior is controlled by the system preference storage.user.home.auto ( Boolean , default value is false ). It controls whether the home storages shall be created automatically. If it is set to true - new storage will be created for the user automatically simultaneously with the user creation. Also the just-created user is being granted OWNER permissions for the new storage. The \"home\" storage automatic creation is being driven by a template. The template is being described as JSON element in the other new system preference - storage.user.home.template . In this preference for the template, being described: settings for the storage permissions on the storage Example of the configured preferences: So, after the user creation, the new storage according to the settings in template is being created: The newly created storage is being set as a \"default\" storage in the user's profile: For more details and examples see here . SSH tunnel to the running compute instance In the current version, a new ability to access Cloud Pipeline run instances from local workstations is implemented. Now, Cloud Pipeline run instance can be accessed via SSH directly using special network tunnels . Such tunnels can be established between a local Windows or Linux workstation and a Cloud Pipeline run. pipe CLI provides a set of command to manage such network tunnels. pipe CLI automatically manages SSH keys and configures passwordless SSH access . As a result no manual SSH keys management is required to access Cloud Pipeline run from the local workstation. SSH tunnels to Cloud Pipeline runs can be used for interactive SSH sessions, files transferring and third-party applications which depends on SSH protocol. The command that runs ports tunnelling operations: pipe tunnel COMMAND [ARGS] Where COMMAND - one of the following commands: start <RUN_ID> - establishes tunnel connection to specified run instance port and serves it as a local port stop <RUN_ID - stops background tunnel processes with specified run For the start command there are two mandatory options: -lp / --local-port - specifies local port to establish connection from -rp / --remote-port - specifies remote port to establish connection to Example of the command that establishes tunnel connection to the run: pipe tunnel start 12345 -lp 4567 -rp 22 --ssh Here: 12345 is the Run ID , 4567 is just a random free local port and 22 is the Cloud Pipeline run SSH port . Additional --ssh flag enables passwordless SSH access. For more details and examples see here . Updates of Metadata object In the current version, several enhancements were implemented for the Metadata objects displaying and working with: Controls placement reorganization Several controls (for adding a new instance, upload and delete metadata, transfer to the cloud and showing attributes) were moved to Additional parameters control (gear icon): See details here . Bulk operation panel is hidden/disabled until at least one instance is selected in a table, e.g.: To manage selected items, click the V button next to the \" Show only selected items \" control to open the corresponding menu: See details here . Ability to show only selected instances The ability is implemented to show separately only selected metadata instances. All unselected items will be hidden. For that: select items of interest (they can be at different pages too) and click the \" Show only selected items \" button at the Bulk operation panel, e.g.: For shown selected items, all functionality as for the general table is available except filtering. Improvements in the search over the metadata users can search over any attribute values (not only over ID as previously) the metadata search field supports multiple terms search - in this case, multiple terms for the search should be specified space separated, e.g. sample1 sample2 the metadata search field supports a key:value search, where key is an attribute name (column header) and value is a term that shall be searched in that attribute values, e.g. ID:D703 See details here . Ability to filter instances The ability is implemented to filter instances of an entity in a table. Now, user can click a special control in a header of the desired column and set one or several filters for the column values - to restrict the output table, e.g.: See details here . Displaying of the creation date info For all Metadata entities the \" Created date \" fields are displayed. This column appears and filled in automatically when the Metadata is uploaded or created manually, e.g.: Sorting by several columns Ability to sort a list of entities by several columns is implemented. For that, a list is being sorted by one column, then user should click the second column he(she) wants to sort by, then the third, etc.: See details here . Autofill An autofill feature for metadata cells was implemented. It allows to fill metadata instances with data that are based on data in other instances in the same column/row, e.g.: click the right-bottom corner of the cell you wish to copy and move the mouse holding the left button - vertically or horizontally, e.g.: once you will release the mouse button - selected cells will be autofilled by the value of the cell that you've dragged: See details here . Entity ID autogeneration In some cases, it could be convenient not to specify entity ID during import. Therefore Metadata entities support IDs autogeneration (in the UUID4 format). This works and for the import Metadata operation (for empty ID fields), and for the manual instance creation, e.g.: See after the creation: Note : IDs still should be unique Ability to add SampleSet item via GUI Now, users may create SampleSets or other \"Container-like\" entities from the GUI (previously it was possible via the CSV import only). This feature could be useful, if the Samples were imported using the IDs autogeneration, as it could be complicated to grab those IDs and copy to the CSV . To create a new SampleSet: Click the + Add instance button in the Metadata section and choose the SampleSet instance type: Provide the information for the new SampleSet and click the Browse button to select a list of Samples, which will be associated with the creating SampleSet: After creation, the new SampleSet will appear in the corresponding metadata class: See details here . Preselect instances for a rerun Additionally, if metadata instances were used for the run via the expansion expressions in the parameters - then for the rerun of such run, the ability to choose was implemented - to use the same resolved expressions values from the initial run or preselect another metadata instance(s) for a coming rerun, e.g.: imagine, that some run was launched from the detached configuration. Moreover, one configuration parameter uses the expansion expression: for the run, some instance was selected after the run is completed, user tries to rerun this run. The Launch form will appear. By default, parameters are substituted fully the same as they were in the initial run: If click the Launch button in this case - during the rerun, all parameter(s) will use their resolved values from the initial run (previous behavior). but now, the ability to preselect another metadata instance for the re-launch is implemented. For that, user can click \" v \" button near the launch button and in the appeared list click \" Select metadata entries and launch \" item: In this case, the pop-up will appear to select a metadata instance for which the rerun will be launched. And during the rerun, all parameter(s) that use expansion expression(s) will be resolved according to a new selected metadata instance(s): See example here . Custom node images Previously, Cloud Pipeline allowed to run instances only using some default predefined node images. For example, some node image was used for all CPU instance types, another - for GPU ones. Nevertheless there are cases than some of the tools or pipelines require special node images. For example, some tool may require specific nvidia driver version which default GPU node image doesn't have. In the current version, the ability to use a custom node image was implemented. If a custom node image is specified for a pipeline, tool or just a single launch then cloud instance with the required node image will be used for their runs. In a pipeline config, a custom node is specified in format: \"instance_image\": \"<custom_node_image>\" . For runs launched via API , a custom node is specified in format: \"instanceImage\": \"<custom_node_image>\" . In both cases, <custom_node_image> is the name of the custom image. For example, to use a custom node for a pipeline: open pipeline's CODE tab open config.json file in the configuration, specify a custom node image: save changes when launching such pipeline, you can observe that specified image is used for a node: See an example for a pipeline in details here . Launch a tool with \"hosted\" applications \"Long-running\" Cloud Pipeline applications may occasionally failed. And one of the main task caused this situation - saving the internal access to the services (e.g. if a database was hosted) as the IP and name (which match the pod) are being changed during the default run restarts. To resolve that, a special option to assign an internal DNS name to the run was implemented. Name of the service and a list of ports can be supplied by the user in the GUI, at the Launch form before the run: Configured DNS service is shown at the Launch form: And FQDN of all configured services are shown during the run - at the Run logs page: Checking that the run is launched with a \"hosted\" application: For more details see here . Advanced global search with faceted filters In v0.14 , the Global search over the platform was introduced . Now, in v0.17 , the new version of the search was implemented - Advanced search . Advanced search repeats the functionality of the Simple search but has some advanced capabilities. To open the Advanced search click the Search icon in the main menu: Please note, the previous form of the global search is still available - by pressing \"Ctrl+F\". Currently, Advanced search is available for admins only To start searching, a query string shall be entered (search can be triggered by pressing the \"Enter\" button or by the correspoding Search button near the search bar), e.g.: As in the previous form, you can: restrict search results selecting desired object types from all results scope (the corresponding panel with the object types selector is placed under the search bar): open the \"Preview\" pane for the certain result hovering mouse point over it and click the Info icon: scroll search results to view more or use the paging control New features: \" Faceted filters \" panel at the left side of the search form. It allows to search objects by their attributes (tags). Operating principle is similar to the E-Commerce sites. Tags' keys and values displayed in this panel are loaded from separate System Dictionaries marked as filter sources. User can restrict (filter) search results - by checking/unchecking desired filters. In the search results, only objects were associated with the checked filter value (dictionary entry) will remain, e.g.: I.e. only objects tagged by this dictionary entry remained in the search results. You can select several filters values from different facets. Each time, other filters will be updated, and also displayed search results will be changed according to the selected filters. You can hover over any displayed search result and click the Info icon to check that the object is really tagged by selected filters (attributes), e.g.: More details - how to add dictionaries to the \"Faceted filters\" panel, how to configure filters and use them for the search - see here . Changeable view of the search results output: Results output in the Simple search has the view as simple list of the object names only. In the Advanced search , that output contains the additional info - according to the entity type of the certain result - it can be OWNER , DESCRIPTION , DATE_CHANGED , PATH , etc. Also user can switch between output view formats - list and table - by special control : list view ( default ) - each result is presented by the \"row\" in the list, additional info is placed in line, e.g.: table view - all results are presented by the single table, additional info is placed in columns, e.g.: Also the table view can be customized by the admin user. To the existing columns, user can add ones for the object attributes (tags) values, where the attribute key becomes the column header. If the object in results has the value for that attribute - it will be displayed in the corresponding column. Customizing of additional attribute columns is being performed by the new system preference - search.elastic.index.metadata.fields : For more details about the view of the results output see here . For more details about Advanced search see here . Explicitly \"immutable\" pipeline parameters Previously, if the pipeline parameter had a default value - it could not be changed in the detached configuration that used this pipeline. In different cases, it might be convenient to provide the ability to specify the own parameter value before the configuration launch or vice versa - the ability to launch the configuration with only defaults parameter values. In the current version, the special option was implemented that allows/denies the parameter value overriding for described cases - no_override ( boolean ). This option can be specified for the pipeline parameter via config.json file: if a pipeline parameter has a default value and no_override is true - the parameter field will be read-only in the detached configuration that uses this pipeline: if a pipeline parameter has a default value and no_override is false or not set - the parameter field will be writable in the detached configuration that uses this pipeline: if a pipeline parameter has no default value - no_override is ignored and the parameter field will be writable in the detached configuration that uses this pipeline Disable Hyper-Threading Hyper-Threading technology makes a single physical processor appear as multiple logical processors. To do this, there is one copy of the architecture state for each logical processor, and the logical processors share a single set of physical execution resources. Hyper-Threading technology is enabled by default for all nodes in Cloud Pipeline . But not in all cases this technology is useful. In cases when threads are operating primarily on very close or relatively close instructions or data, the overall throughput occasionally decreases compared to non-interleaved, serial execution of the lines. For example, at a high performance computing that relies heavily on floating point calculations, the two threads in each core share a single floating point unit (FPU) and are often blocked by one another. In such case Hyper-Threading technology only slows computations. In the current version, the ability to disable Hyper-Threading for a specific job was implemented. So, this technology can be turned on or off, as is best for a particular application at the user's discretion. In Cloud Provider environment, each vCPU is a thread of a physical processor core. All cores of the instance has two threads. Disabling of Hyper-Threading disables the set of vCPUs that are relied to the second thread, set of first thread vCPUs stays enabled (see details for AWS here ). To disable Hyper-Threading technology for a job: set the corresponding option in \" Run capabilities \" before the run: check that Hyper-Threading was disabled via the following command after the run is launched: Here you can check that only 1 thread per core is set, virtual CPUs 4-7 are offline. Only one thread is enabled (set of CPUs 0-3). For more details see here . Saving of interim data for jobs stopped by a timeout Previously, if for a job a timeout was set and it has elapsed - the job was stopped and all the data was erased. In the current version, the solution to extract the current data from the timed-out jobs was implemented. Now, if a job has timed out - it will not be stopped immediately. Instead, the new OutputData task will be triggered. During this task performing, all the contents of the $ANALYSIS_DIR directory will be copied to all output storages - in the same manner, as if the job has succeeded. This feature doesn't require additional actions from the user side. Only $ANALYSIS_DIR and output paths should be defined. Additionally, a new system parameter was added - CP_EXEC_TIMEOUT . This parameter allows to define a timeout period after which the job shall be stopped. The essence of the parameter is the same as the configured value in the \" Timeout \" field. If both values are specified - for a job, CP_EXEC_TIMEOUT value will be used: Resolve variables for a rerun Previously, if the user launched some job containing environment variables in its parameters and then, after the job was completed, user tried to rerun that job - all environment variables from the job parameters had been resolving again during the new run. But for some cases, it might be needed to use in the rerun all the same values of environment variables that were in the initial run. In the current version, the ability to choose was implemented - for the rerun, to resolve such variables in a new run or use their initial values. For that, when user tries to rerun some completed run that used environment variables in its parameters - at the Launch form, the checkbox \" Use resolved values \" appears in the Parameters section, e.g.: By default, this checkbox is disabled. In this case, all environment variables are shown as is and will be resolved only during the new (re-launched) run - with the values corresponding to this new run. If this checkbox is ticked, all environment variables will be resolved with the values of the initial run. Correspondingly, parameters that use environment variables will not be changed during the new launch, e.g.: See example here . NAT gateway Previously, if the Cloud Pipeline Platform was being deployed in some private subnet, it could be quite difficult for the admin to expose a network endpoint for some service to use in a Platform. This required manual execution of a number of tasks on the Platform Core instance and, accordingly, might lead to errors. To resolve this, in the current version, the convenient way to manage network routes (creating/removing) from the GUI was implemented. For that, a new NAT gateway subtab was added to the System Management section of the System Settings . The NAT gateway subtab allows to configure network routes: To add a route, admin shall: click the ADD ROUTE button: in the appeared pop-up, specify details of an external resource: server name, IP ( if needs ), port(s) and comment to route ( optionally ), e.g.: just-added external server will appear in the list. Admin should click the SAVE button to confirm made changes: once the route creation will be done, the route details will appear in the INTERNAL CONFIG fields and near the route, the status will be shown as ACTIVE : For more details see here . Custom Run capabilities Previously, users might select only predefined \"system\" Run capabilities for a job. In some cases or deployments, these capabilities may not be enough. In the current version, the ability for admins to add custom Run capabilities was implemented. Use them for a job/tool run all users can. Managing of the custom capabilities is being performed via the new system preference launch.capabilities . This preference contains an array of capability descriptions in JSON -format and has the following structure: { \"<capability_name_1>\": { \"description\": \"<Description of the capability>\", \"commands\": [ \"<command_1>\", \"<command_2>\", ... ], \"params\": { \"<parameter_1>\": \"<value_1>\", \"<parameter_2>\": \"<value_2>\", ... } }, \"<capability_name_2\": { ... }, ... } For example: Saved capability then can be used for a job/tool: For more details see here . Storage lifecycle management Previously, users had the simplified opportunity to configure the lifecycle of data in storages - via specifying STS/LTS durations in the storage settings. This way is rather primitive and does not allow to fine-tune data archiving/restoring. In the current version, the ability to configure datastorage lifecycle in details was implemented. This new option allows to perform the automatical data transition from standard storage to different types of archival storages by occurance of a certain event and restore that data back as well if needed. Previous functionality (STS/LTS durations) was excluded. For the new one, an additional tab was included to the storage settings - Transition rules : New implemented functionality includes abilities: automatic data archiving/removing according to specified transition rule(s) restoring of previously archived data for the specified period Data archiving is provided by configurable set of transition rules for each separate storage. Each rule defines which files, when (specific date or by the event) and where (different types of archive) shall be automatically transferred: firstly, user creates a rule for a storage - specifying the path and glob pattern for a file(s) name(s) which shall be transferred, e.g.: User can select to transfer files one-by-one or in bulk-mode by the first/last appeared file in a group. Also, an additional condition for the files transition can be configured. then user selects an archive class as the data destination. Here, several destinations can be added (for different dates), e.g.: Note : archive classes depend on the Cloud Provider then user defines the event by which the data shall be transferred - after certain period after the file(s) creation or at the specific date: also, notifications can be configured for the rule events (optionally): recipients list notification title and text ability to specify a delay for data transition - user that receive such notification will have the ability to prolong (delay) transition for some period created rule can be found in the Transition rules tab of the storage settings: after the rule is created, it starts to work. If file matches the condition of any storage rule - it will be transferred to some archive or removed (if Deletion is set as data destination). Transferred file becomes disabled for changing/renaming from the GUI/CLI. At the GUI, near such file a label appears that corresponds to the transition destination, e.g.: For more details see here . Data restoring can be applied to previously archived files. Separate files or whole folders (with sub-folders) can be restored: user selects which files/folders shall be restored, e.g.: user defines the period for which files shall be restored and notification recipients list: after the confirmation, the restore process begins - it is shown by the special status icon: when file is restored - it is shown by the special status icon as well: once the restore period is over, files will be automatically transferred to the archive where they were before For more details see here . Image history Cloud Pipeline performs scanning of the Docker images on a regular basis. This is used to grab the information on: Available software packages Possible vulnerabilities of those packages The users may leverage this feature to choose which docker image to use, depending on the needs for a specific application. But this list of the software packages may not show the full list of the applications as the scanning mechanism uses only the \"well-known\" filesystem locations to collect the applications/versions informations. Some of the apps, might be installed into any custom location and the scanner won't be able to find it. To fulfill this gap and to address some advanced cases, in the current version, a new feature was introduced: now it's possible to view the list of the \"Docker layers\" and corresponding commands, which were used to generate those layers. It can be viewed via the specific tab in the tool version menu - Image history : This allows to get information on the exact commands and settings, which were used to create an image and even reproduce it from scratch. For more details see here . Environments synchronization via pipectl In some cases, admins need to synchronize two different environments of the Cloud Pipeline. New special routine in the pipectl utility is implemented for that - pipectl sync . It allows to synchronize from the source environment to the destination one the following objects: users / user groups / user roles docker registry / tool groups / tools Synchronization can be performed with or without synchronization of attributes (metadata) for the specified Platform objects. During the synchronization, changes are being performed only in the destination environment, the source environment remains the same. For details and examples see here . Data access audit In the current version, System logs were expanded - now, all actions related to any access to the data stored in the object storages are being logged. This includes logging of operations READ / WRITE / DELETE , listing operation is not logged. For logs of data access events, a new item was added to the \" Type \" filter of the System logs - audit type: By this type, the following data access operations are being logged: access to the Object storages data from the Platform GUI access to the Object storages data from the pipe CLI access to the mounted Object storages' data - both from GUI and CLI Examples of logs: For more details see here . System Jobs Sometimes it's desirable for admin to get some statistics or system information about current Cloud Pipeline deployment state - for example, collect information about all storages that have specific size or list all unattached EBS volumes, or set some specific policy to all storages, etc. For such purposes, specific scripts can be written and launched in some way. Number of these \"admin\" scripts can grow very quickly and it would be convenient to have some solution to create and run such scripts in a system, and also view results (logs) and store them. In the current version, a new solution was implemeted for \"admin\" scripts - System Jobs . System jobs solution uses the existing Cloud-Pipeline infrastructure, to reduce number of preparation steps to be done to get desire output. In a nutshell, the System Jobs solution includes the following: special prepared system pipeline, that contains system jobs scripts. Admin can add new scripts or edit/delete existing ones. Also, pipeline config contains: Kubernetes service account to perform kubectl commands from such pipeline during the system job run special assign policy that allows to assign the pipeline to one of the running system node ( MASTER node, for example). It is convenient as no additional instances (waiting or initializing ones) are required to perform a job special prepared docker image that includes pre-installed packages such as system packages ( curl , nano , git , etc.), kubectl , pipe CLI, Cloud CLI ( AWS / Azure / GCP ), LustreFS client separate System Jobs form that displays all available system job scripts, allows to run existing scripts and view results of their performing Userjourney looks like: Admin creates a new system job script and places it to the specific path inside the special system pipeline ( pipeline and path are defined by System Preferences ), e.g.: Admin opens the System Jobs panel from the System Settings : Here, there is the whole list of stored system scripts and results of their runs. To run a script - admin selects any script and launch it: When admin launches a system job - the system instance ( MASTER instance, by default) is used for the job performing. At that system instance, the docker-container is launched from the special prepared docker-image for system jobs. In the launched docker-container, the system job script is being performed. At the System jobs form, states of the performing job are shown similar to the pipeline states, e.g.: Once the script is performed, the state will be changed to Success : By the button LOG , the script performing output can be viewed: For more details see here . Cluster run usage Previously, user can view the state of the cluster run (master and its nested runs) via the Run logs page of the cluster master node. But this information was actual only at the specific time moment. It would be convenient to view how the cluster usage has been changing over the whole cluster run duration. This is especially useful information for auto-scaled clusters, as the number of worker nodes in such clusters can vary greatly over time. In v0.17 , such ability was added. User can view a specific cluster's usage over time - by click the corresponding hyperlink at the Run logs page of the cluster's master node: The chart pop-up will be opened, e.g.: The chart shows a cluster usage - number of all active instances (including the master node) of the current cluster over time. For more details see here . Cluster run estimation price Previously, Cloud Pipeline allowed to view a price estimation for the single instance jobs. But the clusters did not provide such information (summary). Users could see a price only for a master node. Now, Cloud Pipeline offers a cost estimation, when any compute instances are running: Standalone instance - reports it's own cost: Dashboard: Run's list: Static cluster - reports the full cluster cost (summary for a master node and all workers), since it is started: Dashboard: Run's list - master node's cost is reported in the brackets as well: Autoscaled cluster - reports the costs, based on the workers lifetime (summary for a master node and all workers). As the workers may be created and terminated all the time - there costs are computed only for the RUNNING state: Dashboard: Run's list - master node's cost is reported in the brackets as well: Terminal view From the current version, users have the ability to configure the view of the SSH terminal session: Dark (default) Light Required color schema can be configured in two ways: Persistent - schema is being stored in the user profile and used any time SSH session is opened: Temporary - schema is being used during a current SSH session only - toggling Dark <-> Light can be performed via the special control in the terminal frame: For details see here . AWS: seamless authentication In some cases, users are faced with the following scenarios: Some jobs are running in the Cloud Pipeline and accessing data/services located in the external accounts (e.g. Amazon S3 , Amazon DynamoDB ). This requires the user to specify the authentication keys explicitly (either in the shell session or in the R / Python scripts). This is not user-friendly and not secure, if the users include the credentials into the scripts. There are also users who would like to leverage R / Python libraries, that have embedded Amazon S3 support. Users have to download data locally first (via pipe ) and then perform the processing. In the current version, a new mechanism of the seamless AWS authentication was implemented. It allows users to execute any request to the AWS API, from inside the Cloud Pipeline environment, without an authentication request. The following mechanism automates the Cloud Provider authentication for the user\u2019s scripts: Administrator is able to configure the user\u2019s access permissions in the Cloud Pipeline account of the Cloud Provider or provide credentials for the external Cloud Provider account All the requests to the Cloud Provider authentication are handled by the certain Cloud Pipeline service, which authenticates the user with the configured credentials Users are able to use the Cloud Provider API without the authentication request Administrator can create specific interfaces - Cloud Credentials Profiles , that contain the following fields: Provider - to specify the Cloud Provider Name - to specify the profile name Assumed Role - to specify the role received from the Cloud Provider that will be used for the authentication to the Cloud Provider API Policy - to specify the Cloud Provider policy of the objects access It could be configured in Cloud Provider settings, e.g.: Administrator can assign profiles to User/Role/Group entity. For each entity many profiles can be assigned. Also, from the profiles assigned to the certain User/Role/Group the one can be selected as default . If the default profile isn't selected - during the authentication operation there shall be selected the profile to use. It could be configured via the User management panel, e.g.: Usage of the assigned profiles is being configured via the new Cloud Region option - \" Mount Credentials Rule \" with the following allowed values: NONE - for runs in this region, credentials will not be configured SAME CLOUD - for runs in this region, the set user credentials will be configured only allowed for the same Cloud Provider ALL - for runs in this region, the all user credentials will be configured As example, if for the user such AWS credential profile is assigned and the mount rule is allowed - he/she can use AWS CLI directly to the bucket (defined and allowed by profile policy) without extra-authentication: For details and example see here . AWS: transfer objects between AWS regions using pipe storage cp / mv commands Previously, pipe storage cp / pipe storage mv commands allowed to transfer objects only within one AWS region. In the current version, the ability to transfer objects between storages from different AWS regions is implemented. The commands themselves remain the same. Example: AWS: switching of Cloud Regions for launched jobs in case of insufficient capacity Previously, if user started an AWS job and there were not enough instances of specified type to launch that job in a region - it would fail. In the current version, the ability to automatically relaunch such runs in other AWS region(s) was implemented. For that functionality, a new setting was added to the Cloud Region configuration - \" Run shift policy \": If this setting is enabled for some AWS region 1 and for some AWS region 2 - then a job launched in the AWS region 1 will automatically try to be relaunched in the AWS region 2 in case when there are not enough instances of selected type in the AWS region 1 ( InsufficientInstanceCapacity error): Original job is being automatically stopped, new job with the same instance type as in the original run but in the AWS region 2 will be launched. If a new instance is not available with a new region - relaunch will be performed in one more region as long as there are AWS regions in the Platform with the enabled option \" Run shift policy \". Feature is not available: for spot runs for runs that have any Cloud dependent parameter for worker or cluster runs More details see here . Notable Bug fixes Unable to view pipeline sources for previous draft versions #1353 Previously, Pipeline's \" Documents \" and \" Code \" tabs always showed content of the last \" draft \" version of pipeline, even if one of the previous versions was forcibly specified in the url. pipe storage ls works incorrectly with the option --page #1339 Previously, the pipe CLI storage listing worked incorrectly with --page ( -p ) option with S3 provider. All items were displayed without pagination. AWS deployment: unable to list more than 1000 files in the S3 bucket #1312 Previously, when s3 bucket contained more than 1000 files - user could list all the files in the bucket via the GUI, but only first 1000 files via any pipe CLI capabilities ( pipe storage ls , pipe storage mount , etc.). Size of tool version created from original tool without any changes is a lot larger than original one #1270 Previously, the size of the tool version that had created from the original tool without any changes or after resume operation for paused run - by COMMIT operation - was a lot larger than original version. pipe storage cp fails in Windows for the GCS with sslv3 error #1268 Previously, the sslv3 issue happened when data to/from the GCS was copying using the Windows workstation. Shared endpoint for anonymous users is being opened from the second time #1265 Previously, when anonymous user tried to open a hyperlink with the shared endpoint - he/she got the Platform's \"Access denied\" page. But if user tried to open the page in the second time - it was being opened correctly. Attempt to view permissions on a pipeline via the pipe view-pipes throws an error #1216 Previously, when trying to view permissions of a pipeline via the pipe view-pipes -r command - the command execution failed. Scale down \"cold\" SGE autoscaling cluster #1123 Previously, SGE autoscaling cluster didn't scale down until at least one running job appears in queue. Currently, SGE autoscaling cluster is being scaled down even if there weren't any running jobs yet. \"Launch Command\" functionality issues #1086 , #1090 Previously, if a user specified the values of the parameters with \"spaces\" (e.g. selection of the input parameter value from the GUI bucket browser) - this broke the command format. Also, the Launch Command generation function used single-quotes to wrap the -cmd value. This was causing to fail when running the generate commands from the Windows environment. As the Windows CMD shell can't resolve it correctly (the command value is still split by the space). Inner data storages navigation bar fails to navigate #1077 Previously, navigation bar for so-called \"inner\" data storages produced You cannot navigate to another storage in case of any interaction with it. Region is being set incorrectly when trying to rerun pipeline #1066 Previously, when tried to rerun any run - the default region was being set in the Cloud Region field. But the instance type wasn't being changed automatically and remained the same as was set before the run. This could lead to inconsistencies. PAUSE and COMMIT operations fail for the jobs with an autoscaled disk #998 Previously, PAUSE and COMMIT operations failed with the NullPointerException error for the jobs with an autoscaled disk. Broken layouts #1504 , #1505 In Groups / Roles membership view, the vertical scrollbar was shown even if there was a plenty of space below the list. Currently, the list size is increased to the pop up size. At the Billing reports page, if the whole header menu didn't not fit the screen width - the \"discounts\" links overflew the regions selector. Currently, row breaks feature is implemeted for this page.","title":"v.0.17"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#cloud-pipeline-v017-release-notes","text":"Billing reports enhancements System dictionaries Cloud Data application Sending of email notifications enhancements Allowed price types for a cluster master node \"Max\" data series in the resources Monitoring User management enhancements Allowed instance count Export custom user's attributes User management and export in read-only mode Batch users import User states Usage report GUI impersonation \"All pipelines\" and \"All storages\" repositories Sensitive storages Versioned storages Updates of \"Limit mounts\" for object storages Hot node pools FS quotas Export cluster utilization in Excel format Export cluster utilization via pipe Pause/resume runs via pipe Home storage for each user SSH tunnel to the running compute instance Updates of Metadata object Custom node images Launch a tool with \"hosted\" applications Advanced global search with faceted filters Explicitly \"immutable\" pipeline parameters Disable Hyper-Threading Saving of interim data for jobs stopped by a timeout Resolve variables for a rerun NAT gateway Custom Run capabilities Storage lifecycle management Image history Environments synchronization via pipectl Data access audit System Jobs Cluster run usage Cluster run estimation price Terminal view AWS: seamless authentication AWS: transfer objects between AWS regions AWS: switching of regions for launched jobs in case of insufficient capacity Notable Bug fixes Unable to view pipeline sources for previous draft versions pipe storage ls works incorrectly with the option --page AWS deployment: unable to list more than 1000 files in the S3 bucket Size of tool version created from original tool without any changes is a lot larger than original one pipe storage cp fails in Windows for the GCS Shared endpoint for anonymous users is being opened from the second time Attempt to view permissions on a pipeline via the pipe view-pipes throws an error Scale down \"cold\" SGE autoscaling cluster \"Launch Command\" functionality issues Inner data storages navigation bar fails to navigate Region is being set incorrectly when trying to rerun pipeline PAUSE and COMMIT operations fail for the jobs with an autoscaled disk","title":"Cloud Pipeline v.0.17 - Release notes"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#billing-reports-enhancements","text":"In the previous version, the Billing reports functionality was introduced (see details here ). In v0.17 , several useful features for the Billing reports were implemented.","title":"Billing reports enhancements"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#access-to-billing-reports-for-non-admin-users","text":"Previously, only admins had access to the Billing reports Dashboard and can view Platform's spendings data. In some cases, it is convenient that non-admin users also have the access to specific cost reports info. In the current version, such ability was implemented - in two ways: a new role was added into the predefined roles list - ROLE_BILLING_MANAGER . If that role is assigned to the user - for him/her the Billing reports Dashboard becomes available. And all possible filters, charts and their types, discounts configuration, export feature and etc. become available too. So, users who are granted this role are able to view the whole Billing reports info of the platform (as if they were admins). Note : this behavior is enabled by the new system preference billing.reports.enabled.admins . It allows to configure Billing reports visibility for admins and billing managers. Default value is true . base access to the Billing reports for \"general\" users that allows to view some information - about users' own spendings: this behavior is enabled by the new system preference billing.reports.enabled . If this preference is set, all \"general\" users can access personal billing information - runs/storages where the user is an owner. Also \"general\" users can use filters, change chart types, make reports export. the following restrictions are set for \"general\" users when \"base\" billing access is enabled: all showing charts are being displayed only spendings of the current user there isn't an ability to configure discounts, the button \"Configure discounts\" is disabled \"Billing centers (TOP 10)\" chart isn't displayed For example, the view of the Billing reports Dashboard for the \"general\" user when the system preference billing.reports.enabled is enabled:","title":"Access to Billing reports for non-admin users"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#storage-data-consumption","text":"Previously, Cloud Pipeline allowed to show only costs for the data storages. But it would be convenient to understand what is the total consumption of the data usage (volume of storages usage in GB) across all operational groups or individual by specific user. Currently, this ability is implemented. In all \"Storages\" reports, for the TOP 10 Storages... chart, the Volume in GB for each storage is displayed in the table, e.g.: Additionally for each storage, its Billing Center is displayed (if it's defined) and the storage Type ( S3 / GS / NFS / LustreFS /etc.). There are two Volume values displaying for each storage: Avg. Vol. is the average storage volume, in GB. It means that the exact volumes for each day of the selected report period were brought and then the average value was calculated Cur. Vol. is the current storage volume, in GB. This volume is a real volume for a current moment/last day of the given period The user can switch the view of the TOP 10 Storages... charts by a new control - By default, Costs displaying is selected. When Volume displaying is being selected, \"average\" volumes of the corresponding storages (in GB) will be displayed in the chart: These new columns ( Average Volume , Current Volume , Billing Center , Type ) also are being exported in tables reports.","title":"Storage data consumption"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#regionprovider-filter","text":"Previously, Billing reports allowed displaying the different Cloud Providers' instance types and their usage. But there was no way to get the overall per-Cloud or per-Region information. In v0.17 , these abilities were implemented. Now the user can use the following filters: specific Cloud Provider(s) ( for multi-Provider deployments ) specific Region(s) of the specific Cloud Provider They all can be specified via the \" Regions/Providers \" dropdown list in the top of any Billing reports page, e.g.:","title":"Region/Provider filter"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#custom-date-range-for-the-report","text":"Previously, Cloud Pipeline allowed to configure date range on the Billing reports dashboard for different periods ( year , quarter , month(s) ), but the minimum period for any report was only month . Sometimes, it is needed to view cost utilization for a specific period in days. In v0.17 , it was implemented - the user can view Billing reports with manually configured period accurate to the day: Select the \"Custom\" period and click the \"Calendar\" control: Select \"From\" and \"To\" dates, confirm the selection: Reports (charts and tables) will be rebuilt for the configured custom date range:","title":"Custom date range for the report"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#spendings-for-old-versions-of-object-storages","text":"As object storages supports versioning, it is convenient to view spendings for the old (previous) versions of the storage data. Old versions include all non-last (previous) versions of the versioning object storage. From the current version, the Object storages report supports the displaying of the corresponding related information. At the summary chart, new dashed lines of the same colors (as for current and previous periods) appeared - these lines show summary spendings on the data usage for all old versions of object storages: On all other object storage charts, bars are presented as stacks of current version spendings / old versions spendings. Current version spendings are shown with solid filling, old versions spendings are shown without filling, e.g.: Also now, the detailed spendings table for object storages shows the info for spendings/usage in the format total spendings/usage for all versions / spendings/usage for old versions only : Breakdown by versions is shown in the CSV report export as well.","title":"Spendings for old versions of object storages"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#spendings-for-object-storages-archive-layers","text":"As object storages supports archiving data into different archive tiers (layers), it is convenient to view spendings separately for each layer. From the current version, the Object storages report supports the displaying of the corresponding related information. This information is shown on the separate chart - bar chart with division to different tiers (archive types). This chart does not contain any information for previous period. Only layers used for data storing in the current period according to selected filters are shown. Up to 4 layers can be here: Standard , Glacier , Glacier IR , Deep Archive . Example: Object storage layers chart can show the information as storages usage costs - in $ or as average storages volumes - in Gb : If data in the storage is storing in different tiers (archive types), this can be viewed in a tooltip of other object storages charts - there will be a division of spendings by the used tiers, e.g.: Breakdown by archive layers is shown in the CSV report export as well. User can select one of the object storage layers - by click it on this new chart. In this case, all charts and tables will be updated - only storages, that contain files in the selected layer type, will be shown in forms. Also, shown spendings/data volume will be related only to files in the selected layer, not for the whole storage(s) or other layers. For example, Glasier IR was selected:","title":"Spendings for object storages' archive layers"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#spendings-in-runs-cost-layers","text":"From the current version, the Compute instances report (and sub-reports - for CPU/GPU) supports the displaying of the runs cost division into layers: Compute - cost of compute instances used in runs Disk - cost of EBS drives connected to runs during their performing This information is shown on the new Cost details chart - bar chart with division to these layers. This chart does not contain any information for previous period - only cost of runs' layers in the current period according to selected filters are shown. Additionally, information about cost division are shown in details tables under charts Instance types , Pipelines , Tools - as separate columns, e.g.: User can select one of the runs cost layers - by click it in the Cost details chart. In this case: summary runs cost chart will be updated - only summary spendings, that correspond to the selected layer ( Compute or Disk ), will be shown charts Instance types , Pipelines , Tools will be updated - only spendings, that correspond to the selected layer ( Compute or Disk ), will be shown data in tables under charts will not be changed, but the sorting column will be set the same as the selected layer For example, if the Compute layer of the runs cost is selected:","title":"Spendings in runs cost layers"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#displaying-different-users-attributes-in-the-billing-reports","text":"Previously, in all the Billing reports , info about users was displayed as user ID only. In some cases, it would be more convenient to display user names or emails - to take a more readable form. In the current version, this ability is implemented. A new System Preference is introduced: billing.reports.user.name.attribute It defines which user's attribute shall be used to display the users in the Billing reports . If it is set, specified attribute will be used in all billing charts, tables, export reports. Possible values for described preference: userName , FirstName , LastName , etc.","title":"Displaying different user's attributes in the Billing reports"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#export-reports-in-csv-from-any-billing-page","text":"Previously, Cloud Pipeline allowed to export the Billing reports data into the CSV format via the \"General\" section only. But in separate sections - \"Storages\" and \"Compute Instances\" - the user could export data as PNG image format only. Currently, CSV export has been added to all the reports sections (\"Storages\"/\"Compute instances\" and all sub-sections): reports display the same structure as in the GUI - the top 10 records of the corresponding entities (e.g. storages or instances) for the reports, which contain more than one table - all the tables are exported one after another export in CSV from the \"General\" page remains the same Example of an export from the \"CPU\" page:","title":"Export reports in CSV from any Billing page"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#breakdown-the-billing-reports-by-month","text":"Cloud Pipeline allows exporting billing reports in the CSV . Previously, the values were shown as aggregates for the whole selected period. In some cases, it is more convenient to change this view to a breakdown by month. In the current version, this ability is implemented. Now, if any period - longer than a month is selected (including a custom period), the CSV -report contains an aggregate for each month of that period. The whole period summary is being included as well (as previously). Example of the report for a custom period:","title":"Breakdown the billing reports by month"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#billing-general-export-broken-by-the-user","text":"Previously, Cloud Pipeline could export the \"General\" billing report split by the \"Cost Center\". For some use cases, needs to have this report broken by the user as well. Now, this ability is implemented. User can specify which dimension to use for the export: by Cost Center - in this case, the \"General\" billing report will be split by the \"Cost Center\" (as it was previosly) by User - in this case, export will be in the same format as for the \"Cost Center\", but split the values by the user (using billing.reports.user.name.attribute to display the username) Format of the report is being selected before the export: Example of the report broken by the user:","title":"\"Billing General\" export broken by the user"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#system-dictionaries","text":"Often admins have to set attributes (metadata) for \"general\" users manually. In case, when such metadata keys aren't different for each user and has certain amount of values, it is convenient to select these values from the predefined values list, not to specify them manually each time. In the current version, the ability to create System Dictionaries was implemented. Each dictionary is the categorical attribute. I.e. it is attribute which values are predefined. Each dictionary has its name and values, e.g.: If the dictionary exists in the system, then admins can use it when specifying attributes for any Platform object ( Pipeline , Folder , Storage , Project , Tool ), and also for User , Group or Role . In this case, it is enough to specify only the dictionary name as the attribute key, the list of dictionary values will appear automatically in the value field: Also, the different dictionaries may be connected (linked). I.e. admins can create two dictionaries, which values are mapped 1-1 or 1-many , e.g.: In the GUI, such connection is being handled in the following way: Admin specifies the links between the dictionaries items (e.g. for the example above ProjectID : BRCA1 -> DataStorage : <path> ). Links have the \"autofill\" attribute. If the admin selects the source key ( ProjectID : BRCA1 ) as attribute key for any object - the destination key will be specified automatically ( DataStorage will be added with the <path> selection): For more details see here .","title":"System dictionaries"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#cloud-data-application","text":"Previously, there were several ways to manage data between local workstation and Cloud data storages, including CLI, GUI, mounting data storages as a network drives, and others. In the current version, a new Platform capability was implemented that provides a simple and convenient way to manage files, copy/move them between Cloud data storage and local workstation or even FTP-server. This introduces via the new separate application that can be downloaded from the Cloud Pipeline Platform and launched at the local machine - Cloud Data application. Cloud Data application allows you manage files/folders as in a file commander. Main application form contains two panels (left and right). In each panel, one of the following sources can be opened: local workstation / FTP server / Cloud data (datastorages). the local content shows files and folders of the local workstation (by default, home user's directory). Navigation between and inside folders is available: the Cloud data content includes: all FS mounts from the Cloud Pipeline environment - to which current user has permissions. They are shown as simple folders those object storages from the Cloud Pipeline environment - to which current user has permissions and \"File system access\" was requested. They are shown with storage icon Navigation between and inside folders/storages is available the ftp content shows files and folders of the FTP/SFTP server. Navigation between and inside folders is available: The base scenario of the application usage: User selects desired source and destination in panels, e.g. FTP server and object datastorage correspondingly: Users selects desired files/folders in the source and clicks the data management button in the source panel - according to the action user wants to perform, e.g. to copy a file: Action will be performed, content of the panels will be updated: For more details see here .","title":"Cloud Data application"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#sending-of-email-notifications-enhancements","text":"Several additions and updates were implemented in the current version for the System Email notifications. You can view the general mechanism of the Cloud PIpeline email notifications sending described here .","title":"Sending of email notifications enhancements"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#additional-options-for-idlehigh-consumed-runs-notifications","text":"Previously, to customize a platform behavior with respect to idle or high-consumed runs, admin had to set a number of settings in two different system forms - Preferences and Email Notifications . It was inconvenient and could confused users. It would be nice to duplicate input fields for some preferences into the Email Notifications section - for faster and more convenient input of their values, and to avoid possible confusion and mistakes. In the current version, it was implemented. Now: For HIGH_CONSUMED_RESOURCES notification type settings, the following input fields were added: \" Threshold of disk consume (%) \" that duplicates system.disk.consume.threshold preference value \" Threshold of memory consume (%) \" that duplicates system.memory.consume.threshold preference value Saving of the listed values changes at the Email Notifications form will automatically change the corresponding values in the Preferences , and vice versa. For IDLE_RUN , IDLE_RUN_PAUSED , IDLE_RUN_STOPPED notification types settings, the following input fields were added: \" Max duration of idle (min) \" that duplicates system.max.idle.timeout.minutes preference value \" Action delay (min) \" that duplicates system.idle.action.timeout.minutes preference value \" CPU idle threshold (%) \" that duplicates system.idle.cpu.threshold preference value \" Action \" that should duplicates system.idle.action preference value These 4 fields are united into a single section for all idle notification types - you may configure these fields from any idle notification settings tab. Saving of the listed values changes at the Email Notifications form will automatically change the corresponding values in the Preferences , and vice versa. For all these fields, help tooltips were added to clarify their destination, e.g.:","title":"Additional options for IDLE/HIGH-CONSUMED runs notifications"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#notifications-for-long-paused-runs","text":"In v0.17 , new email notification types were added: LONG_PAUSED - the notification that is being sent when the run is in the PAUSED state for a long time. This new notification type has the following additional configurable parameters: Threshold (sec) - it is a time interval of the run PAUSED state after which the notification will be sent Resend delay (sec) - it is a delay after which the notification will be sent again, if the run is still in the PAUSED state LONG_PAUSED_STOPPED - the notification that is being sent when the run that has been in the PAUSED state for a long time, has been stopped by the system. This new notification type has the following additional configurable parameter: Threshold (sec) - it is a time interval of the run PAUSED state after which the notification will be sent and the run will be terminated There is a common setting for the both described notification types - Action . This setting could be only NOTIFY or STOP . It defines the system behavior with the long paused runs: if the Action is NOTIFY - for the appropriate run, the notification LONG_PAUSED will being sent according to its settings if the Action is STOP - for the appropriate run, the notification LONG_PAUSED_STOPPED will be sent once and the run will be terminated Action type also can be configured via the Systemp preference system.long.paused.action . Saving of the Action setting value changes at the Email Notifications form will automatically change the corresponding value in the Preferences , and vice versa.","title":"Notifications for long paused runs"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#resend-setting-for-idle-runs","text":"Previously, IDLE_RUN notifications were sent only once and then configured action had being performed. In the current version, the ability to resend this notifications was implemented. It could be configured via the corresponding field at the IDLE_RUN notification type form: If the Resend delay is specified and the Action for the idle runs is set as NOTIFY , then the IDLE_RUN notification will being resent every appropriate time interval.","title":"\"Resend\" setting for IDLE runs"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#allow-to-exclude-certain-node-type-from-the-specific-notifications","text":"For quite small/cheap nodes, the users may not want to receive the following email notifications for the run: IDLE_RUN LONG_PAUSED LONG_RUNNING So, a new System preference system.notifications.exclude.instance.types was implemented to control that behavior. If the node type is specified in this preference, listed above notifications will not be submitted to the jobs, that use this node type. This preference allows a comma-separated list of the node types and wildcards, e.g.:","title":"Allow to exclude certain node type from the specific notifications"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#push-notifications","text":"Previously, Cloud Pipeline platform sent notifications to users via email only. For many cases it would be useful to show such notifications in the GUI as well. In the current version, such ability was implemented. Now, all email notifications, that are sending by the platform, are also duplicated as push notifications. This allows to view notifications right in the Cloud Pipeline GUI. Push notifications do not require additional configuring - they are fully the same as corresponding email notifications, i.e. have the same header, content, recepients list, frequency and trigger of sending, etc. Once any system event is occurred and its trigger for sending email notification has fired, email will be sent to the configured recipients. Simultaneously, the push notification (with the same subject and body as in the email) will be \"sent\" to the same recipients, e.g.: Click it to view the whole notification - it will be opened in a pop-up: Additionally, a new section appeared in the main menu - Notifications . It allows to view all push notifications/emails sent to the current user, e.g.: User can switch notifications lists - to display only new \"unread\" notifications or only \"read\" ones. To view the notification full details, user can click it - notification will be opened in a pop-up: For more details see here .","title":"Push notifications"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#allowed-price-types-for-a-cluster-master-node","text":"Previously, Cloud Pipeline allowed the user to choose whether the cluster master node be a spot or on-demand instance. While spots are acceptable for the worker nodes, as they can be recreated in failure cases - master node failure will terminate the whole cluster. To make things easy for the end-users, an optional restriction on the specific price types usage for the master nodes was implemented. There is a new string system preference - cluster.allowed.price.types.master - that force the clusters' master node price type. Default value : \"spot,on_demand\" - so, both types are accessible for the user when he/she wants to launch a cluster. Possible values : \"spot\", \"on_demand\" or both together comma-separated. Specified value for that preference defines which price type(s) will be shown in the drop-down, when the cluster run is being configured. For example: set in the Preferences: once the user selects any cluster configuration in the \"Exec environment\" section - available price types becomes equal to the set value: Note : cluster.allowed.price.types.master preference doesn't apply on the price types for single-node jobs","title":"Allowed price types for a cluster master node"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#max-data-series-at-the-resource-monitoring-dashboard","text":"Previously, Cloud Pipeline displayed the resources utilization as an average value. This could hide some spikes (which resulted in job failure), when reviewing at a high zoom-level (e.g. several days). In the current version, to the \"CPU Usage\" and the \"Memory Usage\" charts additional data-series (\"lines\") were added, which are calculated as a max function in each moment. Existing lines are kept as well, but were renamed to average . For example: For more details see here .","title":"\"Max\" data series at the \"Resource Monitoring\" dashboard"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#user-management-enhancements","text":"","title":"User management enhancements"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#allowed-instance-count","text":"Sometimes users' scripts may spawn hundreds of machines without a real need. This could lead to different bugs on the Platform. To prevent such situation, a new setting - Allowed instance max count - was added to the user's options. It allows to restrict the number of instances a user can run at the same time: Behavior is configured by the following way: for example, if this setting for the user is specified to 5 - they can launch only 5 jobs at a maximum. This includes worker nodes of the clusters. If the user tries to launch a job, but it exceeds a current limit (e.g. limit is 5 and user starts a new instance which is going to be a 6th job), GUI will warn the user before submitting a job: And if the user confirms a run operation - it will be rejected: Even if the user will try to start a new job via pipe CLI - it will be rejected as well, e.g.: Such restrictions could be set not only for a user, but on another levels too (in descending order of priority): User-level - i.e. specified for a user. This overrides any other limit for a particular user. See details here . User group level - i.e. specified for a group/role. Count of jobs of each member of the group/role is summed and compared to this parameter. If a number of jobs exceeds a limit - the job submission is rejected. This level is configured via the Allowed instance max count setting for a group/role. See details here . globally via the system preference launch.max.runs.user.global - it can be used to set a global default restriction for all the users. I.e. if it set to 5, each Platform user can launch 5 jobs at a maximum. Additionally, a new command was added to pipe CLI that allows to show the count of instances running by the user at the moment, and also all possible restrictions to the allowed count of instances to launch - pipe users instances : See details here .","title":"Allowed instance count"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#export-custom-users-attributes","text":"Previously, user's metadata attributes couldn't be exported in an automatic way. In the current version, such feature is implemented. Now, before the users export, there is the ability to select which user's metadata attributes shall be exported. Previous export settings remain the same. Click the \" Export users \" button at the USER MANAGEMENT tab of the System Settings . Select the \"Custom configuration\": In the export pop-up, select additional metadata keys you wish to export with general user's info: Exported metadata will be included into the export file as separate columns, e.g. (part of the output): For more details about users export see here .","title":"Export custom user's attributes"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#user-management-and-export-in-read-only-mode","text":"Previously, only admins had access to the users info/metadata. In the current version, a new \"built-in\" role ROLE_USER_READER was added. This role allows: read-only access to the API endpoints, responsible for the users, groups, roles information in the GUI , users with this role can: get \"general\" user/groups information in read-only mode - name/email/etc. - without users' metadata get access to the user management tab in read-only mode - without users' metadata and launch options export users list - including users' metadata For more details about user roles see here .","title":"User management and export in read-only mode"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#batch-users-import","text":"Previously, Cloud Pipeline allowed creating users only one-by-one via the GUI. If a number of users shall be created - it could be quite complicated to perform those operation multiple times. To address this, a new feature was implemented in the current version - now, admins can import users from a CSV file using GUI and CLI. CSV format of the file for the batch import: UserName,Groups,<AttributeItem1>,<AttributeItem2>,<AttributeItemN> <user1>,<group1>,<Value1>,<Value2>,<ValueN> <user2>,<group2>|<group3>,<Value3>,<Value4>,<ValueN> <user3>,,<Value3>,<Value4>,<ValueN> <user4>,<group4>,,, Where: UserName - contains the user name Groups - contains the \"permission\" groups, which shall be assigned to the user <AttributeItem1> , <AttributeItem2> ... <AttributeItemN> - set of optional columns, which correspond to the user attributes (they could be existing or new) The import process takes a number of inputs: CSV file Users/Groups/Attributes creation options , which control if a corresponding object shall be created if not found in the database. If a creation option is not specified - the object creation won't happen: \" create-user \" \" create-group \" \" create-<ATTRIBUTE_ITEM_NAME> \"","title":"Batch users import"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#import-users-via-gui","text":"Import users from a CSV file via GUI can be performed at the USER MANAGEMENT section of the System Settings . Click the \" Import users \" button: Select a CSV file for the import. The GUI will show the creation options selection, e.g.: After the options are selected, click the IMPORT button, e.g.: Once the import is done - you can review the import results: Users and groups have been created Users were assigned to the specified groups Attributes were assigned to the users as well For more details and examples see here .","title":"Import users via GUI"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#import-users-via-cli","text":"Also in the current version, a new pipe command was implemented to import users from a CSV file via CLI: pipe users import [OPTIONS] FILE_PATH Where FILE_PATH - defines a path to the CSV file with users list Possible options: -cu / --create-user - allows the creation of new users -cg / --create-group - allows the creation of new groups -cm / --create-metadata <KEY> - allows the creation of a new metadata with specified key Results of the command execution are similar to the users import operation via GUI. For more details and examples see here .","title":"Import users via CLI"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#user-states","text":"Previously, admins could monitor Platform usage, for example, by list of ACTIVE RUNS or via CLUSTER STATE pages. But for some cases, it can be useful to know which users do utilize the Platform in the current moment. In the current version, the displaying of user states in the \"User management\" system tab was implemented - now, that state is shown as an circle icon near the user name: Possible states: Online (green circle) - for users who are logged in and use the Platform in the moment Offline (blank white circle) - for users who are not logged in at the moment/do not use the Platform for some time By hover over the Offline icons - admin can know when the specific user has utilized the Platform the last time, e.g.:","title":"User states"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#usage-report","text":"It is convenient to have the ability to view Platform statistics of users activity. E.g. when creating different schedulers or node pools and info about number of online users can be helpful. For that, the Usage report subtab, showing the Platform's statistics of users activity, was added to the \"User Management\" system tab. At this subtab, the summary info about total count of Platform users that were online at different time moments during the certain period is displayed in a chart form: User can configure the showing chart by the following ways: select the type of period of view - day ( by default ) or month select to display data for a specific day/month from the calendar restrict the displayed data for specific user(s) or user group(s)/role(s) only For more details see here .","title":"Usage report"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#gui-impersonation","text":"While performing administrating, it is common to help users resolve issues, which can't be reproduced from the administrative accounts. This requires to perform operations on the users' behalf. To assist with such tasks, Cloud Pipeline offers \"Impersonation\" feature. It allows admins to login as a selected user into the Cloud Pipeline GUI and have the same permissions/level of access as the user. To start the impersonation, admin shall: Open the Users subtab of the \"User Management\" section of the system-level settings Load the user profile on whom behalf you are going to impersonate and click the Impersonate button in the top-right corner, e.g.: Platform GUI will be reloaded using the selected user: While in the \"Impersonation\" mode, the following changes happen to the GUI: Main menu turns orange, indicating that the impersonation mode is ON Logout button is being changed to the Stop impersonation button To stop the \"Impersonation\" mode, user shall click the Stop impersonation button. For more details see here .","title":"GUI impersonation"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#all-pipelines-and-all-storages-repositories","text":"There are several ways for users to find the appropriate storage/pipeline object in the Cloud Pipeline Platform - manually via the Library , using the Search ability or via the corresponding panels of the main Dashboard. It would be convenient to get all lists of the storages/pipelines accessible to the user in one place with short info about each object and easy access to it. In the current version, such ability was implemented: there are two new controls displaying at the Library page, above the library-tree - separate \"repositories\" for storages and pipelines: each \"repository\" displays the full list of the corresponding objects accessible by the current user, e.g. for pipelines: for each object in the \"repository\" are displayed: object name object description (if it is available) OWNER user name additionally for pipelines, the Run button - if the pipeline is available for execute for the user additionally for storages, Cloud Region / Provider icons for multi-provider deployments if the user clicks any object in the list - its regular page is being opened for each \"repository\", there is a search field for the quick search over objects list","title":"\"All pipelines\" and \"All storages\" repositories"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#sensitive-storages","text":"Previously, Cloud Pipeline platform allows performing upload/download operations for any authorized data storage. But certain storages may contain sensitive data, which shall not be copied anywhere outside that storage. For storing such data, special \"sensitive\" storages are implemented. Sensitive data from that storages can be used for calculations or different other jobs, but this data cannot be copy/download to another regular storage/local machine/via the Internet etc. Viewing of the sensitive data is also partially restricted. Sensitive storage is being created similar to general object storage, user only should tick the corresponding checkbox: Via the GUI, the sensitive storage looks similar to the regular object storage, but there are some differences (even for admin/storage OWNER ): I.e. files/folders in the sensitive storage can be created/renamed/removed but can't be downloaded/viewed or edited by any user. Sensitive storages can be mounted to the run. In this case, the run will become sensitive too. In sensitive runs, all storages selected for the mounting including sensitive are being mounted in readonly mode to exclude any copy/move operations between storages. Files from the sensitive storages can be viewed inside the sensitive run and also copied into the inner instance disk, but not to any other storage: Files from the sensitive storages can't be viewed outside the sensitive run or copied/moved anywhere (for example, when using not the web-terminal version of pipe SSH): For more details and restrictions that are imposed by using of sensitive storages see here .","title":"Sensitive storages"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#versioned-storages","text":"In some cases, users want to have a full-fledged system of the revision control of their stored data - to view revisions, history of changes, diffs between revisions. So far, for separate storages types (e.g. AWS s3 buckets), there is the ability to enable the versioning option. But it is not enough. Such versioning allows to manage the versions of the certain file, not the revisions of the full storage, which revision can contain changes of several files or folders. For the needs of full version control of the storing data, there was implemented a special storage type - Versioned storage . These storages are GitLab repositories under the hood, all changes performed in their data are versioned. Users can view the history of changes, diffs, etc. Versioned storages are created via the special menu: The view of the versioned storage is similar to regular data storage with some differences: For each file/folder in the storage, additional info is displayed: Revision - latest revision (SHA-1 hash of the latest commit) touched that file/folder Date changed - date and time of the latest commit touched that file/folder Author - user name who performed the latest commit touched that file/folder Message - message of the latest commit touched that file/folder Moreover, there are extra controls for this storage type: RUN button - allows to run the tool with cloning of the opened versioned storage into the instance Generate report button - allows to configure and then download the report of the storage usage (commit history, diffs, etc.) as the Microsoft Word document ( docx format) Show history button - allows to open the panel with commit history info of the current versioned storage or selected folder Each change in a such storage - is a commit by the fact, therefore each change has its related comment message - explicit or automatic created: One of the important advantages of versioned storages in condition with regular object storages - ability to view commit history and all changes that were performed with the data in details. Users can view the commit history of the file in the versioned storage - i.e. history of all commits that touched this file, e.g.: Using the commit history of the file, users can: revert the content of the file to the selected commit view/download revert version of the file corresponding to the specific commit view diffs between the content of the specific file in the selected commit and in the previous commit, e.g.: Besides that, users can view the commit history of the folder or the whole versioned storage - i.e. history of all changes touched files inside that folder or its subfolders, e.g.: Using the commit history of the folder, users can view diffs between the content of the specific folder in the selected commit and in the previous commit. Versioned storages can be also mounted during the runs, data can be used for the computations and results can be comitted back to such storages - with all the benefits of a version control system. For that, new management controls were added to the menu of the active runs: Via this controls, users can: clone the versioned storage(s) to the existing running instance check differences between cloned and current changed versions of the versioned storage save (commit) changes performed in the cloned version of the storage during the run checkout revision of the cloned storage in the run resolve conflicts appeared during the save or checkout operation The main scenario of using versioned storage during the run looks like: user clones selected versioned storage to the run: cloned versioned storages are available inside the run by the path /versioned-data/<storage_name>/ : user works with the data, performed changes can be viewed at any moment, e.g.: user saves performed changes (i.e. creates a new commit): saved changes become available in the origin versioned storage: For more details about versioned storages and operations with them see here .","title":"Versioned storages"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#updates-of-limit-mounts-for-object-storages","text":"","title":"Updates of \"Limit mounts\" for object storages"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#displaying-of-the-cp_cap_limit_mounts-in-a-user-friendly-manner","text":"Previously, Cloud Pipeline displayed the run-enabled data storages (selected via \"Limit mounts\" feature before the launch) as a list of IDs at the Run logs page (as the CP_CAP_LIMIT_MOUNTS parameter). In the current version, this viewing was changed to more \"friendly\" for users: The data storage names are being displayed instead of the IDs Showing names are hyperlinks, pointing to the data storage in the Cloud Pipeline GUI \"Sensitive\" storages are being highlighted appropriately See details here .","title":"Displaying of the CP_CAP_LIMIT_MOUNTS in a user-friendly manner"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#allow-to-create-run-without-mounts","text":"Previously, users could select all/several storages (from the available scope) to be mounted during the run. But in some cases, it might be needed to launch runs without mounts at all. In the current version, such ability was implemented. For that, the separate checkbox was added to the \"Limit mounts\" settings section: If this checkbox is set - there are no storages will be mounted during the run initialization: The ability to set \"Do not mount storages\" is added to all forms where limit mounts can be configured.","title":"Allow to create run without mounts"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#warning-in-case-of-a-risk-of-oom-due-to-the-number-of-the-object-storage-mounts","text":"If the user has 100+ object storages available - they all are mounted to the jobs, by default. When using rather small nodes - this leads to the OOM errors, as the 100+ mount processes may oversubscribe the memory. Even if the memory consumption will be greatly optmized - the user may still face such issues, if the number of object storages grow. So in the current version, a sort of hard-limit was implemented to warn the user if there is risk of OOM . A new System preference is introduced - storage.mounts.per.gb.ratio ( int ). This preference allows to specify the \"safe\" number of storages per Gb of RAM (by default, it is 5 - i.e. \"5 storages per each Gb of RAM \"). When launching a job - the user's available object storages count is being calculated and checked that this count does not exceed the selected instance type RAM multiplied by the storage.mounts.per.gb.ratio . If it's exceeded - the user is being warned with the following wording and asked to reduce a number of mounts via the Limit mounts feature, e.g.: Note : Warning does not prohibit the run launching, user can start it at his own discretion changing nothing. If the storage.mounts.per.gb.ratio is not set - no checks are being performed, no warning appears. Before the launch, only the object storages count is being calculated, file mounts do not introduce this limitation.","title":"Warning in case of a risk of OOM due to the number of the object storage mounts"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#hot-node-pools","text":"For some jobs, a waiting for a node launch can be too long. It is convenient to have some scope of the running nodes in the background that will be always or on schedule be available. In the current version, the mechanism of \" Hot node pools \" was implemented. It allows controlling the number of persistent compute nodes (of the certain configuration) in the cluster during the certain schedule. This is useful to speed up the compute instances creation process (as the nodes are already up and running). Admins can create node pools: each pool contains one or several identical nodes - admin specifies the node configuration (instance type, disk, Cloud Region , etc.) and a corresponding number of such nodes. This count can be fixed or flexible (\"autoscaled\") each pool has the schedule of these nodes creation/termination . E.g. the majority of the new compute jobs are started during the workday, so no need to keep these persistent instances over the weekends. For the pool, several schedules can be specified for each pool can be configured additional filters - to restrict its usage by the specific users/groups or for the specific pipelines/tools etc. When the pool is created, corresponding nodes are being up ( according to pool's schedule(s) ) and waiting in the background: If the user starts a job in this time ( pool's schedule(s) ) and the instance requested for a job matches to the pool's node - such running node from the pool is automatically being assigned to the job, e.g.: Note : pools management is available only for admins. Usage of pool nodes is available for any user. For more details and examples see here .","title":"Hot node pools"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#fs-quotas","text":"In some cases, users may store lots of extra files that are not needed more for them in FS storages. Such amount of extra files may lead to unnecessary storage costs. To prevent extra spending in this case, in the current version a new ability was implemented - FS quotas. There is a feature that allows admins to configure quota(s) to the FS storage volume that user can occupy. On exceeding such quota(s), different actions can be applied - e.g., just user notifying or fully read-only mode for the storage. This allows to minimize the shared filesystem costs by limiting the amount of data being stored in them and to notify the users/admins when FS storage is running out of the specific volume. To configure notifications/quota settings for the storage, admin shall: click the Configure notifications hyperlink in the Attributes panel of the storage: in the appeared pop-up, specify the username(s) or a groupname(s) in the Recipients input to choose who will get the FS quota notifications via emails and push notifications, e.g.: then click the Add notification to configure rules/thresholds: Put a threshold in Gb or % of the total volume and choose which action shall be performed when that threshold is reached. The following actions can be taken by the platform: Send email - just notify the recipients that a quota has been reached (notification will be resent each hour) Disable mount - used to let the users cleanup the data: GUI will still allow to perform the modification of this storage ( read-write mode ) In existing nodes (launched runs), FS storage mount will be switched to a read-only mode (if it was mounted previously) This FS storage will be mounted in a read-only mode to the new launched compute nodes Make read-only - used to stop any data activities from the users, only admins can cleanup the data per a request: GUI will show this FS storage in a read-only mode Existing nodes (launched runs) will turn this mounted FS storage in a read-only mode as well This FS storage will be mounted in a read-only mode to the new launched compute nodes The notification/quota rules can be combined in any form. E.g., the following example sets three levels of the thresholds. Each level notifies the users about the threshold exceeding and also introduces a new restriction: For example, if admin will configure notifications/quotas for the storage as described above: when user(s) will create/upload some files in the storage and summary FS size will exceed 5 Gb threshold - only notifications will be sent to recipients when user(s) will create/upload some more files in the storage and summary FS size will exceed 10 Gb threshold: for active jobs (that were already launched), filesystem mount becomes read-only and users will not be able to perform any modification for new jobs, filesystem will be mounted as read-only in GUI: permissions will not be changed. Write operations can be performed, according to the permissions \" Warning \" icon will be displayed in the storage page. It will show MOUNT DISABLED state: Storage size will be more than 10 Gb: when user(s) will create/upload some more files in the storage (e.g. via GUI) and summary FS size will exceed 20 Gb threshold: for active jobs (that were already launched), filesystem mount will remain read-only and users will not be able to perform any modification for new jobs, filesystem will be mounted as read-only in GUI: storage will become read-only . User will not be able to perform any modification to the filesystem \"Warning\" icon will be still displayed. It will show READ ONLY state Storage size will be more than 20 Gb: Please note, these restrictions will be applied to \"general\" users only. Admins will not be affected by the restrictions. Even if the storage is in read-only state - they can perform READ and WRITE operations. For more details about FS quotas, their settings and options see here .","title":"FS quotas"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#export-cluster-utilization-in-excel-format","text":"Previously, users could export Cluster Node Monitor reports only in CSV format. From now, the ability to export these reports in XLSX format is implemented. Users can choose the format of the report before the download: Excel -reports contain not only raw monitoring data but the graphical info (diagrams) too as users can see on the GUI. Example of the Excel -report sheets: For more details how to configure Cluster Node Monitor reports see here .","title":"Export cluster utilization in Excel format"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#export-cluster-utilization-via-pipe","text":"Also in the current version, the ability to export Cluster Node Monitor reports by pipe CLI is introduced. The command to download the node usage metrics: pipe cluster monitor [OPTIONS] The one of the below options should be specified: -i / --instance-id {ID} - allows to specify the cloud instance ID. This option cannot be used in conjunction with the --run-id option -r / --run-id {RUN_ID} - allows to specify the pipeline run ID. This option cannot be used in conjunction with the --instance-id option Using non-required options, user can specify desired format of the exported file, statistics intervals, report period, etc. For details and examples see here .","title":"Export cluster utilization via pipe"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#pauseresume-runs-via-pipe","text":"Previously, users could automate the pause and resume operation for the pipeline execution only via the API calls. In the current version, pipe pause and pipe resume operations are exposed to the CLI. The command to pause a specific running pipeline: pipe pause [OPTIONS] RUN_ID Possible options: --check-size - to check firstly if free disk space is enough for the commit operation -s / --sync - to perform operation in a sync mode. This option blocks the terminal until the PAUSED status won't be returned for the pausing pipeline The command to resume a specific paused pipeline: pipe resume [OPTIONS] RUN_ID Possible option: -s / --sync - to perform operation in a sync mode. This option blocks the terminal until the RUNNING status won't be returned for the resuming pipeline For details and examples see here - pause command and resume command .","title":"Pause/resume runs via pipe"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#home-storage-for-each-user","text":"Typically each general user stores personal assets in the data storage, that is created for him/her by the Administrator. This is treated as a \"home\" storage and is used a lot. But the creation of multiple users becomes a tedious task (create the user/create storage/grant permissions for the user). To facilitate this task, in the current version the ability (optionally) to create home storages for the newly created users in automatic mode was implemented. This behavior is controlled by the system preference storage.user.home.auto ( Boolean , default value is false ). It controls whether the home storages shall be created automatically. If it is set to true - new storage will be created for the user automatically simultaneously with the user creation. Also the just-created user is being granted OWNER permissions for the new storage. The \"home\" storage automatic creation is being driven by a template. The template is being described as JSON element in the other new system preference - storage.user.home.template . In this preference for the template, being described: settings for the storage permissions on the storage Example of the configured preferences: So, after the user creation, the new storage according to the settings in template is being created: The newly created storage is being set as a \"default\" storage in the user's profile: For more details and examples see here .","title":"Home storage for each user"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#ssh-tunnel-to-the-running-compute-instance","text":"In the current version, a new ability to access Cloud Pipeline run instances from local workstations is implemented. Now, Cloud Pipeline run instance can be accessed via SSH directly using special network tunnels . Such tunnels can be established between a local Windows or Linux workstation and a Cloud Pipeline run. pipe CLI provides a set of command to manage such network tunnels. pipe CLI automatically manages SSH keys and configures passwordless SSH access . As a result no manual SSH keys management is required to access Cloud Pipeline run from the local workstation. SSH tunnels to Cloud Pipeline runs can be used for interactive SSH sessions, files transferring and third-party applications which depends on SSH protocol. The command that runs ports tunnelling operations: pipe tunnel COMMAND [ARGS] Where COMMAND - one of the following commands: start <RUN_ID> - establishes tunnel connection to specified run instance port and serves it as a local port stop <RUN_ID - stops background tunnel processes with specified run For the start command there are two mandatory options: -lp / --local-port - specifies local port to establish connection from -rp / --remote-port - specifies remote port to establish connection to Example of the command that establishes tunnel connection to the run: pipe tunnel start 12345 -lp 4567 -rp 22 --ssh Here: 12345 is the Run ID , 4567 is just a random free local port and 22 is the Cloud Pipeline run SSH port . Additional --ssh flag enables passwordless SSH access. For more details and examples see here .","title":"SSH tunnel to the running compute instance"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#updates-of-metadata-object","text":"In the current version, several enhancements were implemented for the Metadata objects displaying and working with:","title":"Updates of Metadata object"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#controls-placement-reorganization","text":"Several controls (for adding a new instance, upload and delete metadata, transfer to the cloud and showing attributes) were moved to Additional parameters control (gear icon): See details here . Bulk operation panel is hidden/disabled until at least one instance is selected in a table, e.g.: To manage selected items, click the V button next to the \" Show only selected items \" control to open the corresponding menu: See details here .","title":"Controls placement reorganization"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#ability-to-show-only-selected-instances","text":"The ability is implemented to show separately only selected metadata instances. All unselected items will be hidden. For that: select items of interest (they can be at different pages too) and click the \" Show only selected items \" button at the Bulk operation panel, e.g.: For shown selected items, all functionality as for the general table is available except filtering.","title":"Ability to show only selected instances"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#improvements-in-the-search-over-the-metadata","text":"users can search over any attribute values (not only over ID as previously) the metadata search field supports multiple terms search - in this case, multiple terms for the search should be specified space separated, e.g. sample1 sample2 the metadata search field supports a key:value search, where key is an attribute name (column header) and value is a term that shall be searched in that attribute values, e.g. ID:D703 See details here .","title":"Improvements in the search over the metadata"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#ability-to-filter-instances","text":"The ability is implemented to filter instances of an entity in a table. Now, user can click a special control in a header of the desired column and set one or several filters for the column values - to restrict the output table, e.g.: See details here .","title":"Ability to filter instances"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#displaying-of-the-creation-date-info","text":"For all Metadata entities the \" Created date \" fields are displayed. This column appears and filled in automatically when the Metadata is uploaded or created manually, e.g.:","title":"Displaying of the creation date info"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#sorting-by-several-columns","text":"Ability to sort a list of entities by several columns is implemented. For that, a list is being sorted by one column, then user should click the second column he(she) wants to sort by, then the third, etc.: See details here .","title":"Sorting by several columns"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#autofill","text":"An autofill feature for metadata cells was implemented. It allows to fill metadata instances with data that are based on data in other instances in the same column/row, e.g.: click the right-bottom corner of the cell you wish to copy and move the mouse holding the left button - vertically or horizontally, e.g.: once you will release the mouse button - selected cells will be autofilled by the value of the cell that you've dragged: See details here .","title":"Autofill"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#entity-id-autogeneration","text":"In some cases, it could be convenient not to specify entity ID during import. Therefore Metadata entities support IDs autogeneration (in the UUID4 format). This works and for the import Metadata operation (for empty ID fields), and for the manual instance creation, e.g.: See after the creation: Note : IDs still should be unique","title":"Entity ID autogeneration"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#ability-to-add-sampleset-item-via-gui","text":"Now, users may create SampleSets or other \"Container-like\" entities from the GUI (previously it was possible via the CSV import only). This feature could be useful, if the Samples were imported using the IDs autogeneration, as it could be complicated to grab those IDs and copy to the CSV . To create a new SampleSet: Click the + Add instance button in the Metadata section and choose the SampleSet instance type: Provide the information for the new SampleSet and click the Browse button to select a list of Samples, which will be associated with the creating SampleSet: After creation, the new SampleSet will appear in the corresponding metadata class: See details here .","title":"Ability to add SampleSet item via GUI"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#preselect-instances-for-a-rerun","text":"Additionally, if metadata instances were used for the run via the expansion expressions in the parameters - then for the rerun of such run, the ability to choose was implemented - to use the same resolved expressions values from the initial run or preselect another metadata instance(s) for a coming rerun, e.g.: imagine, that some run was launched from the detached configuration. Moreover, one configuration parameter uses the expansion expression: for the run, some instance was selected after the run is completed, user tries to rerun this run. The Launch form will appear. By default, parameters are substituted fully the same as they were in the initial run: If click the Launch button in this case - during the rerun, all parameter(s) will use their resolved values from the initial run (previous behavior). but now, the ability to preselect another metadata instance for the re-launch is implemented. For that, user can click \" v \" button near the launch button and in the appeared list click \" Select metadata entries and launch \" item: In this case, the pop-up will appear to select a metadata instance for which the rerun will be launched. And during the rerun, all parameter(s) that use expansion expression(s) will be resolved according to a new selected metadata instance(s): See example here .","title":"Preselect instances for a rerun"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#custom-node-images","text":"Previously, Cloud Pipeline allowed to run instances only using some default predefined node images. For example, some node image was used for all CPU instance types, another - for GPU ones. Nevertheless there are cases than some of the tools or pipelines require special node images. For example, some tool may require specific nvidia driver version which default GPU node image doesn't have. In the current version, the ability to use a custom node image was implemented. If a custom node image is specified for a pipeline, tool or just a single launch then cloud instance with the required node image will be used for their runs. In a pipeline config, a custom node is specified in format: \"instance_image\": \"<custom_node_image>\" . For runs launched via API , a custom node is specified in format: \"instanceImage\": \"<custom_node_image>\" . In both cases, <custom_node_image> is the name of the custom image. For example, to use a custom node for a pipeline: open pipeline's CODE tab open config.json file in the configuration, specify a custom node image: save changes when launching such pipeline, you can observe that specified image is used for a node: See an example for a pipeline in details here .","title":"Custom node images"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#launch-a-tool-with-hosted-applications","text":"\"Long-running\" Cloud Pipeline applications may occasionally failed. And one of the main task caused this situation - saving the internal access to the services (e.g. if a database was hosted) as the IP and name (which match the pod) are being changed during the default run restarts. To resolve that, a special option to assign an internal DNS name to the run was implemented. Name of the service and a list of ports can be supplied by the user in the GUI, at the Launch form before the run: Configured DNS service is shown at the Launch form: And FQDN of all configured services are shown during the run - at the Run logs page: Checking that the run is launched with a \"hosted\" application: For more details see here .","title":"Launch a tool with \"hosted\" applications"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#advanced-global-search-with-faceted-filters","text":"In v0.14 , the Global search over the platform was introduced . Now, in v0.17 , the new version of the search was implemented - Advanced search . Advanced search repeats the functionality of the Simple search but has some advanced capabilities. To open the Advanced search click the Search icon in the main menu: Please note, the previous form of the global search is still available - by pressing \"Ctrl+F\". Currently, Advanced search is available for admins only To start searching, a query string shall be entered (search can be triggered by pressing the \"Enter\" button or by the correspoding Search button near the search bar), e.g.: As in the previous form, you can: restrict search results selecting desired object types from all results scope (the corresponding panel with the object types selector is placed under the search bar): open the \"Preview\" pane for the certain result hovering mouse point over it and click the Info icon: scroll search results to view more or use the paging control New features: \" Faceted filters \" panel at the left side of the search form. It allows to search objects by their attributes (tags). Operating principle is similar to the E-Commerce sites. Tags' keys and values displayed in this panel are loaded from separate System Dictionaries marked as filter sources. User can restrict (filter) search results - by checking/unchecking desired filters. In the search results, only objects were associated with the checked filter value (dictionary entry) will remain, e.g.: I.e. only objects tagged by this dictionary entry remained in the search results. You can select several filters values from different facets. Each time, other filters will be updated, and also displayed search results will be changed according to the selected filters. You can hover over any displayed search result and click the Info icon to check that the object is really tagged by selected filters (attributes), e.g.: More details - how to add dictionaries to the \"Faceted filters\" panel, how to configure filters and use them for the search - see here . Changeable view of the search results output: Results output in the Simple search has the view as simple list of the object names only. In the Advanced search , that output contains the additional info - according to the entity type of the certain result - it can be OWNER , DESCRIPTION , DATE_CHANGED , PATH , etc. Also user can switch between output view formats - list and table - by special control : list view ( default ) - each result is presented by the \"row\" in the list, additional info is placed in line, e.g.: table view - all results are presented by the single table, additional info is placed in columns, e.g.: Also the table view can be customized by the admin user. To the existing columns, user can add ones for the object attributes (tags) values, where the attribute key becomes the column header. If the object in results has the value for that attribute - it will be displayed in the corresponding column. Customizing of additional attribute columns is being performed by the new system preference - search.elastic.index.metadata.fields : For more details about the view of the results output see here . For more details about Advanced search see here .","title":"Advanced global search with faceted filters"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#explicitly-immutable-pipeline-parameters","text":"Previously, if the pipeline parameter had a default value - it could not be changed in the detached configuration that used this pipeline. In different cases, it might be convenient to provide the ability to specify the own parameter value before the configuration launch or vice versa - the ability to launch the configuration with only defaults parameter values. In the current version, the special option was implemented that allows/denies the parameter value overriding for described cases - no_override ( boolean ). This option can be specified for the pipeline parameter via config.json file: if a pipeline parameter has a default value and no_override is true - the parameter field will be read-only in the detached configuration that uses this pipeline: if a pipeline parameter has a default value and no_override is false or not set - the parameter field will be writable in the detached configuration that uses this pipeline: if a pipeline parameter has no default value - no_override is ignored and the parameter field will be writable in the detached configuration that uses this pipeline","title":"Explicitly \"immutable\" pipeline parameters"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#disable-hyper-threading","text":"Hyper-Threading technology makes a single physical processor appear as multiple logical processors. To do this, there is one copy of the architecture state for each logical processor, and the logical processors share a single set of physical execution resources. Hyper-Threading technology is enabled by default for all nodes in Cloud Pipeline . But not in all cases this technology is useful. In cases when threads are operating primarily on very close or relatively close instructions or data, the overall throughput occasionally decreases compared to non-interleaved, serial execution of the lines. For example, at a high performance computing that relies heavily on floating point calculations, the two threads in each core share a single floating point unit (FPU) and are often blocked by one another. In such case Hyper-Threading technology only slows computations. In the current version, the ability to disable Hyper-Threading for a specific job was implemented. So, this technology can be turned on or off, as is best for a particular application at the user's discretion. In Cloud Provider environment, each vCPU is a thread of a physical processor core. All cores of the instance has two threads. Disabling of Hyper-Threading disables the set of vCPUs that are relied to the second thread, set of first thread vCPUs stays enabled (see details for AWS here ). To disable Hyper-Threading technology for a job: set the corresponding option in \" Run capabilities \" before the run: check that Hyper-Threading was disabled via the following command after the run is launched: Here you can check that only 1 thread per core is set, virtual CPUs 4-7 are offline. Only one thread is enabled (set of CPUs 0-3). For more details see here .","title":"Disable Hyper-Threading"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#saving-of-interim-data-for-jobs-stopped-by-a-timeout","text":"Previously, if for a job a timeout was set and it has elapsed - the job was stopped and all the data was erased. In the current version, the solution to extract the current data from the timed-out jobs was implemented. Now, if a job has timed out - it will not be stopped immediately. Instead, the new OutputData task will be triggered. During this task performing, all the contents of the $ANALYSIS_DIR directory will be copied to all output storages - in the same manner, as if the job has succeeded. This feature doesn't require additional actions from the user side. Only $ANALYSIS_DIR and output paths should be defined. Additionally, a new system parameter was added - CP_EXEC_TIMEOUT . This parameter allows to define a timeout period after which the job shall be stopped. The essence of the parameter is the same as the configured value in the \" Timeout \" field. If both values are specified - for a job, CP_EXEC_TIMEOUT value will be used:","title":"Saving of interim data for jobs stopped by a timeout"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#resolve-variables-for-a-rerun","text":"Previously, if the user launched some job containing environment variables in its parameters and then, after the job was completed, user tried to rerun that job - all environment variables from the job parameters had been resolving again during the new run. But for some cases, it might be needed to use in the rerun all the same values of environment variables that were in the initial run. In the current version, the ability to choose was implemented - for the rerun, to resolve such variables in a new run or use their initial values. For that, when user tries to rerun some completed run that used environment variables in its parameters - at the Launch form, the checkbox \" Use resolved values \" appears in the Parameters section, e.g.: By default, this checkbox is disabled. In this case, all environment variables are shown as is and will be resolved only during the new (re-launched) run - with the values corresponding to this new run. If this checkbox is ticked, all environment variables will be resolved with the values of the initial run. Correspondingly, parameters that use environment variables will not be changed during the new launch, e.g.: See example here .","title":"Resolve variables for a rerun"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#nat-gateway","text":"Previously, if the Cloud Pipeline Platform was being deployed in some private subnet, it could be quite difficult for the admin to expose a network endpoint for some service to use in a Platform. This required manual execution of a number of tasks on the Platform Core instance and, accordingly, might lead to errors. To resolve this, in the current version, the convenient way to manage network routes (creating/removing) from the GUI was implemented. For that, a new NAT gateway subtab was added to the System Management section of the System Settings . The NAT gateway subtab allows to configure network routes: To add a route, admin shall: click the ADD ROUTE button: in the appeared pop-up, specify details of an external resource: server name, IP ( if needs ), port(s) and comment to route ( optionally ), e.g.: just-added external server will appear in the list. Admin should click the SAVE button to confirm made changes: once the route creation will be done, the route details will appear in the INTERNAL CONFIG fields and near the route, the status will be shown as ACTIVE : For more details see here .","title":"NAT gateway"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#custom-run-capabilities","text":"Previously, users might select only predefined \"system\" Run capabilities for a job. In some cases or deployments, these capabilities may not be enough. In the current version, the ability for admins to add custom Run capabilities was implemented. Use them for a job/tool run all users can. Managing of the custom capabilities is being performed via the new system preference launch.capabilities . This preference contains an array of capability descriptions in JSON -format and has the following structure: { \"<capability_name_1>\": { \"description\": \"<Description of the capability>\", \"commands\": [ \"<command_1>\", \"<command_2>\", ... ], \"params\": { \"<parameter_1>\": \"<value_1>\", \"<parameter_2>\": \"<value_2>\", ... } }, \"<capability_name_2\": { ... }, ... } For example: Saved capability then can be used for a job/tool: For more details see here .","title":"Custom Run capabilities"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#storage-lifecycle-management","text":"Previously, users had the simplified opportunity to configure the lifecycle of data in storages - via specifying STS/LTS durations in the storage settings. This way is rather primitive and does not allow to fine-tune data archiving/restoring. In the current version, the ability to configure datastorage lifecycle in details was implemented. This new option allows to perform the automatical data transition from standard storage to different types of archival storages by occurance of a certain event and restore that data back as well if needed. Previous functionality (STS/LTS durations) was excluded. For the new one, an additional tab was included to the storage settings - Transition rules : New implemented functionality includes abilities: automatic data archiving/removing according to specified transition rule(s) restoring of previously archived data for the specified period Data archiving is provided by configurable set of transition rules for each separate storage. Each rule defines which files, when (specific date or by the event) and where (different types of archive) shall be automatically transferred: firstly, user creates a rule for a storage - specifying the path and glob pattern for a file(s) name(s) which shall be transferred, e.g.: User can select to transfer files one-by-one or in bulk-mode by the first/last appeared file in a group. Also, an additional condition for the files transition can be configured. then user selects an archive class as the data destination. Here, several destinations can be added (for different dates), e.g.: Note : archive classes depend on the Cloud Provider then user defines the event by which the data shall be transferred - after certain period after the file(s) creation or at the specific date: also, notifications can be configured for the rule events (optionally): recipients list notification title and text ability to specify a delay for data transition - user that receive such notification will have the ability to prolong (delay) transition for some period created rule can be found in the Transition rules tab of the storage settings: after the rule is created, it starts to work. If file matches the condition of any storage rule - it will be transferred to some archive or removed (if Deletion is set as data destination). Transferred file becomes disabled for changing/renaming from the GUI/CLI. At the GUI, near such file a label appears that corresponds to the transition destination, e.g.: For more details see here . Data restoring can be applied to previously archived files. Separate files or whole folders (with sub-folders) can be restored: user selects which files/folders shall be restored, e.g.: user defines the period for which files shall be restored and notification recipients list: after the confirmation, the restore process begins - it is shown by the special status icon: when file is restored - it is shown by the special status icon as well: once the restore period is over, files will be automatically transferred to the archive where they were before For more details see here .","title":"Storage lifecycle management"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#image-history","text":"Cloud Pipeline performs scanning of the Docker images on a regular basis. This is used to grab the information on: Available software packages Possible vulnerabilities of those packages The users may leverage this feature to choose which docker image to use, depending on the needs for a specific application. But this list of the software packages may not show the full list of the applications as the scanning mechanism uses only the \"well-known\" filesystem locations to collect the applications/versions informations. Some of the apps, might be installed into any custom location and the scanner won't be able to find it. To fulfill this gap and to address some advanced cases, in the current version, a new feature was introduced: now it's possible to view the list of the \"Docker layers\" and corresponding commands, which were used to generate those layers. It can be viewed via the specific tab in the tool version menu - Image history : This allows to get information on the exact commands and settings, which were used to create an image and even reproduce it from scratch. For more details see here .","title":"Image history"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#environments-synchronization-via-pipectl","text":"In some cases, admins need to synchronize two different environments of the Cloud Pipeline. New special routine in the pipectl utility is implemented for that - pipectl sync . It allows to synchronize from the source environment to the destination one the following objects: users / user groups / user roles docker registry / tool groups / tools Synchronization can be performed with or without synchronization of attributes (metadata) for the specified Platform objects. During the synchronization, changes are being performed only in the destination environment, the source environment remains the same. For details and examples see here .","title":"Environments synchronization via pipectl"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#data-access-audit","text":"In the current version, System logs were expanded - now, all actions related to any access to the data stored in the object storages are being logged. This includes logging of operations READ / WRITE / DELETE , listing operation is not logged. For logs of data access events, a new item was added to the \" Type \" filter of the System logs - audit type: By this type, the following data access operations are being logged: access to the Object storages data from the Platform GUI access to the Object storages data from the pipe CLI access to the mounted Object storages' data - both from GUI and CLI Examples of logs: For more details see here .","title":"Data access audit"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#system-jobs","text":"Sometimes it's desirable for admin to get some statistics or system information about current Cloud Pipeline deployment state - for example, collect information about all storages that have specific size or list all unattached EBS volumes, or set some specific policy to all storages, etc. For such purposes, specific scripts can be written and launched in some way. Number of these \"admin\" scripts can grow very quickly and it would be convenient to have some solution to create and run such scripts in a system, and also view results (logs) and store them. In the current version, a new solution was implemeted for \"admin\" scripts - System Jobs . System jobs solution uses the existing Cloud-Pipeline infrastructure, to reduce number of preparation steps to be done to get desire output. In a nutshell, the System Jobs solution includes the following: special prepared system pipeline, that contains system jobs scripts. Admin can add new scripts or edit/delete existing ones. Also, pipeline config contains: Kubernetes service account to perform kubectl commands from such pipeline during the system job run special assign policy that allows to assign the pipeline to one of the running system node ( MASTER node, for example). It is convenient as no additional instances (waiting or initializing ones) are required to perform a job special prepared docker image that includes pre-installed packages such as system packages ( curl , nano , git , etc.), kubectl , pipe CLI, Cloud CLI ( AWS / Azure / GCP ), LustreFS client separate System Jobs form that displays all available system job scripts, allows to run existing scripts and view results of their performing Userjourney looks like: Admin creates a new system job script and places it to the specific path inside the special system pipeline ( pipeline and path are defined by System Preferences ), e.g.: Admin opens the System Jobs panel from the System Settings : Here, there is the whole list of stored system scripts and results of their runs. To run a script - admin selects any script and launch it: When admin launches a system job - the system instance ( MASTER instance, by default) is used for the job performing. At that system instance, the docker-container is launched from the special prepared docker-image for system jobs. In the launched docker-container, the system job script is being performed. At the System jobs form, states of the performing job are shown similar to the pipeline states, e.g.: Once the script is performed, the state will be changed to Success : By the button LOG , the script performing output can be viewed: For more details see here .","title":"System Jobs"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#cluster-run-usage","text":"Previously, user can view the state of the cluster run (master and its nested runs) via the Run logs page of the cluster master node. But this information was actual only at the specific time moment. It would be convenient to view how the cluster usage has been changing over the whole cluster run duration. This is especially useful information for auto-scaled clusters, as the number of worker nodes in such clusters can vary greatly over time. In v0.17 , such ability was added. User can view a specific cluster's usage over time - by click the corresponding hyperlink at the Run logs page of the cluster's master node: The chart pop-up will be opened, e.g.: The chart shows a cluster usage - number of all active instances (including the master node) of the current cluster over time. For more details see here .","title":"Cluster run usage"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#cluster-run-estimation-price","text":"Previously, Cloud Pipeline allowed to view a price estimation for the single instance jobs. But the clusters did not provide such information (summary). Users could see a price only for a master node. Now, Cloud Pipeline offers a cost estimation, when any compute instances are running: Standalone instance - reports it's own cost: Dashboard: Run's list: Static cluster - reports the full cluster cost (summary for a master node and all workers), since it is started: Dashboard: Run's list - master node's cost is reported in the brackets as well: Autoscaled cluster - reports the costs, based on the workers lifetime (summary for a master node and all workers). As the workers may be created and terminated all the time - there costs are computed only for the RUNNING state: Dashboard: Run's list - master node's cost is reported in the brackets as well:","title":"Cluster run estimation price"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#terminal-view","text":"From the current version, users have the ability to configure the view of the SSH terminal session: Dark (default) Light Required color schema can be configured in two ways: Persistent - schema is being stored in the user profile and used any time SSH session is opened: Temporary - schema is being used during a current SSH session only - toggling Dark <-> Light can be performed via the special control in the terminal frame: For details see here .","title":"Terminal view"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#aws-seamless-authentication","text":"In some cases, users are faced with the following scenarios: Some jobs are running in the Cloud Pipeline and accessing data/services located in the external accounts (e.g. Amazon S3 , Amazon DynamoDB ). This requires the user to specify the authentication keys explicitly (either in the shell session or in the R / Python scripts). This is not user-friendly and not secure, if the users include the credentials into the scripts. There are also users who would like to leverage R / Python libraries, that have embedded Amazon S3 support. Users have to download data locally first (via pipe ) and then perform the processing. In the current version, a new mechanism of the seamless AWS authentication was implemented. It allows users to execute any request to the AWS API, from inside the Cloud Pipeline environment, without an authentication request. The following mechanism automates the Cloud Provider authentication for the user\u2019s scripts: Administrator is able to configure the user\u2019s access permissions in the Cloud Pipeline account of the Cloud Provider or provide credentials for the external Cloud Provider account All the requests to the Cloud Provider authentication are handled by the certain Cloud Pipeline service, which authenticates the user with the configured credentials Users are able to use the Cloud Provider API without the authentication request Administrator can create specific interfaces - Cloud Credentials Profiles , that contain the following fields: Provider - to specify the Cloud Provider Name - to specify the profile name Assumed Role - to specify the role received from the Cloud Provider that will be used for the authentication to the Cloud Provider API Policy - to specify the Cloud Provider policy of the objects access It could be configured in Cloud Provider settings, e.g.: Administrator can assign profiles to User/Role/Group entity. For each entity many profiles can be assigned. Also, from the profiles assigned to the certain User/Role/Group the one can be selected as default . If the default profile isn't selected - during the authentication operation there shall be selected the profile to use. It could be configured via the User management panel, e.g.: Usage of the assigned profiles is being configured via the new Cloud Region option - \" Mount Credentials Rule \" with the following allowed values: NONE - for runs in this region, credentials will not be configured SAME CLOUD - for runs in this region, the set user credentials will be configured only allowed for the same Cloud Provider ALL - for runs in this region, the all user credentials will be configured As example, if for the user such AWS credential profile is assigned and the mount rule is allowed - he/she can use AWS CLI directly to the bucket (defined and allowed by profile policy) without extra-authentication: For details and example see here .","title":"AWS: seamless authentication"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#aws-transfer-objects-between-aws-regions-using-pipe-storage-cpmv-commands","text":"Previously, pipe storage cp / pipe storage mv commands allowed to transfer objects only within one AWS region. In the current version, the ability to transfer objects between storages from different AWS regions is implemented. The commands themselves remain the same. Example:","title":"AWS: transfer objects between AWS regions using pipe storage cp/mv commands"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#aws-switching-of-cloud-regions-for-launched-jobs-in-case-of-insufficient-capacity","text":"Previously, if user started an AWS job and there were not enough instances of specified type to launch that job in a region - it would fail. In the current version, the ability to automatically relaunch such runs in other AWS region(s) was implemented. For that functionality, a new setting was added to the Cloud Region configuration - \" Run shift policy \": If this setting is enabled for some AWS region 1 and for some AWS region 2 - then a job launched in the AWS region 1 will automatically try to be relaunched in the AWS region 2 in case when there are not enough instances of selected type in the AWS region 1 ( InsufficientInstanceCapacity error): Original job is being automatically stopped, new job with the same instance type as in the original run but in the AWS region 2 will be launched. If a new instance is not available with a new region - relaunch will be performed in one more region as long as there are AWS regions in the Platform with the enabled option \" Run shift policy \". Feature is not available: for spot runs for runs that have any Cloud dependent parameter for worker or cluster runs More details see here .","title":"AWS: switching of Cloud Regions for launched jobs in case of insufficient capacity"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#notable-bug-fixes","text":"","title":"Notable Bug fixes"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#unable-to-view-pipeline-sources-for-previous-draft-versions","text":"#1353 Previously, Pipeline's \" Documents \" and \" Code \" tabs always showed content of the last \" draft \" version of pipeline, even if one of the previous versions was forcibly specified in the url.","title":"Unable to view pipeline sources for previous draft versions"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#pipe-storage-ls-works-incorrectly-with-the-option-page","text":"#1339 Previously, the pipe CLI storage listing worked incorrectly with --page ( -p ) option with S3 provider. All items were displayed without pagination.","title":"pipe storage ls works incorrectly with the option --page"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#aws-deployment-unable-to-list-more-than-1000-files-in-the-s3-bucket","text":"#1312 Previously, when s3 bucket contained more than 1000 files - user could list all the files in the bucket via the GUI, but only first 1000 files via any pipe CLI capabilities ( pipe storage ls , pipe storage mount , etc.).","title":"AWS deployment: unable to list more than 1000 files in the S3 bucket"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#size-of-tool-version-created-from-original-tool-without-any-changes-is-a-lot-larger-than-original-one","text":"#1270 Previously, the size of the tool version that had created from the original tool without any changes or after resume operation for paused run - by COMMIT operation - was a lot larger than original version.","title":"Size of tool version created from original tool without any changes is a lot larger than original one"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#pipe-storage-cp-fails-in-windows-for-the-gcs-with-sslv3-error","text":"#1268 Previously, the sslv3 issue happened when data to/from the GCS was copying using the Windows workstation.","title":"pipe storage cp fails in Windows for the GCS with sslv3 error"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#shared-endpoint-for-anonymous-users-is-being-opened-from-the-second-time","text":"#1265 Previously, when anonymous user tried to open a hyperlink with the shared endpoint - he/she got the Platform's \"Access denied\" page. But if user tried to open the page in the second time - it was being opened correctly.","title":"Shared endpoint for anonymous users is being opened from the second time"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#attempt-to-view-permissions-on-a-pipeline-via-the-pipe-view-pipes-throws-an-error","text":"#1216 Previously, when trying to view permissions of a pipeline via the pipe view-pipes -r command - the command execution failed.","title":"Attempt to view permissions on a pipeline via the pipe view-pipes throws an error"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#scale-down-cold-sge-autoscaling-cluster","text":"#1123 Previously, SGE autoscaling cluster didn't scale down until at least one running job appears in queue. Currently, SGE autoscaling cluster is being scaled down even if there weren't any running jobs yet.","title":"Scale down \"cold\" SGE autoscaling cluster"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#launch-command-functionality-issues","text":"#1086 , #1090 Previously, if a user specified the values of the parameters with \"spaces\" (e.g. selection of the input parameter value from the GUI bucket browser) - this broke the command format. Also, the Launch Command generation function used single-quotes to wrap the -cmd value. This was causing to fail when running the generate commands from the Windows environment. As the Windows CMD shell can't resolve it correctly (the command value is still split by the space).","title":"\"Launch Command\" functionality issues"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#inner-data-storages-navigation-bar-fails-to-navigate","text":"#1077 Previously, navigation bar for so-called \"inner\" data storages produced You cannot navigate to another storage in case of any interaction with it.","title":"Inner data storages navigation bar fails to navigate"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#region-is-being-set-incorrectly-when-trying-to-rerun-pipeline","text":"#1066 Previously, when tried to rerun any run - the default region was being set in the Cloud Region field. But the instance type wasn't being changed automatically and remained the same as was set before the run. This could lead to inconsistencies.","title":"Region is being set incorrectly when trying to rerun pipeline"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#pause-and-commit-operations-fail-for-the-jobs-with-an-autoscaled-disk","text":"#998 Previously, PAUSE and COMMIT operations failed with the NullPointerException error for the jobs with an autoscaled disk.","title":"PAUSE and COMMIT operations fail for the jobs with an autoscaled disk"},{"location":"release_notes/v.0.17/v.0.17_-_Release_notes/#broken-layouts","text":"#1504 , #1505 In Groups / Roles membership view, the vertical scrollbar was shown even if there was a plenty of space below the list. Currently, the list size is increased to the pop up size. At the Billing reports page, if the whole header menu didn't not fit the screen width - the \"discounts\" links overflew the regions selector. Currently, row breaks feature is implemeted for this page.","title":"Broken layouts"}]}