#!/usr/bin/env bash

# Copyright 2017-2020 EPAM Systems, Inc. (https://www.epam.com/)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

SLURM_COMMON_CONFIG_DIR="$SHARED_FOLDER/.slurm"
SLURM_WORKER_SETUP_TASK="SLURMWorkerSetup"
CURRENT_PID=$$

resolve_node_resources() {
    _TASK="$1"

    export _NODE_CPU_COUNT=$(nproc)
    pipe_log_info "$_NODE_CPU_COUNT CPUs found" "$_TASK"

    export _NODE_RAM_COUNT=$(grep MemTotal /proc/meminfo | awk '{print int($2 / 1024)}')
    pipe_log_info "${_NODE_RAM_COUNT}M RAM found" "$_TASK"

    export _NODE_GPUS_COUNT=$(nvidia-smi -L 2>/dev/null | wc -l)
    export _NODE_GPUS_COUNT="${_NODE_GPUS_COUNT:-0}"
    pipe_log_info "$_NODE_GPUS_COUNT GPUs found" "$_TASK"
}

configure_consumable_gpu_resource() {
    _RESOURCE_VALUE="$1"

    if (( _RESOURCE_VALUE > 0 )); then
        for device in $(ls /dev/ | grep -E "nvidia[0-9]+") ; do
            echo "Name=gpu File=/dev/$device" >> "$_SLURM_CONFIG_LOCATION/gres.conf"
        done
    fi
}

configure_slurm() {
    _SLURM_CONFIG_LOCATION=$( slurm_config_location )

    cp ${SLURM_COMMON_CONFIG_DIR}/munge.key /etc/munge/
    chown munge: /etc/munge/munge.key
    chmod 0700 /etc/munge/munge.key
    su -c /usr/sbin/munged -s /bin/bash munge

    ln -s  ${SLURM_COMMON_CONFIG_DIR}/slurm.conf "$_SLURM_CONFIG_LOCATION"
    ln -s  ${SLURM_COMMON_CONFIG_DIR}/cgroup.conf "$_SLURM_CONFIG_LOCATION"
    mkdir /var/spool/slurmd
    chown slurm: /var/spool/slurmd
    chmod 755 /var/spool/slurmd
    touch /var/log/slurmd.log
    chown slurm: /var/log/slurmd.log

    resolve_node_resources "$SLURM_WORKER_SETUP_TASK"
    configure_consumable_gpu_resource "$_NODE_GPUS_COUNT"
}

check_last_exit_code() {
   exit_code=$1
   msg_if_success=$2
   msg_if_fail=$3
   if [[ "$exit_code" -ne 0 ]]; then
        pipe_log_fail "$msg_if_fail" "${SLURM_WORKER_SETUP_TASK}"
        kill -s "$CURRENT_PID"
        exit 1
    else
        pipe_log_info "$msg_if_success" "${SLURM_WORKER_SETUP_TASK}"
    fi
}

slurm_config_location() {
   _SLURM_CONFIG_DIR=$(ls /etc/ | grep slurm | head -n 1)
   if [ "$_SLURM_CONFIG_DIR" != "" ]; then
      echo "/etc/$_SLURM_CONFIG_DIR"
   else
      pipe_log_fail "Failed to find SLURM config dir" "${SLURM_WORKER_SETUP_TASK}"
      kill -s "$CURRENT_PID"
      exit 1
   fi
}

install_slurm() {
    create_slurm_user

    if [ "$CP_OS_FAMILY" == "debian" ]; then
        if [ "$CP_OS" == "ubuntu" ] && [ "$CP_VER" == "16.04" ]; then
            pipe_log_fail "SLURM support for Ubuntu 16.04 has been dropped. Please use Ubuntu 18.04 instead." "$SLURM_WORKER_SETUP_TASK"
            return 1
        fi
        install_slurm_debian
    elif [ "$CP_OS_FAMILY" == "rhel" ]; then
        install_slurm_rhel
    else
        return 1
    fi
}

create_slurm_user() {
    export SLURMUSER=992
    groupadd -g $SLURMUSER slurm
    useradd -m -c "SLURM workload manager" -d /var/lib/slurm -u $SLURMUSER -g slurm  -s /bin/bash slurm
}

install_slurm_debian() {
    apt-get update -y
    apt-get install -y munge
#    todo: Check if there is need to install libmunge2 / libmunge-dev is needed

    mkdir -p /var/run/munge \
             /run/munge
    chown munge: /var/run/munge \
                 /run/munge
    chmod +t /run/munge

    wget -q "$CP_SLURM_PACKAGE_DEB_URL" -O /tmp/slurm.deb
    if ! apt-get install -y -f /tmp/slurm.deb; then
        return 1
    fi
    rm -f /tmp/slurm.deb

    mkdir -p /etc/slurm \
             /etc/slurm/prolog.d \
             /etc/slurm/epilog.d
    chown slurm: /etc/slurm \
                 /etc/slurm/prolog.d \
                 /etc/slurm/epilog.d

    ln -s /usr/local/slurm/bin/* /usr/bin/
    ln -s /usr/local/slurm/sbin/* /usr/sbin/

    return 0
}

install_slurm_rhel() {
    yum install epel-release -y
    yum install munge munge-libs munge-devel openssl openssl-devel \
                pam-devel numactl hwloc hwloc-devel lua lua-devel readline-devel  \
                rrdtool-devel ncurses-devel man2html libibmad libibumad rpm-build perl-devel perl-CPAN -y

    yum --nogpgcheck localinstall /common/slurm-pkgs/* -y
}

pipe_log_info "Installing SLURM worker" "$SLURM_WORKER_SETUP_TASK"

export CP_SLURM_SOURCE_URL="${CP_SLURM_SOURCE_URL:-"${GLOBAL_DISTRIBUTION_URL}tools/slurm/slurm-22.05.5.tar.bz2"}"
export CP_SLURM_PACKAGE_DEB_URL="${CP_SLURM_PACKAGE_DEB_URL:-"${GLOBAL_DISTRIBUTION_URL}tools/slurm/deb/slurm_22.05.5_amd64.deb"}"
export CP_SLURM_PACKAGE_RPM_URL="${CP_SLURM_PACKAGE_RPM_URL:-"${GLOBAL_DISTRIBUTION_URL}tools/slurm/rpm/slurm-22.05.5-1.el7.x86_64.tar"}"

MASTER_INFO_RESULT=$(eval "${CP_PYTHON2_PATH} ${COMMON_REPO_DIR}/scripts/cluster_wait_for_master.py --master-id ${parent_id} --task-name SLURMMasterSetup")
_MASTER_AWAIT_RESULT=$?
MASTER_INFO=($MASTER_INFO_RESULT)
MASTER_IP=${MASTER_INFO[-1]}
MASTER_NAME=${MASTER_INFO[-2]}
check_last_exit_code $_MASTER_AWAIT_RESULT "Master info received: $MASTER_NAME : $MASTER_IP" "Fail to install SLURM worker. Unable to get master information"

if check_cp_cap CP_CAP_SLURM_PREINSTALLED; then
    pipe_log_info "Skipping SLURM packages installation because packages are preinstalled" "$SLURM_WORKER_SETUP_TASK"
else
    pipe_log_info "Installing SLURM packages" "$SLURM_WORKER_SETUP_TASK"
    install_slurm
    check_last_exit_code $? "SLURM packages have been installed" "Can't install SLURM packages"
fi

pipe_log_info "Configuring SLURM cluster" "$SLURM_WORKER_SETUP_TASK"
configure_slurm
pipe_log_info "SLURM cluster has been configured" "$SLURM_WORKER_SETUP_TASK"

pipe_log_info "Adding worker node to SLURM cluster" "$SLURM_WORKER_SETUP_TASK"
if [ "${cluster_role_type}" == "additional" ]; then
    slurmd -Z --conf "CPUs=$_NODE_CPU_COUNT RealMemory=$_NODE_RAM_COUNT Gres=gpu:$_NODE_GPUS_COUNT State=FUTURE"
else
    slurmd
fi
check_last_exit_code $? "SLURM worker daemon has started" "Fail to start SLURM worker daemon"

pipe_log_success "SLURM worker node has been successfully configured" "$SLURM_WORKER_SETUP_TASK"
