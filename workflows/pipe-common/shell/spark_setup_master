#!/usr/bin/env bash

# Copyright 2017-2019 EPAM Systems, Inc. (https://www.epam.com/)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

function update_spark_config_prop {
    local prop_name="$1"
    local prop_value="$2"
    local spark_config_file="$3"

    if [ -z "$prop_name" ] || [ -z "$prop_name" ] || [ -z "$prop_name" ]; then
        return 1
    fi

    local prop_value_existing=
    if grep -q "$prop_name " "$spark_config_file"  2>/dev/null; then
        prop_value_existing="$(grep "$prop_name" "$spark_config_file" | cut -d\  -f2-)"
        sed -i "/$prop_name /d" "$spark_config_file"
    fi
    echo "$prop_name $prop_value_existing $prop_value" >> "$spark_config_file"
}

function get_proxy_value {
    local proxy_var_name="$1"

    # Try to get the proxy value in the lower case (e.g. http_proxy)
    proxy_var_name="$(echo $proxy_var_name | tr '[:upper:]' '[:lower:]')"
    local proxy_var_value="${!proxy_var_name}"
    [ "$proxy_var_value" ] && echo "$proxy_var_value" && return

    # If a lower case variable has no value - try an upper case (e.g. HTTP_PROXY)
    proxy_var_name="$(echo $proxy_var_name | tr '[:lower:]' '[:upper:]')"
    echo "${!proxy_var_name}"
}

function get_proxy_for_java {
    local proxy_type="$1"
    
    local proxy_var_name="${proxy_type}_proxy"
    local proxy_value="$(get_proxy_value $proxy_var_name)"
    [ -z "$proxy_value" ] && return

    local proxy_host_name="$(parse_url $proxy_value hostname)"
    local proxy_port="$(parse_url "$proxy_value" port)"
    ([ -z "$proxy_host_name" ] || [ -z "$proxy_port" ]) && return

    echo "-D$proxy_type.proxyHost=$proxy_host_name -D$proxy_type.proxyPort=$proxy_port"
}

function get_spark_workers_count {
    local api_endpoint="$1"

    local workers_to_skip=1
    [ "$CP_CAP_SPARK_MASTER_NO_SCHEDULE" == "true" ] && workers_to_skip=0

    local workers_count=$(( $(curl -s $api_endpoint | jq -r '.aliveworkers') - $workers_to_skip ))
    (( "$workers_count" < 0 )) && workers_count=0
    echo "$workers_count"
}

function setup_executors {
    local spark_config_file="$1"

    nodes_total=$(( ${node_count:-0} + 1 ))

    mem_per_node=$(grep MemTotal /proc/meminfo | awk '{print $2}')
    mem_per_node=$(( $mem_per_node / 1024 / 1024 ))
    mem_total=$(( $mem_per_node * $nodes_total ))

    cpu_per_node=$(nproc)
    cpu_total=$(( $cpu_per_node * $nodes_total ))

    executor_cores=${CP_CAP_SPARK_EXECUTOR_CORES:-2}
    executors_num=$(( $cpu_total / $executor_cores ))
    executor_mem=$(( ($mem_total / $executors_num) - 1 ))

    if grep -q "spark.executor.cores" "$spark_config_file"  2>/dev/null; then
        pipe_log_warn "Spark config file at $spark_config_file already contains executors configuration, it WILL NOT be updated" "$SPARK_MASTER_SETUP_TASK"
    else
        cat >> "$spark_config_file" << EOF
spark.executor.cores        $executor_cores
spark.executor.memory       ${executor_mem}G
EOF
    fi
}

# FIXME: this sets up access only to AWS/S3 and only for "current" region (which is used to run a compute node). GCP/Azure shall be introduced later as well
function setup_object_storage_access {
    local spark_config_file="$1"

    pipe_log_info "Starting Object Storage access configuration" "$SPARK_MASTER_SETUP_TASK"

    if [ -z "$CLOUD_REGION_ID" ]; then
        pipe_log_fail "Current region ID is not set via CLOUD_REGION_ID, object storages access WILL NOT be configured" "$SPARK_MASTER_SETUP_TASK"
        return 1
    fi

    local region_var_name="CP_ACCOUNT_REGION_${CLOUD_REGION_ID}"
    export AWS_DEFAULT_REGION="${!region_var_name}"

    # Auth settings
    CP_CAP_SPARK_MASTER_S3A_AUTH="${CP_CAP_SPARK_MASTER_S3A_AUTH:-cp}"
    pipe_log_info "Object Storage '$CP_CAP_SPARK_MASTER_S3A_AUTH' auth shema will be used" "$SPARK_MASTER_SETUP_TASK"

    if [[ "$CP_CAP_SPARK_MASTER_S3A_AUTH" == "keys" ]]; then
        local key_id_var_name="CP_ACCOUNT_ID_${CLOUD_REGION_ID}"
        local access_key_var_name="CP_ACCOUNT_KEY_${CLOUD_REGION_ID}"
        export AWS_ACCESS_KEY_ID="${!key_id_var_name}"
        export AWS_SECRET_ACCESS_KEY="${!access_key_var_name}"

        if [ -z "$AWS_ACCESS_KEY_ID" ] || [ -z "$AWS_SECRET_ACCESS_KEY" ] || [ -z "$AWS_DEFAULT_REGION" ]; then
            unset AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_DEFAULT_REGION
            pipe_log_fail "Credentials for the Cloud Region $CLOUD_REGION_ID cannot be found, object storages access WILL NOT be configured" "$SPARK_MASTER_SETUP_TASK"
            return 1
        fi

        spark_env_file="$(dirname spark_config_file)/spark-env.sh"
        if grep -q "AWS_" "$spark_env_file" 2>/dev/null; then
            pipe_log_warn "Spark environment file at $spark_env_file already contains S3A configuration, it WILL NOT be updated" "$SPARK_MASTER_SETUP_TASK"
        else
            cat >> "$spark_env_file" << EOF
AWS_ACCESS_KEY_ID="$AWS_ACCESS_KEY_ID"
AWS_SECRET_ACCESS_KEY="$AWS_SECRET_ACCESS_KEY"
EOF
        fi
    elif [[ "$CP_CAP_SPARK_MASTER_S3A_AUTH" == "cp" ]]; then
        CP_CAP_SPARK_MASTER_S3A_ADDITIONAL_ARGS=$(printf "$CP_CAP_SPARK_MASTER_S3A_ADDITIONAL_ARGS\nspark.hadoop.fs.s3a.aws.credentials.provider com.epam.pipeline.hadoop.fs.s3a.CPCredentialsProvider")
    fi

    # General settings
    if grep -q "spark.hadoop.fs.s3a" "$spark_config_file"  2>/dev/null; then
        pipe_log_warn "Spark config file at $spark_config_file already contains S3A configuration, it WILL NOT be updated" "$SPARK_MASTER_SETUP_TASK"
    else
        cat >> "$spark_config_file" << EOF
# S3A config
spark.hadoop.fs.s3a.impl                                org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.endpoint                            s3.${AWS_DEFAULT_REGION}.amazonaws.com
spark.hadoop.fs.s3a.server-side-encryption-algorithm    AES256
spark.executor.extraJavaOptions                         -Dcom.amazonaws.services.s3.enableV4=true
${CP_CAP_SPARK_MASTER_S3A_ADDITIONAL_ARGS}

# Parquet I/O Settings (https://spark.apache.org/docs/2.4.3/cloud-integration.html#parquet-io-settings)
spark.hadoop.parquet.enable.summary-metadata            false
spark.sql.parquet.mergeSchema                           false
spark.sql.parquet.filterPushdown                        true
spark.sql.hive.metastorePartitionPruning                true

# ORC I/O Settings (https://spark.apache.org/docs/2.4.3/cloud-integration.html#orc-io-settings)
spark.sql.orc.filterPushdown                            true
spark.sql.orc.splits.include.file.footer                true
spark.sql.orc.cache.stripe.details.size                 10000
spark.sql.hive.metastorePartitionPruning                true
EOF
        update_spark_config_prop "spark.driver.extraJavaOptions" "-Dcom.amazonaws.services.s3.enableV4=true" "$spark_config_file"
    fi

    pipe_log_info "Object Storage access configuration is done" "$SPARK_MASTER_SETUP_TASK"
    return 0
}

function setup_proxy_config {
    local spark_config_file="$1"

    java_proxy_options="$(get_proxy_for_java http)"
    java_proxy_options="$java_proxy_options $(get_proxy_for_java https)"
    java_proxy_options="$java_proxy_options $(get_proxy_for_java ftp)"

    # Add all the spark nodes' names/ips to the no_proxy to allow internal http communication
    local spark_cluster_no_proxy=""
    if [ -f "$DEFAULT_HOSTFILE" ]; then
        while read cluster_node; do
            spark_cluster_no_proxy="${spark_cluster_no_proxy},${cluster_node}"

            cluster_node_ip="$(cat /etc/hosts | grep "$cluster_node$" | cut -f1)"
            [ -z "$cluster_node_ip" ] && continue
            spark_cluster_no_proxy="${spark_cluster_no_proxy},${cluster_node_ip}"
        done <"$DEFAULT_HOSTFILE"
    fi

    # Translate no_proxy shell to java notation:
    # , -> | (delimiter)
    # . -> *. (wildcard domain name)
    # e.g.: "127.0.0.1,localhost,.local" -> "127.0.0.1|localhost|*.local"
    java_no_proxy="$(get_proxy_value no_proxy)"
    if [ "$java_no_proxy" ]; then
        java_no_proxy="${java_no_proxy}${spark_cluster_no_proxy}"
        export no_proxy="$java_no_proxy"
        java_no_proxy="$(echo "$java_no_proxy" | sed 's/,/|/g' | sed 's/|\./|*./g')"
        java_proxy_options="$java_proxy_options -Dhttp.nonProxyHosts=$java_no_proxy -Dftp.nonProxyHosts=$java_no_proxy"
    fi

    # Write the proxy options only if the java_proxy_options is not empty or does not consist of empty spaces
    if [ "${java_proxy_options// }" ]; then
        update_spark_config_prop "spark.driver.extraJavaOptions" "$java_proxy_options" "$spark_config_file"
    fi
}

function setup_common_spark_conf {
    local spark_config_file="$1"

    CP_CAP_SPARK_DERBY_SYSTEM_HOME="${CP_CAP_SPARK_DERBY_SYSTEM_HOME:-$CP_CAP_SPARK_INSTALL_DIR/derby}"
    mkdir -p "$CP_CAP_SPARK_DERBY_SYSTEM_HOME"

    CP_CAP_SPARK_WAREHOUSE_DIR="${CP_CAP_SPARK_WAREHOUSE_DIR:-$CP_CAP_SPARK_INSTALL_DIR/spark-warehouse}"
    mkdir -p "$CP_CAP_SPARK_WAREHOUSE_DIR"

    # Setup additional driver options
    local extra_java_opts="-Dderby.system.home=$CP_CAP_SPARK_DERBY_SYSTEM_HOME"
    update_spark_config_prop "spark.driver.extraJavaOptions" "$extra_java_opts" "$spark_config_file"

    # Setup spark warehouse dir
    update_spark_config_prop "spark.sql.warehouse.dir" "${CP_CAP_SPARK_WAREHOUSE_DIR}" "$spark_config_file"
}

SPARK_MASTER_SETUP_TASK="SparkMasterSetup"
SPARK_MASTER_SETUP_TASK_WORKERS="SparkMasterSetupWorkers"

export SHARED_FOLDER=${SHARED_FOLDER:-"/common"}
export CP_CAP_SPARK_INSTALL_DIR="${CP_CAP_SPARK_INSTALL_DIR:-$SHARED_FOLDER/spark}"
export CP_CAP_SPARK_VERSION="${CP_CAP_SPARK_VERSION:-2.4.3}"
# INFO: A custom Spark distribution is built using https://github.com/sidoruka/cloud-pipeline-spark-build (see README and build script for the customization reasons)
export CP_CAP_SPARK_DIST_URL="${CP_CAP_SPARK_DIST_URL:-https://cloud-pipeline-oss-builds.s3.amazonaws.com/tools/spark/spark-$CP_CAP_SPARK_VERSION.tgz}"
export CP_CAP_SPARK_HOST="${CP_CAP_SPARK_HOST:-$(hostname)}"
export CP_CAP_SPARK_PORT="${CP_CAP_SPARK_PORT:-7077}"
export CP_CAP_SPARK_UI_PORT="${CP_CAP_SPARK_UI_PORT:-8087}"
export CP_CAP_SPARK_UI_PROXY_PORT="${CP_CAP_SPARK_UI_PROXY_PORT:-8088}"
export CP_CAP_SPARK_UI_PROXY_ENDPOINT_ID="${CP_CAP_SPARK_UI_PROXY_ENDPOINT_ID:-1000}"

rm -rf "$CP_CAP_SPARK_INSTALL_DIR"
mkdir -p "$(dirname $CP_CAP_SPARK_INSTALL_DIR)"

# Install Spark master
pipe_log_info "Installing Spark $CP_CAP_SPARK_VERSION using $CP_CAP_SPARK_DIST_URL" "$SPARK_MASTER_SETUP_TASK"

install_tmp="$(mktemp -d)"

# "--no-same-owner" is used to keep current user's ownership. Otherwise extracted directory will have owner/group IDs from the archive
cd $install_tmp && \
curl -s "$CP_CAP_SPARK_DIST_URL" -o spark.tar.gz && \
tar -zxf spark.tar.gz --no-same-owner && \
rm -f spark.tar.gz && \
mv spark* "$CP_CAP_SPARK_INSTALL_DIR"

if [ $? -ne 0 ]; then
    pipe_log_fail "Spark installation failed, aborting" "$SPARK_MASTER_SETUP_TASK"
    exit 1
fi

export SPARK_HOME="$CP_CAP_SPARK_INSTALL_DIR"
if [ -f "$CP_ENV_FILE_TO_SOURCE" ]; then
    echo "export SPARK_HOME=$SPARK_HOME" >> "$CP_ENV_FILE_TO_SOURCE"
    echo "export PATH=\${PATH}:$CP_CAP_SPARK_INSTALL_DIR/bin" >> "$CP_ENV_FILE_TO_SOURCE"
fi

pipe_log_info "Spark installed into $CP_CAP_SPARK_INSTALL_DIR" "$SPARK_MASTER_SETUP_TASK"

# Install java
if ! java -version > /dev/null 2>&1 || [ ! -d "$JAVA_HOME" ]; then
    export CP_CAP_SPARK_JAVA_VERSION="${CP_CAP_SPARK_JAVA_VERSION:-1.8.0_222}"
    export CP_CAP_SPARK_JAVA_DIST_URL="${CP_CAP_SPARK_JAVA_DIST_URL:-https://cloud-pipeline-oss-builds.s3.amazonaws.com/tools/java/openjdk-${CP_CAP_SPARK_JAVA_VERSION}_linux-x64_bin.tar.gz}"

    pipe_log_info "JAVA not found, it will be installed using $CP_CAP_SPARK_JAVA_DIST_URL" "$SPARK_MASTER_SETUP_TASK"

    export JAVA_HOME="$CP_CAP_SPARK_INSTALL_DIR/jdk"
    export PATH="$JAVA_HOME/bin:$PATH"
    
    cd $install_tmp && \
    curl -s "$CP_CAP_SPARK_JAVA_DIST_URL" -o openjdk.tar.gz && \
    tar -zxf openjdk.tar.gz --no-same-owner && \
    rm -f openjdk.tar.gz && \
    mv *jdk* "$JAVA_HOME"

    if [ $? -ne 0 ]; then
        pipe_log_fail "JAVA installation failed, aborting Spark installation" "$SPARK_MASTER_SETUP_TASK"
        exit 1
    fi

    if ! java -version > /dev/null 2>&1; then
        pipe_log_fail "JAVA installed without errors, but 'java -version' failed, aborting Spark installation" "$SPARK_MASTER_SETUP_TASK"
        exit 1
    fi

    if [ -f "$CP_ENV_FILE_TO_SOURCE" ]; then
        echo "export JAVA_HOME=$JAVA_HOME" >> "$CP_ENV_FILE_TO_SOURCE"
        echo "export PATH=\${PATH}:$JAVA_HOME/bin" >> "$CP_ENV_FILE_TO_SOURCE"
    fi

    pipe_log_info "JAVA installed into $JAVA_HOME" "$SPARK_MASTER_SETUP_TASK"
fi

# Configuration file setup
spark_config_file="$CP_CAP_SPARK_INSTALL_DIR/conf/spark-defaults.conf"

# Setup proxies for Spark, if defined in the environment variables
setup_proxy_config "$spark_config_file"

# Setup Object Storages access for Spark (if not restricted)
if [ "$CP_CAP_SPARK_NO_OBJECT_STORAGES" == "true" ]; then
    pipe_log_info " Object Storage access configuration WILL NO be performed (prohibited by CP_CAP_SPARK_NO_OBJECT_STORAGES=true)" "$SPARK_MASTER_SETUP_TASK"
else
    setup_object_storage_access "$spark_config_file"
fi

# Setup executors num/cores/mem configuration
setup_executors "$spark_config_file"

# Setup other configuration options
setup_common_spark_conf "$spark_config_file"

# Start Spark master
spark_master_url="spark://${CP_CAP_SPARK_HOST}:${CP_CAP_SPARK_PORT}"
pipe_log_info "Starting Spark master service $spark_master_url with Web UI on port $CP_CAP_SPARK_UI_PORT" "$SPARK_MASTER_SETUP_TASK"
$CP_CAP_SPARK_INSTALL_DIR/sbin/start-master.sh -h "$CP_CAP_SPARK_HOST" \
                                               -p "$CP_CAP_SPARK_PORT" \
                                               --webui-port "$CP_CAP_SPARK_UI_PORT"
if [ $? -ne 0 ]; then
    pipe_log_fail "Spark master startup failed, aborting. Please review the logs in $CP_CAP_SPARK_INSTALL_DIR/logs" "$SPARK_MASTER_SETUP_TASK"
    exit 1
fi

# Add the master's URL to the config
[ -f "$spark_config_file" ] && sed -i '/spark.master/d' "$spark_config_file"
echo "spark.master $spark_master_url" >> "$spark_config_file"
export SPARK_MASTER="$spark_master_url"
if [ -f "$CP_ENV_FILE_TO_SOURCE" ]; then
    echo "export SPARK_MASTER=$SPARK_MASTER" >> "$CP_ENV_FILE_TO_SOURCE"
fi    

# Fix permissions for the SPARK_HOME
# If the SPARK_HOME is located in the LFS mountpoint - ACLs are ignored: https://github.com/lizardfs/lizardfs/issues/816
if [ "$OWNER" ]; then
    chown "$OWNER" "$SPARK_HOME" -R
fi

# Configure Spark UI proxy
# Construct EDGE-compatible URL prefix for the proxy, e.g. /pipeline-1111-8088-1000/
export URL_PREFIX="/$(hostname)-${CP_CAP_SPARK_UI_PROXY_PORT}-${CP_CAP_SPARK_UI_PROXY_ENDPOINT_ID}/"
nohup $CP_PYTHON2_PATH $COMMON_REPO_DIR/shell/spark_setup_ui \
        "${CP_CAP_SPARK_HOST}:${CP_CAP_SPARK_UI_PORT}" \
        "${CP_CAP_SPARK_UI_PROXY_PORT}" > /dev/null 2>&1 &
spark_setup_ui=$!
sleep 1
if ! kill -0 $spark_setup_ui > /dev/null 2>&1; then
    pipe_log_warn "Spark UI proxy failed to start on port $CP_CAP_SPARK_UI_PROXY_PORT --> ${CP_CAP_SPARK_HOST}:${CP_CAP_SPARK_UI_PORT}" "$SPARK_MASTER_SETUP_TASK"
else
    pipe_log_info "Spark UI proxy started on port $CP_CAP_SPARK_UI_PROXY_PORT --> ${CP_CAP_SPARK_HOST}:${CP_CAP_SPARK_UI_PORT}" "$SPARK_MASTER_SETUP_TASK"
fi

pipe_log_success "Spark master is started" "$SPARK_MASTER_SETUP_TASK"

# Add a current master as a worker (if not restricted)
if [ "$CP_CAP_SPARK_MASTER_NO_SCHEDULE" == "true" ]; then
    pipe_log_info "Spark master host WILL NOT be added as a worker (prohibited by CP_CAP_SPARK_MASTER_NO_SCHEDULE=true)" "$SPARK_MASTER_SETUP_TASK"
else
    pipe_log_info "Adding Spark master host as a worker" "$SPARK_MASTER_SETUP_TASK"
    spark_setup_worker "$CP_CAP_SPARK_HOST"
    if [ $? -ne 0 ]; then
        pipe_log_fail "Cannot add a Spark master host as a worker, treating this as critical error and aborting" "$SPARK_MASTER_SETUP_TASK"
        exit 1
    fi
fi

# Wait for worker nodes to initiate and connect to the master
if [ -z "$node_count" ] || (( "$node_count" == 0 )); then
    pipe_log_success "Worker nodes count is not defined. Won't wait for them" "$SPARK_MASTER_SETUP_TASK_WORKERS"
else
    _MASTER_API_ENDPOINT="http://$CP_CAP_SPARK_HOST:$CP_CAP_SPARK_UI_PORT/json/"
    _MASTER_EXEC_WAIT_ATTEMPTS=${_MASTER_EXEC_WAIT_ATTEMPTS:-60}
    _MASTER_EXEC_WAIT_SEC=${_MASTER_EXEC_WAIT_SEC:-10}
    _CURRENT_WORKERS_COUNT=$(get_spark_workers_count "$_MASTER_API_ENDPOINT")
    while [ "$node_count" -gt "$_CURRENT_WORKERS_COUNT" ]; do
        pipe_log_info "Waiting for workers to connect. $_CURRENT_WORKERS_COUNT out of $node_count are ready" "$SPARK_MASTER_SETUP_TASK_WORKERS"
        sleep $_MASTER_EXEC_WAIT_SEC
        _CURRENT_WORKERS_COUNT=$(get_spark_workers_count "$_MASTER_API_ENDPOINT")
        _MASTER_EXEC_WAIT_ATTEMPTS=$(( _MASTER_EXEC_WAIT_ATTEMPTS-1 ))

        if (( $_MASTER_EXEC_WAIT_ATTEMPTS <= 0 )); then
            pipe_log_success "NOT all execution hosts are connected. But we are giving up waiting as threshold has been reached" "$SPARK_MASTER_SETUP_TASK_WORKERS"
            exit 0
        fi
    done
    pipe_log_success "All workers are connected" "$SPARK_MASTER_SETUP_TASK_WORKERS"
fi
