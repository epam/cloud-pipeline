#!/bin/bash

# Copyright 2017-2020 EPAM Systems, Inc. (https://www.epam.com/)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

SLURM_MASTER_SETUP_TASK="SLURMMasterSetup"
SLURM_MASTER_SETUP_TASK_WORKERS="SLURMMasterSetupWorkers"
CURRENT_PID=$$

configure_slurm() {

    mkdir /var/spool/slurmctld
    chown slurm: /var/spool/slurmctld
    chmod 755 /var/spool/slurmctld
    touch /var/log/slurmctld.log
    chown slurm: /var/log/slurmctld.log
    touch /var/log/slurm_jobacct.log /var/log/slurm_jobcomp.log
    chown slurm: /var/log/slurm_jobacct.log /var/log/slurm_jobcomp.log

    cat > /common/cgroup.conf <<EOL
CgroupAutomount=yes
CgroupMountpoint=/cgroup
ConstrainCores=no
ConstrainRAMSpace=no
EOL

    cat > /common/slurm.conf <<EOL
ControlMachine=$HOSTNAME
#
#MailProg=/bin/mail
MpiDefault=none
ProctrackType=proctrack/cgroup
ReturnToService=1
SlurmUser=root
SlurmdUser=root
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid
SlurmdSpoolDir=/var/spool/slurmd
StateSaveLocation=/var/spool/slurmctld
SwitchType=switch/none
TaskPlugin=task/none

# SCHEDULING
SchedulerType=sched/backfill
SelectType=select/cons_tres
SelectTypeParameters=CR_CPU

# LOGGING AND ACCOUNTING
ClusterName=cloudpipeline
AccountingStorageType=accounting_storage/none
JobAcctGatherType=jobacct_gather/none
SlurmctldLogFile=/var/log/slurmctld.log
SlurmdLogFile=/var/log/slurmd.log

#
# COMPUTE NODES
EOL

    _WORKER_CORES=$(nproc)
    _NODE_GPUS_COUNT=$(nvidia-smi -L 2>/dev/null | wc -l)
    _NODE_RAM_COUNT=$(grep MemTotal /proc/meminfo | awk '{print int($2 / 1024)}')
    CP_CAP_SGE_MASTER_CORES="${CP_CAP_SGE_MASTER_CORES:-999999999}"
    _WORKER_CORES=$((_WORKER_CORES < CP_CAP_SGE_MASTER_CORES ? _WORKER_CORES : CP_CAP_SGE_MASTER_CORES))

    if (( _NODE_GPUS_COUNT > 0 ))
    then
        echo "GresTypes=gpu" >> /common/slurm.conf
        for device in $(ls /dev/ | grep -E "nvidia[0-9]+") ; do
            echo "Name=gpu File=/dev/$device" >> /$_SLURM_CONFIG_LOCATION/gres.conf
        done
    fi

    IFS=$'\n' read -d '' -r -a _NODE_NAMES < $DEFAULT_HOSTFILE
    for _NODE in ${_NODE_NAMES[*]} ; do
        if (( _NODE_GPUS_COUNT > 0 ))
        then
            echo "NodeName=$_NODE NodeHostname=$_NODE NodeAddr=$(getent hosts $_NODE | awk '{ print $1 }') CPUs=$_WORKER_CORES RealMemory=$_NODE_RAM_COUNT Gres=gpu:$_NODE_GPUS_COUNT State=UNKNOWN" >> /common/slurm.conf
        else
            echo "NodeName=$_NODE NodeHostname=$_NODE NodeAddr=$(getent hosts $_NODE | awk '{ print $1 }') CPUs=$_WORKER_CORES RealMemory=$_NODE_RAM_COUNT State=UNKNOWN" >> /common/slurm.conf
        fi
    done
    echo "PartitionName=main.q Nodes=`echo ${_NODE_NAMES[*]} | xargs |  awk -v OFS="," '{$1=$1;print}'` Default=YES MaxTime=INFINITE State=UP" >> /common/slurm.conf

    if [ ! -z $CP_SLURM_LICENSES ]; then
        echo "Licenses=$CP_SLURM_LICENSES" >> /common/slurm.conf
    fi
}

get_linux_dist() {
    result=
    command -v apt-get > /dev/null
    if [ $? -eq 0 ]; then
        result="debian"
    fi

    command -v yum > /dev/null
    if [ $? -eq 0 ]; then
        result="redhat"
    fi

    echo "$result"
}

check_last_exit_code() {
   exit_code=$1
   msg_if_success=$2
   msg_if_fail=$3
   if [[ "$exit_code" -ne 0 ]]; then
        pipe_log_fail "$msg_if_fail" "${SLURM_MASTER_SETUP_TASK}"
        kill -s "$CURRENT_PID"
        exit 1
    else
        pipe_log_info "$msg_if_success" "${SLURM_MASTER_SETUP_TASK}"
    fi
}

slurm_config_location() {
   _SLURM_CONFIG_DIR=$(ls /etc/ | grep slurm | head -n 1)
   if [ "$_SLURM_CONFIG_DIR" != "" ]; then
      echo "/etc/$_SLURM_CONFIG_DIR"
   else
      pipe_log_fail "Failed to find SLURM config dir" "${SLURM_MASTER_SETUP_TASK}"
      kill -s "$CURRENT_PID"
      exit 1
   fi
}

pipe_log_info "Installing SLURM master" "$SLURM_MASTER_SETUP_TASK"

LINUX_DISTRIBUTION=$( get_linux_dist )

if [ "$LINUX_DISTRIBUTION" = "debian" ]; then
    apt-get update && apt-get install -y munge slurm-wlm
    mkdir -p /var/run/munge
    chown munge: /var/run/munge
    mkdir -p /run/munge
    chown munge: /run/munge
    chmod +t /run/munge
elif [ "$LINUX_DISTRIBUTION" = "redhat" ]; then

    if [ -z $CP_SLURM_PACKAGE_URL ]; then
        CP_SLURM_PACKAGE_URL="${GLOBAL_DISTRIBUTION_URL}tools/slurm/rpm/slurm-19.05.5-1.el7.x86_64.tar"
        CP_SLURM_SOURCE_URL="https://download.schedmd.com/slurm/slurm-19.05.5.tar.bz2"
    fi

    export SLURMUSER=992
    groupadd -g $SLURMUSER slurm
    useradd  -m -c "SLURM workload manager" -d /var/lib/slurm -u $SLURMUSER -g slurm  -s /bin/bash slurm

    yum install epel-release -y
    yum install gcc gcc-c++ mariadb-server mariadb-devel  munge munge-libs munge-devel openssl openssl-devel \
                pam-devel numactl hwloc hwloc-devel lua lua-devel readline-devel rrdtool-devel ncurses-devel  \
                man2html libibmad libibumad rpm-build perl-devel perl-CPAN -y

    _TMP_PACKAGE_DIR=/tmp/localinstall && mkdir -p $_TMP_PACKAGE_DIR
    mkdir -p /common/slurm-pkgs
    wget $CP_SLURM_PACKAGE_URL --directory-prefix=$_TMP_PACKAGE_DIR
    if [[ $? -ne 0 ]]; then
        rm -rf /root/rpmbuild/RPMS/x86_64/*
        wget $CP_SLURM_SOURCE_URL --directory-prefix=$_TMP_PACKAGE_DIR
        rpmbuild -ta $_TMP_PACKAGE_DIR/slurm*.tar.bz2
        cp /root/rpmbuild/RPMS/x86_64/* /common/slurm-pkgs/
    else
        tar -xf $_TMP_PACKAGE_DIR/slurm*.tar --directory $_TMP_PACKAGE_DIR
        mv $_TMP_PACKAGE_DIR/slurm*/* /common/slurm-pkgs/
    fi

    yum --nogpgcheck localinstall /common/slurm-pkgs/* -y
    check_last_exit_code $? "Successfully install SLURM packages" "Can't install SLURM packages from /common/slurm-pkgs/"
fi

_SLURM_CONFIG_LOCATION=$( slurm_config_location )
configure_slurm

dd if=/dev/urandom bs=1 count=1024 > /common/munge.key
cp /common/munge.key /etc/munge/
chown munge: /etc/munge/munge.key
chmod 400 /etc/munge/munge.key
su -c /usr/sbin/munged -s /bin/bash munge

cp /common/slurm.conf "$_SLURM_CONFIG_LOCATION"
cp /common/cgroup.conf "$_SLURM_CONFIG_LOCATION"

slurmctld && slurmd

check_last_exit_code $? "SLURM demons have started" "Fail to start SLURM demons."

pipe_log_success "Master ENV is ready" "$SLURM_MASTER_SETUP_TASK"


# Verify whether it is a resumed run - if so, do not reinstall SLURM, instead - just rerun master and worker daemon
if [ "$RESUMED_RUN" = true ]; then
    pipe_log_info "Run is resumed - SLURM master node won't be reconfigured. Starting master and worker daemons" "$SLURM_MASTER_SETUP_TASK"
    slurmctld -f /etc/slurm/slurm.conf && slurmd -f /etc/slurm/slurm.conf
    _SLURM_RESUME_RESULT=$?

    if [ $_SLURM_RESUME_RESULT -eq 0 ]; then
        pipe_log_success "SLURM daemons started" "$SLURM_MASTER_SETUP_TASK"
    else
        pipe_log_fail "SLURM daemons start failed. See any errors in the ConsoleOutput" "$SLURM_MASTER_SETUP_TASK"
    fi
    exit 0
fi

# Wait for worker nodes to initiate and connect to the master
if [ -z "$node_count" ] || (( "$node_count" == 0 )); then
    pipe_log_success "Worker nodes count is not defined. Won't wait for execution hosts" "$SLURM_MASTER_SETUP_TASK_WORKERS"
else
    _MASTER_EXEC_WAIT_ATTEMPTS=${_MASTER_EXEC_WAIT_ATTEMPTS:-60}
    _MASTER_EXEC_WAIT_SEC=${_MASTER_EXEC_WAIT_SEC:-10}
    _CURRENT_EXEC_HOSTS_COUNT=$(( $(sinfo -N | grep 'idle' | wc -l) - 1))
    while [ "$node_count" -gt "$_CURRENT_EXEC_HOSTS_COUNT" ]; do
        pipe_log_info "Waiting for execution hosts to connect. $_CURRENT_EXEC_HOSTS_COUNT out of $node_count are ready" "$SLURM_MASTER_SETUP_TASK_WORKERS"
        sleep $_MASTER_EXEC_WAIT_SEC
        _CURRENT_EXEC_HOSTS_COUNT=$(( $(sinfo -N | grep 'idle' | wc -l) - 1))
        _MASTER_EXEC_WAIT_ATTEMPTS=$(( _MASTER_EXEC_WAIT_ATTEMPTS-1 ))

        if (( $_MASTER_EXEC_WAIT_ATTEMPTS <= 0 )); then
            pipe_log_success "NOT all execution hosts are connected. But we are giving up waiting as threshold has been reached" "$SLURM_MASTER_SETUP_TASK_WORKERS"
            exit 0
        fi
    done
    pipe_log_success "All SLURM hosts are connected" "$SLURM_MASTER_SETUP_TASK_WORKERS"
    rm -rf /common/slurm.conf /common/cgroup.conf /common/munge.key
fi

