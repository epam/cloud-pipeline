#!/usr/bin/env bash

# Copyright 2017-2019 EPAM Systems, Inc. (https://www.epam.com/)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

SLURM_WORKER_SETUP_TASK="SLURMWorkerSetup"
CURRENT_PID=$$

add_worker() {

    cp /common/munge.key /etc/munge/
    chown munge: /etc/munge/munge.key
    chmod 0700 /etc/munge/munge.key
    su -c /usr/sbin/munged -s /bin/bash munge

    cp /common/slurm.conf "$_SLURM_CONFIG_LOCATION"
    mkdir /var/spool/slurmd
    chown slurm: /var/spool/slurmd
    chmod 755 /var/spool/slurmd
    touch /var/log/slurmd.log
    chown slurm: /var/log/slurmd.log

    _NODE_GPUS_COUNT=$(nvidia-smi -L 2>/dev/null | wc -l)
    if (( _NODE_GPUS_COUNT > 0 ))
    then
        for device in $(ls /dev/ | grep -E "nvidia[0-9]+") ; do
            echo "Name=gpu File=/dev/$device" >> /$_SLURM_CONFIG_LOCATION/gres.conf
        done
    fi

    slurmd
}

get_linux_dist() {
    result=
    command -v apt-get > /dev/null
    if [ $? -eq 0 ]; then
        result="debian"
    fi

    command -v yum > /dev/null
    if [ $? -eq 0 ]; then
        result="redhat"
    fi

    echo "$result"
}

check_last_exit_code() {
   exit_code=$1
   msg_if_success=$2
   msg_if_fail=$3
   if [[ "$exit_code" -ne 0 ]]; then
        pipe_log_fail "$msg_if_fail" "${SLURM_WORKER_SETUP_TASK}"
        kill -s "$CURRENT_PID"
        exit 1
    else
        pipe_log_info "$msg_if_success" "${SLURM_WORKER_SETUP_TASK}"
    fi
}

LINUX_DISTRIBUTION=$( get_linux_dist )

pipe_log_info "Installing SLURM worker" "$SLURM_WORKER_SETUP_TASK"

MASTER_INFO_RESULT=$(eval "${CP_PYTHON2_PATH} ${COMMON_REPO_DIR}/scripts/cluster_wait_for_master.py --master-id ${parent_id} --task-name SLURMMasterSetup")
_MASTER_AWAIT_RESULT=$?
MASTER_INFO=($MASTER_INFO_RESULT)
MASTER_IP=${MASTER_INFO[-1]}
MASTER_NAME=${MASTER_INFO[-2]}
check_last_exit_code $_MASTER_AWAIT_RESULT "Master info received: $MASTER_NAME : $MASTER_IP" "Fail to install SLURM worker. Unable to get master information"

if [ "$LINUX_DISTRIBUTION" = "debian" ]; then
    apt install -y munge slurm-wlm
    mkdir -p /var/run/munge
    chown munge: /var/run/munge
    export _SLURM_CONFIG_LOCATION=/etc/slurm-llnl/
elif [ "$LINUX_DISTRIBUTION" = "redhat" ]; then
    export _SLURM_CONFIG_LOCATION=/etc/slurm/
    export SLURMUSER=992
    groupadd -g $SLURMUSER slurm
    useradd  -m -c "SLURM workload manager" -d /var/lib/slurm -u $SLURMUSER -g slurm  -s /bin/bash slurm

    yum install epel-release -y
    yum install munge munge-libs munge-devel openssl openssl-devel \
                pam-devel numactl hwloc hwloc-devel lua lua-devel readline-devel  \
                rrdtool-devel ncurses-devel man2html libibmad libibumad rpm-build perl-devel perl-CPAN -y

    yum --nogpgcheck localinstall /common/slurm-pkgs/* -y
fi

pipe_log_info "Adding worker node to SLURM cluster" "$SLURM_WORKER_SETUP_TASK"
add_worker
check_last_exit_code $? "Host was added as a worker to the SLURM cluster" "Fail to add SLURM worker host"

pipe_log_success "SLURM worker node was successfully configured" "$SLURM_WORKER_SETUP_TASK"
