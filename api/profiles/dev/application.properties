#tomcat configuration
server.port=9999
server.context-path=/pipeline
server.compression.enabled=true
server.compression.min-response-size=2048
server.compression.mime-types=text/html,text/xml,application/json,application/javascript
server.connection-timeout=20000
server.api.token=
spring.http.encoding.charset=UTF-8
spring.http.encoding.force=true
spring.http.encoding.force-response=true
spring.session.store-type=jdbc

#Security
api.security.anonymous.urls=${CP_API_SECURITY_ANONYMOUS_URLS:/restapi/route}
api.security.public.urls=${CP_API_SECURITY_PUBLIC_URLS:/launch.sh,/launch.py,/PipelineCLI.tar.gz,/pipe-common.tar.gz,/commit-run-scripts/**,/pipe,/fsbrowser.tar.gz,/pipe.zip,/pipe.tar.gz,/pipe-el6,/pipe-el6.tar.gz,/pipe-osx,/pipe-osx.tar.gz,/cloud-data-linux.tar.gz,/cloud-data-win64.zip,/fsautoscale.sh,/data-transfer-service.jar,/data-transfer-service-windows.zip,/DeployDts.ps1}

#db configuration
database.url=jdbc:postgresql://localhost:5432/pipeline
database.username=pipeline
database.password=pipeline
database.driverClass=org.postgresql.Driver
database.max.pool.size=10
database.initial.pool.size=5

# Jpa
spring.jpa.properties.hibernate.default_schema=${DATASOURCE_SCHEMA:pipeline}
spring.jpa.generate-ddl=false
spring.jpa.properties.hibernate.dialect=com.epam.pipeline.hibernate.JsonPostgreSqlDialect

#monitoring configuration
monitoring.backend=elastic
monitoring.stats.export.xls.template=${CP_API_MONITORING_XLS_TEMPLATE_PATH:classpath:/templates/monitoring_report_template.xls}

#monitoring Elaticsearch configuration
monitoring.elasticsearch.url=
monitoring.elasticsearch.port=80

#flyway configuration
flyway.sql-migration-prefix=v
flyway.locations=classpath:db/migration
flyway.schemas=${DATASOURCE_SCHEMA:pipeline}
flyway.placeholders.default.admin=pipe_admin
flyway.placeholders.default.admin.id=1

#sso config
server.ssl.enabled-protocols=TLSv1.1,TLSv1.2
server.ssl.ciphers=HIGH:!RC4:!aNULL:!MD5:!kEDH
server.ssl.key-store=file:${CP_API_SSO_CONFIG}/store.jks
server.ssl.metadata=${CP_API_SSO_CONFIG}/cp-api-srv-fed-meta.xml
server.ssl.key-store-password=changeit
server.ssl.keyAlias=ssl
saml.sign.key=sso
server.ssl.endpoint.id=https://localhost:9999/pipeline/
saml.authn.request.binding=urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect
saml.authorities.attribute.names=groups
saml.user.attributes=Email=http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress,Name=http://schemas.xmlsoap.org/ws/2005/05/identity/claims/name
saml.user.blocked.attribute=
saml.user.blocked.attribute.true.val=true
# Create a user if it is not present in the database. Available strategies: AUTO, EXPLICIT, EXPLICIT_GROUP
saml.user.auto.create=${CP_SAML_USER_AUTO_CREATE:EXPLICIT}
saml.user.allow.anonymous=${CP_SAML_USER_ALLOW_ANONYMOUS:false}
saml.authn.max.authentication.age=93600

#applicaion configuration
working.directory=${CP_API_WORKDIR}

#run.pipeline.init.task.name=InitializeEnvironment
#commit run scripts
commit.run.scripts.root.url=
commit.run.script.starter.url=
docker.registry.login.script=

#pause/resume run scripts
pause.run.script.url=
pause.pool.size=3
run.scheduling=true

#scheduled tasks
scheduled.pool.size=5

run.as.pool.size=5

#luigi
kube.namespace=default
luigi.graph.script=

#git
git.src.directory=src/
git.docs.directory=docs/

#s3 bucket policy
# in bytes
storage.clone.name.suffix=storage

#cluster management

#AWS Scripts
cluster.nodeup.script=
cluster.nodedown.script=
cluster.reassign.script=
cluster.node.terminate.script=

#Azure Scripts
cluster.azure.nodeup.script=
cluster.azure.nodedown.script=
cluster.azure.reassign.script=
cluster.azure.node.terminate.script=

#GCP Scripts
cluster.gcp.nodeup.script=
cluster.gcp.nodedown.script=
cluster.gcp.reassign.script=
cluster.gcp.node.terminate.script=

cluster.networks.config=
cluster.cadvisor.port=4194
cluster.cadvisor.request.period=15
cluster.cadvisor.timeout=10

api.host=
launch.script.url.linux=
launch.script.url.windows=
jwt.key.public=${CP_API_JWT_KEY_PUBLIC}
jwt.key.private=${CP_API_JWT_KEY_PRIVATE}
kube.edge.label=EDGE
kube.edge.ip.label=
kube.edge.port.label=
kube.master.ip=
kube.kubeadm.token=
kube.kubeadm.cert.hash=
kube.node.token=
kube.protected.node.labels=cloud-pipeline/role=EDGE
kube.master.pod.check.url=http://localhost:4040
kube.current.pod.name=${CP_API_CURRENT_POD_NAME:localhost}
ha.deploy.enabled=false

#templates
templates.directory=
templates.default.template=
templates.folder.directory=

# Tool's Security Policy options.
# Denies running a Tool, if the number of it's vulnerabilities exceeds the threshold. To disable the policy, set to -1

# Root mount point for mounting NFS file systems. Must be a directory, where pipeline's user has write access
data.storage.nfs.root.mount.point=
# Mount options for NFS
data.storage.nfs.options.rsize=1048576
data.storage.nfs.options.wsize=1048576

# Enables logging filter using CommonsRequestLoggingFilter
logging.level.org.springframework.web.filter.CommonsRequestLoggingFilter=DEBUG

#Firecloud
firecloud.auth.client.id=
firecloud.auth.client.secret=

cluster.disable.task.monitoring=${CP_API_DISABLE_POD_MONITOR:false}
cluster.disable.autoscaling=${CP_API_DISABLE_AUTOSCALER:false}

#Billing API
billing.index.common.prefix=cp-billing
billing.empty.report.value=unknown
billing.center.key=${CP_BILLING_CENTER_KEY:billing-center}


#logging
log.security.elastic.index.prefix=${CP_SECURITY_LOGS_ELASTIC_PREFIX:security_log}*

migration.alias.file=${CP_API_MIGRATION_ALIAS_FILE:}

#Cache
cache.type=MEMORY

#edge
edge.internal.host=${CP_EDGE_INTERNAL_HOST:cp-edge.default.svc.cluster.local}
edge.internal.port=${CP_EDGE_INTERNAL_PORT:31081}
