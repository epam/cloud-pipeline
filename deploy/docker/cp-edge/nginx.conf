# Copyright 2017-2022 EPAM Systems, Inc. (https://www.epam.com/)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

user             root;
worker_processes auto;
error_log        /etc/nginx/logs/error.log warn;
pid              /usr/local/openresty/nginx/logs/nginx.pid;
include          /usr/share/nginx/modules/*.conf;

events {
    worker_connections 1024;
}

env JWT_PUB_KEY;
env API;
env API_EXTERNAL;
env API_TOKEN;
env EDGE_EXTERNAL;
env EDGE_EXTERNAL_SCHEMA;
env CP_API_SRV_SAML_ALLOW_ANONYMOUS_USER;
env CP_EDGE_CONNECT_PROXY_AUTHENTICATION_WHITELIST;

# - if Cloud Pipeline is NOT in maintenance mode, traffic will be proxied to the port 8181 (HTTP Reverse proxy
#   for interactive web and in-browser sessions); otherwise, in maintenance mode, SSL/TLS traffic will go to
#   port 8383 for maintenance mode check;
#   /etc/maintenance/maintenance.py script is responsible to switch ports (8181 <-> 8383) and gracefully restart nginx
# - While plain HTTP traffic will go to a forward proxy and will be considered as HTTP CONNECT method
#   (this is typically used for the noMachine sessions, which does not support HTTPS CONNECT)
stream {
    upstream reverse_proxy {
        server 127.0.0.1:8181;
    }

    upstream forward_proxy {
        server 127.0.0.1:8282;
    }

    map $ssl_preread_protocol $proxy_type {
        "" forward_proxy;
        default reverse_proxy;
    }

    server {
        listen 8080;

        # This can be one of:
        # $proxy_type - let both NoMachine and General traffic (forward_proxy or reverse_proxy)
        # forward_proxy - only NoMachine
        # reverse_proxy - only General traffic
        proxy_pass ${CP_EDGE_ROOT_PROXY_PASS_DESTINATION};
        ssl_preread on;
    }
}

http {
    # Configs to proxy the Cloud Pipeline services (e.g. API) in the "ingress" mode
    include                 /etc/nginx/ingress/cp-*.conf;

    # Configs to serve/proxy any external apps (e.g. rstudio/rshiny)
    include                 /etc/nginx/external-apps/*.conf;

    # CP_EDGE_WEB_CLIENT_MAX_SIZE is replaced in the `init` script
    client_max_body_size            $CP_EDGE_WEB_CLIENT_MAX_SIZE;
    proxy_http_version              1.1;
    proxy_buffering                 off;
    proxy_request_buffering         off;
    # This is required, as some of the auto-generated server names (via cloud hosted zone)
    # may exceed a default bucket_size=64
    server_names_hash_bucket_size   128; 
    log_format                      main '$remote_addr - $remote_user [$time_local] "$request" '
                                    '$status $body_bytes_sent "$http_referer" '
                                    '"$http_user_agent" "$http_x_forwarded_for"';
    log_format                      dav  '[$time_local] "$request" "$http_destination" '
                                    '$status $body_bytes_sent';

    access_log              /etc/nginx/logs/access.log  main;
    lua_package_path        "/usr/local/openresty/lualib/?.lua;;/etc/nginx/?.lua;";

    proxy_set_header Cloud-Pipeline-Edge-Region "${CP_EDGE_REGION}";

    # We're requiring "maintenance" package (/etc/nginx/maintenance.lua) here at the
    # initialize phase to prevent logs poluting with lua warnings ("assigning to global variable").
    # Subsequent "require" calls (at the maintenance-*.lua scripts) won't log such warning
    # because of lua package caching.
    init_worker_by_lua_block {
	    require "maintenance"
    }

    # Health endpoints
    server {
        listen 8888 default_server;

        # A basic nginx health check, which ensures that nginx is up/running and can server the requests
        location /edge-health {
            access_log off;
            return 200 "healthy\n";
        }

        # Web TTY ping, which ensures that SSH service is there
        location /wetty-health {
            access_log off;
            proxy_pass http://127.0.0.1:32000/;
            proxy_connect_timeout   5s;
            proxy_send_timeout      5s;
            proxy_read_timeout      5s;
        }
    }

    # Maintenance redirection rules:
    # if we're at the maintenance mode (on) - we should proxy pass request to the /maintenance endpoint;
    # otherwise (off) - proxy pass request to the port 8181 (reverse proxy)
    map $maintenance $maintenance_redirect {
        off "https://127.0.0.1:8181";
        on "https://127.0.0.1:8383/maintenance";
    }

    # Maintenance handling
    server {
        listen 8383 ssl default_server;
        # SSL
        ssl_certificate             /etc/edge/pki/ssl-public-cert.pem;
        ssl_certificate_key         /etc/edge/pki/ssl-private-key.pem;

        set $maintenance off;
        # if flag file exists, switch $maintenance variable on
        if (-f /etc/maintenance/active) {
            set $maintenance on;
        }

        # Calling lua script that initializes $maintenance_user_allowed variable:
        # - "allow" value: user is allowed to proceed with request (even at the maintenance mode)
        # - "deny" value: /maintenance page should be displayed
        # - "redirect" value: we should redirect current request (except API request, see below)
        #                     to the authentication endpoint to receive user token and re-check access permissions;
        #                     API methods do not follow redirect, 'cloud-pipeline-mode':'maintenance' header
        #                     is exposed instead.
        # Also, maintenance_check_user.lua script modifies $maintenance variable at some cases (e.g., if user
        # is allowed to proceed with request, $maintenance var is switched off)
        #
        # $maintenance_user_allowed variable is used within the -redirect, -authenticate and -process_restapi
        # lua scripts below.
        # We're using 'set_by_lua_file' directive to make sure is it called BEFORE 'access_by_lua_file' one -
        # we need both $maintenance_user_allowed and $maintenance variables to be properly set.
        set_by_lua_file $maintenance_user_allowed /etc/nginx/maintenance_check_user.lua;

        # Default endpoint - all requests except '/maintenance', '/restapi/route', 'saml' and 'sso'
        # will be handled here
        location / {
            # maintenance_redirect_user.lua will redirect request to the authentication endpoint (/restapi/route)
            # if we don't know user roles (i.e., bearer token is missing, like for the GUI calls)
            access_by_lua_file /etc/nginx/maintenance_redirect_user.lua;
            # If request was not redirected, proxy pass it to the default endpoint OR /maintenance page
            # (depending on the $maintenance mode)
            proxy_pass $maintenance_redirect;
            proxy_set_header Host $host;
        }

        # Allow restapi/route, SSO endpoints even at the maintenance mode - proxy pass to the default handler
        location ~* /(restapi[/]+route|sso) {
            proxy_pass https://127.0.0.1:8181;
            proxy_set_header Host $host;
        }

        # Allow saml, impersonation endpoints even at the maintenance mode, proxy pass to the default handler and
        # clear MAINTENANCE cookie
        location ~* /(saml|impersonation/stop|impersonation/start) {
            proxy_pass https://127.0.0.1:8181;
            proxy_set_header Host $host;
            add_header Set-Cookie "MAINTENANCE=; Path=/; Expires=Thu, 01 Jan 1970 00:00:00 GMT;";
        }

        # Restapi methods should follow the general rule ("allow" - proxy pass to default endpoint, "deny" - show disclaimer),
        # but DO NOT redirect if $maintenance_user_allowed == 'redirect'. This logic is handled
        # at "maintenance_process_restapi.lua" script
        location ~* /restapi {
            access_by_lua_file /etc/nginx/maintenance_process_restapi.lua;
            proxy_pass $maintenance_redirect;
            proxy_set_header Host $host;
        }

        # Maintenance disclaimer page
        location /maintenance {
            types {
                text/css css;
                application/javascript js;
                image/png png;
                image/svg+xml svg svgz;
                application/json json;
                text/html html htm shtml;
            }
            root /etc/nginx;
            add_header Cache-Control no-cache;
            expires 0;
            try_files $uri $uri/index.html index.html;
        }

        # Maintenance check endpoint
        location ^~ /maintenance/check {
            if (-f /etc/maintenance/active) {
                return 200 "ON";
            }
            return 200 "OFF";
        }

        # Maintenance auth endpoint.
        # If request was redirected to the authentication route - i.e., "restapi/route" - this endpoint
        # is specified as fallback ("restapi/route?url=/maintenance/auth");
        location ^~ /maintenance/auth {
            # At the fallback endpoint we're setting MAINTENANCE cookie to prevent further authentication calls.
            access_by_lua_file /etc/nginx/maintenance_authenticate_user.lua;
        }
    }

    # "Endpoints" routes with custom domains
    # These routes are defined in the separate "server { server_name }" blocks with the server_name
    include /etc/nginx/sites-enabled/custom-domains/*.srv.conf;

    # Main proxy setup
    server {
        # "default_server" is required for "ingress" use case, when all other services are proxied by their "server_name"
        # EDGE itself will be a default server, and there will be no need to specify its server_name explicitely
        listen                      8181 ssl default_server;

        # SSL
        ssl_certificate             /etc/edge/pki/ssl-public-cert.pem;
        ssl_certificate_key         /etc/edge/pki/ssl-private-key.pem;

        # Server block shared configuration for all the "Endpoints"
        include                     /etc/nginx/endpoints-config/server.common.conf;

        # "Endpoints" routes WITHOUT custom domains
        # These routes are defined in the "location" blocks
        include                     /etc/nginx/sites-enabled/*.loc.conf;

        # External app hosting, which do not require custom domain
        include                     /etc/nginx/external-locs/*.conf;

        location /_ping {
            access_log off;
            add_header Content-Length 0;
            add_header Content-Type text/plain;
            add_header Access-Control-Allow-Headers *;
            add_header Access-Control-Allow-Methods *;
            add_header Access-Control-Allow-Origin ${CP_EDGE_FSBROWSER_CORS};
            add_header Vary Origin;
            return 200;
        }

        location /ssh {
            default_type        text/html;
            access_by_lua_file  /etc/nginx/validate_cookie_ssh.lua;
            proxy_pass          http://127.0.0.1:32000/ssh;
            proxy_http_version  1.1;
            proxy_set_header    Upgrade $http_upgrade;
            proxy_set_header    Connection "upgrade";
            proxy_read_timeout  43200000;

            proxy_set_header    X-Real-IP $remote_addr;
            proxy_set_header    X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header    Host $http_host;
            proxy_set_header    X-NginX-Proxy true;
        }

        location ${CP_EDGE_INVALIDATE_AUTH_PATH} {
            add_header Access-Control-Allow-Credentials "true";
            add_header Access-Control-Allow-Origin ${CP_EDGE_INVALIDATE_URL_CORS};
            add_header Access-Control-Allow-Headers *;
            add_header Vary Origin;
            add_header Set-Cookie "bearer=; path=/; SameSite=None; Secure; expires=Thu, 01 Jan 1970 00:00:00 GMT";
            add_header Set-Cookie "MAINTENANCE=; Path=/; SameSite=None; Secure; Expires=Thu, 01 Jan 1970 00:00:00 GMT";
            return 200;
        }

        location /fsbrowser/ {
            if ($request_method = OPTIONS ) {
                add_header Content-Length 0;
                add_header Content-Type text/plain;
                add_header Access-Control-Allow-Headers *;
                add_header Access-Control-Allow-Origin  ${CP_EDGE_FSBROWSER_CORS};
                add_header Vary Origin;
                return 200;
            }
            if ($request_uri ~ '^(\/fsbrowser\/(\d+)$)') {
     	        return 301 $1/;
            }
            default_type          text/html;
            resolver              ${CP_EDGE_CLUSTER_RESOLVER};
            set $fsbrowser_target '';
            set $fsbrowser_auth   '';
            set $fsbrowser_req_uri '';
            access_by_lua_file    /etc/nginx/validate_cookie_fsbrowser.lua;
            add_header            Access-Control-Allow-Origin ${CP_EDGE_FSBROWSER_CORS};
            add_header            Vary Origin;
            add_header            Access-Control-Allow-Headers  *;
            proxy_set_header      Authorization $fsbrowser_auth;
            proxy_pass            http://$fsbrowser_target:${CP_PREF_STORAGE_FSBROWSER_PORT}$fsbrowser_req_uri;
        }

        location /${CP_DAV_URL_PATH} {
            access_log logs/webdav.access.log dav;
            default_type            text/html;
            access_by_lua_file      /etc/nginx/validate_cookie_dav.lua;
            # Internal cluster DNS is used to resolve the .cluster.local correctly
            # As the NGINX resolves the names only at a startup time
            # If the DAV service is recreated - it's IP is changed and EDGE will throw Gateway Timeout
            resolver                ${CP_EDGE_CLUSTER_RESOLVER} valid=${CP_EDGE_CLUSTER_RESOLVER_TIMEOUT_SEC}s ipv6=off;
            # We also need to set the backend (proxied host) as a variable, so nginx will force name resolution each "valid" interval
            # See https://serverfault.com/questions/240476/how-to-force-nginx-to-resolve-dns-of-a-dynamic-hostname-everytime-when-doing-p/593003#593003
            set                     $cp_dav_backend "http://${CP_DAV_INTERNAL_HOST}:${CP_DAV_INTERNAL_PORT}";
            proxy_pass              $cp_dav_backend;
            proxy_set_header        Host $http_host;
            proxy_pass_header       Server;
            proxy_redirect          "http://" "${EDGE_EXTERNAL_SCHEMA}://";
            client_max_body_size    0;

            # Rewrite the "Destination" header (which is used to specify dest for the MOVE operation) to the internal address of the upstream DAV server, e.g.:
            # Client: "Destination: https://1.1.1.1:31081/webdav/folder/file.txt"
            # Nginx rewrites to: "Destination: http://cp-dav.default.cluster.local:8080/webdav/folder/file.txt"
            # Substitution happens in the lua script at /etc/nginx/validate_cookie_dav.lua
            # The script sets dav_dest_path variable
            set $dav_dest_path "";
            proxy_hide_header Destination;
            proxy_set_header  Destination $dav_dest_path;
        }

        location /${CP_DAV_AUTH_URL_PATH}/ {
            default_type        text/html;
            access_by_lua_file  /etc/nginx/create_cookie_dav.lua;
            root                /etc/nginx/dav;
        }

        location ~ ^/${CP_DAV_URL_PATH}/extra/(.*) {
            default_type            text/html;
            access_by_lua_file      /etc/nginx/validate_cookie_ssh.lua;
            resolver                ${CP_EDGE_CLUSTER_RESOLVER} valid=${CP_EDGE_CLUSTER_RESOLVER_TIMEOUT_SEC}s ipv6=off;
            set                     $cp_dav_backend "http://${CP_DAV_INTERNAL_HOST}:${CP_DAV_EXTRA_INTERNAL_PORT}/$1/";
            proxy_pass              $cp_dav_backend;
            client_max_body_size    0;
        }
    }

    server {
        listen                         8282;

        # dns resolver used by forward proxying
        resolver                       ${CP_EDGE_CLUSTER_RESOLVER};

        # If CP_EDGE_CONNECT_PROXY_AUTHENTICATION_ENABLED is set to "true"
        # then lua script will be added to perform the JWT validation before 
        # passing the request through.
        ${CP_EDGE_CONNECT_PROXY_AUTHENTICATION}

        # forward proxy for CONNECT request
        proxy_connect;
        proxy_connect_allow            all;
        proxy_connect_connect_timeout  3600s;
        proxy_connect_read_timeout     3600s;
        proxy_connect_send_timeout     3600s;
    }
}
