{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Cloud Pipeline Introduction Cloud Pipeline Introduction Why Cloud Pipeline Components Why Cloud Pipeline Cloud Pipeline solution from EPAM provides an easy and scalable approach to perform a wide range of analysis tasks in the cloud environment. This solution takes the best of two approaches: classic HPC solutions (based on GridEngine schedulers family) and SaaS cloud solutions. Feature Classic \"HPC\" \"Cloud Pipeline\" SaaS Pipelines customization High . Direct scripting - any level of customization High . Direct scripting - any level of customization Low . Provide specific pipeline definition languages or even only a graphical editor Backward compatibility N/A High . \"Classic\" HPC scripts and NGS tools can be run without any changes Low . Scripts have to be rewritten according to the supported languages and storage structures User Interface Command Line Graphical Interface for User interaction and a Command Line Interface for automation scripts Graphical Interface Calculation power scalability Low . New nodes shall be deployed and supported on-premises. Idle nodes are still consuming resources High . New nodes are started according to the job request and terminated as soon as they are not needed anymore. Each job can precisely define required CPU/RAM/Disk resources or even select optimal node up to speed up execution (e.g. memory optimized nodes for cellranger pipelines) High . Scalable as \"Cloud Pipeline\" but sometimes limits user to predefined nodes setup Deployment and vendor-lock Deployed on-premises and introduces no vendor-lock Can be deployed in AWS/GCP/Azure or on-premises, thus introduces no vendor-lock Consumed as an Internet service (no on-premises deployment available), all processes are tied to this specific vendor Security High . All data and analysis processes are located in a controlled network. High . All data and analysis processes are located in a controlled cloud VPC. All security configurations are performed by user's security officers Low . No direct control over security configuration. SaaS vendor has full access to the data storages. Components The main components of the Cloud Pipeline are shown below:","title":"Introduction"},{"location":"#cloud-pipeline-introduction","text":"Cloud Pipeline Introduction Why Cloud Pipeline Components","title":"Cloud Pipeline Introduction"},{"location":"#why-cloud-pipeline","text":"Cloud Pipeline solution from EPAM provides an easy and scalable approach to perform a wide range of analysis tasks in the cloud environment. This solution takes the best of two approaches: classic HPC solutions (based on GridEngine schedulers family) and SaaS cloud solutions. Feature Classic \"HPC\" \"Cloud Pipeline\" SaaS Pipelines customization High . Direct scripting - any level of customization High . Direct scripting - any level of customization Low . Provide specific pipeline definition languages or even only a graphical editor Backward compatibility N/A High . \"Classic\" HPC scripts and NGS tools can be run without any changes Low . Scripts have to be rewritten according to the supported languages and storage structures User Interface Command Line Graphical Interface for User interaction and a Command Line Interface for automation scripts Graphical Interface Calculation power scalability Low . New nodes shall be deployed and supported on-premises. Idle nodes are still consuming resources High . New nodes are started according to the job request and terminated as soon as they are not needed anymore. Each job can precisely define required CPU/RAM/Disk resources or even select optimal node up to speed up execution (e.g. memory optimized nodes for cellranger pipelines) High . Scalable as \"Cloud Pipeline\" but sometimes limits user to predefined nodes setup Deployment and vendor-lock Deployed on-premises and introduces no vendor-lock Can be deployed in AWS/GCP/Azure or on-premises, thus introduces no vendor-lock Consumed as an Internet service (no on-premises deployment available), all processes are tied to this specific vendor Security High . All data and analysis processes are located in a controlled network. High . All data and analysis processes are located in a controlled cloud VPC. All security configurations are performed by user's security officers Low . No direct control over security configuration. SaaS vendor has full access to the data storages.","title":"Why Cloud Pipeline"},{"location":"#components","text":"The main components of the Cloud Pipeline are shown below:","title":"Components"},{"location":"manual/Cloud_Pipeline_-_Manual/","text":"Cloud Pipeline - Manual 1. Quick start 2. Getting started 3. Overview 4. Manage Folder 4.1. Create an object in Folder 4.2. Rename folder 4.3. Delete Folder 4.4. Clone a folder 4.5. Lock a folder 5. Manage Metadata 5.1. Add/Delete metadata items 5.2. Upload metadata 5.3. Customize view of the entity instance table 5.4. Launch a run configuration on metadata 5.5. Download data from external resources to the cloud data storage 6. Manage Pipeline 6.1. Create and configure pipeline 6.1.1 Building WDL pipeline with graphical PipelineBuilder 6.2. Launch a pipeline 7. Manage Detached configuration 7.1. Create and customize Detached configuration 7.2. Launch Detached Configuration 7.3. Expansion Expressions 7.4. Remove Detached configuration 8. Manage Data Storage 8.1. Create and edit storage 8.2. Upload/Download data 8.3. Create and Edit text files 8.4. Control File versions 8.5. Delete and unregister Data Storage 8.6. Delete Files and Folders from Storage 8.7. Create shared file system 8.8. Data sharing 9. Manage Cluster nodes 10. Manage Tools 10.1. Add/Edit a Docker registry 10.2. Add/Edit a Tool group 10.3. Add a Tool 10.4. Edit a Tool 10.5. Launch a Tool 10.6. Tool security check 10.7. Tool version menu 11. Manage Runs 11.1. Pause/resume Runs 11.2. Auto-commit Docker image 11.3. Sharing with other users or groups of users 12. Manage Settings 12.1. Add a new system event 12.2. Edit a system event 12.3. Create a new user 12.4. Edit/delete a user 12.5. Create a group 12.6. Edit a group/role 12.7. Delete a group 12.8. Change a set of roles/groups for a user 12.9. Change email notification 12.10. Manage system-level settings 13. Permissions 14. Command-line interface (CLI) 14.1. Install and setup CLI 14.2. View and manage Attributes via CLI 14.3. Manage Storage via CLI 14.4. View and manage Permissions via CLI 15. Interactive services 15.1 Starting an Interactive application 15.2 Using Terminal access 16. Issues 17. CP objects tagging by additional attributes 18. Home page Appendix A. EC2 Instance and Docker container lifecycles Appendix B. Working with a Project 1. Quick start This chapter will give you a basic knowledge of pipeline running procedure. 2. Getting started Find Thesaurus, list of supported browsers and authentication details. 3. Overview Get familiar with a Cloud Pipeline Graphical User Interface (GUI). 4. Manage Folder Learn how you can create your own hierarchical structured space. 5. Manage Metadata Learn how you can create a complex analysis environment using custom data entities. 6. Manage Pipeline Get details on pipeline creation, configuration, and launching. Get descriptions of \"Launch pipeline\" page. There is also a description of graphical \"Pipeline builder\" for WDL pipelines. 7. Manage Detached configuration Learn how to run a cluster of differently configurated instances. \"Launch cluster\" box vs LaunchofClusterconfiguration. 8. Manage Data Storage Create a new storage and learn how to manage it: download and upload, copy, move data, delete and unregister storages. 9. Manage Cluster Nodes Learn about active nodes, how to terminate and refresh them. Get familiar with a related dashboard. 10. Manage Tools Add and edit Docker registry. Learn how to add, modify and launch Tools. 11. Manage Runs Learn about Runs space and what information you can get from it. 12. Manage Settings Here you can find information about: how to install CLI, add a new system event, managing roles. 13. Permissions Learn how to manage permissions for Cloud Pipeline objects. 14. Command-line interface (CLI) Get familiar with a Cloud Pipeline Command Line Interface (CLI). 15. Interactive services Learn how to setup non-batch application in a cloud infrastructure and access it via a web interface. 16. Issues Learn how to share results with other users or get feedback. 17. CP objects tagging by additional attributes Learn how to manage custom sets of \"key-values\" attributes for data storage and files. 18. Home page Get details about homepage widgets, how to configure homepage view. Appendix A. EC2 Instance and Docker container lifecycles Learn basics about EC2 Instance and Docker container lifecycle. Appendix B. Working with a Project Learn basics about working with a Project.","title":"Contents"},{"location":"manual/Cloud_Pipeline_-_Manual/#cloud-pipeline-manual","text":"1. Quick start 2. Getting started 3. Overview 4. Manage Folder 4.1. Create an object in Folder 4.2. Rename folder 4.3. Delete Folder 4.4. Clone a folder 4.5. Lock a folder 5. Manage Metadata 5.1. Add/Delete metadata items 5.2. Upload metadata 5.3. Customize view of the entity instance table 5.4. Launch a run configuration on metadata 5.5. Download data from external resources to the cloud data storage 6. Manage Pipeline 6.1. Create and configure pipeline 6.1.1 Building WDL pipeline with graphical PipelineBuilder 6.2. Launch a pipeline 7. Manage Detached configuration 7.1. Create and customize Detached configuration 7.2. Launch Detached Configuration 7.3. Expansion Expressions 7.4. Remove Detached configuration 8. Manage Data Storage 8.1. Create and edit storage 8.2. Upload/Download data 8.3. Create and Edit text files 8.4. Control File versions 8.5. Delete and unregister Data Storage 8.6. Delete Files and Folders from Storage 8.7. Create shared file system 8.8. Data sharing 9. Manage Cluster nodes 10. Manage Tools 10.1. Add/Edit a Docker registry 10.2. Add/Edit a Tool group 10.3. Add a Tool 10.4. Edit a Tool 10.5. Launch a Tool 10.6. Tool security check 10.7. Tool version menu 11. Manage Runs 11.1. Pause/resume Runs 11.2. Auto-commit Docker image 11.3. Sharing with other users or groups of users 12. Manage Settings 12.1. Add a new system event 12.2. Edit a system event 12.3. Create a new user 12.4. Edit/delete a user 12.5. Create a group 12.6. Edit a group/role 12.7. Delete a group 12.8. Change a set of roles/groups for a user 12.9. Change email notification 12.10. Manage system-level settings 13. Permissions 14. Command-line interface (CLI) 14.1. Install and setup CLI 14.2. View and manage Attributes via CLI 14.3. Manage Storage via CLI 14.4. View and manage Permissions via CLI 15. Interactive services 15.1 Starting an Interactive application 15.2 Using Terminal access 16. Issues 17. CP objects tagging by additional attributes 18. Home page Appendix A. EC2 Instance and Docker container lifecycles Appendix B. Working with a Project 1. Quick start This chapter will give you a basic knowledge of pipeline running procedure. 2. Getting started Find Thesaurus, list of supported browsers and authentication details. 3. Overview Get familiar with a Cloud Pipeline Graphical User Interface (GUI). 4. Manage Folder Learn how you can create your own hierarchical structured space. 5. Manage Metadata Learn how you can create a complex analysis environment using custom data entities. 6. Manage Pipeline Get details on pipeline creation, configuration, and launching. Get descriptions of \"Launch pipeline\" page. There is also a description of graphical \"Pipeline builder\" for WDL pipelines. 7. Manage Detached configuration Learn how to run a cluster of differently configurated instances. \"Launch cluster\" box vs LaunchofClusterconfiguration. 8. Manage Data Storage Create a new storage and learn how to manage it: download and upload, copy, move data, delete and unregister storages. 9. Manage Cluster Nodes Learn about active nodes, how to terminate and refresh them. Get familiar with a related dashboard. 10. Manage Tools Add and edit Docker registry. Learn how to add, modify and launch Tools. 11. Manage Runs Learn about Runs space and what information you can get from it. 12. Manage Settings Here you can find information about: how to install CLI, add a new system event, managing roles. 13. Permissions Learn how to manage permissions for Cloud Pipeline objects. 14. Command-line interface (CLI) Get familiar with a Cloud Pipeline Command Line Interface (CLI). 15. Interactive services Learn how to setup non-batch application in a cloud infrastructure and access it via a web interface. 16. Issues Learn how to share results with other users or get feedback. 17. CP objects tagging by additional attributes Learn how to manage custom sets of \"key-values\" attributes for data storage and files. 18. Home page Get details about homepage widgets, how to configure homepage view. Appendix A. EC2 Instance and Docker container lifecycles Learn basics about EC2 Instance and Docker container lifecycle. Appendix B. Working with a Project Learn basics about working with a Project.","title":"Cloud Pipeline - Manual"},{"location":"manual/01_Quick_start/1._Quick_start/","text":"1. Quick start To launch a Pipeline you need to have EXECUTE permissions for that Pipeline . For more information see 13. Permissions . This quick start will help you to get a general idea of data processing via the Cloud Pipeline platform. Pipelines are set of data processing steps that are dependent on each other. Here we will describe an example of a pipeline launch. Note : Pipeline launch steps remain the same for different pipelines. Pipeline running procedure for basic users The first step you need to perform is to upload the data that you will work with. Generally, data in the Cloud Pipeline is kept in data storages . So, the first thing you need to do is to find a data storage in the Library tab on the left side of the screen. Then upload data to it. In this example, we will choose the data storage named \" INPUT \" ( 1 ) and then create a folder ( 2 ) named \" gromacs_benchmark \" ( 3 ). Then we will navigate to this folder by clicking on its name and click \" Upload \" button to upload your input data to the storage. Note : make sure that the size of data doesn't exceed 5 Gb. To upload more than 5 Gb you shall use CLI (see details here ). Then decide where to put results of pipeline processing. A good practice is to not mix input and output data in a one data storage, so we will use another data storage for output data. In this example, we will use data storage named \" ANALYSIS \" to store pipeline output data . In the \" Search \" field of the Library tab, find a pipeline that will process the input data . In this example, we will use the \" Gromacs \" pipeline that performs molecular modeling. Choose the pipeline version and click the Run button. On the \" Launch a pipeline \" page you'll see pipeline execution parameters . On the bottom, you'll see a \" Parameter \" section with the input parameter and output parameter. Here you can specify the paths to your folders in storages, which you've created in step 1 and step 2 . Click the controls to choose your folders. Check your folder in the list and click \"OK\" . Tip: If you want to create a new folder for the analysis, you can put /$RUN_ID at the end of the output string. The system will create a new folder with Run ID name. Click the Launch button to run the pipeline. You'll be redirected to the Runs tab of the CP . Here you can find launched \"Gromacs\" pipeline. Click a run with the pipeline name to see run logs . When all tasks of pipeline finish their execution, you'll be able to find the pipeline run in the Completed Runs section of the Runs tab. To see the output data, you shall navigate to the data storage with output data - ANALYSIS - and find the results of the pipeline run.","title":"1. Quick start"},{"location":"manual/01_Quick_start/1._Quick_start/#1-quick-start","text":"To launch a Pipeline you need to have EXECUTE permissions for that Pipeline . For more information see 13. Permissions . This quick start will help you to get a general idea of data processing via the Cloud Pipeline platform. Pipelines are set of data processing steps that are dependent on each other. Here we will describe an example of a pipeline launch. Note : Pipeline launch steps remain the same for different pipelines.","title":"1. Quick start"},{"location":"manual/01_Quick_start/1._Quick_start/#pipeline-running-procedure-for-basic-users","text":"The first step you need to perform is to upload the data that you will work with. Generally, data in the Cloud Pipeline is kept in data storages . So, the first thing you need to do is to find a data storage in the Library tab on the left side of the screen. Then upload data to it. In this example, we will choose the data storage named \" INPUT \" ( 1 ) and then create a folder ( 2 ) named \" gromacs_benchmark \" ( 3 ). Then we will navigate to this folder by clicking on its name and click \" Upload \" button to upload your input data to the storage. Note : make sure that the size of data doesn't exceed 5 Gb. To upload more than 5 Gb you shall use CLI (see details here ). Then decide where to put results of pipeline processing. A good practice is to not mix input and output data in a one data storage, so we will use another data storage for output data. In this example, we will use data storage named \" ANALYSIS \" to store pipeline output data . In the \" Search \" field of the Library tab, find a pipeline that will process the input data . In this example, we will use the \" Gromacs \" pipeline that performs molecular modeling. Choose the pipeline version and click the Run button. On the \" Launch a pipeline \" page you'll see pipeline execution parameters . On the bottom, you'll see a \" Parameter \" section with the input parameter and output parameter. Here you can specify the paths to your folders in storages, which you've created in step 1 and step 2 . Click the controls to choose your folders. Check your folder in the list and click \"OK\" . Tip: If you want to create a new folder for the analysis, you can put /$RUN_ID at the end of the output string. The system will create a new folder with Run ID name. Click the Launch button to run the pipeline. You'll be redirected to the Runs tab of the CP . Here you can find launched \"Gromacs\" pipeline. Click a run with the pipeline name to see run logs . When all tasks of pipeline finish their execution, you'll be able to find the pipeline run in the Completed Runs section of the Runs tab. To see the output data, you shall navigate to the data storage with output data - ANALYSIS - and find the results of the pipeline run.","title":"Pipeline running procedure for basic users"},{"location":"manual/02_Getting_started/2._Getting_started/","text":"2. Getting started Thesaurus Objects - Cloud Pipeline (CP) entities such as Folder, Pipeline, Data Storage, Run configuration, Cluster node, Docker Registry, Tool group, Tool. Folder , or directory - CP object. It is an entity similar to the directories in the file system. Folders are used to structure other CP objects. Project - a special type of Folder. It might be used to organize data and metadata (see below) and simplify analysis runs for a large data set. Pipeline - CP object. Represents a workflow script with versioned source code, documentation, and configuration. Under the hood, it is a git repository. Data Storage - CP object. It is an S3 bucket (see below) representation in a folder hierarchy. NFS Storage - a data storage based on NFS (Network File System). It is a distributed file system that can be used by several nodes during High-performance computing jobs. Docker Registry - CP object. It is a Docker registry representation in the CP. Docker registry stores docker images. For more details refer to https://docs.docker.com/registry/ . Tool - CP object. It is a Docker image representation in CP. Tool group - CP object. It allows organizing different Tools into groups. Detached configuration - Run configuration that is not attached to any particular pipeline. Run configuration - CP object specifying parameters of the run: what pipeline or tool, type of instance to run with what parameters. There are Run Configurations for Detached configuration and for Pipeline configuration. History - CP object. It shows all runs that are related to a project. Cluster configuration - CP object. It is Run Configuration of Detached Configuration. Represents a set of pipelines or tools that run as one cluster. Also is used to associate pipeline parameters with some Metadata Entity. Metadata - CP object that defines custom data entities associated with raw data files (fastq, bcl, etc.) or data parameters. Batch job - an automated job that doesn't require interaction with a human. Interactive application - type of an application that requires an interaction with a human to achieve certain results. Docker image - a stand-alone, executable package that includes everything needed to run a piece of software. Containerized software will always run the same, regardless of the environment. Docker container - a runtime instance of docker image - what the image turns into when actually executed. Run - Executed \"Pipeline\" or \"Tool\" object, contains log information and parameters of an execution process. S3 bucket , or bucket - Amazon cloud data storage. For more details please refer to https://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html . EC2 instance , or just instance - virtual computing environment. For more details refer to https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html . On-Demand Instances \u2013 \"Stable\" type of instances that can't be overbought while in use. Spot Instances \u2013 EC2 instances, which has significantly lower costs. This type of instances are rented and can be overbought and shut down by Amazon. EC2 instances may have any type (number of CPU cores, processor type, amount of RAM and disk space, etc.) that describes optimization and available features for the instance. Cluster node (Calculation node) - EC2 instance used for a \"Run\" object. Task - a discrete Pipeline step. Attributes - data that provides information about other data. CP allows creating a custom metadata set of \"key=values\" (attributes) for its objects. It helps to maintain traceability and use this information for search queries. Cloud Pipeline CLI - command line interface to the CP. It allows interaction with CP via command line instead of GUI. STS (Short-Term Storage) - S3 buckets (in case of Amazon) that are used for frequently accessed files. LTS (Long-Term Storage) - Amazon Glacier archive (in case of Amazon) that is for infrequently accessed files. Access to data becomes more complicated but it's cheaper. Pipeline template - it is a template structure for creating a pipeline. Typically it includes source code folder structure with the executable script, doc file, and configuration file. Token - authentication key to getting access to an application. Endpoint - link to access a launched application. Pipeline version - A particular version of pipeline source code, documentation and configuration. Log - automatically produced and time-stamped documentation of events relevant to a particular system. Cluster - a collection of instances which are connected so that they can be used together on a task. Access Control List - a list of pairs of attributes: user id and permissions: read/write/execute. Supported Browsers The following web-browsers are supported at the moment: Google Chrome (best option) Firefox Microsoft EDGE Internet Explorer 11 Authentication SAML Authentication protocol is currently used in a Cloud Pipeline.","title":"2. Getting started"},{"location":"manual/02_Getting_started/2._Getting_started/#2-getting-started","text":"","title":"2. Getting started"},{"location":"manual/02_Getting_started/2._Getting_started/#thesaurus","text":"Objects - Cloud Pipeline (CP) entities such as Folder, Pipeline, Data Storage, Run configuration, Cluster node, Docker Registry, Tool group, Tool. Folder , or directory - CP object. It is an entity similar to the directories in the file system. Folders are used to structure other CP objects. Project - a special type of Folder. It might be used to organize data and metadata (see below) and simplify analysis runs for a large data set. Pipeline - CP object. Represents a workflow script with versioned source code, documentation, and configuration. Under the hood, it is a git repository. Data Storage - CP object. It is an S3 bucket (see below) representation in a folder hierarchy. NFS Storage - a data storage based on NFS (Network File System). It is a distributed file system that can be used by several nodes during High-performance computing jobs. Docker Registry - CP object. It is a Docker registry representation in the CP. Docker registry stores docker images. For more details refer to https://docs.docker.com/registry/ . Tool - CP object. It is a Docker image representation in CP. Tool group - CP object. It allows organizing different Tools into groups. Detached configuration - Run configuration that is not attached to any particular pipeline. Run configuration - CP object specifying parameters of the run: what pipeline or tool, type of instance to run with what parameters. There are Run Configurations for Detached configuration and for Pipeline configuration. History - CP object. It shows all runs that are related to a project. Cluster configuration - CP object. It is Run Configuration of Detached Configuration. Represents a set of pipelines or tools that run as one cluster. Also is used to associate pipeline parameters with some Metadata Entity. Metadata - CP object that defines custom data entities associated with raw data files (fastq, bcl, etc.) or data parameters. Batch job - an automated job that doesn't require interaction with a human. Interactive application - type of an application that requires an interaction with a human to achieve certain results. Docker image - a stand-alone, executable package that includes everything needed to run a piece of software. Containerized software will always run the same, regardless of the environment. Docker container - a runtime instance of docker image - what the image turns into when actually executed. Run - Executed \"Pipeline\" or \"Tool\" object, contains log information and parameters of an execution process. S3 bucket , or bucket - Amazon cloud data storage. For more details please refer to https://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html . EC2 instance , or just instance - virtual computing environment. For more details refer to https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html . On-Demand Instances \u2013 \"Stable\" type of instances that can't be overbought while in use. Spot Instances \u2013 EC2 instances, which has significantly lower costs. This type of instances are rented and can be overbought and shut down by Amazon. EC2 instances may have any type (number of CPU cores, processor type, amount of RAM and disk space, etc.) that describes optimization and available features for the instance. Cluster node (Calculation node) - EC2 instance used for a \"Run\" object. Task - a discrete Pipeline step. Attributes - data that provides information about other data. CP allows creating a custom metadata set of \"key=values\" (attributes) for its objects. It helps to maintain traceability and use this information for search queries. Cloud Pipeline CLI - command line interface to the CP. It allows interaction with CP via command line instead of GUI. STS (Short-Term Storage) - S3 buckets (in case of Amazon) that are used for frequently accessed files. LTS (Long-Term Storage) - Amazon Glacier archive (in case of Amazon) that is for infrequently accessed files. Access to data becomes more complicated but it's cheaper. Pipeline template - it is a template structure for creating a pipeline. Typically it includes source code folder structure with the executable script, doc file, and configuration file. Token - authentication key to getting access to an application. Endpoint - link to access a launched application. Pipeline version - A particular version of pipeline source code, documentation and configuration. Log - automatically produced and time-stamped documentation of events relevant to a particular system. Cluster - a collection of instances which are connected so that they can be used together on a task. Access Control List - a list of pairs of attributes: user id and permissions: read/write/execute.","title":"Thesaurus"},{"location":"manual/02_Getting_started/2._Getting_started/#supported-browsers","text":"The following web-browsers are supported at the moment: Google Chrome (best option) Firefox Microsoft EDGE Internet Explorer 11","title":"Supported Browsers"},{"location":"manual/02_Getting_started/2._Getting_started/#authentication","text":"SAML Authentication protocol is currently used in a Cloud Pipeline.","title":"Authentication"},{"location":"manual/03_Overview/3._Overview/","text":"3. Overview User Journey in a nutshell GUI menu tab bar Home Library Cluster Nodes Tools Runs Settings Search Logout User Journey in a nutshell Cloud Pipeline (CP) is a cloud-based web application which allows users to solve a wide range of analytical tasks and includes: Data processing : you can create data processing pipelines and run them in the cloud in an automated way. Data storing : create your data storage, download or upload data from it or edit text files within CP UI. File version control is supported. Tool management : you can create and deploy your own calculation environment using Docker's container concept. This Manual is mostly around data processing lifecycle which, in a nutshell, can be described in these several steps: To run a user's calculation script it shall be registered in CP as a pipeline . The script could be created in CP environment or uploaded from the local machine. See more details 6. Manage Pipeline . Note : If you need to run a pipeline in different environments simultaneously or set a specific type of data, you can use detach configuration object. See more 7. Manage Detached configuration . To store pipeline's inputs and outputs data files the Data Storage shall be determined in CP . Learn more 8. Manage Data Storage . Almost every pipeline requires a specific package of software to run it, which is defined in a docker image. So when a user starts a pipeline, CP starts a new cloud instance (nodes) and runs a docker image at it. See more details 9. Manage Cluster nodes and 10. Manage Tools . When the environment is set, pipeline starts execution. A user in CP can change and save configurations of the run. Learn more 6.2. Launch a pipeline . A user can monitor the status of active and completed runs and usage of active instances in CP . Learn more 11. Manage Runs and 9. Manage Cluster nodes . Note : CP can run a docker image at instance without any pipeline at all if needed. There just will be an instance with some installed and running software. A user can SSH to it or use it in the interactive mode. Note : CP uses Amazon's products to store data and process it by default: Amazon AWS buckets, Amazon EC2. Also, CP supports CLI, which duplicates some of GUI features and has extra features unavailable via GUI, such as automation of interaction with CP during the pipeline script running, or uploading considerable amount of data (more than 5 Gb), etc. The basics of CLI you can learn here . GUI menu tab bar There are several menu tabs at the left edge of CP window. Home Home space is opened by default when Cloud Pipeline is loaded. It provides \"personal\" and often-used information for a user. By default, this dashboard shows 3 widgets: Active Runs (see picture below, 1 ) displays a list of user's active runs. See more details 11. Manage Runs . Note : press Explore all active runs to see a list of all runs. Tools (see picture below, 2 ) shows a list of Tools in a user's personal repository and Tools available to your group. Learn more 10. Manage Tools and 13. Permissions . Note : group-level Tools will be shown on the top of the Tools list. Note : user can search for a particular Tool by using the Search tools text box. System will suggest Tools based on a user's query. Data (see picture below, 3 ) displays available Data Storages for a user - user shall have OWNER rights or READ/WRITE access to a Data Storage. See more 8. Manage Data Storage and 13. Permissions . Note : personal Data storages (i.e. a user is an OWNER of this Storage) will be shown on top, WRITE - second priority, READ - third priority. Each widget has the Help icon . Hover this icon to get a brief description of a widget. Learn more about Home tab 18. Home page . Library Library space supports a hierarchical view of its content: Pipelines and its versions Data Storages Folders Configurations Metadata objects . The tab consists of two panels: \"Hierarchy\" view (see the picture below, 2 ) displays a hierarchical-structured list of pipelines, folders, storages and machine configurations, etc. Use the \" Search \" field to find a CP object by a name. \" Collapse/Expand \" button (see the picture below, 1 ) at the bottom-left corner of the screen: use it to collapse or expand \"Hierarchy\" view. \"Details\" view (see the picture below, 3 ) shows details of a selected item in hierarchy panel. Depends on a selected type of object it has a very different view. You can learn about each on respective pages of the manual. Note : Each time \"Hierarchy\" view loads, the first Folder in the hierarchy that has more than 1 child object (Folder, Pipeline, Data Storage, etc) is automatically selected. Contents of that Folder are automatically expanded. Cluster Nodes This space provides a list of working nodes. You can get information on their usage and terminate them in this tab. See more details 9. Manage Cluster nodes . Tools Tools space displays available Docker images and their tools, organized in tool groups and docker registries. You can edit information about them and run nodes with any Docker image in this tab. See more details 10. Manage Tools . Runs This space helps you monitor the state of your run instances. You can get parameters and logs of a specific run, stop a run, rerun a completed run. Learn more 11. Manage Runs . Settings This tab opens a Settings window which allows: generate a CLI installation and configuration commands to set CLI for CP , manage system events notifications, manage roles and permissions. See more details 12. Manage Settings . Search It's a search field which allows searching specific runs by its \"Run ID\" or \"Parameters\". Logout This is a Logout button which logs you out. Note : if automatic logging-in is configured, you will be re-logged at once. Note : if any changes occur in the CP application during an authorized session, the changes are applied after re-login.","title":"3. Overview"},{"location":"manual/03_Overview/3._Overview/#3-overview","text":"User Journey in a nutshell GUI menu tab bar Home Library Cluster Nodes Tools Runs Settings Search Logout","title":"3. Overview"},{"location":"manual/03_Overview/3._Overview/#user-journey-in-a-nutshell","text":"Cloud Pipeline (CP) is a cloud-based web application which allows users to solve a wide range of analytical tasks and includes: Data processing : you can create data processing pipelines and run them in the cloud in an automated way. Data storing : create your data storage, download or upload data from it or edit text files within CP UI. File version control is supported. Tool management : you can create and deploy your own calculation environment using Docker's container concept. This Manual is mostly around data processing lifecycle which, in a nutshell, can be described in these several steps: To run a user's calculation script it shall be registered in CP as a pipeline . The script could be created in CP environment or uploaded from the local machine. See more details 6. Manage Pipeline . Note : If you need to run a pipeline in different environments simultaneously or set a specific type of data, you can use detach configuration object. See more 7. Manage Detached configuration . To store pipeline's inputs and outputs data files the Data Storage shall be determined in CP . Learn more 8. Manage Data Storage . Almost every pipeline requires a specific package of software to run it, which is defined in a docker image. So when a user starts a pipeline, CP starts a new cloud instance (nodes) and runs a docker image at it. See more details 9. Manage Cluster nodes and 10. Manage Tools . When the environment is set, pipeline starts execution. A user in CP can change and save configurations of the run. Learn more 6.2. Launch a pipeline . A user can monitor the status of active and completed runs and usage of active instances in CP . Learn more 11. Manage Runs and 9. Manage Cluster nodes . Note : CP can run a docker image at instance without any pipeline at all if needed. There just will be an instance with some installed and running software. A user can SSH to it or use it in the interactive mode. Note : CP uses Amazon's products to store data and process it by default: Amazon AWS buckets, Amazon EC2. Also, CP supports CLI, which duplicates some of GUI features and has extra features unavailable via GUI, such as automation of interaction with CP during the pipeline script running, or uploading considerable amount of data (more than 5 Gb), etc. The basics of CLI you can learn here .","title":"User Journey in a nutshell"},{"location":"manual/03_Overview/3._Overview/#gui-menu-tab-bar","text":"There are several menu tabs at the left edge of CP window.","title":"GUI menu tab bar"},{"location":"manual/03_Overview/3._Overview/#home","text":"Home space is opened by default when Cloud Pipeline is loaded. It provides \"personal\" and often-used information for a user. By default, this dashboard shows 3 widgets: Active Runs (see picture below, 1 ) displays a list of user's active runs. See more details 11. Manage Runs . Note : press Explore all active runs to see a list of all runs. Tools (see picture below, 2 ) shows a list of Tools in a user's personal repository and Tools available to your group. Learn more 10. Manage Tools and 13. Permissions . Note : group-level Tools will be shown on the top of the Tools list. Note : user can search for a particular Tool by using the Search tools text box. System will suggest Tools based on a user's query. Data (see picture below, 3 ) displays available Data Storages for a user - user shall have OWNER rights or READ/WRITE access to a Data Storage. See more 8. Manage Data Storage and 13. Permissions . Note : personal Data storages (i.e. a user is an OWNER of this Storage) will be shown on top, WRITE - second priority, READ - third priority. Each widget has the Help icon . Hover this icon to get a brief description of a widget. Learn more about Home tab 18. Home page .","title":"Home"},{"location":"manual/03_Overview/3._Overview/#library","text":"Library space supports a hierarchical view of its content: Pipelines and its versions Data Storages Folders Configurations Metadata objects . The tab consists of two panels: \"Hierarchy\" view (see the picture below, 2 ) displays a hierarchical-structured list of pipelines, folders, storages and machine configurations, etc. Use the \" Search \" field to find a CP object by a name. \" Collapse/Expand \" button (see the picture below, 1 ) at the bottom-left corner of the screen: use it to collapse or expand \"Hierarchy\" view. \"Details\" view (see the picture below, 3 ) shows details of a selected item in hierarchy panel. Depends on a selected type of object it has a very different view. You can learn about each on respective pages of the manual. Note : Each time \"Hierarchy\" view loads, the first Folder in the hierarchy that has more than 1 child object (Folder, Pipeline, Data Storage, etc) is automatically selected. Contents of that Folder are automatically expanded.","title":"Library"},{"location":"manual/03_Overview/3._Overview/#cluster-nodes","text":"This space provides a list of working nodes. You can get information on their usage and terminate them in this tab. See more details 9. Manage Cluster nodes .","title":"Cluster Nodes"},{"location":"manual/03_Overview/3._Overview/#tools","text":"Tools space displays available Docker images and their tools, organized in tool groups and docker registries. You can edit information about them and run nodes with any Docker image in this tab. See more details 10. Manage Tools .","title":"Tools"},{"location":"manual/03_Overview/3._Overview/#runs","text":"This space helps you monitor the state of your run instances. You can get parameters and logs of a specific run, stop a run, rerun a completed run. Learn more 11. Manage Runs .","title":"Runs"},{"location":"manual/03_Overview/3._Overview/#settings","text":"This tab opens a Settings window which allows: generate a CLI installation and configuration commands to set CLI for CP , manage system events notifications, manage roles and permissions. See more details 12. Manage Settings .","title":"Settings"},{"location":"manual/03_Overview/3._Overview/#search","text":"It's a search field which allows searching specific runs by its \"Run ID\" or \"Parameters\".","title":"Search"},{"location":"manual/03_Overview/3._Overview/#logout","text":"This is a Logout button which logs you out. Note : if automatic logging-in is configured, you will be re-logged at once. Note : if any changes occur in the CP application during an authorized session, the changes are applied after re-login.","title":"Logout"},{"location":"manual/04_Manage_Folder/4.1._Create_an_object_in_Folder/","text":"4.1. Create an object in Folder How to create Example: Create Folder To create new object in the Folder you need WRITE permissions for that folder and an appropriate ROLE, e.g. to create new folder you need to have FOLDER_MANAGER role. For more information see 13. Permissions . You can create Pipelines , Storages , Detached configurations and Folders in a Folder. How to create Navigate to the Folder . Click \" Create \". Choose a type of the object to be created from a drop-down list. Set properties of the object (such as a name for the object). See Create pipeline , Create storage , Create Detached Configuration pages. Click OK . Example: Create Folder Navigate to a folder. Click Create . Choose Folder from the drop-down list. Enter the name of the folder. Note : Folder name must be unique in its hierarchical layer. Click OK .","title":"4.1 Create an object in Folder"},{"location":"manual/04_Manage_Folder/4.1._Create_an_object_in_Folder/#41-create-an-object-in-folder","text":"How to create Example: Create Folder To create new object in the Folder you need WRITE permissions for that folder and an appropriate ROLE, e.g. to create new folder you need to have FOLDER_MANAGER role. For more information see 13. Permissions . You can create Pipelines , Storages , Detached configurations and Folders in a Folder.","title":"4.1. Create an object in Folder"},{"location":"manual/04_Manage_Folder/4.1._Create_an_object_in_Folder/#how-to-create","text":"Navigate to the Folder . Click \" Create \". Choose a type of the object to be created from a drop-down list. Set properties of the object (such as a name for the object). See Create pipeline , Create storage , Create Detached Configuration pages. Click OK .","title":"How to create"},{"location":"manual/04_Manage_Folder/4.1._Create_an_object_in_Folder/#example-create-folder","text":"Navigate to a folder. Click Create . Choose Folder from the drop-down list. Enter the name of the folder. Note : Folder name must be unique in its hierarchical layer. Click OK .","title":"Example: Create Folder"},{"location":"manual/04_Manage_Folder/4.2._Rename_folder/","text":"4.2. Rename folder To edit name of a Folder you need WRITE permission for that folder. For more information see 13. Permissions . You can rename a Folder . To do that: Navigate to a Folder . Click icon and choose Edit folder - the settings window will open. Enter the new name. Note : Folder name must be unique in its hierarchical layer. Click OK . Note : another way to rename folder is: Hover over the title of the folder - the editing icon will appear. Click on the highlight field - it's switched to edit-mode. Type a name. Click out of the active field - the name will be saved.","title":"4.2 Rename Folder"},{"location":"manual/04_Manage_Folder/4.2._Rename_folder/#42-rename-folder","text":"To edit name of a Folder you need WRITE permission for that folder. For more information see 13. Permissions . You can rename a Folder . To do that: Navigate to a Folder . Click icon and choose Edit folder - the settings window will open. Enter the new name. Note : Folder name must be unique in its hierarchical layer. Click OK . Note : another way to rename folder is: Hover over the title of the folder - the editing icon will appear. Click on the highlight field - it's switched to edit-mode. Type a name. Click out of the active field - the name will be saved.","title":"4.2. Rename folder"},{"location":"manual/04_Manage_Folder/4.3._Delete_Folder/","text":"4.3. Delete Folder To delete a Folder you need WRITE permissions for that folder and a FOLDER_MANAGER role. For more information see 13. Permissions . Note : If the folder is not empty it will not be deleted. If a folder contains only metadata, the folder will be deleted. Navigate to the folder contains the folder to delete. Click the icon in the line with desired object to delete. To delete current (selected folder): Click button. Click Delete . Click OK .","title":"4.3 Delete Folder"},{"location":"manual/04_Manage_Folder/4.3._Delete_Folder/#43-delete-folder","text":"To delete a Folder you need WRITE permissions for that folder and a FOLDER_MANAGER role. For more information see 13. Permissions . Note : If the folder is not empty it will not be deleted. If a folder contains only metadata, the folder will be deleted. Navigate to the folder contains the folder to delete. Click the icon in the line with desired object to delete. To delete current (selected folder): Click button. Click Delete . Click OK .","title":"4.3. Delete Folder"},{"location":"manual/04_Manage_Folder/4.4._Clone_a_folder/","text":"4.4. Clone a folder This feature allows a user to clone any folder to a specific destination: to user's personal folder, or user's project. It would be helpful to create a new project way faster due to copying metadata, configurations, storages from another project. To copy a folder, you need READ permissions for the copied folder and WRITE permissions for a folder selected as a destination. For more information see 13. Permissions . Note : learn more about metadata here . To clone a folder, the following steps shall be performed: Navigate to the desired folder page. Click icon and choose \"Clone\" menu item. The \"Select destination folder\" will pop-up. Note : The pop-up window will be open with parent folder of a copied folder as a default destination. Choose the destination if it needed by classical navigation actions. Note : The \"Clone\" button ( 1 ) displays which destination is selected at the moment. Name the new clone of the folder ( 2 ). Note : The name shall not break the uniqueness principal: there shouldn't be two folders with the same name in one destination. Click \"Clone\" button - and the folder will be cloned. The page of the clone of the folder will be open automatically. All child of the copied folder will be copied. Note : The exception is pipelines. Pipelines won't be copied as far as it may cause the collision. Note : The storages are copied by creating a new empty one with a unique name and path. No file will be copied from a copied storage to a new one.","title":"4.4 Clone a Folder"},{"location":"manual/04_Manage_Folder/4.4._Clone_a_folder/#44-clone-a-folder","text":"This feature allows a user to clone any folder to a specific destination: to user's personal folder, or user's project. It would be helpful to create a new project way faster due to copying metadata, configurations, storages from another project. To copy a folder, you need READ permissions for the copied folder and WRITE permissions for a folder selected as a destination. For more information see 13. Permissions . Note : learn more about metadata here . To clone a folder, the following steps shall be performed: Navigate to the desired folder page. Click icon and choose \"Clone\" menu item. The \"Select destination folder\" will pop-up. Note : The pop-up window will be open with parent folder of a copied folder as a default destination. Choose the destination if it needed by classical navigation actions. Note : The \"Clone\" button ( 1 ) displays which destination is selected at the moment. Name the new clone of the folder ( 2 ). Note : The name shall not break the uniqueness principal: there shouldn't be two folders with the same name in one destination. Click \"Clone\" button - and the folder will be cloned. The page of the clone of the folder will be open automatically. All child of the copied folder will be copied. Note : The exception is pipelines. Pipelines won't be copied as far as it may cause the collision. Note : The storages are copied by creating a new empty one with a unique name and path. No file will be copied from a copied storage to a new one.","title":"4.4. Clone a folder"},{"location":"manual/04_Manage_Folder/4.5._Lock_a_folder/","text":"4.5. Lock a folder To lock or unlock a folder, a user shall have OWNER permissions. For more permissions details see 13. Permissions . If you want to save your folder from the changes, you may lock it. Other users won't be able to make any changes in a locked folder and its children. To lock a folder the following steps shall be performed: Navigate to a folder you want to lock. Click icon \u2192 Lock . Confirm you decision in the dialog window. The folder will be locked and marked with a specific symbol . To unlock the folder, you just need to click icon \u2192 Unlock and confirm it in the dialog box.","title":"4.5 Lock a Folder"},{"location":"manual/04_Manage_Folder/4.5._Lock_a_folder/#45-lock-a-folder","text":"To lock or unlock a folder, a user shall have OWNER permissions. For more permissions details see 13. Permissions . If you want to save your folder from the changes, you may lock it. Other users won't be able to make any changes in a locked folder and its children. To lock a folder the following steps shall be performed: Navigate to a folder you want to lock. Click icon \u2192 Lock . Confirm you decision in the dialog window. The folder will be locked and marked with a specific symbol . To unlock the folder, you just need to click icon \u2192 Unlock and confirm it in the dialog box.","title":"4.5. Lock a folder"},{"location":"manual/04_Manage_Folder/4._Manage_Folder/","text":"4. Manage Folder \"Details\" view Controls Upload metadata + Create \"Displays\" icon \"Gear\" icon Child objects controls One of the key objects represented in the \" Library \" space is Folder . It is an entity similar to the directories in the file system. Folders are used to structure other CP objects. Folders can be arranged into a tree like a file system, which helps to store information divided by departments, users, programs or other logical groups. Note : there is a special type of Folder - a Project . For details see here . \"Details\" view \" Details \" panel shows contents of the selected folder: subfolders, files, pipelines etc. Controls The following buttons are available at the top of the \" Details \" view of the folder: Upload metadata \" Upload metadata \" ( 1 ) control allows a user to upload new metadata entities (e.g. Samples, Participants) from the .csv/.tsv/.tdf files in a project or another folder selected by a user. It helps to organize and manage metadata in a user's workspace (project or some sandbox folder for collecting metadata). Learn more 5.2. Upload metadata . + Create A user can add: a CP objects to the selected folder with the Create button ( 2 ). See 4.1. Create an object in Folder ; create a CP object from a template. See Appendix B. Working with a Project . Note : The list of templates could be extended. \"Displays\" icon The control (3) includes options to change view of the page: Feature Description Descriptions This feature makes visible addition description for child objects: child objects' attributes description in creating form if it exists. Attributes Attributes control opens/closes attributes pane. Folder's attributes - keys (a) and values (b) will be represented in the metadata pane on the right: Note : If a selected folder has any defined attribute, Attributes pane is shown by default. See how to edit attributes here . Issues This feature shows/hides the issues of the current folder to discuss. To learn more see here \"Gear\" icon There are the following features in \"Gear\" (4) icon: Feature Description Edit folder Rename a folder and set permissions for the folder. See 4.2. Rename folder and 13. Permissions . Clone This feature helps to copy the current folder and its child objects. See more 4.4. Clone a folder . Lock/Unlock You can save your folder and its content from changes by locking it. Learn more 4.5. Lock a folder . Delete A user can delete a folder with the Delete icon. A folder will be deleted if metadata is stored only. See 4.3. Delete Folder . Child objects controls Control CP objects Description Delete Folder Delete ( 1 ) a current folder. Learn more 4.3. Delete Folder . Discussion Folder, Pipeline This icon ( 2 ) allows to create discussion threads in child objects: you can create a topic, leave comments and address them to a specific user. To learn more see here . Run Pipeline Allows running a pipeline from the parent folder. Click on the icon ( 3 ) and the Launch form page will be open. See 6.2. Launch a pipeline . Edit Pipeline, Data storage, Run configuration Click this icon ( 4 ) to edit basic CP object's information: name, description, etc.","title":"4.0 Overview"},{"location":"manual/04_Manage_Folder/4._Manage_Folder/#4-manage-folder","text":"\"Details\" view Controls Upload metadata + Create \"Displays\" icon \"Gear\" icon Child objects controls One of the key objects represented in the \" Library \" space is Folder . It is an entity similar to the directories in the file system. Folders are used to structure other CP objects. Folders can be arranged into a tree like a file system, which helps to store information divided by departments, users, programs or other logical groups. Note : there is a special type of Folder - a Project . For details see here .","title":"4. Manage Folder"},{"location":"manual/04_Manage_Folder/4._Manage_Folder/#details-view","text":"\" Details \" panel shows contents of the selected folder: subfolders, files, pipelines etc.","title":"\"Details\"\u00a0view"},{"location":"manual/04_Manage_Folder/4._Manage_Folder/#controls","text":"The following buttons are available at the top of the \" Details \" view of the folder:","title":"Controls"},{"location":"manual/04_Manage_Folder/4._Manage_Folder/#upload-metadata","text":"\" Upload metadata \" ( 1 ) control allows a user to upload new metadata entities (e.g. Samples, Participants) from the .csv/.tsv/.tdf files in a project or another folder selected by a user. It helps to organize and manage metadata in a user's workspace (project or some sandbox folder for collecting metadata). Learn more 5.2. Upload metadata .","title":"Upload metadata"},{"location":"manual/04_Manage_Folder/4._Manage_Folder/#create","text":"A user can add: a CP objects to the selected folder with the Create button ( 2 ). See 4.1. Create an object in Folder ; create a CP object from a template. See Appendix B. Working with a Project . Note : The list of templates could be extended.","title":"+ Create"},{"location":"manual/04_Manage_Folder/4._Manage_Folder/#displays-icon","text":"The control (3) includes options to change view of the page: Feature Description Descriptions This feature makes visible addition description for child objects: child objects' attributes description in creating form if it exists. Attributes Attributes control opens/closes attributes pane. Folder's attributes - keys (a) and values (b) will be represented in the metadata pane on the right: Note : If a selected folder has any defined attribute, Attributes pane is shown by default. See how to edit attributes here . Issues This feature shows/hides the issues of the current folder to discuss. To learn more see here","title":"\"Displays\" icon"},{"location":"manual/04_Manage_Folder/4._Manage_Folder/#gear-icon","text":"There are the following features in \"Gear\" (4) icon: Feature Description Edit folder Rename a folder and set permissions for the folder. See 4.2. Rename folder and 13. Permissions . Clone This feature helps to copy the current folder and its child objects. See more 4.4. Clone a folder . Lock/Unlock You can save your folder and its content from changes by locking it. Learn more 4.5. Lock a folder . Delete A user can delete a folder with the Delete icon. A folder will be deleted if metadata is stored only. See 4.3. Delete Folder .","title":"\"Gear\" icon"},{"location":"manual/04_Manage_Folder/4._Manage_Folder/#child-objects-controls","text":"Control CP objects Description Delete Folder Delete ( 1 ) a current folder. Learn more 4.3. Delete Folder . Discussion Folder, Pipeline This icon ( 2 ) allows to create discussion threads in child objects: you can create a topic, leave comments and address them to a specific user. To learn more see here . Run Pipeline Allows running a pipeline from the parent folder. Click on the icon ( 3 ) and the Launch form page will be open. See 6.2. Launch a pipeline . Edit Pipeline, Data storage, Run configuration Click this icon ( 4 ) to edit basic CP object's information: name, description, etc.","title":"Child objects controls"},{"location":"manual/05_Manage_Metadata/5.1._Add_Delete_metadata_items/","text":"5.1. Add/Delete metadata items To manage metadata items, a user shall have WRITE permission for the parent folder and a role ENTITY_MANAGER . For more information see 13. Permissions . A user is able to add metadata item manually. Add metadata item To add metadata item the following steps shall be performed: Navigate to Metadata of the desired folder or project. Note : or navigate to specific metadata entity folder, e.g. Participant, Sample, etc. Click control - the pop-up window is open. Fill up the required fields: ID . It should be a unique identification for a new metadata item. Type . Choose the metadata item type, e.g. Participant, Sample, etc Note : if you clicked control from the specific metadata entity folder, the type would be set by default, but you would be able to change it. Click Add parameter to set attributes for the new metadata instance. It could be: String attribute. You can add an attribute with any name and value. Link to a metadata entity. You can choose a link to what a metadata entity you want to add as an attribute, e.g. set a link to a participant existing in the CP as a sample's attribute. Click Create - the new metadata item will be created and shown in the chosen metadata entity table. Delete metadata item To delete metadata item the following steps shall be performed: Navigate to the metadata entity table that contains the metadata item you want to delete. Tick one metadata item or more - the bulk operation group of buttons is enabled. Click button. Confirm your choice in the dialog window. The items are removed.","title":"5.1 Add/delete Metadata items"},{"location":"manual/05_Manage_Metadata/5.1._Add_Delete_metadata_items/#51-adddelete-metadata-items","text":"To manage metadata items, a user shall have WRITE permission for the parent folder and a role ENTITY_MANAGER . For more information see 13. Permissions . A user is able to add metadata item manually.","title":"5.1. Add/Delete metadata items"},{"location":"manual/05_Manage_Metadata/5.1._Add_Delete_metadata_items/#add-metadata-item","text":"To add metadata item the following steps shall be performed: Navigate to Metadata of the desired folder or project. Note : or navigate to specific metadata entity folder, e.g. Participant, Sample, etc. Click control - the pop-up window is open. Fill up the required fields: ID . It should be a unique identification for a new metadata item. Type . Choose the metadata item type, e.g. Participant, Sample, etc Note : if you clicked control from the specific metadata entity folder, the type would be set by default, but you would be able to change it. Click Add parameter to set attributes for the new metadata instance. It could be: String attribute. You can add an attribute with any name and value. Link to a metadata entity. You can choose a link to what a metadata entity you want to add as an attribute, e.g. set a link to a participant existing in the CP as a sample's attribute. Click Create - the new metadata item will be created and shown in the chosen metadata entity table.","title":"Add metadata item"},{"location":"manual/05_Manage_Metadata/5.1._Add_Delete_metadata_items/#delete-metadata-item","text":"To delete metadata item the following steps shall be performed: Navigate to the metadata entity table that contains the metadata item you want to delete. Tick one metadata item or more - the bulk operation group of buttons is enabled. Click button. Confirm your choice in the dialog window. The items are removed.","title":"Delete metadata item"},{"location":"manual/05_Manage_Metadata/5.2._Upload_metadata/","text":"5.2. Upload metadata To upload a Metadata to a Folder you need to have WRITE permission for that folder and a role ENTITY_MANAGER . For more information see 13. Permissions . Each folder allows uploading metadata to its space. But to get the full set of features it is advised to upload the metadata into the special types of folders called Projects . How to create a project described here . Uploading could be executed from the csv or tsv file. The structure of load file should be as in Table 1 . Table 1 - Load file structure (referenced_entity_type):ID membership:(referenced_entity_type):ID AttrubuteName referenced_uid_value referenced_uid_value attribute_value referenced_uid_value referenced_uid_value attribute_value referenced_uid_value referenced_uid_value attribute_value Examples: wes-11-rep-samples.csv , wes-11-rep-set.csv (referenced_entity_type):ID - here you set up what type of entity you are uploading and what is an ID of it, e.g. Sample:ID = FZ700059549. membership:(referenced_entity_type):ID - this is a reference to an instance of another entity. If you have a sample owned by a participant, you can show this connection: Participant:Participant:ID = TY90000044343. Note : make sure, that the reference entity id already exists in the system. AttrubuteName - any attribute name of an instance, e.g. RNA or DNA type, Blood or Tissue, etc. Upload instances of an entity Click on button. The Uploading metadata window is open. Select the appropriate file. Confirm choice by clicking OK . Note : if your file contains a link to the instance that doesn't exist, the whole metadata file won't be uploaded. For example, you set a link to a participant ID that doesn't exist in the file with samples. The new metadata is uploaded in the folder of a specific entity in the project. Note : the instances will be uploaded to the entity with a name, related to the first column if exists (e.g. \"Sample\"). Otherwise, the new entity folder will be created. Note : if the file contains instances already exist in the system, the attributes of it will be updated.","title":"5.2 Upload Metadata"},{"location":"manual/05_Manage_Metadata/5.2._Upload_metadata/#52-upload-metadata","text":"To upload a Metadata to a Folder you need to have WRITE permission for that folder and a role ENTITY_MANAGER . For more information see 13. Permissions . Each folder allows uploading metadata to its space. But to get the full set of features it is advised to upload the metadata into the special types of folders called Projects . How to create a project described here . Uploading could be executed from the csv or tsv file. The structure of load file should be as in Table 1 . Table 1 - Load file structure (referenced_entity_type):ID membership:(referenced_entity_type):ID AttrubuteName referenced_uid_value referenced_uid_value attribute_value referenced_uid_value referenced_uid_value attribute_value referenced_uid_value referenced_uid_value attribute_value Examples: wes-11-rep-samples.csv , wes-11-rep-set.csv (referenced_entity_type):ID - here you set up what type of entity you are uploading and what is an ID of it, e.g. Sample:ID = FZ700059549. membership:(referenced_entity_type):ID - this is a reference to an instance of another entity. If you have a sample owned by a participant, you can show this connection: Participant:Participant:ID = TY90000044343. Note : make sure, that the reference entity id already exists in the system. AttrubuteName - any attribute name of an instance, e.g. RNA or DNA type, Blood or Tissue, etc.","title":"5.2. Upload metadata"},{"location":"manual/05_Manage_Metadata/5.2._Upload_metadata/#upload-instances-of-an-entity","text":"Click on button. The Uploading metadata window is open. Select the appropriate file. Confirm choice by clicking OK . Note : if your file contains a link to the instance that doesn't exist, the whole metadata file won't be uploaded. For example, you set a link to a participant ID that doesn't exist in the file with samples. The new metadata is uploaded in the folder of a specific entity in the project. Note : the instances will be uploaded to the entity with a name, related to the first column if exists (e.g. \"Sample\"). Otherwise, the new entity folder will be created. Note : if the file contains instances already exist in the system, the attributes of it will be updated.","title":"Upload instances of an entity"},{"location":"manual/05_Manage_Metadata/5.3._Customize_view_of_the_entity_instance_table/","text":"5.3. Customize view of the entity instance table To customize view of the table with instances of an entity you need to have READ permissions for the folder with that metadata. For more information see 13. Permissions . This page describes how a user can customize an entity instance table and make it more easy to read. Any user, who has permissions for reading, will be able to customize the view of the table: change set of viewed attributes, change the order of attributes and reset the default settings. Press the \"Change view\" control. A list of accessible attributes shows up. The list contains all attributes of entities, uploaded in the current project. The attributes of current view are ticked. Choose desired attributes by ticking. Note : The last tick will be disabled, so you can't clear all checkboxes. To change the order, click a control in front of the desired attribute and pulling it up or down. The changes are applied instantly. The table view has only selected attributes. Note : This customization saved only for logged in user. To leave the form a user shall click outside the form. Note : you are able to restore the initial order of columns. To do that, click Reset Columns control.","title":"5.3 Customize view of the entity instance table"},{"location":"manual/05_Manage_Metadata/5.3._Customize_view_of_the_entity_instance_table/#53-customize-view-of-the-entity-instance-table","text":"To customize view of the table with instances of an entity you need to have READ permissions for the folder with that metadata. For more information see 13. Permissions . This page describes how a user can customize an entity instance table and make it more easy to read. Any user, who has permissions for reading, will be able to customize the view of the table: change set of viewed attributes, change the order of attributes and reset the default settings. Press the \"Change view\" control. A list of accessible attributes shows up. The list contains all attributes of entities, uploaded in the current project. The attributes of current view are ticked. Choose desired attributes by ticking. Note : The last tick will be disabled, so you can't clear all checkboxes. To change the order, click a control in front of the desired attribute and pulling it up or down. The changes are applied instantly. The table view has only selected attributes. Note : This customization saved only for logged in user. To leave the form a user shall click outside the form. Note : you are able to restore the initial order of columns. To do that, click Reset Columns control.","title":"5.3. Customize view of the entity instance table"},{"location":"manual/05_Manage_Metadata/5.4._Launch_a_run_configuration_on_metadata/","text":"5.4. Launch a run configuration on metadata To launch a run configuration on metadata a user shall have the following permissions: READ permissions a for folder contains metadata items; READ and EXECUTE permissions for a run configuration. For more information see 13. Permissions . A user could launch a run configuration on selected metadata from the metadata space. To launch a run configuration, the following steps shall be performed: Navigate to the desired folder with metadata items. Tick desired metadata items (see the picture below, 1 ). Click Run button (see the picture above, 2 ) - and the \"Select configuration\" pop-up window will be open. Note : Clicking on a configuration item, you'll see configuration's parameters. The second click will hide the parameters. Note : if you want to choose a run configuration with Root entity that differs from selected metadata items, please, specify it in \"Define expression\" field. To learn more about expressions, see here . Click \"OK\" button - and the runs will be scheduled. You'll be redirected into Run space automatically.","title":"5.4 Launch a Run configuration"},{"location":"manual/05_Manage_Metadata/5.4._Launch_a_run_configuration_on_metadata/#54-launch-a-run-configuration-on-metadata","text":"To launch a run configuration on metadata a user shall have the following permissions: READ permissions a for folder contains metadata items; READ and EXECUTE permissions for a run configuration. For more information see 13. Permissions . A user could launch a run configuration on selected metadata from the metadata space. To launch a run configuration, the following steps shall be performed: Navigate to the desired folder with metadata items. Tick desired metadata items (see the picture below, 1 ). Click Run button (see the picture above, 2 ) - and the \"Select configuration\" pop-up window will be open. Note : Clicking on a configuration item, you'll see configuration's parameters. The second click will hide the parameters. Note : if you want to choose a run configuration with Root entity that differs from selected metadata items, please, specify it in \"Define expression\" field. To learn more about expressions, see here . Click \"OK\" button - and the runs will be scheduled. You'll be redirected into Run space automatically.","title":"5.4. Launch a run configuration on metadata"},{"location":"manual/05_Manage_Metadata/5.5._Download_data_from_external_resources_to_the_cloud_data_storage/","text":"5.5. Download data from external resources to the cloud data storage Overview Upload metadata with list of external resources from CSV/TSV file Download data from external resources Overview Users often get the raw datasets from the external partners for processing. CP provides comfortable way to load a list of such files as external links to the CP GUI and launch a data load procedure to the cloud storage in background mode. Users can provide CSV/TSV files with the external links and submit a data transfer job, so that the files will be moved to the cloud storage in the background. Upload metadata with list of external resources from CSV/TSV file Note : To upload a Metadata to a Folder you need to have WRITE permission for that folder and a role ENTITY_MANAGER . For more information see 13. Permissions . Note : file with list of external resources should have PATH column and http/ftp links in this column (example: sample.csv ) Navigate on folder. Click on button. Navigate to CSV/TSV file with list of external resources in appeared dialog and select file. Click \" Open \" button. Click on \"Metadata\" object: Click on \"Sample\" class: List of samples with the external links will be appeared: Download data from external resources Above the list of external links, uploaded on previous step, click \" Transfer to the cloud \" button: The pop-up window for preparing for transferring will appear: In this window fill fields: a . Input data storage which will be use as a destination for downloading data. Click on button. In opened pop-up window select required storage (on the left panel with folder-tree) ( 1 ). In selected storage set the checkbox opposite the folder name, where external data will be downloaded ( 2 ). Click \" Ok \" button ( 3 ): Note : you need to have READ and WRITE permissions for folder and S3 storage, that contain folder for downloading. b . Select CSV/TSV columns (only from PATH columns), which shall be used to get external URLs (if several columns contain URLs - all can be used). c . ( optionally ) Select whether to rename resulting path to some other value (can be specified as another column cell, e.g. sample name). For do that click on button and select from dropdown list. d . ( optionally ) Input max threads count, if needs to limit. e . ( optionally ) Set if needs to create new folders within destination in case when several columns are selected for \" Path fields \" option ( b ). E.g. if two columns contain URLs and both are selected - then folders will be created for the corresponding column name and used for appropriate files storage. f . ( optionally ) Set if needs to update external URL within a table to the new location of the files. If set - http/ftp URLs will be changed to data storage path. Such data structure can be then used for a processing by a pipeline: URLs are changed to the S3-clickable hyperlinks (checkbox is set): URLs aren't changed and not-clickable hyperlinks (checkbox isn't set): Once you filled the form, click \" Start download \" button: System pipeline (transfer job) will be started automatically: Once the transfer job will be finished successfully, files will be located in the selected S3 storage:","title":"5.5 Download data from external resources"},{"location":"manual/05_Manage_Metadata/5.5._Download_data_from_external_resources_to_the_cloud_data_storage/#55-download-data-from-external-resources-to-the-cloud-data-storage","text":"Overview Upload metadata with list of external resources from CSV/TSV file Download data from external resources","title":"5.5. Download data from external resources to the cloud data storage"},{"location":"manual/05_Manage_Metadata/5.5._Download_data_from_external_resources_to_the_cloud_data_storage/#overview","text":"Users often get the raw datasets from the external partners for processing. CP provides comfortable way to load a list of such files as external links to the CP GUI and launch a data load procedure to the cloud storage in background mode. Users can provide CSV/TSV files with the external links and submit a data transfer job, so that the files will be moved to the cloud storage in the background.","title":"Overview"},{"location":"manual/05_Manage_Metadata/5.5._Download_data_from_external_resources_to_the_cloud_data_storage/#upload-metadata-with-list-of-external-resources-from-csvtsv-file","text":"Note : To upload a Metadata to a Folder you need to have WRITE permission for that folder and a role ENTITY_MANAGER . For more information see 13. Permissions . Note : file with list of external resources should have PATH column and http/ftp links in this column (example: sample.csv ) Navigate on folder. Click on button. Navigate to CSV/TSV file with list of external resources in appeared dialog and select file. Click \" Open \" button. Click on \"Metadata\" object: Click on \"Sample\" class: List of samples with the external links will be appeared:","title":"Upload metadata with list of external resources from CSV/TSV file"},{"location":"manual/05_Manage_Metadata/5.5._Download_data_from_external_resources_to_the_cloud_data_storage/#download-data-from-external-resources","text":"Above the list of external links, uploaded on previous step, click \" Transfer to the cloud \" button: The pop-up window for preparing for transferring will appear: In this window fill fields: a . Input data storage which will be use as a destination for downloading data. Click on button. In opened pop-up window select required storage (on the left panel with folder-tree) ( 1 ). In selected storage set the checkbox opposite the folder name, where external data will be downloaded ( 2 ). Click \" Ok \" button ( 3 ): Note : you need to have READ and WRITE permissions for folder and S3 storage, that contain folder for downloading. b . Select CSV/TSV columns (only from PATH columns), which shall be used to get external URLs (if several columns contain URLs - all can be used). c . ( optionally ) Select whether to rename resulting path to some other value (can be specified as another column cell, e.g. sample name). For do that click on button and select from dropdown list. d . ( optionally ) Input max threads count, if needs to limit. e . ( optionally ) Set if needs to create new folders within destination in case when several columns are selected for \" Path fields \" option ( b ). E.g. if two columns contain URLs and both are selected - then folders will be created for the corresponding column name and used for appropriate files storage. f . ( optionally ) Set if needs to update external URL within a table to the new location of the files. If set - http/ftp URLs will be changed to data storage path. Such data structure can be then used for a processing by a pipeline: URLs are changed to the S3-clickable hyperlinks (checkbox is set): URLs aren't changed and not-clickable hyperlinks (checkbox isn't set): Once you filled the form, click \" Start download \" button: System pipeline (transfer job) will be started automatically: Once the transfer job will be finished successfully, files will be located in the selected S3 storage:","title":"Download data\u00a0from external resources"},{"location":"manual/05_Manage_Metadata/5._Manage_Metadata/","text":"5. Manage Metadata Overview \"Details\" view Controls Search field Sorting control \"Change view\" + Add instance Upload metadata Show attributes/Hide attributes Bulk operation panel Overview Metadata is a CP object that defines custom data entities (see the definition below) associated with raw data files (fastq, bcl, etc.) or data parameters (see the picture below, arrow 1 ). By using this object a user can create a complex analysis environment. For example, you can customize your analysis to work with a subset of your data. Two important concepts of the metadata object is an Entity and an Instance of an entity . Entity - abstract category of comparable objects. For example, entity \"Sample\" can contain sequencing data from different people (see the picture below, arrow 2 ). An Instance of an entity - a specific representation of an entity. For example, sequencing data from a particular patient in the \"Sample\" entity is an instance of that entity (see the picture below, arrow 3 ). \"Details\" view \"Details\" panel displays content as a table of entity instances. Each column is an attribute of an instance, which is duplicated in the \"Attribute\" panel. Note : more about managing instance's attribute you can learn here . Controls The following buttons are available in the metadata entity space: Search field To find a particular instance of an entity a user shall use the Search field (see the picture above, 1 ), which is searching for the occurrence of entered text in the ID column of the table. Sorting control To sort instances of an entity in a table, a user shall click a header of the desired column: 1 click sorts a list in an ascending order, the next click sorts a list in a descending order, the next click reset sorting. \"Change view\" This control (see the picture above, 2 ) allows customizing the view of the table with instances of an entity. For more information see 5.3. Customize view of the entity instance table . + Add instance To add a new instance in the current metadata container, click + Add instance control (see the picture above, 3 ). For more information see 5.1. Add/Delete metadata items . Upload metadata Use this control (see the picture above, 4 ) to create the metadata object or to add entities to the metadata object/to add instances of an entity to the existing entity. See here for more information - 5.2. Upload metadata . Show attributes/Hide attributes This button (see the picture above, 5 ) allows to view or edit attributes of a particular instance of an entity. For more information see 17. CP objects tagging by additional attributes . Bulk operation panel This panel allows to execute operations for more than one item. You can tick desired items and the panel switch to active mode. Control Description Delete To delete one or more metadata item ( 1 ). See more details here . Clear selection Clears all selected items ( 2 ). The panel is deactivated. Run Allows to execute run configurations for the selected items ( 3 ). See details here .","title":"5.0 Overview"},{"location":"manual/05_Manage_Metadata/5._Manage_Metadata/#5-manage-metadata","text":"Overview \"Details\" view Controls Search field Sorting control \"Change view\" + Add instance Upload metadata Show attributes/Hide attributes Bulk operation panel","title":"5. Manage Metadata"},{"location":"manual/05_Manage_Metadata/5._Manage_Metadata/#overview","text":"Metadata is a CP object that defines custom data entities (see the definition below) associated with raw data files (fastq, bcl, etc.) or data parameters (see the picture below, arrow 1 ). By using this object a user can create a complex analysis environment. For example, you can customize your analysis to work with a subset of your data. Two important concepts of the metadata object is an Entity and an Instance of an entity . Entity - abstract category of comparable objects. For example, entity \"Sample\" can contain sequencing data from different people (see the picture below, arrow 2 ). An Instance of an entity - a specific representation of an entity. For example, sequencing data from a particular patient in the \"Sample\" entity is an instance of that entity (see the picture below, arrow 3 ).","title":"Overview"},{"location":"manual/05_Manage_Metadata/5._Manage_Metadata/#details-view","text":"\"Details\" panel displays content as a table of entity instances. Each column is an attribute of an instance, which is duplicated in the \"Attribute\" panel. Note : more about managing instance's attribute you can learn here .","title":"\"Details\"\u00a0view"},{"location":"manual/05_Manage_Metadata/5._Manage_Metadata/#controls","text":"The following buttons are available in the metadata entity space:","title":"Controls"},{"location":"manual/05_Manage_Metadata/5._Manage_Metadata/#search-field","text":"To find a particular instance of an entity a user shall use the Search field (see the picture above, 1 ), which is searching for the occurrence of entered text in the ID column of the table.","title":"Search field"},{"location":"manual/05_Manage_Metadata/5._Manage_Metadata/#sorting-control","text":"To sort instances of an entity in a table, a user shall click a header of the desired column: 1 click sorts a list in an ascending order, the next click sorts a list in a descending order, the next click reset sorting.","title":"Sorting control"},{"location":"manual/05_Manage_Metadata/5._Manage_Metadata/#change-view","text":"This control (see the picture above, 2 ) allows customizing the view of the table with instances of an entity. For more information see 5.3. Customize view of the entity instance table .","title":"\"Change view\""},{"location":"manual/05_Manage_Metadata/5._Manage_Metadata/#add-instance","text":"To add a new instance in the current metadata container, click + Add instance control (see the picture above, 3 ). For more information see 5.1. Add/Delete metadata items .","title":"+ Add instance"},{"location":"manual/05_Manage_Metadata/5._Manage_Metadata/#upload-metadata","text":"Use this control (see the picture above, 4 ) to create the metadata object or to add entities to the metadata object/to add instances of an entity to the existing entity. See here for more information - 5.2. Upload metadata .","title":"Upload metadata"},{"location":"manual/05_Manage_Metadata/5._Manage_Metadata/#show-attributeshide-attributes","text":"This button (see the picture above, 5 ) allows to view or edit attributes of a particular instance of an entity. For more information see 17. CP objects tagging by additional attributes .","title":"Show attributes/Hide\u00a0attributes"},{"location":"manual/05_Manage_Metadata/5._Manage_Metadata/#bulk-operation-panel","text":"This panel allows to execute operations for more than one item. You can tick desired items and the panel switch to active mode. Control Description Delete To delete one or more metadata item ( 1 ). See more details here . Clear selection Clears all selected items ( 2 ). The panel is deactivated. Run Allows to execute run configurations for the selected items ( 3 ). See details here .","title":"Bulk operation panel"},{"location":"manual/06_Manage_Pipeline/6.1.1_Building_WDL_pipeline_with_graphical_PipelineBuilder/","text":"6.1.1 Building WDL pipeline with graphical PipelineBuilder Overview Creating a new pipeline with a Pipeline Builder Overriding docker image for a specific task Example Pipeline To create a new WDL pipeline in a Folder you need to have WRITE permissions for that folder and the PIPELINE_MANAGER role. For more information see 13. Permissions . Overview Cloud Pipeline allows creating pipelines using graphical IDE called \" PipelineBuilder \". \" PipelineBuilder \" provides GUI approach to construct WDL pipeline workflow supported dependencies, loops, etc without programming. \" PipelineBuilder \" is based on WDL language (by Broad Institute, https://github.com/openwdl/wdl ) that is executed by \"Cromwell\" service. Creating a new pipeline with a Pipeline Builder To start using Pipeline Builder - create a new pipeline from \" WDL \" template: + Create \u2192 Pipeline \u2192 WDL . Name it (e.g. \"pipeline-builder-test\"). This will create a new pipeline with a draft version. Click on the created pipeline and open the pipeline draft version. Navigate to the GRAPH tab. Default pipeline will be generated with a single task \" HelloWorld_print \". To add more tasks - click the ADD TASK button in the top right corner. The ADD SCATTER button allows adding scatters . This will bring a task editor with the following fields: Name - a name of a task (it will be used for visualizing in a workflow and logging). Input - a list of parameters that a task can accept from upstream tasks. Output - a list of parameters that a task will pass to the downstream tasks. Command - a shell script that will be executed within a task. Use another docker image - if ticked - docker image, that is used within a task, can be overridden (i.e. different tools/images can be used for each task of the workflow) See more info below . The following picture presents an example of a basic task creating: Click Add and then Save and Commit . The following visualization will be generated - new \"task1\" with one output will appear. To create a \"real\" workflow - create a second task with one input. Then Save and Commit . Link task1 output with task2 input with a mouse cursor (click \"output1\" and slide to \"input1\"). Then click Save and Commit . Note : \" HelloWorld_print \" task isn't linked to other tasks. When code generates from a graph, tasks without links to other tasks can be executed in any order (e.g. task1 \u2192 HelloWorld_print \u2192 task2 or HelloWorld_print \u2192 task1 \u2192 task2 , etc). If you want to delete task - click its name and then the Delete button. Overriding docker image for a specific task By default, all tasks (and their commands) will run within a docker image that is specified for the initial run. This is useful when all tools/libraries are packed into a single docker image. But if a specific step requires tools that are not packed into the same docker image - \" PipelineBuilder \" allows to specify another docker image: Open any task details and check the Use another docker image option. This will bring a docker image selector. Choose registry , Tool group . Select an image and its version. Then click the OK button. Specified docker image will be used instead of the initial one. This means that a command specified for a task will be executed in another docker container. Example Pipeline As an example - R-based scRNA secondary analysis script. This script uses 10xGenomics matrix as input. Workflow diagram: Note : this workflow uses \"overridden\" docker image for the last task to show how it behaves (as described in Overriding docker image for a specific task section).","title":"6.1.1 Building WDL Pipeline with graphical PipelineBuilder"},{"location":"manual/06_Manage_Pipeline/6.1.1_Building_WDL_pipeline_with_graphical_PipelineBuilder/#611-building-wdl-pipeline-with-graphical-pipelinebuilder","text":"Overview Creating a new pipeline with a Pipeline Builder Overriding docker image for a specific task Example Pipeline To create a new WDL pipeline in a Folder you need to have WRITE permissions for that folder and the PIPELINE_MANAGER role. For more information see 13. Permissions .","title":"6.1.1 Building WDL pipeline with graphical PipelineBuilder"},{"location":"manual/06_Manage_Pipeline/6.1.1_Building_WDL_pipeline_with_graphical_PipelineBuilder/#overview","text":"Cloud Pipeline allows creating pipelines using graphical IDE called \" PipelineBuilder \". \" PipelineBuilder \" provides GUI approach to construct WDL pipeline workflow supported dependencies, loops, etc without programming. \" PipelineBuilder \" is based on WDL language (by Broad Institute, https://github.com/openwdl/wdl ) that is executed by \"Cromwell\" service.","title":"Overview"},{"location":"manual/06_Manage_Pipeline/6.1.1_Building_WDL_pipeline_with_graphical_PipelineBuilder/#creating-a-new-pipeline-with-a-pipeline-builder","text":"To start using Pipeline Builder - create a new pipeline from \" WDL \" template: + Create \u2192 Pipeline \u2192 WDL . Name it (e.g. \"pipeline-builder-test\"). This will create a new pipeline with a draft version. Click on the created pipeline and open the pipeline draft version. Navigate to the GRAPH tab. Default pipeline will be generated with a single task \" HelloWorld_print \". To add more tasks - click the ADD TASK button in the top right corner. The ADD SCATTER button allows adding scatters . This will bring a task editor with the following fields: Name - a name of a task (it will be used for visualizing in a workflow and logging). Input - a list of parameters that a task can accept from upstream tasks. Output - a list of parameters that a task will pass to the downstream tasks. Command - a shell script that will be executed within a task. Use another docker image - if ticked - docker image, that is used within a task, can be overridden (i.e. different tools/images can be used for each task of the workflow) See more info below . The following picture presents an example of a basic task creating: Click Add and then Save and Commit . The following visualization will be generated - new \"task1\" with one output will appear. To create a \"real\" workflow - create a second task with one input. Then Save and Commit . Link task1 output with task2 input with a mouse cursor (click \"output1\" and slide to \"input1\"). Then click Save and Commit . Note : \" HelloWorld_print \" task isn't linked to other tasks. When code generates from a graph, tasks without links to other tasks can be executed in any order (e.g. task1 \u2192 HelloWorld_print \u2192 task2 or HelloWorld_print \u2192 task1 \u2192 task2 , etc). If you want to delete task - click its name and then the Delete button.","title":"Creating a new pipeline with a Pipeline Builder"},{"location":"manual/06_Manage_Pipeline/6.1.1_Building_WDL_pipeline_with_graphical_PipelineBuilder/#overriding-docker-image-for-a-specific-task","text":"By default, all tasks (and their commands) will run within a docker image that is specified for the initial run. This is useful when all tools/libraries are packed into a single docker image. But if a specific step requires tools that are not packed into the same docker image - \" PipelineBuilder \" allows to specify another docker image: Open any task details and check the Use another docker image option. This will bring a docker image selector. Choose registry , Tool group . Select an image and its version. Then click the OK button. Specified docker image will be used instead of the initial one. This means that a command specified for a task will be executed in another docker container.","title":"Overriding docker image for a specific task"},{"location":"manual/06_Manage_Pipeline/6.1.1_Building_WDL_pipeline_with_graphical_PipelineBuilder/#example-pipeline","text":"As an example - R-based scRNA secondary analysis script. This script uses 10xGenomics matrix as input. Workflow diagram: Note : this workflow uses \"overridden\" docker image for the last task to show how it behaves (as described in Overriding docker image for a specific task section).","title":"Example Pipeline"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/","text":"6.1. Create and configure pipeline Create a pipeline in a Library space Customize a pipeline version Edit documentation (optional) Edit code section Edit pipeline configuration (optional) Add/delete storage rules (optional) Example: Create Pipeline Pipeline input data Pipeline output folder Configure the main_file Configure pipeline input/output parameters via GUI Check the results of pipeline execution Example: Add pipeline configuration Example: Create a configuration that uses Pipeline CLI for data uploading To create a Pipeline in a Folder you need to have WRITE permission for that folder and a role PIPELINE_MANAGER . To edit pipeline you need just WRITE permissions for a pipeline. For more information see 13. Permissions . To create a working pipeline version you need: Create a pipeline in a Library space Customize a pipeline version: Edit documentation (optional) Edit Code file Edit Configuration, Add new configuration (optional) Add storage rules (optional) . Create a pipeline in a Library space Go to the \"Library\" tab and select a folder. Click + Create \u2192 Pipeline and choose one of the built-in pipeline templates (Pure Python, Shell, Luigi, SAMPLE_SHEET_BATCH, FOLDER_BATCH, WDL). By default, Luigi template will be chosen. Default pipeline templates define the programming language for a pipeline. As templates are empty user shall write pipeline logic on his own. \"SAMPLE_SHEET_BATCH\" and \"FOLDER_BATCH\" templates include batch scripts and are devoted to batch processing of NGS files. Enter pipeline's name (pipeline description is optional) in the popped-up form. Click the Create button. A new pipeline will appear in the folder. Note : To configure repository where to store pipeline versions click the Edit repository settings button. Click on the button and two additional fields will appear: Repository (repository address) and Token (password to access a repository). The new pipeline will appear in a Library space. Customize a pipeline version Click a pipeline version to start its configuration process. Edit documentation (optional) This option allows you to make a detailed description of your pipelines. Navigate to the Documents tab and: Click Edit . Change the document using a markdown language . Click the Save button. Enter a description of the change and click Commit . Changes are saved. Edit code section It is not optional because you need to create a pipeline that will be tailored to your specific needs. For that purpose, you need to extend basic pipeline templates/add new files. Navigate to the Code tab. Click on any file you want to edit. Note : each pipeline version has a default code file: it named after a pipeline and has a respective extension. A new window with file contents will open. Click the Edit button and change the code file in the desired way. When you are done, click the Save button. You'll be asked to write a Commit message (e.g. 'added second \"echo\" command'). Then click the Commit button. After that changes will be applied to your file. Note : all code files are downloaded to the node to run the pipeline. Just adding a new file to the Code section doesn't change anything. You need to specify the order of scripts execution by yourself. E.g. you have three files in your pipeline: first.sh ( main_file ), second.sh and config.json . cmd_template parameter is chmod +x $SCRIPTS_DIR/src/* && $SCRIPTS_DIR/src/[main_file] . So in the first.sh file you need to explicitly specify execution of second.sh script for them both to run inside your pipeline, otherwise this file will be ignored. Edit pipeline configuration (optional) See details about pipeline configuration parameters here . Every pipeline has default pipeline configuration from the moment it was created. To change default pipeline configuration: Navigate to the Configuration tab. Expand \"Exec environment\" and \"Advanced\" tabs to see a full list of pipeline parameters. \"Parameters\" tab is opened by default. Change any parameter you need. In this example, we will set Cloud Region to Europe Ireland, Disk to 40 Gb and set the Timeout to 400 mins. Click the Save button. Now this will be the default pipeline configuration for the pipeline execution. Add/delete storage rules (optional) This section allows configuring what data will be transferred to an STS after pipeline execution. To add a new rule: Click the Add new rule button. A pop-up will appear. Enter File mask and then tick the box \"Move to STS\" to move pipeline output data to STS after pipeline execution. Note : If many rules with different Masks are present all of them are checked one by one. If a file corresponds to any of rules - it will be uploaded to the bucket. To delete storage rule click the Delete button in the right part of the storage rule's row. Example: Create Pipeline We will create a simple Shell pipeline (Shell template used). For that purpose, we will click + Create \u2192 Pipeline \u2192 SHELL . Then we will write Pipeline name ( 1 ), Pipeline description ( 2 ) and click Create ( 3 ). This pipeline will: Download a file. Rename it. Upload renamed the file to the bucket. Pipeline input data This is where pipeline input data is stored. This path will be used in pipeline parameters later on. Pipeline output folder This is where pipeline output data will be stored after pipeline execution. This path will be used in pipeline parameters later on. Configure the main_file The pipeline will consist of 2 files: main_file and config.json . Let's extend the main_file so that it renames the input file and puts it into the $ANALYSIS_DIR folder on the node from which data will be uploaded to the bucket. To do that click the main_file name and click the Edit button. Then type all the pipeline instructions. Configure pipeline input/output parameters via GUI Click the Run button. In the pipeline run configuration select the arrow near the Add parameter button and select the \"Input path parameter\" option from the drop-down list. Name the parameter (e.g. \"input\") and click on the grey \"download\" icon to select the path to the pipeline input data (we described pipeline input data above ). For pipeline output folder parameter choose the \"Output path parameter\" option from the drop-down list, name it and click on the grey \"upload\" icon to select the path to the pipeline output folder (we described pipeline output data above ). This is how everything looks after these parameters are set: Leave all other parameters default and click the Launch button. Check the results of pipeline execution After pipeline finished its execution, you can find the renamed file in the output folder. Example: Add pipeline configuration In this example, we will create a new pipeline configuration for the example pipeline and set it as default one. To add new pipeline configuration perform the following steps: Select a pipeline Select a pipeline version Navigate to the CONFIGURATION tab Click the + ADD button in the upper-right corner of the screen Specify Configuration name , Description (optionally) and the Template - this is a pipeline configuration, from which the new pipeline configuration will inherit its parameters (right now only the \"default\" template is available). Click the Create button. As you can see, the new configuration has the same parameters as the default configuration. Use Delete ( 1 ), Set as default ( 2 ) or Save ( 3 ) buttons to delete, set as default or save this configuration respectively. Expand the Exec environment section ( 1 ) and then Specify 30 GB Disk size ( 2 ), click the control to choose another Docker image ( 3 ). Click the Save button ( 4 ). Set \"new-configuration\" as default with the Set as default button. Navigate to the CODE tab. As you can see, config.json file now contains information about two configurations: \"default\" and \"new-configuration\". \"new-configuration\" is default one for pipeline execution. Example: Create a configuration that uses Pipeline CLI for data uploading To use Pipeline CLI instead of AWS CLI, the system parameter shall be added. In the Configuration page navigate to the Advanced tab and press Add system parameter button. Select the CP_USE_PIPE_FOR_CP option and click OK . Save the configuration - now the configuration will use CP CLI to upload running results. To see the difference, click \"Run\" to launch it. Edit or add any parameters you want and click \"Launch\" . In the Active Runs tab press the pipeline name. When pipeline finishes its execution, click the output link in the Parameters tab. On the opened page press the output file name and on the bottom-right you'll see a list of tags that was automatically set for the chosen file. This is a feature of uploading uses the CP CLI - files are automatically tagged with the following attributes: Note : The exception is that the storage is based on NFS share. Files in such data storage don't have attributes at all. Name Value CP_OWNER User ID CP_SOURCE Local path used to upload CP_CALC_CONFIG Instance type CP_DOCKER_IMAGE Tool that was used CP_JOB_CONFIGURATION Pipeline configuration CP_JOB_ID Pipeline ID CP_JOB_NAME Pipeline name CP_JOB_VERSION Pipeline version CP_RUN_ID Run ID","title":"6.1 Create and configure Pipeline"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#61-create-and-configure-pipeline","text":"Create a pipeline in a Library space Customize a pipeline version Edit documentation (optional) Edit code section Edit pipeline configuration (optional) Add/delete storage rules (optional) Example: Create Pipeline Pipeline input data Pipeline output folder Configure the main_file Configure pipeline input/output parameters via GUI Check the results of pipeline execution Example: Add pipeline configuration Example: Create a configuration that uses Pipeline CLI for data uploading To create a Pipeline in a Folder you need to have WRITE permission for that folder and a role PIPELINE_MANAGER . To edit pipeline you need just WRITE permissions for a pipeline. For more information see 13. Permissions . To create a working pipeline version you need: Create a pipeline in a Library space Customize a pipeline version: Edit documentation (optional) Edit Code file Edit Configuration, Add new configuration (optional) Add storage rules (optional) .","title":"6.1. Create and configure pipeline"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#create-a-pipeline-in-a-library-space","text":"Go to the \"Library\" tab and select a folder. Click + Create \u2192 Pipeline and choose one of the built-in pipeline templates (Pure Python, Shell, Luigi, SAMPLE_SHEET_BATCH, FOLDER_BATCH, WDL). By default, Luigi template will be chosen. Default pipeline templates define the programming language for a pipeline. As templates are empty user shall write pipeline logic on his own. \"SAMPLE_SHEET_BATCH\" and \"FOLDER_BATCH\" templates include batch scripts and are devoted to batch processing of NGS files. Enter pipeline's name (pipeline description is optional) in the popped-up form. Click the Create button. A new pipeline will appear in the folder. Note : To configure repository where to store pipeline versions click the Edit repository settings button. Click on the button and two additional fields will appear: Repository (repository address) and Token (password to access a repository). The new pipeline will appear in a Library space.","title":"Create a pipeline in a Library space"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#customize-a-pipeline-version","text":"Click a pipeline version to start its configuration process.","title":"Customize a pipeline version"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#edit-documentation-optional","text":"This option allows you to make a detailed description of your pipelines. Navigate to the Documents tab and: Click Edit . Change the document using a markdown language . Click the Save button. Enter a description of the change and click Commit . Changes are saved.","title":"Edit documentation (optional)"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#edit-code-section","text":"It is not optional because you need to create a pipeline that will be tailored to your specific needs. For that purpose, you need to extend basic pipeline templates/add new files. Navigate to the Code tab. Click on any file you want to edit. Note : each pipeline version has a default code file: it named after a pipeline and has a respective extension. A new window with file contents will open. Click the Edit button and change the code file in the desired way. When you are done, click the Save button. You'll be asked to write a Commit message (e.g. 'added second \"echo\" command'). Then click the Commit button. After that changes will be applied to your file. Note : all code files are downloaded to the node to run the pipeline. Just adding a new file to the Code section doesn't change anything. You need to specify the order of scripts execution by yourself. E.g. you have three files in your pipeline: first.sh ( main_file ), second.sh and config.json . cmd_template parameter is chmod +x $SCRIPTS_DIR/src/* && $SCRIPTS_DIR/src/[main_file] . So in the first.sh file you need to explicitly specify execution of second.sh script for them both to run inside your pipeline, otherwise this file will be ignored.","title":"Edit code section"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#edit-pipeline-configuration-optional","text":"See details about pipeline configuration parameters here . Every pipeline has default pipeline configuration from the moment it was created. To change default pipeline configuration: Navigate to the Configuration tab. Expand \"Exec environment\" and \"Advanced\" tabs to see a full list of pipeline parameters. \"Parameters\" tab is opened by default. Change any parameter you need. In this example, we will set Cloud Region to Europe Ireland, Disk to 40 Gb and set the Timeout to 400 mins. Click the Save button. Now this will be the default pipeline configuration for the pipeline execution.","title":"Edit pipeline configuration (optional)"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#adddelete-storage-rules-optional","text":"This section allows configuring what data will be transferred to an STS after pipeline execution. To add a new rule: Click the Add new rule button. A pop-up will appear. Enter File mask and then tick the box \"Move to STS\" to move pipeline output data to STS after pipeline execution. Note : If many rules with different Masks are present all of them are checked one by one. If a file corresponds to any of rules - it will be uploaded to the bucket. To delete storage rule click the Delete button in the right part of the storage rule's row.","title":"Add/delete storage rules (optional)"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#example-create-pipeline","text":"We will create a simple Shell pipeline (Shell template used). For that purpose, we will click + Create \u2192 Pipeline \u2192 SHELL . Then we will write Pipeline name ( 1 ), Pipeline description ( 2 ) and click Create ( 3 ). This pipeline will: Download a file. Rename it. Upload renamed the file to the bucket.","title":"Example: Create Pipeline"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#pipeline-input-data","text":"This is where pipeline input data is stored. This path will be used in pipeline parameters later on.","title":"Pipeline input data"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#pipeline-output-folder","text":"This is where pipeline output data will be stored after pipeline execution. This path will be used in pipeline parameters later on.","title":"Pipeline output folder"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#configure-the-main_file","text":"The pipeline will consist of 2 files: main_file and config.json . Let's extend the main_file so that it renames the input file and puts it into the $ANALYSIS_DIR folder on the node from which data will be uploaded to the bucket. To do that click the main_file name and click the Edit button. Then type all the pipeline instructions.","title":"Configure the main_file"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#configure-pipeline-inputoutput-parameters-via-gui","text":"Click the Run button. In the pipeline run configuration select the arrow near the Add parameter button and select the \"Input path parameter\" option from the drop-down list. Name the parameter (e.g. \"input\") and click on the grey \"download\" icon to select the path to the pipeline input data (we described pipeline input data above ). For pipeline output folder parameter choose the \"Output path parameter\" option from the drop-down list, name it and click on the grey \"upload\" icon to select the path to the pipeline output folder (we described pipeline output data above ). This is how everything looks after these parameters are set: Leave all other parameters default and click the Launch button.","title":"Configure pipeline input/output parameters via GUI"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#check-the-results-of-pipeline-execution","text":"After pipeline finished its execution, you can find the renamed file in the output folder.","title":"Check the results of pipeline execution"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#example-add-pipeline-configuration","text":"In this example, we will create a new pipeline configuration for the example pipeline and set it as default one. To add new pipeline configuration perform the following steps: Select a pipeline Select a pipeline version Navigate to the CONFIGURATION tab Click the + ADD button in the upper-right corner of the screen Specify Configuration name , Description (optionally) and the Template - this is a pipeline configuration, from which the new pipeline configuration will inherit its parameters (right now only the \"default\" template is available). Click the Create button. As you can see, the new configuration has the same parameters as the default configuration. Use Delete ( 1 ), Set as default ( 2 ) or Save ( 3 ) buttons to delete, set as default or save this configuration respectively. Expand the Exec environment section ( 1 ) and then Specify 30 GB Disk size ( 2 ), click the control to choose another Docker image ( 3 ). Click the Save button ( 4 ). Set \"new-configuration\" as default with the Set as default button. Navigate to the CODE tab. As you can see, config.json file now contains information about two configurations: \"default\" and \"new-configuration\". \"new-configuration\" is default one for pipeline execution.","title":"Example: Add pipeline configuration"},{"location":"manual/06_Manage_Pipeline/6.1._Create_and_configure_pipeline/#example-create-a-configuration-that-uses-pipeline-cli-for-data-uploading","text":"To use Pipeline CLI instead of AWS CLI, the system parameter shall be added. In the Configuration page navigate to the Advanced tab and press Add system parameter button. Select the CP_USE_PIPE_FOR_CP option and click OK . Save the configuration - now the configuration will use CP CLI to upload running results. To see the difference, click \"Run\" to launch it. Edit or add any parameters you want and click \"Launch\" . In the Active Runs tab press the pipeline name. When pipeline finishes its execution, click the output link in the Parameters tab. On the opened page press the output file name and on the bottom-right you'll see a list of tags that was automatically set for the chosen file. This is a feature of uploading uses the CP CLI - files are automatically tagged with the following attributes: Note : The exception is that the storage is based on NFS share. Files in such data storage don't have attributes at all. Name Value CP_OWNER User ID CP_SOURCE Local path used to upload CP_CALC_CONFIG Instance type CP_DOCKER_IMAGE Tool that was used CP_JOB_CONFIGURATION Pipeline configuration CP_JOB_ID Pipeline ID CP_JOB_NAME Pipeline name CP_JOB_VERSION Pipeline version CP_RUN_ID Run ID","title":"Example: Create a configuration that\u00a0uses Pipeline CLI for data uploading"},{"location":"manual/06_Manage_Pipeline/6.2._Launch_a_pipeline/","text":"6.2. Launch a pipeline To launch a pipeline you need to have EXECUTE permissions for the pipeline. For more information see 13. Permissions . Select a pipeline in the \" Library \" menu ( 3. Overview ). Select a pipeline version to run. Click the Run button. Launch pipeline page will open. Feel free to change settings of run configuration if you need to. See an example of editing configuration here . Click Launch . You'll be redirected to the \"Runs\" area. Here you'll find your pipeline running. You can monitor status of your run and see additional information (see 11. Manage Runs ). Note : after some initialization time, a new node will appear in the \" Cluster nodes \" tab. See 9. Manage Cluster nodes . Note : to learn about launching a pipeline as an Interactive service, refer to 15. Interactive services .","title":"6.2 Launch a Pipeline"},{"location":"manual/06_Manage_Pipeline/6.2._Launch_a_pipeline/#62-launch-a-pipeline","text":"To launch a pipeline you need to have EXECUTE permissions for the pipeline. For more information see 13. Permissions . Select a pipeline in the \" Library \" menu ( 3. Overview ). Select a pipeline version to run. Click the Run button. Launch pipeline page will open. Feel free to change settings of run configuration if you need to. See an example of editing configuration here . Click Launch . You'll be redirected to the \"Runs\" area. Here you'll find your pipeline running. You can monitor status of your run and see additional information (see 11. Manage Runs ). Note : after some initialization time, a new node will appear in the \" Cluster nodes \" tab. See 9. Manage Cluster nodes . Note : to learn about launching a pipeline as an Interactive service, refer to 15. Interactive services .","title":"6.2. Launch a pipeline"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/","text":"6. Manage Pipeline Pipeline object GUI \"Details\" view pane \"Details\" controls Pipeline versions GUI Pipeline controls Pipeline launching page Pipeline version tabs DOCUMENTS CODE CONFIGURATION HISTORY STORAGE RULES GRAPH Default environment variables Pipelines represent a sequence of tasks that are executed along with each other in order to process some data. They help to automate complex tasks that consist of many sub-tasks. This chapter describes Pipeline space GUI and the main working scenarios. Pipeline object GUI As far as the pipeline is one of CP objects which stored in \" Library \" space, the Pipeline workspace is separated into two panes: \"Hierarchy\" view pane \"Details\" view pane. \"Details\" view pane The \"Details\" view pane displays content of a selected object. In case of a pipeline, you will see: a list of pipeline versions with a description of last update and date of the last update; specific space's controls . \"Details\" controls Control Description Displays icon This icon includes: \" Attributes \" control ( 1 ) opens Attributes pane. Here you can see a list of \"key=value\" attributes of the pipeline. For more info see here . Note : If the selected pipeline has any defined attribute, Attributes pane is shown by default. Issues shows/hides the issues of the current pipeline to discuss. To learn more see here . \"Gear\" icon This control ( 2 ) allows to rename, change the description, delete and edit repository settings i.e. Repository address and Token Git repository \" Git repository \" control ( 3 ) shows a gitlab repository address where pipeline versions are stored, which could be copied and pasted into a browser address field. Also a user can work directly with git from the console on the running node. To clone a pipeline a user shall have READ permissions, to push WRITE permission are also needed. For more info see 13. Permissions . Release \" Release \" control ( 4 ) is used to tag a particular pipeline version with a name. A draft pipeline version has the control only. Note : you can edit the last pipeline version only. Run Each pipeline version item of the selected pipeline's list has a \" Run \" control ( 5 ) to launch a pipeline version. Pipeline versions GUI Pipeline version interface displays full information about a pipeline version: supporting documentation, code files, and configurations, history of version runnings, etc. Pipeline controls The following buttons are available to manage this space. Control Description Run This button launch a pipeline version. When a user clicks the button, the \"Launch a pipeline\" page opens. \"Gear\" icon This control allows to rename, change the description, delete and edit repository settings i.e. Repository address and Token . Git repository Shows a git repository address where pipeline versions are stored, which could be copied and pasted in a browser line. Also a user can work directly with git from the console on the running node. To clone a pipeline a user shall have READ permissions, to push WRITE permission are also needed. For more info see 13. Permissions . Pipeline launching page \" Launch a pipeline \" page shows parameters of a default configuration the pipeline version. This page has the same view as a \"Configuration\" tab of a pipeline version. Here you can select any other configuration from the list and/or change parameters for this specific run (changes in configuration will be applied only to this specific run). Pipeline version tabs Pipeline version space dramatically differs from the Pipeline space. You can open it: just click on it. The whole information is organized into the following tabs in \"Details\" view pane. DOCUMENTS The \"Documents\" tab contains documentation associated with the pipeline, e.g. README, pipeline description, etc. See an example here . Note : README.md file is created automatically and contains default text which could be easily edited by a user. Documents tab controls The following buttons are available to manage this section: Control Description Upload ( a ) This control ( a ) allows to upload documentation files. Delete ( b ) \"Delete\" control ( b ) helps to delete a file. Rename ( c ) To rename a file a user shall use a \"Rename\" control ( c ). Download ( d ) This control ( d ) allows downloading pipeline documentation file to your local machine. Edit ( e ) \"Edit\" control ( e ) helps a user to edit any text files (e.g. README) here in a text editor using a markdown language . CODE This section contains a list of scripts to run a pipeline. Here you can create new files, folders and upload files here. Each script file could be edited (see details here ). Note : .json configuration file can also be edited in the Configuration tab via GUI. Code tab controls The following controls are available: Control Description Plus button ( a ) This control is to create a new folder in a pipeline version. The folder's name shall be specified. + New file ( b ) To create a new file in the current folder. Upload ( c ) To upload files from your local file system to a pipeline version. Rename ( d ) Each file or folder has a \"Rename\" control which allows renaming a file/folder. Delete ( e ) Each file or folder has a \"Delete\" control which deletes a file/folder. The list of system files All newly created pipelines have at least 2 starting files no matter what pipeline template you've chosen. main_file This file contains a pipeline scenario. By default, it is named after a pipeline, but this may be changed in the configuration file. Note : the main_file is usually an entry point to start pipeline execution. To create your own scenario the default template of the main file shall be edited (see details here ). Example : below is the piece of the main_file of the Gromacs pipeline: config.json This file contains pipeline execution parameters. You can not rename or delete it because of it's used in pipeline scripts and they will not work without it. Note : it is advised that pipeline execution settings are modified via CONFIGURATION tab (e.g. if you want to change default settings for pipeline execution) or via Launch pipeline page (e.g. if you want to change pipeline settings for a current run). Manual config.json editing should be used only for advanced users (primarily developers) since json format is not validated in this case. Note : all attributes from config.json are available as environment variables for pipeline execution. A config.json files for every pipeline template have the following settings: Setting Description main_file A name of the main file for that pipeline. instance_size AWS EC2 instance size (e.g. \"m4.xlarge\") that specifies an amount of RAM in Gb and CPU cores number. instance_disk An instance's disk size in Gb. docker_image A name of the Docker image that will be used in the current pipeline. cmd_template Command line template that will be executed at the running instance in the pipeline. cmd_template can use environment variables: To address the main_file parameter value, use the following construction - [main_file] To address all other parameters, usual Linux environment variables style shall be used (e.g. $docker_image ) parameters Pipeline execution parameters (e.g. path to the data storage with input data). A parameter has a name and set of attributes. There are 3 possible keys for each parameter: \"type\" - key specifies a type for current parameter, \"value\" - key specifies default value for parameter, \"required\" - key specifies whether this parameter must be set ( \"required\": true ) or might not ( \"required\": false ) Example : config.json file of the Gromacs pipeline: Note : In addition to main_file and config.json you can add any number of files to the CODE section and combine it in one whole scenario. CONFIGURATION This section represents pipeline execution parameters which are set in config.json file. The parameters can be changed here and config.json file will be changed respectively. See how to edit configuration here . A configuration specifies: Section Control Description Name Pipeline and its configuration names. Estimated price per hour Control shows machine hours prices. If you navigate mouse to \"info\" icon, you'll see the maximum, minimum and average price for a particular pipeline version run as well as the price per hour. Exec environment This section lists execution environment parameters. Docker image A name of a Docker image to use for a pipeline execution (e.g. \"library/gromacs-gpu\"). Node type An instance type in terms of Amazon: CPU, RAM, GPU (e.g. 2 CPU cores, 8 Gb RAM, 0 GPU cores). Disk Size of a disk in gigabytes, that will be attached to the instance in Gb. Configure cluster button On-click, pop-up window will be shown: Here you can configure cluster or auto-scaled cluster. Cluster is a collection of instances which are connected so that they can be used together on a task. In both cases, a number of additional worker nodes with some main node as cluster head are launching (total number of pipelines = \"number of working nodes\" + 1) having NFS network file system within one node. See v.0.14 - 7.2. Launch Detached Configuration for details. In case of using cluster , an exact count of worker nodes is directly defined by the user before launching the task and could not changing during the run. In case of using auto-scaled cluster , a max count of worker nodes is defined by the user before launching the task but really used count of worker nodes can change during the run depending on the jobs queue load. For configure cluster: in opened window click Cluster button specify a number of child nodes (workers' count) and click Ok button: When user selects Cluster option, information on total cluster resources is shown. Resources are calculated as (CPU/RAM/GPU)*(NUMBER_OF_WORKERS+1) : For configure auto-scaled cluster : in opened window click Auto-scaled cluster button specify a number of child nodes (workers' count) in field Auto-scaled up to and click Ok button: Note : that number is meaning total count of \"auto-scaled\" nodes - it is the max count of worker nodes that could be attached to the main node to work together as cluster. These nodes will be attached to the cluster only in case if some jobs are in waiting state longer than a specific time. Also these nodes will be dropped from the cluster in case when jobs queue is empty or all jobs are running and there are some idle nodes longer than a specific time. Note : about timeout periods for scale-up and scale-down of auto-scaled cluster see here . also you may specify a number of \"persistent\" child nodes (workers' count) - click Setup default child nodes count , input Default child nodes number and click Ok button: These default child nodes will be never \"scaled-down\" during the run regardless of jobs queue load. In the example above, total count of \"auto-scaled\" nodes - 3, and 1 of them is \"persistent\". Note : total count of child nodes always must be greater than count of default (\"persistent\") child nodes. if you don't want to use default (\"persistent\") child nodes in your auto-scaled cluster - click Reset button opposite the Default child nodes field. When user selects Auto-scaled cluster , information on total cluster resources is shown as interval - from the \"min\" configuration to \"max\" configuration: \"min\" configuration resources are calculated as (CPU\\/RAM\\/GPU)*(NUMBER_OF_DEFAULT_WORKERS+1) \"max\" configuration resources are calculated as (CPU\\/RAM\\/GPU)*(TOTAL_NUMBER_OF_WORKERS+1) E.g. for auto-scaled cluster with 2 child nodes and without default (\"persistent\") child nodes (NUMBER_OF_DEFAULT_WORKERS = 0; TOTAL_NUMBER_OF_WORKERS = 2) : E.g. for auto-scaled cluster with 2 child nodes and 1 default (\"persistent\") child node (NUMBER_OF_DEFAULT_WORKERS = 1; TOTAL_NUMBER_OF_WORKERS = 2) : Note : if you don't want to use any cluster - click Single node button and then click Ok button. Cloud Region A region, where computing nodes are located. This select allows to decrease time of data movement for huge data volumes by choosing the nearest region. Please note that a \"default\" region for all runs is \"US East\". If a non-default region is selected - certain CP features may be unavailable: EFS storages usage from the another region (e.g. EU West region cannot use EFS storages from the \"US East\"). S3 buckets will be still available If a specific tool, used for a run, requires an on-premise license server (e.g. monolix, matlab, schrodinger, etc.) - such instances shall be run in a region, that hosts those license servers. Advanced Price type Choose spot or on-demand type of instance. You can look information about price types hovering \"Info\" icon and based on it make your choice. Timeout (min) After this time pipeline will shut down (optional). Limit mounts Allow to specify storages that should be mounted. Cmd template A shell command that will be executed to start a pipeline. \"Start idle\" The flag sets cmd_template to sleep infinity . For more information about starting a job in this mode refer to 15. Interactive services . Parameters This section lists pipeline specific parameters that can be used during a pipeline run. Pipeline parameters can be assigned the following types: String - generic scalar value (e.g. Sample name). Path - path in a data storage hierarchy. Input - path in a data storage hierarchy. During pipeline initialization, this path will be used to download input data on the calculation node for processing from a storage. Output - path in a data storage hierarchy. During pipeline finalization, this path will be used to upload resulting data to a storage. Common - path in a data storage hierarchy. Similar to \"Input\" type, but this data will not be erased from a calculation node, when a pipeline is finished (this is useful for reference data, as it can be reused by further pipeline runs that share the same reference). Note : You can use Project attribute values as parameters for the Run : Click an empty parameter value field. Enter \" project. \" In the drop-down list select the Project attribute value: Add parameter This control helps to add an additional parameter to a configuration. Configuration tab controls Control Description Add To create a customized configuration for the pipeline, click the + ADD button in the upper-right corner of the screen. For more details see here . Save This button saves changes in a configuration. HISTORY This section contains information about all the current pipeline version's runs. Runs info is organized into a table with the following columns: Run - run name that consists of pipeline name and run id. Parent-run - id of the run that executed current run (this field is non-empty only for runs that are executed by other runs). Pipeline - pipeline name. Version - pipeline version. Started - time pipeline started running. Completed - time pipeline finished execution. Elapsed time - pipeline running time. Estimated price - estimated price of run, which is calculated based on the run duration and instance type. Owner - user who launched run. You can filter runs by clicking the filter icon . By using the filter control you can choose whether display runs for current pipeline version or display runs for all pipeline versions. History tab controls Control Description Pause (a) To pause running pipeline press this control. This control is available only for on-demand instances. Resume (b) To resume pausing pipeline press this control. This control is available only for on-demand instances. Stop (c) To stop running pipeline press this control. Rerun (d) This control reruns completed pipeline's runs. Log (e) \"Log\" control opens detailed information about the run. You'll be redirected to \" Runs \" space (see 11. Manage Runs ). Pipeline run's states Icons at the left represent the current state of the pipeline runs: Rotating - a run is scheduled but is waiting for a calculation node to appear. Download - now pipeline Docker image is downloaded to the node. Play - The pipeline is running. The node is appearing and pipeline input data is being downloaded to the node before the \" InitializeEnvironment \" service task appears. OK - successful pipeline execution. Caution - unsuccessful pipeline execution. Time - a pipeline manually stopped. STORAGE RULES This section displays a list of rules used to upload data to the output data storage, once pipeline finished. It helps to store only data you need and minimize the amount of interim data in S3 buckets. Info is organized into a table with the following columns: Mask column contains a relative path from the $ANALYSIS_DIR folder (see Default environment variables section below for more information). Mask uses bash syntax to specify the data that you want to upload from the $ANALYSIS_DIR. Data from the specified path will be uploaded to the bucket from the pipeline node. Note : by default whole $ANALYSIS_DIR folder is uploaded to the cloud bucket (default Mask is - \"*\"). For example, \"*.txt*\" mask specifies that all files with .txt extension need to be uploaded from the $ANALYSIS_DIR to the data storage. Note : Be accurate when specifying masks - if wildcard mask (\"*\") is specified, all files will be uploaded, no matter what additional masks are specified. The Created column shows date and time of rules creation. Move to Short-Term Storage column indicates whether pipeline output data will be moved to a short-term storage. Storage rules tab control Control Description Add new rule (a) This control allows adding a new data managing rule. Delete (b) To delete a data managing rule press this control. GRAPH This section represents the sequence of pipeline tasks as a directed graph. Tasks are graph vertices, edges represent execution order. A task can be executed only when all input edges - associated tasks - are completed (see more information about creating a pipeline with GRAPH section here ). Note : only for Luigi and WDL pipelines. Note : If main_file has mistakes, pipeline workflow won't be visualized. Graph tab controls When a PipelineBuilder graph is loaded, the following layout controls become available to the user. Control Description Layout performs graph linearization, make it more readable. Fit zooms graph to fit the screen. Show links enables/disables workflow level links to the tasks. It is disabled by default, as for large workflows it overwhelms the visualization. Revert reverts all changes to the last saving. Save saves changes. Default environment variables Pipeline scripts (e.g. main_file ) use default environmental variables for pipeline execution. These variables are set in internal CP scripts: RUN_ID - pipeline run ID. PIPELINE_NAME - pipeline name. COMMON_DIR - directory where pipeline common data (parameter with \"type\": \"common\" ) will be stored. ANALYSIS_DIR - directory where output data of the pipeline (parameter with \"type\": \"output\" ) will be stored. INPUT_DIR - directory where input data of the pipeline (parameter with \"type\": \"input\" ) will be stored. SCRIPTS_DIR - directory where all pipeline scripts and config.json file will be stored.","title":"6.0 Overview"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#6-manage-pipeline","text":"Pipeline object GUI \"Details\" view pane \"Details\" controls Pipeline versions GUI Pipeline controls Pipeline launching page Pipeline version tabs DOCUMENTS CODE CONFIGURATION HISTORY STORAGE RULES GRAPH Default environment variables Pipelines represent a sequence of tasks that are executed along with each other in order to process some data. They help to automate complex tasks that consist of many sub-tasks. This chapter describes Pipeline space GUI and the main working scenarios.","title":"6. Manage Pipeline"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#pipeline-object-gui","text":"As far as the pipeline is one of CP objects which stored in \" Library \" space, the Pipeline workspace is separated into two panes: \"Hierarchy\" view pane \"Details\" view pane.","title":"Pipeline object GUI"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#details-view-pane","text":"The \"Details\" view pane displays content of a selected object. In case of a pipeline, you will see: a list of pipeline versions with a description of last update and date of the last update; specific space's controls .","title":"\"Details\" view pane"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#details-controls","text":"Control Description Displays icon This icon includes: \" Attributes \" control ( 1 ) opens Attributes pane. Here you can see a list of \"key=value\" attributes of the pipeline. For more info see here . Note : If the selected pipeline has any defined attribute, Attributes pane is shown by default. Issues shows/hides the issues of the current pipeline to discuss. To learn more see here . \"Gear\" icon This control ( 2 ) allows to rename, change the description, delete and edit repository settings i.e. Repository address and Token Git repository \" Git repository \" control ( 3 ) shows a gitlab repository address where pipeline versions are stored, which could be copied and pasted into a browser address field. Also a user can work directly with git from the console on the running node. To clone a pipeline a user shall have READ permissions, to push WRITE permission are also needed. For more info see 13. Permissions . Release \" Release \" control ( 4 ) is used to tag a particular pipeline version with a name. A draft pipeline version has the control only. Note : you can edit the last pipeline version only. Run Each pipeline version item of the selected pipeline's list has a \" Run \" control ( 5 ) to launch a pipeline version.","title":"\"Details\" controls"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#pipeline-versions-gui","text":"Pipeline version interface displays full information about a pipeline version: supporting documentation, code files, and configurations, history of version runnings, etc.","title":"Pipeline versions GUI"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#pipeline-controls","text":"The following buttons are available to manage this space. Control Description Run This button launch a pipeline version. When a user clicks the button, the \"Launch a pipeline\" page opens. \"Gear\" icon This control allows to rename, change the description, delete and edit repository settings i.e. Repository address and Token . Git repository Shows a git repository address where pipeline versions are stored, which could be copied and pasted in a browser line. Also a user can work directly with git from the console on the running node. To clone a pipeline a user shall have READ permissions, to push WRITE permission are also needed. For more info see 13. Permissions .","title":"Pipeline controls"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#pipeline-launching-page","text":"\" Launch a pipeline \" page shows parameters of a default configuration the pipeline version. This page has the same view as a \"Configuration\" tab of a pipeline version. Here you can select any other configuration from the list and/or change parameters for this specific run (changes in configuration will be applied only to this specific run).","title":"Pipeline launching page"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#pipeline-version-tabs","text":"Pipeline version space dramatically differs from the Pipeline space. You can open it: just click on it. The whole information is organized into the following tabs in \"Details\" view pane.","title":"Pipeline version tabs"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#documents","text":"The \"Documents\" tab contains documentation associated with the pipeline, e.g. README, pipeline description, etc. See an example here . Note : README.md file is created automatically and contains default text which could be easily edited by a user.","title":"DOCUMENTS"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#documents-tab-controls","text":"The following buttons are available to manage this section: Control Description Upload ( a ) This control ( a ) allows to upload documentation files. Delete ( b ) \"Delete\" control ( b ) helps to delete a file. Rename ( c ) To rename a file a user shall use a \"Rename\" control ( c ). Download ( d ) This control ( d ) allows downloading pipeline documentation file to your local machine. Edit ( e ) \"Edit\" control ( e ) helps a user to edit any text files (e.g. README) here in a text editor using a markdown language .","title":"Documents tab controls"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#code","text":"This section contains a list of scripts to run a pipeline. Here you can create new files, folders and upload files here. Each script file could be edited (see details here ). Note : .json configuration file can also be edited in the Configuration tab via GUI.","title":"CODE"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#code-tab-controls","text":"The following controls are available: Control Description Plus button ( a ) This control is to create a new folder in a pipeline version. The folder's name shall be specified. + New file ( b ) To create a new file in the current folder. Upload ( c ) To upload files from your local file system to a pipeline version. Rename ( d ) Each file or folder has a \"Rename\" control which allows renaming a file/folder. Delete ( e ) Each file or folder has a \"Delete\" control which deletes a file/folder.","title":"Code tab controls"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#the-list-of-system-files","text":"All newly created pipelines have at least 2 starting files no matter what pipeline template you've chosen.","title":"The list of system files"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#main_file","text":"This file contains a pipeline scenario. By default, it is named after a pipeline, but this may be changed in the configuration file. Note : the main_file is usually an entry point to start pipeline execution. To create your own scenario the default template of the main file shall be edited (see details here ). Example : below is the piece of the main_file of the Gromacs pipeline:","title":"main_file"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#configjson","text":"This file contains pipeline execution parameters. You can not rename or delete it because of it's used in pipeline scripts and they will not work without it. Note : it is advised that pipeline execution settings are modified via CONFIGURATION tab (e.g. if you want to change default settings for pipeline execution) or via Launch pipeline page (e.g. if you want to change pipeline settings for a current run). Manual config.json editing should be used only for advanced users (primarily developers) since json format is not validated in this case. Note : all attributes from config.json are available as environment variables for pipeline execution. A config.json files for every pipeline template have the following settings: Setting Description main_file A name of the main file for that pipeline. instance_size AWS EC2 instance size (e.g. \"m4.xlarge\") that specifies an amount of RAM in Gb and CPU cores number. instance_disk An instance's disk size in Gb. docker_image A name of the Docker image that will be used in the current pipeline. cmd_template Command line template that will be executed at the running instance in the pipeline. cmd_template can use environment variables: To address the main_file parameter value, use the following construction - [main_file] To address all other parameters, usual Linux environment variables style shall be used (e.g. $docker_image ) parameters Pipeline execution parameters (e.g. path to the data storage with input data). A parameter has a name and set of attributes. There are 3 possible keys for each parameter: \"type\" - key specifies a type for current parameter, \"value\" - key specifies default value for parameter, \"required\" - key specifies whether this parameter must be set ( \"required\": true ) or might not ( \"required\": false ) Example : config.json file of the Gromacs pipeline: Note : In addition to main_file and config.json you can add any number of files to the CODE section and combine it in one whole scenario.","title":"config.json"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#configuration","text":"This section represents pipeline execution parameters which are set in config.json file. The parameters can be changed here and config.json file will be changed respectively. See how to edit configuration here . A configuration specifies: Section Control Description Name Pipeline and its configuration names. Estimated price per hour Control shows machine hours prices. If you navigate mouse to \"info\" icon, you'll see the maximum, minimum and average price for a particular pipeline version run as well as the price per hour. Exec environment This section lists execution environment parameters. Docker image A name of a Docker image to use for a pipeline execution (e.g. \"library/gromacs-gpu\"). Node type An instance type in terms of Amazon: CPU, RAM, GPU (e.g. 2 CPU cores, 8 Gb RAM, 0 GPU cores). Disk Size of a disk in gigabytes, that will be attached to the instance in Gb. Configure cluster button On-click, pop-up window will be shown: Here you can configure cluster or auto-scaled cluster. Cluster is a collection of instances which are connected so that they can be used together on a task. In both cases, a number of additional worker nodes with some main node as cluster head are launching (total number of pipelines = \"number of working nodes\" + 1) having NFS network file system within one node. See v.0.14 - 7.2. Launch Detached Configuration for details. In case of using cluster , an exact count of worker nodes is directly defined by the user before launching the task and could not changing during the run. In case of using auto-scaled cluster , a max count of worker nodes is defined by the user before launching the task but really used count of worker nodes can change during the run depending on the jobs queue load. For configure cluster: in opened window click Cluster button specify a number of child nodes (workers' count) and click Ok button: When user selects Cluster option, information on total cluster resources is shown. Resources are calculated as (CPU/RAM/GPU)*(NUMBER_OF_WORKERS+1) : For configure auto-scaled cluster : in opened window click Auto-scaled cluster button specify a number of child nodes (workers' count) in field Auto-scaled up to and click Ok button: Note : that number is meaning total count of \"auto-scaled\" nodes - it is the max count of worker nodes that could be attached to the main node to work together as cluster. These nodes will be attached to the cluster only in case if some jobs are in waiting state longer than a specific time. Also these nodes will be dropped from the cluster in case when jobs queue is empty or all jobs are running and there are some idle nodes longer than a specific time. Note : about timeout periods for scale-up and scale-down of auto-scaled cluster see here . also you may specify a number of \"persistent\" child nodes (workers' count) - click Setup default child nodes count , input Default child nodes number and click Ok button: These default child nodes will be never \"scaled-down\" during the run regardless of jobs queue load. In the example above, total count of \"auto-scaled\" nodes - 3, and 1 of them is \"persistent\". Note : total count of child nodes always must be greater than count of default (\"persistent\") child nodes. if you don't want to use default (\"persistent\") child nodes in your auto-scaled cluster - click Reset button opposite the Default child nodes field. When user selects Auto-scaled cluster , information on total cluster resources is shown as interval - from the \"min\" configuration to \"max\" configuration: \"min\" configuration resources are calculated as (CPU\\/RAM\\/GPU)*(NUMBER_OF_DEFAULT_WORKERS+1) \"max\" configuration resources are calculated as (CPU\\/RAM\\/GPU)*(TOTAL_NUMBER_OF_WORKERS+1) E.g. for auto-scaled cluster with 2 child nodes and without default (\"persistent\") child nodes (NUMBER_OF_DEFAULT_WORKERS = 0; TOTAL_NUMBER_OF_WORKERS = 2) : E.g. for auto-scaled cluster with 2 child nodes and 1 default (\"persistent\") child node (NUMBER_OF_DEFAULT_WORKERS = 1; TOTAL_NUMBER_OF_WORKERS = 2) : Note : if you don't want to use any cluster - click Single node button and then click Ok button. Cloud Region A region, where computing nodes are located. This select allows to decrease time of data movement for huge data volumes by choosing the nearest region. Please note that a \"default\" region for all runs is \"US East\". If a non-default region is selected - certain CP features may be unavailable: EFS storages usage from the another region (e.g. EU West region cannot use EFS storages from the \"US East\"). S3 buckets will be still available If a specific tool, used for a run, requires an on-premise license server (e.g. monolix, matlab, schrodinger, etc.) - such instances shall be run in a region, that hosts those license servers. Advanced Price type Choose spot or on-demand type of instance. You can look information about price types hovering \"Info\" icon and based on it make your choice. Timeout (min) After this time pipeline will shut down (optional). Limit mounts Allow to specify storages that should be mounted. Cmd template A shell command that will be executed to start a pipeline. \"Start idle\" The flag sets cmd_template to sleep infinity . For more information about starting a job in this mode refer to 15. Interactive services . Parameters This section lists pipeline specific parameters that can be used during a pipeline run. Pipeline parameters can be assigned the following types: String - generic scalar value (e.g. Sample name). Path - path in a data storage hierarchy. Input - path in a data storage hierarchy. During pipeline initialization, this path will be used to download input data on the calculation node for processing from a storage. Output - path in a data storage hierarchy. During pipeline finalization, this path will be used to upload resulting data to a storage. Common - path in a data storage hierarchy. Similar to \"Input\" type, but this data will not be erased from a calculation node, when a pipeline is finished (this is useful for reference data, as it can be reused by further pipeline runs that share the same reference). Note : You can use Project attribute values as parameters for the Run : Click an empty parameter value field. Enter \" project. \" In the drop-down list select the Project attribute value: Add parameter This control helps to add an additional parameter to a configuration.","title":"CONFIGURATION"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#configuration-tab-controls","text":"Control Description Add To create a customized configuration for the pipeline, click the + ADD button in the upper-right corner of the screen. For more details see here . Save This button saves changes in a configuration.","title":"Configuration tab controls"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#history","text":"This section contains information about all the current pipeline version's runs. Runs info is organized into a table with the following columns: Run - run name that consists of pipeline name and run id. Parent-run - id of the run that executed current run (this field is non-empty only for runs that are executed by other runs). Pipeline - pipeline name. Version - pipeline version. Started - time pipeline started running. Completed - time pipeline finished execution. Elapsed time - pipeline running time. Estimated price - estimated price of run, which is calculated based on the run duration and instance type. Owner - user who launched run. You can filter runs by clicking the filter icon . By using the filter control you can choose whether display runs for current pipeline version or display runs for all pipeline versions.","title":"HISTORY"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#history-tab-controls","text":"Control Description Pause (a) To pause running pipeline press this control. This control is available only for on-demand instances. Resume (b) To resume pausing pipeline press this control. This control is available only for on-demand instances. Stop (c) To stop running pipeline press this control. Rerun (d) This control reruns completed pipeline's runs. Log (e) \"Log\" control opens detailed information about the run. You'll be redirected to \" Runs \" space (see 11. Manage Runs ).","title":"History tab controls"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#pipeline-runs-states","text":"Icons at the left represent the current state of the pipeline runs: Rotating - a run is scheduled but is waiting for a calculation node to appear. Download - now pipeline Docker image is downloaded to the node. Play - The pipeline is running. The node is appearing and pipeline input data is being downloaded to the node before the \" InitializeEnvironment \" service task appears. OK - successful pipeline execution. Caution - unsuccessful pipeline execution. Time - a pipeline manually stopped.","title":"Pipeline run's states"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#storage-rules","text":"This section displays a list of rules used to upload data to the output data storage, once pipeline finished. It helps to store only data you need and minimize the amount of interim data in S3 buckets. Info is organized into a table with the following columns: Mask column contains a relative path from the $ANALYSIS_DIR folder (see Default environment variables section below for more information). Mask uses bash syntax to specify the data that you want to upload from the $ANALYSIS_DIR. Data from the specified path will be uploaded to the bucket from the pipeline node. Note : by default whole $ANALYSIS_DIR folder is uploaded to the cloud bucket (default Mask is - \"*\"). For example, \"*.txt*\" mask specifies that all files with .txt extension need to be uploaded from the $ANALYSIS_DIR to the data storage. Note : Be accurate when specifying masks - if wildcard mask (\"*\") is specified, all files will be uploaded, no matter what additional masks are specified. The Created column shows date and time of rules creation. Move to Short-Term Storage column indicates whether pipeline output data will be moved to a short-term storage.","title":"STORAGE RULES"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#storage-rules-tab-control","text":"Control Description Add new rule (a) This control allows adding a new data managing rule. Delete (b) To delete a data managing rule press this control.","title":"Storage rules tab control"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#graph","text":"This section represents the sequence of pipeline tasks as a directed graph. Tasks are graph vertices, edges represent execution order. A task can be executed only when all input edges - associated tasks - are completed (see more information about creating a pipeline with GRAPH section here ). Note : only for Luigi and WDL pipelines. Note : If main_file has mistakes, pipeline workflow won't be visualized.","title":"GRAPH"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#graph-tab-controls","text":"When a PipelineBuilder graph is loaded, the following layout controls become available to the user. Control Description Layout performs graph linearization, make it more readable. Fit zooms graph to fit the screen. Show links enables/disables workflow level links to the tasks. It is disabled by default, as for large workflows it overwhelms the visualization. Revert reverts all changes to the last saving. Save saves changes.","title":"Graph tab controls"},{"location":"manual/06_Manage_Pipeline/6._Manage_Pipeline/#default-environment-variables","text":"Pipeline scripts (e.g. main_file ) use default environmental variables for pipeline execution. These variables are set in internal CP scripts: RUN_ID - pipeline run ID. PIPELINE_NAME - pipeline name. COMMON_DIR - directory where pipeline common data (parameter with \"type\": \"common\" ) will be stored. ANALYSIS_DIR - directory where output data of the pipeline (parameter with \"type\": \"output\" ) will be stored. INPUT_DIR - directory where input data of the pipeline (parameter with \"type\": \"input\" ) will be stored. SCRIPTS_DIR - directory where all pipeline scripts and config.json file will be stored.","title":"Default environment variables"},{"location":"manual/07_Manage_Detached_configuration/7.1._Create_and_customize_Detached_configuration/","text":"7.1. Create and customize Detached configuration Create Detached configuration Customize Detached configuration Edit detached configuration name and description Edit detached configuration permissions Add run configuration to detached configuration Edit run configuration in the Detached configuration Set a root entity and map configuration parameters Delete run configuration from the Detached configuration Create Detached configuration To create Detached configuration in a Folder you need to have WRITE permission for that folder and a role CONFIGURATION_MANAGER . For more information see 13. Permissions . Note : you can create a specific type of a run configuration which could be used only for a specific type of data. In such type of a run configuration, you can link type of data (e.g. Sample, Participant, etc) and the algorithm - a pipeline. To do that, you shall create your run configuration in a Project folder. Learn how to create a project here . To create a Detached configuration: Navigate to the folder where you want to create. Click + Create \u2192 Configuration . Enter Configuration name and Configuration description in pop-up window. Click Create . The configuration will be shown in the Library. Customize Detached configuration To edit Detached configuration you need WRITE permissions for it. For more information see 13. Permissions . Edit detached configuration name and description Navigate to the Folder where the Detached configuration is stored. Click icon. The \"Edit configuration info\" pop-up window will be open. Change Detached configuration name and description. Click Save . Edit detached configuration permissions Navigate to the Detached configuration and click icon. Note: Also you can navigate to the Folder where the Detached configuration is stored and click \"Pencil\" icon. Go to Permissions tab. Click Add user or Add user group . In the example screenshots, we grant permissions to a user. Enter user's name. Auto-filling will help you. A user will be added to the list. Click User's name to manage user's permissions. Tick appropriate permissions. For more details see 13. Permissions . Add run configuration to detached configuration Select Detached configuration in the Library . Add new Run configuration via + ADD button. Enter the name, description of the new Run configuration . If the Detached configuration already has more than one Run configuration , select the template for the new one. The new configuration will be based on the template . Click Create . New Run Configuration will be represented at Detached configuration details pane. Edit run configuration in the Detached configuration Select Detached configuration in the Library . Go to the tab with the Run configuration that you want to change. Change parameters of the Run configuration. Click Save . Set a root entity and map configuration parameters If your configuration stored in a folder with a Project type, when, to set a \"Root entity\" field, you shall add metadata to your project. After that, you'll be able to select metadata entity type from the drop-down list. Note : learn how to create a project here and about managing metadata here . Click \" Root entity \" combo-box. Choose the object from default values: Participants, Samples, Pairs, Sets of Participants, Sets of Samples, Sets of Pairs. When you select \" Root entity \", you'll be able to map configuration parameters to the root entity metadata attributes. You can set it using expansion expressions. Click an empty parameter value field. Enter \" this. \". \" this \" means that value is attributed to the selected Root entity type . In the drop-down list select the metadata value. Delete run configuration from the Detached configuration Select Detached configuration in the Library . Go to the tab with the Run configuration that you want to delete. Click Remove . Confirm removal.","title":"7.1 Create and customize Detached configuration"},{"location":"manual/07_Manage_Detached_configuration/7.1._Create_and_customize_Detached_configuration/#71-create-and-customize-detached-configuration","text":"Create Detached configuration Customize Detached configuration Edit detached configuration name and description Edit detached configuration permissions Add run configuration to detached configuration Edit run configuration in the Detached configuration Set a root entity and map configuration parameters Delete run configuration from the Detached configuration","title":"7.1. Create and customize Detached configuration"},{"location":"manual/07_Manage_Detached_configuration/7.1._Create_and_customize_Detached_configuration/#create-detached-configuration","text":"To create Detached configuration in a Folder you need to have WRITE permission for that folder and a role CONFIGURATION_MANAGER . For more information see 13. Permissions . Note : you can create a specific type of a run configuration which could be used only for a specific type of data. In such type of a run configuration, you can link type of data (e.g. Sample, Participant, etc) and the algorithm - a pipeline. To do that, you shall create your run configuration in a Project folder. Learn how to create a project here . To create a Detached configuration: Navigate to the folder where you want to create. Click + Create \u2192 Configuration . Enter Configuration name and Configuration description in pop-up window. Click Create . The configuration will be shown in the Library.","title":"Create Detached configuration"},{"location":"manual/07_Manage_Detached_configuration/7.1._Create_and_customize_Detached_configuration/#customize-detached-configuration","text":"To edit Detached configuration you need WRITE permissions for it. For more information see 13. Permissions .","title":"Customize Detached configuration"},{"location":"manual/07_Manage_Detached_configuration/7.1._Create_and_customize_Detached_configuration/#edit-detached-configuration-name-and-description","text":"Navigate to the Folder where the Detached configuration is stored. Click icon. The \"Edit configuration info\" pop-up window will be open. Change Detached configuration name and description. Click Save .","title":"Edit\u00a0detached configuration name and description"},{"location":"manual/07_Manage_Detached_configuration/7.1._Create_and_customize_Detached_configuration/#edit-detached-configuration-permissions","text":"Navigate to the Detached configuration and click icon. Note: Also you can navigate to the Folder where the Detached configuration is stored and click \"Pencil\" icon. Go to Permissions tab. Click Add user or Add user group . In the example screenshots, we grant permissions to a user. Enter user's name. Auto-filling will help you. A user will be added to the list. Click User's name to manage user's permissions. Tick appropriate permissions. For more details see 13. Permissions .","title":"Edit\u00a0detached configuration permissions"},{"location":"manual/07_Manage_Detached_configuration/7.1._Create_and_customize_Detached_configuration/#add-run-configuration-to-detached-configuration","text":"Select Detached configuration in the Library . Add new Run configuration via + ADD button. Enter the name, description of the new Run configuration . If the Detached configuration already has more than one Run configuration , select the template for the new one. The new configuration will be based on the template . Click Create . New Run Configuration will be represented at Detached configuration details pane.","title":"Add\u00a0run configuration\u00a0to detached configuration"},{"location":"manual/07_Manage_Detached_configuration/7.1._Create_and_customize_Detached_configuration/#edit-run-configuration-in-the-detached-configuration","text":"Select Detached configuration in the Library . Go to the tab with the Run configuration that you want to change. Change parameters of the Run configuration. Click Save .","title":"Edit run configuration in the Detached configuration"},{"location":"manual/07_Manage_Detached_configuration/7.1._Create_and_customize_Detached_configuration/#set-a-root-entity-and-map-configuration-parameters","text":"If your configuration stored in a folder with a Project type, when, to set a \"Root entity\" field, you shall add metadata to your project. After that, you'll be able to select metadata entity type from the drop-down list. Note : learn how to create a project here and about managing metadata here . Click \" Root entity \" combo-box. Choose the object from default values: Participants, Samples, Pairs, Sets of Participants, Sets of Samples, Sets of Pairs. When you select \" Root entity \", you'll be able to map configuration parameters to the root entity metadata attributes. You can set it using expansion expressions. Click an empty parameter value field. Enter \" this. \". \" this \" means that value is attributed to the selected Root entity type . In the drop-down list select the metadata value.","title":"Set a root entity and map configuration parameters"},{"location":"manual/07_Manage_Detached_configuration/7.1._Create_and_customize_Detached_configuration/#delete-run-configuration-from-the-detached-configuration","text":"Select Detached configuration in the Library . Go to the tab with the Run configuration that you want to delete. Click Remove . Confirm removal.","title":"Delete\u00a0run configuration\u00a0from the Detached configuration"},{"location":"manual/07_Manage_Detached_configuration/7.2._Launch_Detached_Configuration/","text":"7.2. Launch Detached Configuration To launch a Detached configuration you need to have EXECUTE permissions for it. For more information see 13. Permissions . Detached configuration represents a configuration for running instances as a cluster. For example, when you need different instances running at one task: master machine and one or several worker machines are configured. They may or may not use one docker image and run different scripts. There are different options to run the cluster in Cloud Pipeline : To use Configure cluster button at Launch pipeline page or at Detached configuration page. Click the button, configure cluster - you will be offered to start current configuration at several machines (\"working nodes\"). I.e. there will start several identically configured machines having NFS network file system within each. It is impossible to run all configurations for the pipeline at once as a cluster. For more details see 6. Manage Pipeline . To use Detached configuration . Cluster configuration is a set of configurations for nodes that may or may not start some pipeline. Configurations in Detached configuration page are typically different. From Detached configuration page, you can launch all configurations at once as a cluster or launch configurations one by one as via Launch pipeline tab. Launch detached cluster configuration as a cluster Navigate to Detached Configuration details page. Click Run \u2192 Run cluster . All the Run configurations of the Detached configuration will start execution. Note : Select Run selected to launch only the opened Run configuration. In case of launching Root entity configuration , a pop-up window emerges. Select an appropriate metadata in correspondence with root entity . If root entity is an attribute of the selected metadata, use expansion expression in the Define expression field . Click OK .","title":"7.2 Launch Detached configuration"},{"location":"manual/07_Manage_Detached_configuration/7.2._Launch_Detached_Configuration/#72-launch-detached-configuration","text":"To launch a Detached configuration you need to have EXECUTE permissions for it. For more information see 13. Permissions . Detached configuration represents a configuration for running instances as a cluster. For example, when you need different instances running at one task: master machine and one or several worker machines are configured. They may or may not use one docker image and run different scripts. There are different options to run the cluster in Cloud Pipeline : To use Configure cluster button at Launch pipeline page or at Detached configuration page. Click the button, configure cluster - you will be offered to start current configuration at several machines (\"working nodes\"). I.e. there will start several identically configured machines having NFS network file system within each. It is impossible to run all configurations for the pipeline at once as a cluster. For more details see 6. Manage Pipeline . To use Detached configuration . Cluster configuration is a set of configurations for nodes that may or may not start some pipeline. Configurations in Detached configuration page are typically different. From Detached configuration page, you can launch all configurations at once as a cluster or launch configurations one by one as via Launch pipeline tab.","title":"7.2. Launch Detached Configuration"},{"location":"manual/07_Manage_Detached_configuration/7.2._Launch_Detached_Configuration/#launch-detached-cluster-configuration-as-a-cluster","text":"Navigate to Detached Configuration details page. Click Run \u2192 Run cluster . All the Run configurations of the Detached configuration will start execution. Note : Select Run selected to launch only the opened Run configuration. In case of launching Root entity configuration , a pop-up window emerges. Select an appropriate metadata in correspondence with root entity . If root entity is an attribute of the selected metadata, use expansion expression in the Define expression field . Click OK .","title":"Launch detached\u00a0cluster\u00a0configuration as a cluster"},{"location":"manual/07_Manage_Detached_configuration/7.3._Expansion_Expressions/","text":"7.3. Expansion Expressions As the Root entity has a number of attributes and the entity is connected with other entities and their attributes, we need a special way to define them as parameters of configuration or be able to launch one entity metadata under a configuration with a different type of root entity. For this purpose we use expansion expressions : \"this. ...\" - \"this\" keyword references a specific instance of an entity. It shall be followed by an attribute of an instance. Additionally, set of attributes of corresponding entities can be chained (see examples below). \"this.attribute\" - \"this.fastq\" \"this.attribute.attribute. ...\" - chaining allows creating more complex pipeline runs. In this case, each \"attribute\" keyword will be expanded and used as an input for the next \"attribute\". For example, \" this.control_sample.r1_fastq \". \"this.attribute\" construction is also useful at Root entity configuration launching step when a user selects metadata: Example 1 . The root entity for the configuration is \" Samples \", but user selects a \" Pair \" in the pop-up window. The \" Pair \" has a link to two samples as attributes . A user here should define the desired attribute ( sample ) to launch pipeline with the specified root entity. For example, type \" this.control_sample \" in Define expression field. Example 2 . The root entity is \" Samples \", but user selects \" Sample Set \" and wants to run the analysis for all the samples in the set . A user here should type the following expression \" this.Samples \" in Define expression field.","title":"7.3 Expansion expressions"},{"location":"manual/07_Manage_Detached_configuration/7.3._Expansion_Expressions/#73-expansion-expressions","text":"As the Root entity has a number of attributes and the entity is connected with other entities and their attributes, we need a special way to define them as parameters of configuration or be able to launch one entity metadata under a configuration with a different type of root entity. For this purpose we use expansion expressions : \"this. ...\" - \"this\" keyword references a specific instance of an entity. It shall be followed by an attribute of an instance. Additionally, set of attributes of corresponding entities can be chained (see examples below). \"this.attribute\" - \"this.fastq\" \"this.attribute.attribute. ...\" - chaining allows creating more complex pipeline runs. In this case, each \"attribute\" keyword will be expanded and used as an input for the next \"attribute\". For example, \" this.control_sample.r1_fastq \". \"this.attribute\" construction is also useful at Root entity configuration launching step when a user selects metadata: Example 1 . The root entity for the configuration is \" Samples \", but user selects a \" Pair \" in the pop-up window. The \" Pair \" has a link to two samples as attributes . A user here should define the desired attribute ( sample ) to launch pipeline with the specified root entity. For example, type \" this.control_sample \" in Define expression field. Example 2 . The root entity is \" Samples \", but user selects \" Sample Set \" and wants to run the analysis for all the samples in the set . A user here should type the following expression \" this.Samples \" in Define expression field.","title":"7.3. Expansion Expressions"},{"location":"manual/07_Manage_Detached_configuration/7.4._Remove_Detached_configuration/","text":"7.4. Remove Detached configuration To delete Detached configuration you need to have WRITE permission for that configuration and a role CONFIGURATION_MANAGER . For more information see 13. Permissions . You can remove the Detached configuration . Navigate to the Folder where Cluster configuration is stored. Click icon in the detached configuration line to delete. Click Delete . Confirm the action.","title":"7.4 Remove Detached configuration"},{"location":"manual/07_Manage_Detached_configuration/7.4._Remove_Detached_configuration/#74-remove-detached-configuration","text":"To delete Detached configuration you need to have WRITE permission for that configuration and a role CONFIGURATION_MANAGER . For more information see 13. Permissions . You can remove the Detached configuration . Navigate to the Folder where Cluster configuration is stored. Click icon in the detached configuration line to delete. Click Delete . Confirm the action.","title":"7.4. Remove Detached configuration"},{"location":"manual/07_Manage_Detached_configuration/7._Manage_Detached_configuration/","text":"7. Manage Detached configuration \"Details\" view pane Controls \"Gear\" icon Add Run Remove Save Detached configuration is a run configuration or a set of run configurations that allows running tools and pipelines. Note : In comparison with pipeline configurations, detached configurations do not require a pipeline. \"Details\" view pane At the \" Details view \" pane you can find: Section Control Description Name Detach configuration (a) and its Run configuration (b) names Estimated price per hour Control shows machine hours prices. If you navigate mouse to \" info \" icon (c) , you'll see the maximum, minimum and average price for a particular pipeline version run as well as the price per hour. Exec environment This section lists execution environment parameters. Pipeline (d) A name of the pipeline to be executed (optional). Click on the field to select a pipeline in the pop-up. Execution environment (e) An environment platform for execution the pipeline. Click for select from the list. Docker image (f) A name of a Docker image to use for a pipeline execution (e.g. \"base-generic-centos7\"). Click on the field to select an image in the pop-up. Node type (g) An instance type in terms of Amazon: CPU, RAM and GPU (e.g. 4 CPU cores, 16 Gb RAM and 0 GPU cores). Disk (h) Size of a disk, that will be attached to the instance in Gb. Configure cluster button (i) By clicking on this button you can configure cluster or auto-scaled cluster. Cluster is a collection of instances which are connected so that they can be used together on a task. See here and here for more information. Cloud Region (j) A specific region for a compute node placement. Please note that a \"default\" region for all runs is \"US East\". If a non-default region is selected - certain CP features may be unavailable: EFS storages usage from the another region (e.g. EU West region cannot use EFS storages from the \"US East\"). S3 buckets will be still available If a specific tool, used for a run, requires an on-premise license server (e.g. monolix, matlab, schrodinger, etc.) - such instances shall be run in a region, that hosts those license servers. Total resources (q) Information about total resources that will be used for running pipeline with specified parameters (depends on node type and cluster configuration). See here for more details. Advanced Price type (k) Choose spot or on-demand type of instance. The \"Info\" icon can give you additional information, which helps you to make choice. Timeout (min) (l) After this time pipeline will shut down (optional). Limit mounts (m) Restricts available storages for the tools or pipelines. Cmd template (n) A shell command that will be executed on the running node. \"Start idle\" The flag sets Cmd template to \"sleep infinity\". For more information about starting a job in this mode refer to 15. Interactive services . Parameters This section lists pipeline specific parameters that can be used during a pipeline run. Pipeline parameters can be assigned the following types (p) : String - generic scalar value (e.g. Sample name). Boolean - boolean value. Path - path in a data storage hierarchy. Input - path in a data storage hierarchy. During pipeline initialization, this path will be used to get data from a storage. Output - path in a data storage hierarchy. During pipeline finalization, this path will be used to upload resulting data to a storage. Common - path in a data storage hierarchy. Similar to \"Input\" type, but this data will not be erased from a calculation node, when a pipeline is finished (this is useful for reference data, as it can be reused by further pipeline runs that share the same reference). Note : You can use Project attribute values as parameters for the Run : Click an empty parameter value field. Enter \"project.\" In the drop-down list select the Project attribute value. Add parameter (o) This control helps to add an additional parameter to a configuration. Root entity type Note : This parameter is only available for configurations that are stored in Project type of the Folder and the Project has to store metadata object(s) within. See 7.1. Create and customize Detached configuration . It defines an entity which metadata will be used to process data. Default values: Participants, Samples, Pairs, Sets of Participants, Sets of Samples, Sets of Pairs. Controls There are buttons at the top of the \"Details\" view: \"Gear\" icon Allows changing a name, description of the configuration and permissions for it (a) . See 7.1. Create and customize Detached configuration . Add Allows adding machine configuration (b) . See 7.1. Create and customize Detached configuration . Run Allows launching one machine or all machines as a cluster (c) . See 7.2. Launch Detached Configuration . Remove Allows removing machine configuration (d) . See 7.4. Remove Detached configuration . Save Allow saving machine configuration (e) . See 7.1. Create and customize Detached configuration .","title":"7.0 Overview"},{"location":"manual/07_Manage_Detached_configuration/7._Manage_Detached_configuration/#7-manage-detached-configuration","text":"\"Details\" view pane Controls \"Gear\" icon Add Run Remove Save Detached configuration is a run configuration or a set of run configurations that allows running tools and pipelines. Note : In comparison with pipeline configurations, detached configurations do not require a pipeline.","title":"7. Manage Detached configuration"},{"location":"manual/07_Manage_Detached_configuration/7._Manage_Detached_configuration/#details-view-pane","text":"At the \" Details view \" pane you can find: Section Control Description Name Detach configuration (a) and its Run configuration (b) names Estimated price per hour Control shows machine hours prices. If you navigate mouse to \" info \" icon (c) , you'll see the maximum, minimum and average price for a particular pipeline version run as well as the price per hour. Exec environment This section lists execution environment parameters. Pipeline (d) A name of the pipeline to be executed (optional). Click on the field to select a pipeline in the pop-up. Execution environment (e) An environment platform for execution the pipeline. Click for select from the list. Docker image (f) A name of a Docker image to use for a pipeline execution (e.g. \"base-generic-centos7\"). Click on the field to select an image in the pop-up. Node type (g) An instance type in terms of Amazon: CPU, RAM and GPU (e.g. 4 CPU cores, 16 Gb RAM and 0 GPU cores). Disk (h) Size of a disk, that will be attached to the instance in Gb. Configure cluster button (i) By clicking on this button you can configure cluster or auto-scaled cluster. Cluster is a collection of instances which are connected so that they can be used together on a task. See here and here for more information. Cloud Region (j) A specific region for a compute node placement. Please note that a \"default\" region for all runs is \"US East\". If a non-default region is selected - certain CP features may be unavailable: EFS storages usage from the another region (e.g. EU West region cannot use EFS storages from the \"US East\"). S3 buckets will be still available If a specific tool, used for a run, requires an on-premise license server (e.g. monolix, matlab, schrodinger, etc.) - such instances shall be run in a region, that hosts those license servers. Total resources (q) Information about total resources that will be used for running pipeline with specified parameters (depends on node type and cluster configuration). See here for more details. Advanced Price type (k) Choose spot or on-demand type of instance. The \"Info\" icon can give you additional information, which helps you to make choice. Timeout (min) (l) After this time pipeline will shut down (optional). Limit mounts (m) Restricts available storages for the tools or pipelines. Cmd template (n) A shell command that will be executed on the running node. \"Start idle\" The flag sets Cmd template to \"sleep infinity\". For more information about starting a job in this mode refer to 15. Interactive services . Parameters This section lists pipeline specific parameters that can be used during a pipeline run. Pipeline parameters can be assigned the following types (p) : String - generic scalar value (e.g. Sample name). Boolean - boolean value. Path - path in a data storage hierarchy. Input - path in a data storage hierarchy. During pipeline initialization, this path will be used to get data from a storage. Output - path in a data storage hierarchy. During pipeline finalization, this path will be used to upload resulting data to a storage. Common - path in a data storage hierarchy. Similar to \"Input\" type, but this data will not be erased from a calculation node, when a pipeline is finished (this is useful for reference data, as it can be reused by further pipeline runs that share the same reference). Note : You can use Project attribute values as parameters for the Run : Click an empty parameter value field. Enter \"project.\" In the drop-down list select the Project attribute value. Add parameter (o) This control helps to add an additional parameter to a configuration. Root entity type Note : This parameter is only available for configurations that are stored in Project type of the Folder and the Project has to store metadata object(s) within. See 7.1. Create and customize Detached configuration . It defines an entity which metadata will be used to process data. Default values: Participants, Samples, Pairs, Sets of Participants, Sets of Samples, Sets of Pairs.","title":"\"Details\" view pane"},{"location":"manual/07_Manage_Detached_configuration/7._Manage_Detached_configuration/#controls","text":"There are buttons at the top of the \"Details\" view:","title":"Controls"},{"location":"manual/07_Manage_Detached_configuration/7._Manage_Detached_configuration/#gear-icon","text":"Allows changing a name, description of the configuration and permissions for it (a) . See 7.1. Create and customize Detached configuration .","title":"\"Gear\" icon"},{"location":"manual/07_Manage_Detached_configuration/7._Manage_Detached_configuration/#add","text":"Allows adding machine configuration (b) . See 7.1. Create and customize Detached configuration .","title":"Add"},{"location":"manual/07_Manage_Detached_configuration/7._Manage_Detached_configuration/#run","text":"Allows launching one machine or all machines as a cluster (c) . See 7.2. Launch Detached Configuration .","title":"Run"},{"location":"manual/07_Manage_Detached_configuration/7._Manage_Detached_configuration/#remove","text":"Allows removing machine configuration (d) . See 7.4. Remove Detached configuration .","title":"Remove"},{"location":"manual/07_Manage_Detached_configuration/7._Manage_Detached_configuration/#save","text":"Allow saving machine configuration (e) . See 7.1. Create and customize Detached configuration .","title":"Save"},{"location":"manual/08_Manage_Data_Storage/8.1._Create_and_edit_storage/","text":"8.1. Create and edit storage Create s3 storage Edit storage To create a Storage in a Folder you need to have WRITE permission for that folder and a STORAGE_MANAGER role. For more information see 13. Permissions . You also can create Storage via CLI . See 14.3. Manage Storage via CLI . Create s3 storage Navigate to the folder where you want to create data storage. Click + Create \u2192 Storage \u2192 Create new s3 storage . Note : choose Add existing s3 storage to use an already existing bucket for this data storage. Note : how to create NFS mount see here . Fill in the \" Info \" form: Storage path - path to access the storage (bucket name). If on Data storage tab in Preferences section of system-level settings storage.object.prefix (see v.0.14 - 12.10. Manage system-level settings ) is set - all new storages will be created with this prefix (e.g. \" ds \"): Alias - storage name. Cloud region - location region of a data storage. This select allows to decrease time of data movement for huge data volumes by choosing the nearest region. Please note that a \"default\" region for all runs is \"US East\". If a non-default region is selected - certain CP features may be unavailable: EFS storages usage from the another region (e.g. EU West region cannot use EFS storages from the \"US East\"). S3 buckets will be still available. If a specific tool, used for a run, requires an on-premise license server (e.g. monolix, matlab, schrodinger, etc.) - such instances shall be run in a region, that hosts those license servers. Description - description of the data storage and comments. STS duration - short-term storage duration (days). LTS duration - long-term storage duration (days). Enable versioning box and backup duration - how long backup is stored (days). Note : If you want to store data permanently, leave fields empty. Click \"Create\" button. Edit storage You may change Alias, Description, STS and LTS duration. Example: Select storage. Click icon. Change number of days in STS and LTS duration fields. If you want to store data permanently, leave fields empty. Click \"Save\" button.","title":"8.1 Create and edit Data Storage"},{"location":"manual/08_Manage_Data_Storage/8.1._Create_and_edit_storage/#81-create-and-edit-storage","text":"Create s3 storage Edit storage To create a Storage in a Folder you need to have WRITE permission for that folder and a STORAGE_MANAGER role. For more information see 13. Permissions . You also can create Storage via CLI . See 14.3. Manage Storage via CLI .","title":"8.1. Create and edit storage"},{"location":"manual/08_Manage_Data_Storage/8.1._Create_and_edit_storage/#create-s3-storage","text":"Navigate to the folder where you want to create data storage. Click + Create \u2192 Storage \u2192 Create new s3 storage . Note : choose Add existing s3 storage to use an already existing bucket for this data storage. Note : how to create NFS mount see here . Fill in the \" Info \" form: Storage path - path to access the storage (bucket name). If on Data storage tab in Preferences section of system-level settings storage.object.prefix (see v.0.14 - 12.10. Manage system-level settings ) is set - all new storages will be created with this prefix (e.g. \" ds \"): Alias - storage name. Cloud region - location region of a data storage. This select allows to decrease time of data movement for huge data volumes by choosing the nearest region. Please note that a \"default\" region for all runs is \"US East\". If a non-default region is selected - certain CP features may be unavailable: EFS storages usage from the another region (e.g. EU West region cannot use EFS storages from the \"US East\"). S3 buckets will be still available. If a specific tool, used for a run, requires an on-premise license server (e.g. monolix, matlab, schrodinger, etc.) - such instances shall be run in a region, that hosts those license servers. Description - description of the data storage and comments. STS duration - short-term storage duration (days). LTS duration - long-term storage duration (days). Enable versioning box and backup duration - how long backup is stored (days). Note : If you want to store data permanently, leave fields empty. Click \"Create\" button.","title":"Create s3 storage"},{"location":"manual/08_Manage_Data_Storage/8.1._Create_and_edit_storage/#edit-storage","text":"You may change Alias, Description, STS and LTS duration. Example: Select storage. Click icon. Change number of days in STS and LTS duration fields. If you want to store data permanently, leave fields empty. Click \"Save\" button.","title":"Edit storage"},{"location":"manual/08_Manage_Data_Storage/8.2._Upload_Download_data/","text":"8.2. Upload/Download data Upload data Download data Generate URL To edit a Storage you need to have WRITE permission for the Storage . For more information see 13. Permissions . You also can upload and download data via CLI. See 14.3. Manage Storage via CLI . Upload data Click Upload button in the storage and folder of your choice: Browse file(s) to upload. Note : make sure size doesn't exceed 5 Gb. Note : you can cancel upload process by clicking the \"Cancel\" button. As a result, the file will be uploaded to the CP system. Note: the uploaded file will be tagged with auto-created attribute: CP_OWNER . The value of the attribute will be set as a user ID. The exception is that the storage is based on NFS share. Files in such data storage don't have attributes at all. Download data Click the Download button next to a file name. Specify where to download in the pop-up window. As a result, the file will be downloaded via your browser to the specified location. Generate URL You can use this to generate URLs for a number of files and then download them manually one by one or via scripts. Select files using a checkbox. Click the Generate URL button. A list of URLs (one for each file) will be generated.","title":"8.2 Upload/Download data"},{"location":"manual/08_Manage_Data_Storage/8.2._Upload_Download_data/#82-uploaddownload-data","text":"Upload data Download data Generate URL To edit a Storage you need to have WRITE permission for the Storage . For more information see 13. Permissions . You also can upload and download data via CLI. See 14.3. Manage Storage via CLI .","title":"8.2. Upload/Download data"},{"location":"manual/08_Manage_Data_Storage/8.2._Upload_Download_data/#upload-data","text":"Click Upload button in the storage and folder of your choice: Browse file(s) to upload. Note : make sure size doesn't exceed 5 Gb. Note : you can cancel upload process by clicking the \"Cancel\" button. As a result, the file will be uploaded to the CP system. Note: the uploaded file will be tagged with auto-created attribute: CP_OWNER . The value of the attribute will be set as a user ID. The exception is that the storage is based on NFS share. Files in such data storage don't have attributes at all.","title":"Upload data"},{"location":"manual/08_Manage_Data_Storage/8.2._Upload_Download_data/#download-data","text":"Click the Download button next to a file name. Specify where to download in the pop-up window. As a result, the file will be downloaded via your browser to the specified location.","title":"Download data"},{"location":"manual/08_Manage_Data_Storage/8.2._Upload_Download_data/#generate-url","text":"You can use this to generate URLs for a number of files and then download them manually one by one or via scripts. Select files using a checkbox. Click the Generate URL button. A list of URLs (one for each file) will be generated.","title":"Generate URL"},{"location":"manual/08_Manage_Data_Storage/8.3._Create_and_Edit_text_files/","text":"8.3. Create and Edit text files You can create and edit files via GUI. It may be useful when you add some description or metadata about files in the storage. Create and rename a file View and edit text file's contents View and edit tabular file's contents To edit a Storage you need to have WRITE permission for the Storage . For more information see 13. Permissions . Create and rename a file To create a file: Click + Create \u2192 File . Enter file's name. Enter file's contents (optionally). Click OK . As a result, a new file will be created. Note: the new file will be tagged with auto-created attribute: CP_OWNER . The value of the attribute will be set as a user ID. The exception is that the storage is based on NFS share. Files in such data storage don't have attributes at all. To rename a file: Click the button in the desired file line. Rename it. Click OK . View and edit text file's contents Click on the file. You will see file preview on the right. In case of the file is large you will see only a part of it. Click \"Expand\" icon at the upper right of the preview window. A pop-up text editor window appears. Click Edit and change the file. Click Save to save the changes. View and edit tabular file's contents Click on the tabular file. You will see file preview on the right. In case of the file is large you will see only a part of it. Click \"Expand\" icon at the upper right of the preview window. A pop-up text editor window appears. Click Edit and change file in tabular view or in the text view. Click Save to save the changes.","title":"8.3 Create and edit text files"},{"location":"manual/08_Manage_Data_Storage/8.3._Create_and_Edit_text_files/#83-create-and-edit-text-files","text":"You can create and edit files via GUI. It may be useful when you add some description or metadata about files in the storage. Create and rename a file View and edit text file's contents View and edit tabular file's contents To edit a Storage you need to have WRITE permission for the Storage . For more information see 13. Permissions .","title":"8.3. Create and Edit text files"},{"location":"manual/08_Manage_Data_Storage/8.3._Create_and_Edit_text_files/#create-and-rename-a-file","text":"To create a file: Click + Create \u2192 File . Enter file's name. Enter file's contents (optionally). Click OK . As a result, a new file will be created. Note: the new file will be tagged with auto-created attribute: CP_OWNER . The value of the attribute will be set as a user ID. The exception is that the storage is based on NFS share. Files in such data storage don't have attributes at all. To rename a file: Click the button in the desired file line. Rename it. Click OK .","title":"Create and rename a file"},{"location":"manual/08_Manage_Data_Storage/8.3._Create_and_Edit_text_files/#view-and-edit-text-files-contents","text":"Click on the file. You will see file preview on the right. In case of the file is large you will see only a part of it. Click \"Expand\" icon at the upper right of the preview window. A pop-up text editor window appears. Click Edit and change the file. Click Save to save the changes.","title":"View and edit text file's contents"},{"location":"manual/08_Manage_Data_Storage/8.3._Create_and_Edit_text_files/#view-and-edit-tabular-files-contents","text":"Click on the tabular file. You will see file preview on the right. In case of the file is large you will see only a part of it. Click \"Expand\" icon at the upper right of the preview window. A pop-up text editor window appears. Click Edit and change file in tabular view or in the text view. Click Save to save the changes.","title":"View and edit tabular file's contents"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/","text":"8.4. Control File versions Versioned files management Management of deleted files Management of existing files Object group level (not a version) Delete a file from a data storage Rename a file Download a file Latest version Delete the latest file version Rename the latest file version Download the latest file version Previous version(s) Set a file version as the latest Download one of the previous file versions Change backup duration File versioning management system prevents users from accident deletion of files and loss of data. It allows restoring specific versions of a file. You can also control file versions via CLI. See 14.3. Manage Storage via CLI . Versioned files management To manage file versions in GUI: In the Library tab choose an appropriate data storage. Choose the Show files versions option. Note : an OWNER or a user with ROLE_ADMIN role are able to turn on files versioning view, other users will not be able to see/use that option. After that, you'll be able to see file versions and files marked as deleted (but they are not actually deleted from the data storage yet). Deleted files (i.e. objects where the latest file version is set to a \" Delete marker \") are highlighted in red (\" file2.txt \" in the example below). To expand a list of versions for each file press the \" Plus \" icon. Management of deleted files Deleted files are kept in a data storage for some time. See more details about backup duration later in this document. Available operations for deleted versioned files: on the object group level (not a version) : To delete a file completely from a data storage press the \" Delete \" icon. with the latest version: To delete a file completely press the \" Delete \" icon of its latest version (expand file version list to do it). Works the same way as for object group level. with the previous version(s): These operations are the same as listed in the next section. Management of existing files Available operations for existing versioned files: Object group level (not a version) Delete a file from a data storage To delete an existing file from a data storage press the \" Delete \" icon. If \" Show files versions \" is ON - you will have the following options: \" Set deletion marker \" - \" Delete marker \" will be set and the latest version of the file will NOT be really deleted from a data storage. Users (except \" ADMIN \" and \" OWNER \") will not be able to view this file after that. \" Delete from bucket \" - delete a file from a data storage. Note : if you don't have ROLE_ADMIN or OWNER rights, you won't have these options and you won't be able to delete a file from the data storage completely (\" Delete marker \" will be set). If \" Show files versions \" is OFF - \" Set deletion marker \" option will be used by default. Rename a file To rename an existing files press the \"Rename\" icon . In this example, we will rename \"1.txt\" to \"5.txt\". To confirm the action press OK . After that, the latest version of \"1.txt\" is set to \" Delete marker \". New file appears in the folder. Download a file To download the latest version of an existing file press the \" Download \" button . Latest version All operations below work in the same way as for object group level. Delete the latest file version To delete a file press the \" Delete \" button of its latest version. Rename the latest file version To rename the latest version of an object press the \" Rename \" button of its latest version. Download the latest file version To download the latest version of an object press the \" Download \" button of its latest version. Previous version(s) Set a file version as the latest To set a chosen version as the latest press the \" Restore \" button: Download one of the previous file versions To download an appropriate previous version of the file press the \" Download \" button. Change backup duration To change the backup time in GUI: Press the \" Edit \" button . Turn on \" Enable versioning \" option. Choose Backup duration in days. If not explicitly defined, the global default value will be applied (defined in service configuration). Note : setting such rule for a Data Storage (from GUI or CLI) will tell AWS to delete previous versions of an object that are older than a specified number of days. This configuration option will be available to users with ROLE_ADMIN or OWNER rights.","title":"8.4 Control file versions"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#84-control-file-versions","text":"Versioned files management Management of deleted files Management of existing files Object group level (not a version) Delete a file from a data storage Rename a file Download a file Latest version Delete the latest file version Rename the latest file version Download the latest file version Previous version(s) Set a file version as the latest Download one of the previous file versions Change backup duration File versioning management system prevents users from accident deletion of files and loss of data. It allows restoring specific versions of a file. You can also control file versions via CLI. See 14.3. Manage Storage via CLI .","title":"8.4. Control File versions"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#versioned-files-management","text":"To manage file versions in GUI: In the Library tab choose an appropriate data storage. Choose the Show files versions option. Note : an OWNER or a user with ROLE_ADMIN role are able to turn on files versioning view, other users will not be able to see/use that option. After that, you'll be able to see file versions and files marked as deleted (but they are not actually deleted from the data storage yet). Deleted files (i.e. objects where the latest file version is set to a \" Delete marker \") are highlighted in red (\" file2.txt \" in the example below). To expand a list of versions for each file press the \" Plus \" icon.","title":"Versioned files management"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#management-of-deleted-files","text":"Deleted files are kept in a data storage for some time. See more details about backup duration later in this document. Available operations for deleted versioned files: on the object group level (not a version) : To delete a file completely from a data storage press the \" Delete \" icon. with the latest version: To delete a file completely press the \" Delete \" icon of its latest version (expand file version list to do it). Works the same way as for object group level. with the previous version(s): These operations are the same as listed in the next section.","title":"Management of deleted files"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#management-of-existing-files","text":"Available operations for existing versioned files:","title":"Management of existing files"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#object-group-level-not-a-version","text":"","title":"Object group level (not a version)"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#delete-a-file-from-a-data-storage","text":"To delete an existing file from a data storage press the \" Delete \" icon. If \" Show files versions \" is ON - you will have the following options: \" Set deletion marker \" - \" Delete marker \" will be set and the latest version of the file will NOT be really deleted from a data storage. Users (except \" ADMIN \" and \" OWNER \") will not be able to view this file after that. \" Delete from bucket \" - delete a file from a data storage. Note : if you don't have ROLE_ADMIN or OWNER rights, you won't have these options and you won't be able to delete a file from the data storage completely (\" Delete marker \" will be set). If \" Show files versions \" is OFF - \" Set deletion marker \" option will be used by default.","title":"Delete a file from a data storage"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#rename-a-file","text":"To rename an existing files press the \"Rename\" icon . In this example, we will rename \"1.txt\" to \"5.txt\". To confirm the action press OK . After that, the latest version of \"1.txt\" is set to \" Delete marker \". New file appears in the folder.","title":"Rename a file"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#download-a-file","text":"To download the latest version of an existing file press the \" Download \" button .","title":"Download a file"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#latest-version","text":"All operations below work in the same way as for object group level.","title":"Latest version"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#delete-the-latest-file-version","text":"To delete a file press the \" Delete \" button of its latest version.","title":"Delete the latest file version"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#rename-the-latest-file-version","text":"To rename the latest version of an object press the \" Rename \" button of its latest version.","title":"Rename the latest file version"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#download-the-latest-file-version","text":"To download the latest version of an object press the \" Download \" button of its latest version.","title":"Download the latest file version"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#previous-versions","text":"","title":"Previous version(s)"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#set-a-file-version-as-the-latest","text":"To set a chosen version as the latest press the \" Restore \" button:","title":"Set a file version as the latest"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#download-one-of-the-previous-file-versions","text":"To download an appropriate previous version of the file press the \" Download \" button.","title":"Download one of the previous file versions"},{"location":"manual/08_Manage_Data_Storage/8.4._Control_File_versions/#change-backup-duration","text":"To change the backup time in GUI: Press the \" Edit \" button . Turn on \" Enable versioning \" option. Choose Backup duration in days. If not explicitly defined, the global default value will be applied (defined in service configuration). Note : setting such rule for a Data Storage (from GUI or CLI) will tell AWS to delete previous versions of an object that are older than a specified number of days. This configuration option will be available to users with ROLE_ADMIN or OWNER rights.","title":"Change backup duration"},{"location":"manual/08_Manage_Data_Storage/8.5._Delete_and_unregister_Data_Storage/","text":"8.5. Delete and unregister Data Storage Delete storage Unregister storage To delete a Storage in a Folder you need to have WRITE permission for that folder and a STORAGE_MANAGER role. For more details see 13. Permissions . You can also delete and unregister Storage via CLI . See 14.3. Manage Storage via CLI . Delete storage Note : If storage contains only metadata, it will not prevent deletion. Select a Data storage. Click Edit . Choose Delete . You will be offered to unregister or delete a storage. Click Delete . Unregister storage A user can unregister storage. Cloud bucket with data neither will be deleted nor will be accessible in Cloud Pipeline. Do the same actions as in Delete storage but choose to Unregister in step 4 .","title":"8.5 Delete and unregister Data Storage"},{"location":"manual/08_Manage_Data_Storage/8.5._Delete_and_unregister_Data_Storage/#85-delete-and-unregister-data-storage","text":"Delete storage Unregister storage To delete a Storage in a Folder you need to have WRITE permission for that folder and a STORAGE_MANAGER role. For more details see 13. Permissions . You can also delete and unregister Storage via CLI . See 14.3. Manage Storage via CLI .","title":"8.5. Delete and unregister Data Storage"},{"location":"manual/08_Manage_Data_Storage/8.5._Delete_and_unregister_Data_Storage/#delete-storage","text":"Note : If storage contains only metadata, it will not prevent deletion. Select a Data storage. Click Edit . Choose Delete . You will be offered to unregister or delete a storage. Click Delete .","title":"Delete storage"},{"location":"manual/08_Manage_Data_Storage/8.5._Delete_and_unregister_Data_Storage/#unregister-storage","text":"A user can unregister storage. Cloud bucket with data neither will be deleted nor will be accessible in Cloud Pipeline. Do the same actions as in Delete storage but choose to Unregister in step 4 .","title":"Unregister storage"},{"location":"manual/08_Manage_Data_Storage/8.6._Delete_Files_and_Folders_from_Storage/","text":"8.6. Delete Files and Folders from Storage Note : when you delete a folder, all child folders and files will be removed as well. Click the Delete button in the right side of the row with file or folder you want to delete. Confirm the action.","title":"8.6 Delete Files and Folders from Data Storage"},{"location":"manual/08_Manage_Data_Storage/8.6._Delete_Files_and_Folders_from_Storage/#86-delete-files-and-folders-from-storage","text":"Note : when you delete a folder, all child folders and files will be removed as well. Click the Delete button in the right side of the row with file or folder you want to delete. Confirm the action.","title":"8.6. Delete Files and Folders from Storage"},{"location":"manual/08_Manage_Data_Storage/8.7._Create_shared_file_system/","text":"8.7. Create shared file system Create NFS storage NFS Storage features User shall have ROLE_ADMIN to mount NFS to the Cloud Pipeline. For more information see 13. Permissions . A shared file system is a data storage based on NFS. It has several advantages over S3 and local file system: While S3 is a great option for a long-term storage, it cannot be used as a shared file system (FS) for high-performance computing jobs as it does not support FS-like interface. A local disk cannot be shared across several nodes. A user needs to specify local disk size when scheduling a run. If a user specifies a size that is not enough to finish a job - it will fail. Cloud-based shared file system could be used to workaround this issue. Create NFS storage Navigate to a desired folder and click + Create \u2192 Storages \u2192 Create new NFS mount . Note : For NFS mounts - \"Add existing\" option is not available. Specify Storage path and other optional parameters. Note : Storage path parameter contains NFS mount path nfs://fs-2a5ab373.efs.eu-central-1.amazonaws.com:/ and the name of the NFS storage to be created NFSstorage . Note : NFS storages are just subdirectories of the mounted NFS. One NFS can have multiple NFS storages. When deleted from GUI, NFS storage is unmounted from the Cloud Pipeline. NFS Storage features For NFS mounts GUI doesn't display the following features typical for S3 storages: STS LTS Versioning and Backup duration. When a user selects Input/Common/Output path parameter for a pipeline run - it is impossible to set NFS storage: NFS storages aren't displayed in the \"Browse...\" dialog for Input/Common/Output path parameters; Value of Input/Common/Output path parameters is validated so that user is not able to specify a path to NFS storage manually. The content of files stored in NFS data storage could be previewed as well as in the S3 data storage. Since NFS isn't an object storage like s3, it isn't possible to add metadata tags to files in the NFS storage. Use NFS storage between cluster nodes. If pipeline Tools contain NFS client, NFS storage(s) will be mounted automatically.","title":"8.7 Create shared file system"},{"location":"manual/08_Manage_Data_Storage/8.7._Create_shared_file_system/#87-create-shared-file-system","text":"Create NFS storage NFS Storage features User shall have ROLE_ADMIN to mount NFS to the Cloud Pipeline. For more information see 13. Permissions . A shared file system is a data storage based on NFS. It has several advantages over S3 and local file system: While S3 is a great option for a long-term storage, it cannot be used as a shared file system (FS) for high-performance computing jobs as it does not support FS-like interface. A local disk cannot be shared across several nodes. A user needs to specify local disk size when scheduling a run. If a user specifies a size that is not enough to finish a job - it will fail. Cloud-based shared file system could be used to workaround this issue.","title":"8.7. Create shared file system"},{"location":"manual/08_Manage_Data_Storage/8.7._Create_shared_file_system/#create-nfs-storage","text":"Navigate to a desired folder and click + Create \u2192 Storages \u2192 Create new NFS mount . Note : For NFS mounts - \"Add existing\" option is not available. Specify Storage path and other optional parameters. Note : Storage path parameter contains NFS mount path nfs://fs-2a5ab373.efs.eu-central-1.amazonaws.com:/ and the name of the NFS storage to be created NFSstorage . Note : NFS storages are just subdirectories of the mounted NFS. One NFS can have multiple NFS storages. When deleted from GUI, NFS storage is unmounted from the Cloud Pipeline.","title":"Create NFS storage"},{"location":"manual/08_Manage_Data_Storage/8.7._Create_shared_file_system/#nfs-storage-features","text":"For NFS mounts GUI doesn't display the following features typical for S3 storages: STS LTS Versioning and Backup duration. When a user selects Input/Common/Output path parameter for a pipeline run - it is impossible to set NFS storage: NFS storages aren't displayed in the \"Browse...\" dialog for Input/Common/Output path parameters; Value of Input/Common/Output path parameters is validated so that user is not able to specify a path to NFS storage manually. The content of files stored in NFS data storage could be previewed as well as in the S3 data storage. Since NFS isn't an object storage like s3, it isn't possible to add metadata tags to files in the NFS storage. Use NFS storage between cluster nodes. If pipeline Tools contain NFS client, NFS storage(s) will be mounted automatically.","title":"NFS Storage features"},{"location":"manual/08_Manage_Data_Storage/8.8._Data_sharing/","text":"8.8. Data sharing Create shared s3 storage Upload data to shared s3 storage Download data from shared s3 storage To create a Shared storage in a Folder you need to have READ and WRITE permissions for that folder and a STORAGE_MANAGER role. For more information see 13. Permissions . Users can share s3 data storages within a Cloud Platform for enabling of getting data files by the external partners for processing. Create shared s3 storage For the ability of getting data files by the external partners, users should be considered, that external partner has own CP account and R/W permissions for a bucket. Start creating a new object storage (for more details see here ), fill Info items. Set Enable sharing . Click Create button: Open created storage by clicking on it in the folder tree ( 1 ). Click icon in upper right corner ( 2 ): Choose Permissions tab in opened pop-up window: Click on button, enter user, for whom you want to share created storage. Confirm by clicking \" Ok \" button: Click on user name. If you want your partner can only download data from creating shared space, set Allow checkbox for READ permission, set Deny checkbox for WRITE permission: If you want your partner can download data from creating shared spaced and upload on it, set Allow checkbox both for READ and WRITE permissions: Close pop-up window. Click button. In the pop-up window generated URL will be appeared. It can be shared with the external collaborator. Upload data to shared s3 storage Shared s3 storage's collaboration space can be used to exchange large data files (up to 5Tb per one file). For storage owner : Uploading data to shared s3 storage is doing in the same way as on a regular. For more details see here . For external partner : Note : for uploading to shared bucket, user account shall be registered within CP users catalog and granted READ & WRITE permissions for a bucket. Open new browser window and insert URL, that you receive from the partner. In appeared login page enter user credentials and sign in. The following page will be opened: Click button. In opened pop-up window browse file(s) to upload. Confirm uploading. Note : make sure size doesn't exceed 5 Tb. Note : you can cancel upload process by clicking the \"Cancel\" button: As a result, the file(s) will be uploaded to the shared s3 storage: Download data from shared s3 storage For storage owner : Downloading data from shared s3 storage is doing in the same way as from a regular. For more details see here . For external partner : Note : for downloading from shared bucket, user account shall be registered within CP users catalog and granted READ permission for a bucket. Open new browser window and insert URL, that you receive from the partner. In appeared login page enter user credentials and sign in. The following page will be opened: Click to download required file. Specify where to download in the pop-up window. As a result, the file will be downloaded via your browser to the specified location.","title":"8.8 Data sharing"},{"location":"manual/08_Manage_Data_Storage/8.8._Data_sharing/#88-data-sharing","text":"Create shared s3 storage Upload data to shared s3 storage Download data from shared s3 storage To create a Shared storage in a Folder you need to have READ and WRITE permissions for that folder and a STORAGE_MANAGER role. For more information see 13. Permissions . Users can share s3 data storages within a Cloud Platform for enabling of getting data files by the external partners for processing.","title":"8.8. Data sharing"},{"location":"manual/08_Manage_Data_Storage/8.8._Data_sharing/#create-shared-s3-storage","text":"For the ability of getting data files by the external partners, users should be considered, that external partner has own CP account and R/W permissions for a bucket. Start creating a new object storage (for more details see here ), fill Info items. Set Enable sharing . Click Create button: Open created storage by clicking on it in the folder tree ( 1 ). Click icon in upper right corner ( 2 ): Choose Permissions tab in opened pop-up window: Click on button, enter user, for whom you want to share created storage. Confirm by clicking \" Ok \" button: Click on user name. If you want your partner can only download data from creating shared space, set Allow checkbox for READ permission, set Deny checkbox for WRITE permission: If you want your partner can download data from creating shared spaced and upload on it, set Allow checkbox both for READ and WRITE permissions: Close pop-up window. Click button. In the pop-up window generated URL will be appeared. It can be shared with the external collaborator.","title":"Create shared s3 storage"},{"location":"manual/08_Manage_Data_Storage/8.8._Data_sharing/#upload-data-to-shared-s3-storage","text":"Shared s3 storage's collaboration space can be used to exchange large data files (up to 5Tb per one file). For storage owner : Uploading data to shared s3 storage is doing in the same way as on a regular. For more details see here . For external partner : Note : for uploading to shared bucket, user account shall be registered within CP users catalog and granted READ & WRITE permissions for a bucket. Open new browser window and insert URL, that you receive from the partner. In appeared login page enter user credentials and sign in. The following page will be opened: Click button. In opened pop-up window browse file(s) to upload. Confirm uploading. Note : make sure size doesn't exceed 5 Tb. Note : you can cancel upload process by clicking the \"Cancel\" button: As a result, the file(s) will be uploaded to the shared s3 storage:","title":"Upload data to shared s3 storage"},{"location":"manual/08_Manage_Data_Storage/8.8._Data_sharing/#download-data-from-shared-s3-storage","text":"For storage owner : Downloading data from shared s3 storage is doing in the same way as from a regular. For more details see here . For external partner : Note : for downloading from shared bucket, user account shall be registered within CP users catalog and granted READ permission for a bucket. Open new browser window and insert URL, that you receive from the partner. In appeared login page enter user credentials and sign in. The following page will be opened: Click to download required file. Specify where to download in the pop-up window. As a result, the file will be downloaded via your browser to the specified location.","title":"Download data from shared s3 storage"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/","text":"8. Manage Data Storage Data storage represents S3 bucket and its contents. Controls Select page Show file versions Remove all selected Generate URL Show attributes/Hide attributes \"Gear\" icon Refresh + Create Upload Each-line controls View and edit a text file CLI Storage options Permissions management for a storage is described here . \"Details\" view lists contents of the bucket: files that may be organized into folders. Clicking on the inside folder will open its contents in the \"Details\" view. Note : S3 bucket folders hierarchy will not be represented in the \"Hierarchy\" view panel. Note : you can move Storage to a new parent folder using drag and drop approach. In \"Library tree view\" and \"Library details form\" S3 buckets are tagged with region flag to visually distinguish storage locations. Figure 1 Another option for navigation in the bucket is to use \"breadcrumbs\" control at the top of the \"Details\" view (see picture above, 1 ): Clicking an item will navigate to that folder. Editing a path will allow to copy/paste a path and navigate to any custom location. Controls At the top of the \"Details\" view there are buttons: Select page Clicking this control (see picture Figure 1 above, item 2 ), the whole file and folders on the current page will be selected. It allows to perform bulk operations like deleting. Show file versions Tick this checkbox (see picture Figure 1 above, item 3 ) and the view of a page will changed: the all file versions will be displayed. You can expand each version's list by clicking \" + \" in desired line. Note : the last version will be marked by \" (latest) \". Remove all selected This is a bulk operation control. It is visible, if at least one of the data storage item (folder or file) is selected. Generate URL This control helps to generate URLs for a number of files and then download them manually one by one or via scripts. See details here . Note : the control is available, if only files are selected. Show attributes/Hide attributes Allows see or edit a list of key=value attributes of the data storage (see picture Figure 1 above, item 4 ). Note : If selected storage has any defined attribute, Attributes pane is shown by default. See 17. CP objects tagging by additional attributes . \"Gear\" icon Allows to edit the path, alias, description of the storage, manage its STS and LTS and enable versions control (see picture Figure 1 above, item 5 ). The delete option is also here (if storage contains only metadata, it will be deleted anyway). See 8.1. Create and edit storage . Refresh Allows updating representation of bucket's contents (see picture Figure 1 above, item 6 ). + Create You can also create new folders and files via this button (see picture Figure 1 above, item 7 ). See 8.3. Create and Edit text files . Upload This control allows uploading files to the bucket (see picture Figure 1 above, item 8 ). See 8.2. Upload/Download data . Each-line controls Control Description Download This control calls downloading of selected file. Edit Helps to rename a file or a folder. Delete Delete a file or a folder. View and edit a text file You can view and edit text files. For more details see here . CLI Storage options There are also several options that are only implemented in CLI but not in GUI: To move files and folders from one storage to another or between local file system and storage. To copy files from one storage to another. See here for more details.","title":"8.0 Overview"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#8-manage-data-storage","text":"Data storage represents S3 bucket and its contents. Controls Select page Show file versions Remove all selected Generate URL Show attributes/Hide attributes \"Gear\" icon Refresh + Create Upload Each-line controls View and edit a text file CLI Storage options Permissions management for a storage is described here . \"Details\" view lists contents of the bucket: files that may be organized into folders. Clicking on the inside folder will open its contents in the \"Details\" view. Note : S3 bucket folders hierarchy will not be represented in the \"Hierarchy\" view panel. Note : you can move Storage to a new parent folder using drag and drop approach. In \"Library tree view\" and \"Library details form\" S3 buckets are tagged with region flag to visually distinguish storage locations. Figure 1 Another option for navigation in the bucket is to use \"breadcrumbs\" control at the top of the \"Details\" view (see picture above, 1 ): Clicking an item will navigate to that folder. Editing a path will allow to copy/paste a path and navigate to any custom location.","title":"8. Manage Data Storage"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#controls","text":"At the top of the \"Details\" view there are buttons:","title":"Controls"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#select-page","text":"Clicking this control (see picture Figure 1 above, item 2 ), the whole file and folders on the current page will be selected. It allows to perform bulk operations like deleting.","title":"Select page"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#show-file-versions","text":"Tick this checkbox (see picture Figure 1 above, item 3 ) and the view of a page will changed: the all file versions will be displayed. You can expand each version's list by clicking \" + \" in desired line. Note : the last version will be marked by \" (latest) \".","title":"Show file versions"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#remove-all-selected","text":"This is a bulk operation control. It is visible, if at least one of the data storage item (folder or file) is selected.","title":"Remove all selected"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#generate-url","text":"This control helps to generate URLs for a number of files and then download them manually one by one or via scripts. See details here . Note : the control is available, if only files are selected.","title":"Generate URL"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#show-attributeshide-attributes","text":"Allows see or edit a list of key=value attributes of the data storage (see picture Figure 1 above, item 4 ). Note : If selected storage has any defined attribute, Attributes pane is shown by default. See 17. CP objects tagging by additional attributes .","title":"Show attributes/Hide attributes"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#gear-icon","text":"Allows to edit the path, alias, description of the storage, manage its STS and LTS and enable versions control (see picture Figure 1 above, item 5 ). The delete option is also here (if storage contains only metadata, it will be deleted anyway). See 8.1. Create and edit storage .","title":"\"Gear\" icon"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#refresh","text":"Allows updating representation of bucket's contents (see picture Figure 1 above, item 6 ).","title":"Refresh"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#create","text":"You can also create new folders and files via this button (see picture Figure 1 above, item 7 ). See 8.3. Create and Edit text files .","title":"+ Create"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#upload","text":"This control allows uploading files to the bucket (see picture Figure 1 above, item 8 ). See 8.2. Upload/Download data .","title":"Upload"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#each-line-controls","text":"Control Description Download This control calls downloading of selected file. Edit Helps to rename a file or a folder. Delete Delete a file or a folder.","title":"Each-line controls"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#view-and-edit-a-text-file","text":"You can view and edit text files. For more details see here .","title":"View and edit a text file"},{"location":"manual/08_Manage_Data_Storage/8._Manage_Data_Storage/#cli-storage-options","text":"There are also several options that are only implemented in CLI but not in GUI: To move files and folders from one storage to another or between local file system and storage. To copy files from one storage to another. See here for more details.","title":"CLI Storage options"},{"location":"manual/09_Manage_Cluster_nodes/9._Manage_Cluster_nodes/","text":"9. Manage Cluster nodes \" Cluster nodes \" provides a list of working nodes. You can get information on nodes usage and terminate them in this tab. Overview Controls Node information page GENERAL INFO JOBS MONITOR Note : Nodes remain for the time that is already paid for, even if all runs at the node finished execution. So if you restart pipeline, new nodes will not be initialized saving time and money. Overview This tab shows Active nodes table that has information about: Name - a name of the node. Pipeline - a currently assigned run on the node. Labels of the node - characteristics extracted from the parameters of the node. There are common labels: RUN ID - ID of currently assigned run, MASTER/EDGE - service labels, nodes with this labels may be viewed only by ADMIN users. Addresses - node addresses. Created - a date of creation. Controls Control Description Terminate This control terminates node. Refresh To get currently active nodes list. Node information page Clicking on the row of the table will redirect you to detailed node information page. This page has three tabs. GENERAL INFO This tab allows seeing general info about the node, including: System information ; Addresses of internal network and domain name; Labels of the node automatically generated in accordance with system information; Node type - amounts of available and total memory, number of jobs and CPUs. JOBS \"JOBS\" tab lists jobs being processed at the moment. Name of the job; clicking \" + \" icon next to the name expands a list of containers needed for the job. Namespace for a job to be executed at; Status of the job. MONITOR \"MONITOR\" tab displays a dashboard with following diagrams: Diagram Description CPU usage A diagram represents CPU usage (cores) - time graph. The graph is updated once in 15 seconds. Memory usage A diagram represents memory usage - time graph. The graph is updated once in 15 seconds. Blue graph represents usage in MB according to left vertical axis. Red graph represents usage in % of available amounts of memory according to right vertical axis. Network connection speed A diagram represents connection speed (bytes) - time graph. The graph is updated once in 15 seconds. Blue graph ( TX ) represents \"transceiving\" speed. Red graph ( RX ) represents \"receive\" speed. Drop-down at the top of the section allows changing connection protocol. File system load Represents all the disks of the machine and their loading.","title":"9. Manage cluster nodes"},{"location":"manual/09_Manage_Cluster_nodes/9._Manage_Cluster_nodes/#9-manage-cluster-nodes","text":"\" Cluster nodes \" provides a list of working nodes. You can get information on nodes usage and terminate them in this tab. Overview Controls Node information page GENERAL INFO JOBS MONITOR Note : Nodes remain for the time that is already paid for, even if all runs at the node finished execution. So if you restart pipeline, new nodes will not be initialized saving time and money.","title":"9. Manage Cluster nodes"},{"location":"manual/09_Manage_Cluster_nodes/9._Manage_Cluster_nodes/#overview","text":"This tab shows Active nodes table that has information about: Name - a name of the node. Pipeline - a currently assigned run on the node. Labels of the node - characteristics extracted from the parameters of the node. There are common labels: RUN ID - ID of currently assigned run, MASTER/EDGE - service labels, nodes with this labels may be viewed only by ADMIN users. Addresses - node addresses. Created - a date of creation.","title":"Overview"},{"location":"manual/09_Manage_Cluster_nodes/9._Manage_Cluster_nodes/#controls","text":"Control Description Terminate This control terminates node. Refresh To get currently active nodes list.","title":"Controls"},{"location":"manual/09_Manage_Cluster_nodes/9._Manage_Cluster_nodes/#node-information-page","text":"Clicking on the row of the table will redirect you to detailed node information page. This page has three tabs.","title":"Node information page"},{"location":"manual/09_Manage_Cluster_nodes/9._Manage_Cluster_nodes/#general-info","text":"This tab allows seeing general info about the node, including: System information ; Addresses of internal network and domain name; Labels of the node automatically generated in accordance with system information; Node type - amounts of available and total memory, number of jobs and CPUs.","title":"GENERAL INFO"},{"location":"manual/09_Manage_Cluster_nodes/9._Manage_Cluster_nodes/#jobs","text":"\"JOBS\" tab lists jobs being processed at the moment. Name of the job; clicking \" + \" icon next to the name expands a list of containers needed for the job. Namespace for a job to be executed at; Status of the job.","title":"JOBS"},{"location":"manual/09_Manage_Cluster_nodes/9._Manage_Cluster_nodes/#monitor","text":"\"MONITOR\" tab displays a dashboard with following diagrams: Diagram Description CPU usage A diagram represents CPU usage (cores) - time graph. The graph is updated once in 15 seconds. Memory usage A diagram represents memory usage - time graph. The graph is updated once in 15 seconds. Blue graph represents usage in MB according to left vertical axis. Red graph represents usage in % of available amounts of memory according to right vertical axis. Network connection speed A diagram represents connection speed (bytes) - time graph. The graph is updated once in 15 seconds. Blue graph ( TX ) represents \"transceiving\" speed. Red graph ( RX ) represents \"receive\" speed. Drop-down at the top of the section allows changing connection protocol. File system load Represents all the disks of the machine and their loading.","title":"MONITOR"},{"location":"manual/10_Manage_Tools/10.1._Add_Edit_a_Docker_registry/","text":"10.1. Add/Edit a Docker registry Add registry Edit registry Customize registry permissions A docker registry is a storage and content delivery system, holding named Docker images, available in different tagged versions. This page describes the process of adding and editing Docker registries to the Cloud Pipeline . Also here you will find information about permission management for Docker registries. Only administrators can create Docker registries. To edit its parameters you need to have WRITE permissions. For more information see 13. Permissions . Add registry Configured Docker registry can be added to the Cloud Pipeline with the following commands: In the Tools tab click the Gear icon \u2192 Registry \u2192 + Create . Then set the registry parameters: Path (mandatory field) - IP address/domain name of the machine with configured Docker registry. Description - registry description. Require security scanning - tick the box to allow scheduled security scanning procedure for the Tools in the registry. For more information see 10.6. Tool security check . User name * and Password * (optional field) - if the registry is closed, you have to set a username and a password. Certificate * - if Docker registry uses a self-signed certificate, upload it with the Choose file button to set HTTPS access to the registry. External URL * - URL that can be exploited to push/pull Docker images to/from a registry. Pipeline authentication * - tick to make registry use Cloud Pipeline authentication system. * click Edit credentials to get these fields. You'll see a new registry in the registry list. Edit registry To edit/view registry attributes - see 17. CP objects tagging by additional attributes . Choose registry from the registry list and click the Gear icon \u2192 Registry \u2192 Edit . You'll be able to modify registry parameters in the Info tab ( a ). Note : you can modify all registry parameters except Path . If you wish to detach registry from the CP - click the Delete button ( b ). Customize registry permissions Users with ROLE_ADMIN or OWNER rights can modify permissions for a registry. For detailed instruction refer to this document .","title":"10.1 Add and edit a Docker registry"},{"location":"manual/10_Manage_Tools/10.1._Add_Edit_a_Docker_registry/#101-addedit-a-docker-registry","text":"Add registry Edit registry Customize registry permissions A docker registry is a storage and content delivery system, holding named Docker images, available in different tagged versions. This page describes the process of adding and editing Docker registries to the Cloud Pipeline . Also here you will find information about permission management for Docker registries. Only administrators can create Docker registries. To edit its parameters you need to have WRITE permissions. For more information see 13. Permissions .","title":"10.1. Add/Edit a Docker registry"},{"location":"manual/10_Manage_Tools/10.1._Add_Edit_a_Docker_registry/#add-registry","text":"Configured Docker registry can be added to the Cloud Pipeline with the following commands: In the Tools tab click the Gear icon \u2192 Registry \u2192 + Create . Then set the registry parameters: Path (mandatory field) - IP address/domain name of the machine with configured Docker registry. Description - registry description. Require security scanning - tick the box to allow scheduled security scanning procedure for the Tools in the registry. For more information see 10.6. Tool security check . User name * and Password * (optional field) - if the registry is closed, you have to set a username and a password. Certificate * - if Docker registry uses a self-signed certificate, upload it with the Choose file button to set HTTPS access to the registry. External URL * - URL that can be exploited to push/pull Docker images to/from a registry. Pipeline authentication * - tick to make registry use Cloud Pipeline authentication system. * click Edit credentials to get these fields. You'll see a new registry in the registry list.","title":"Add registry"},{"location":"manual/10_Manage_Tools/10.1._Add_Edit_a_Docker_registry/#edit-registry","text":"To edit/view registry attributes - see 17. CP objects tagging by additional attributes . Choose registry from the registry list and click the Gear icon \u2192 Registry \u2192 Edit . You'll be able to modify registry parameters in the Info tab ( a ). Note : you can modify all registry parameters except Path . If you wish to detach registry from the CP - click the Delete button ( b ).","title":"Edit registry"},{"location":"manual/10_Manage_Tools/10.1._Add_Edit_a_Docker_registry/#customize-registry-permissions","text":"Users with ROLE_ADMIN or OWNER rights can modify permissions for a registry. For detailed instruction refer to this document .","title":"Customize registry permissions"},{"location":"manual/10_Manage_Tools/10.2._Add_Edit_a_Tool_group/","text":"10.2. Add/Edit a Tool group Add a generic Tool group Add a personal Tool group Edit a Tool group Customize Tool group permissions A Tool group is an object of the Cloud Pipeline that allows to organize Tools into groups. This page describes the process of adding and editing Tool groups. Also here you will find information about permission management for Tool groups. To create a Tool group, user need to have WRITE permissions for a Docker registry and a role TOOL_GROUP_MANAGER . To edit Tool group parameters you need to have WRITE permissions for it. For more information see 13. Permissions . Add a generic Tool group A Tool group can be added to a Docker registry in the following way: In the Tools tab click the Gear icon \u2192 Group \u2192 + Create . Give your Tool group a name and an optional description . You'll be automatically redirected to the new Tool group in the current Docker registry. Add a personal Tool group In the Tools tab click the Gear icon \u2192 Group \u2192 + Create personal . Another way to do that is to select personal Tool group from the Tool group list And then press the Create personal tool group button. After that you'll be able to upload Tools to your personal Tool group. Edit a Tool group Choose a Tool group and click the Gear icon \u2192 Group \u2192 Edit . You'll be able to modify Tool group description in the Info tab. If you wish to delete a Tool group - click the Gear icon \u2192 Group \u2192 Delete . Customize Tool group permissions Users with ROLE_ADMIN or OWNER rights can modify permissions for this Tool group. It is convenient when you want to manage access for the whole Tool group and not for the individual Tools. For more details see here .","title":"10.2 Add/edit a Tool group"},{"location":"manual/10_Manage_Tools/10.2._Add_Edit_a_Tool_group/#102-addedit-a-tool-group","text":"Add a generic Tool group Add a personal Tool group Edit a Tool group Customize Tool group permissions A Tool group is an object of the Cloud Pipeline that allows to organize Tools into groups. This page describes the process of adding and editing Tool groups. Also here you will find information about permission management for Tool groups. To create a Tool group, user need to have WRITE permissions for a Docker registry and a role TOOL_GROUP_MANAGER . To edit Tool group parameters you need to have WRITE permissions for it. For more information see 13. Permissions .","title":"10.2. Add/Edit a Tool group"},{"location":"manual/10_Manage_Tools/10.2._Add_Edit_a_Tool_group/#add-a-generic-tool-group","text":"A Tool group can be added to a Docker registry in the following way: In the Tools tab click the Gear icon \u2192 Group \u2192 + Create . Give your Tool group a name and an optional description . You'll be automatically redirected to the new Tool group in the current Docker registry.","title":"Add a generic Tool group"},{"location":"manual/10_Manage_Tools/10.2._Add_Edit_a_Tool_group/#add-a-personal-tool-group","text":"In the Tools tab click the Gear icon \u2192 Group \u2192 + Create personal . Another way to do that is to select personal Tool group from the Tool group list And then press the Create personal tool group button. After that you'll be able to upload Tools to your personal Tool group.","title":"Add a personal Tool group"},{"location":"manual/10_Manage_Tools/10.2._Add_Edit_a_Tool_group/#edit-a-tool-group","text":"Choose a Tool group and click the Gear icon \u2192 Group \u2192 Edit . You'll be able to modify Tool group description in the Info tab. If you wish to delete a Tool group - click the Gear icon \u2192 Group \u2192 Delete .","title":"Edit a Tool group"},{"location":"manual/10_Manage_Tools/10.2._Add_Edit_a_Tool_group/#customize-tool-group-permissions","text":"Users with ROLE_ADMIN or OWNER rights can modify permissions for this Tool group. It is convenient when you want to manage access for the whole Tool group and not for the individual Tools. For more details see here .","title":"Customize Tool group permissions"},{"location":"manual/10_Manage_Tools/10.3._Add_a_Tool/","text":"10.3. Add a Tool This page describes the process of adding new Docker image to the Docker registry. Docker CLI has to be installed. A registry has to be configured to use Cloud Pipeline (CP) authentication system (see 10.1. Add/Edit a Docker registry ). In case of registry doesn't use CP authentication system - contact your system administrators for registry access details. Go to the Tools tab and choose a registry. Click the Gear icon \u2192 How to configure? Copy and paste a command from the Login into cloud registry section ( a ) into the Terminal, then run it. This command installs registry certificate and uses docker login command to access the registry. Note : this command requires \"root\" rights. Copy and run instructions from the Push a local docker image to the cloud registry section ( b ) consequentially to add the Tool to the registry: Create environment variable ( MY_LOCAL_DOCKER_IMAGE ) that holds the name of the Docker image; Tag the image you want to push with the domain name or IP address and the port of the Docker registry; Push the image to the registry. Here we push \" hello-world \" image to the registry \" 18.195.69.178:5000 \", \" library \" Tool group. Troubleshooting section ( c ) contains information about fixing common problems that may appear during the execution of docker login or docker pull/push commands. Make sure that image was pushed to the registry and enabled in the Tools tab. Note : Tool will be automatically enabled only if CP authentication is configured for the registry. Note : If registry doesn't use CP authentication system, after the image was pushed do the following: Navigate to the Tools tab. Choose a Registry and a Tool group . Click the Gear icon \u2192 + Enable Tool . Write the name of the pushed image. Registry and Tool group will be already written for you.","title":"10.3 Add a Tool"},{"location":"manual/10_Manage_Tools/10.3._Add_a_Tool/#103-add-a-tool","text":"This page describes the process of adding new Docker image to the Docker registry. Docker CLI has to be installed. A registry has to be configured to use Cloud Pipeline (CP) authentication system (see 10.1. Add/Edit a Docker registry ). In case of registry doesn't use CP authentication system - contact your system administrators for registry access details. Go to the Tools tab and choose a registry. Click the Gear icon \u2192 How to configure? Copy and paste a command from the Login into cloud registry section ( a ) into the Terminal, then run it. This command installs registry certificate and uses docker login command to access the registry. Note : this command requires \"root\" rights. Copy and run instructions from the Push a local docker image to the cloud registry section ( b ) consequentially to add the Tool to the registry: Create environment variable ( MY_LOCAL_DOCKER_IMAGE ) that holds the name of the Docker image; Tag the image you want to push with the domain name or IP address and the port of the Docker registry; Push the image to the registry. Here we push \" hello-world \" image to the registry \" 18.195.69.178:5000 \", \" library \" Tool group. Troubleshooting section ( c ) contains information about fixing common problems that may appear during the execution of docker login or docker pull/push commands. Make sure that image was pushed to the registry and enabled in the Tools tab. Note : Tool will be automatically enabled only if CP authentication is configured for the registry. Note : If registry doesn't use CP authentication system, after the image was pushed do the following: Navigate to the Tools tab. Choose a Registry and a Tool group . Click the Gear icon \u2192 + Enable Tool . Write the name of the pushed image. Registry and Tool group will be already written for you.","title":"10.3. Add a Tool"},{"location":"manual/10_Manage_Tools/10.4._Edit_a_Tool/","text":"10.4. Edit a Tool Edit a Tool description Edit a Tool version Run/Delete a Tool Commit a Tool Edit a Tool settings To view and edit tool's attributes see 17. CP objects tagging by additional attributes . Edit a Tool description Select a Tool and click its name. Navigate to the Description tab of a Tool. Click the Edit buttons on the right side of the screen to modify the Short description and Full description of the Tool. Edit a Tool version In this tab, you can run a Tool version with custom settings or delete a Tool version. To see an example of a launching a Tool with custom settings see here . Run/Delete a Tool Navigate to the Versions tab of a Tool. Click on the arrow near the Run button. Select Custom settings option to configure run parameters. If you want to delete a Tool version, click the Delete button. Commit a Tool Commit function allows modifying existing Tools. Launch a Tool in the sleep infinity mode. See an example here . SSH into it via SSH button. Change something. Example : here we install a biopython package into this Docker image. Wait for the installation to complete! Go back to the Logs tab and click the COMMIT button. Choose a Docker registry and a Tool group. Change a name for the modified image or add a new version to the current Tool by typing the version name in a separate box ( to add a new version to the existing Tool don't change the original name of the Tool! ). Note : image name and a version name should be written according to the following rules: May contain lowercase letters, digits, and separators. A separator is defined as a period, one or two underscores, or one or more dashes. A name component may not start or end with a separator. Tick boxes if needed: Delete runtime files box - to delete all files from /runs/[pipeline name] folder before committing. Stop pipeline box - to stop the current run after committing. In this example, we will change \"base-generic-centos7\" to \"base-generic-centos7-biopython\". Committing may take some time. When it is complete COMMITING status on the right side of the screen will change to COMMIT SUCCEEDED . Find a modified Tool in the registry. Edit a Tool settings Settings in this tab are applied to all Tool versions (i.e. these settings will be a default for all Tool version). Navigate to the Settings tab. Specify Endpoints for a Tool by click \" Add endpoint \" button: In the example below: the port is 8788 and the endpoint name is rstudio-s3fs : Let's look at the endpoint closer: \"nginx\" - type of the endpoints (only nginx is currently supported) \"port\": XXXX - an application will be deployed on this port on the pipeline node. You can specify additional nginx configuration for that endpoint in the text field bottom in JSON format. Note : optional path parameter may be required in case your application starts on <host:port:>/ path . Note : optional additional parameter may be required in case you need to specify nginx location settings. See more information here . \"name\" - this value will be visible as a hyperlink in the UI. It is especially convenient when user sets more than one endpoint configuration for an interactive tool (learn more about interactive services - 15. Interactive services ). In the example below, we name one endpoint as \"rstudio-s3fs\" and another one as \"shiny\" . This is how everything will look in the Run log window: Click the + New Label button and add a label to the Tool (e.g. \"Awesome Tool\"). Specify \" Execution defaults \": Instance type (e.g. \"m4.large\") Price type (e.g. \"Spot\") Disk size in Gb (e.g. \"20\") select available storages configure cluster, if it's necessary for your Tool write the Default command for the Tool execution (e.g. echo \"Hello world!\" ) If it's necessary for your Tool - add system or custom parameters. For more details see 6.1. Create and configure pipeline . Click the Save button to save these settings.","title":"10.4 Edit a Tool"},{"location":"manual/10_Manage_Tools/10.4._Edit_a_Tool/#104-edit-a-tool","text":"Edit a Tool description Edit a Tool version Run/Delete a Tool Commit a Tool Edit a Tool settings To view and edit tool's attributes see 17. CP objects tagging by additional attributes .","title":"10.4. Edit a Tool"},{"location":"manual/10_Manage_Tools/10.4._Edit_a_Tool/#edit-a-tool-description","text":"Select a Tool and click its name. Navigate to the Description tab of a Tool. Click the Edit buttons on the right side of the screen to modify the Short description and Full description of the Tool.","title":"Edit a Tool description"},{"location":"manual/10_Manage_Tools/10.4._Edit_a_Tool/#edit-a-tool-version","text":"In this tab, you can run a Tool version with custom settings or delete a Tool version. To see an example of a launching a Tool with custom settings see here .","title":"Edit a Tool version"},{"location":"manual/10_Manage_Tools/10.4._Edit_a_Tool/#rundelete-a-tool","text":"Navigate to the Versions tab of a Tool. Click on the arrow near the Run button. Select Custom settings option to configure run parameters. If you want to delete a Tool version, click the Delete button.","title":"Run/Delete a Tool"},{"location":"manual/10_Manage_Tools/10.4._Edit_a_Tool/#commit-a-tool","text":"Commit function allows modifying existing Tools. Launch a Tool in the sleep infinity mode. See an example here . SSH into it via SSH button. Change something. Example : here we install a biopython package into this Docker image. Wait for the installation to complete! Go back to the Logs tab and click the COMMIT button. Choose a Docker registry and a Tool group. Change a name for the modified image or add a new version to the current Tool by typing the version name in a separate box ( to add a new version to the existing Tool don't change the original name of the Tool! ). Note : image name and a version name should be written according to the following rules: May contain lowercase letters, digits, and separators. A separator is defined as a period, one or two underscores, or one or more dashes. A name component may not start or end with a separator. Tick boxes if needed: Delete runtime files box - to delete all files from /runs/[pipeline name] folder before committing. Stop pipeline box - to stop the current run after committing. In this example, we will change \"base-generic-centos7\" to \"base-generic-centos7-biopython\". Committing may take some time. When it is complete COMMITING status on the right side of the screen will change to COMMIT SUCCEEDED . Find a modified Tool in the registry.","title":"Commit a Tool"},{"location":"manual/10_Manage_Tools/10.4._Edit_a_Tool/#edit-a-tool-settings","text":"Settings in this tab are applied to all Tool versions (i.e. these settings will be a default for all Tool version). Navigate to the Settings tab. Specify Endpoints for a Tool by click \" Add endpoint \" button: In the example below: the port is 8788 and the endpoint name is rstudio-s3fs : Let's look at the endpoint closer: \"nginx\" - type of the endpoints (only nginx is currently supported) \"port\": XXXX - an application will be deployed on this port on the pipeline node. You can specify additional nginx configuration for that endpoint in the text field bottom in JSON format. Note : optional path parameter may be required in case your application starts on <host:port:>/ path . Note : optional additional parameter may be required in case you need to specify nginx location settings. See more information here . \"name\" - this value will be visible as a hyperlink in the UI. It is especially convenient when user sets more than one endpoint configuration for an interactive tool (learn more about interactive services - 15. Interactive services ). In the example below, we name one endpoint as \"rstudio-s3fs\" and another one as \"shiny\" . This is how everything will look in the Run log window: Click the + New Label button and add a label to the Tool (e.g. \"Awesome Tool\"). Specify \" Execution defaults \": Instance type (e.g. \"m4.large\") Price type (e.g. \"Spot\") Disk size in Gb (e.g. \"20\") select available storages configure cluster, if it's necessary for your Tool write the Default command for the Tool execution (e.g. echo \"Hello world!\" ) If it's necessary for your Tool - add system or custom parameters. For more details see 6.1. Create and configure pipeline . Click the Save button to save these settings.","title":"Edit a Tool settings"},{"location":"manual/10_Manage_Tools/10.5._Launch_a_Tool/","text":"10.5. Launch a Tool Launch the latest version Launch particular Tool version Launch a Tool with \"friendly\" URL Instance management To launch a Tool you need to have EXECUTE permissions for it. For more information see 13. Permissions . Launch the latest version To run an instance with a selected Tool navigate to the Tools tab and click the Tool name . Click the Run button in the top-right corner of the screen and the latest version with default settings will be launched (these are defined for Cloud Pipeline globally). If you want to change settings, you shall click the arrow near the Run button \u2192 Custom settings . Launch tool page will be open. Define the parameters in the Exec environment , Advanced and Parameters tabs. Click the Launch button in the top-right corner of the screen. Launch particular Tool version To run a particular version click the Versions section. Select a version and click the Run button. The selected version with default settings will be launched (these are defined for Cloud Pipeline globally). If you want to change settings, you shall click the arrow near the Run button \u2192 Custom settings . Launch a tool page will be opened. Define the parameters. Click the \" Launch \" button. Example 1 In this example, we will run the \" base-gui-centos-nomachine \" Tool with custom settings: 20 Gb hard drive, 2 CPU cores, and 8 Gb RAM. Pricing is calculated for a spot instance. Note : \"Start idle\" box is ticked to allow SSH access to the running Tool. To learn more about interactive services see 15. Interactive services . Click the Launch button in the top-right corner of the screen when all parameters are set. After the Tool is launched you will be redirected to the Runs tab: Click the Log button to see run details after instance finishes initialization. Click the SSH button in the Run logs page. You will be redirected to the page with interactive shell session inside the Docker container. For example, we can list \"/\" directory content inside the container. Launch a Tool with \"friendly\" URL User can specify a \"Friendly URL\" for persistent services. This produces endpoint URL in a more friendly/descriptive format: {cloud-pipeline_url}/ friendly_url instead of {cloud-pipeline_url}/ pipeline-XXXX-XXXX . It can be configured at a service launch time in the \" Advanced \" section of the Launch form. Example 2 In this example we will configure a pretty URL for rstudio Tool. Note : for do that, user account shall be registered within CP users catalog and granted READ & EXECUTE permissions for the rstudio Tool. Navigate to the Tools tab. In the Default registry select the library/rstudio Tool: On opened page hover the \" Run \" button and click on appeared \" Custom settings \" point: Click on \" Advanced \" control ( a ), input desired \" Friendly URL \" ( b ) (name shall be unique) and then click \" Launch \" button ( c ): Open logs page of rstudio Tool, wait until tool successfully started. Click on hyperlink opposite \" Endpoint \" label: In a new tab RStudio will be opened. Check, the URL will be in \"pretty\" format, that you inputted on step 4: Instance management Instance management allows to set restrictions on instance types and price types for tool runs. User shall have ROLE_ADMIN or to be an OWNER of the Tool to launch Instance management panel. For more information see 13. Permissions . To open Instance management panel: Click button in the left upper corner of the main tool page. Click \"Instance management\": Such panel will be shown: On this panel you can specify some restrictions on allowed instance types and price types for launching tool. Here you can specify: Field Description Example Allowed tool instance types mask This mask restrict for a tool allowed instance types. If you want for that tool only some of \"large m5...\" instances types will be able, mask would be m5*.large* In that case, before launching tool, dropdown list of available node types will be look like this: Allowed price types In this field you may restrict, what price types will be allowed for a user. If you want \"On-demand\" runs only for that tool will be able, select it in the dropdown list: In that case, before launching tool, dropdown list of price types will be look like this: To apply set restrictions for a tool click button. Setting restrictions on allowed instance types/price types is a convenient way to minimize a number of invalid configurations runs. Such restrictions could be set not only for a tool, but on another levels too. In CP platform next hierarchy is set for applying of inputted allowed instance types (sorted by priority): User level (specified for a user on \"User management\" tab) (see v.0.14 - 12.4. Edit/delete a user ) User group level (specified for a group (role) on \"User management\" tab. If a user is a member of several groups - list of allowed instances will be summarized across all the groups) (see v.0.14 - 12.6. Edit a group/role ) Tool level (specified for a tool on \"Instance management\" panel) (see above ) (global) cluster.allowed.instance.types.docker (specified on \"Cluster\" tab in \"Preferences\" section of system-level settings) (see v.0.14 - 12.10. Manage system-level settings ) (global) cluster.allowed.instance.types (specified on \"Cluster\" tab in \"Preferences\" section of system-level settings) (see v.0.14 - 12.10. Manage system-level settings )","title":"10.5 Launch a Tool"},{"location":"manual/10_Manage_Tools/10.5._Launch_a_Tool/#105-launch-a-tool","text":"Launch the latest version Launch particular Tool version Launch a Tool with \"friendly\" URL Instance management To launch a Tool you need to have EXECUTE permissions for it. For more information see 13. Permissions .","title":"10.5. Launch a Tool"},{"location":"manual/10_Manage_Tools/10.5._Launch_a_Tool/#launch-the-latest-version","text":"To run an instance with a selected Tool navigate to the Tools tab and click the Tool name . Click the Run button in the top-right corner of the screen and the latest version with default settings will be launched (these are defined for Cloud Pipeline globally). If you want to change settings, you shall click the arrow near the Run button \u2192 Custom settings . Launch tool page will be open. Define the parameters in the Exec environment , Advanced and Parameters tabs. Click the Launch button in the top-right corner of the screen.","title":"Launch the latest version"},{"location":"manual/10_Manage_Tools/10.5._Launch_a_Tool/#launch-particular-tool-version","text":"To run a particular version click the Versions section. Select a version and click the Run button. The selected version with default settings will be launched (these are defined for Cloud Pipeline globally). If you want to change settings, you shall click the arrow near the Run button \u2192 Custom settings . Launch a tool page will be opened. Define the parameters. Click the \" Launch \" button.","title":"Launch\u00a0particular Tool version"},{"location":"manual/10_Manage_Tools/10.5._Launch_a_Tool/#example-1","text":"In this example, we will run the \" base-gui-centos-nomachine \" Tool with custom settings: 20 Gb hard drive, 2 CPU cores, and 8 Gb RAM. Pricing is calculated for a spot instance. Note : \"Start idle\" box is ticked to allow SSH access to the running Tool. To learn more about interactive services see 15. Interactive services . Click the Launch button in the top-right corner of the screen when all parameters are set. After the Tool is launched you will be redirected to the Runs tab: Click the Log button to see run details after instance finishes initialization. Click the SSH button in the Run logs page. You will be redirected to the page with interactive shell session inside the Docker container. For example, we can list \"/\" directory content inside the container.","title":"Example\u00a01"},{"location":"manual/10_Manage_Tools/10.5._Launch_a_Tool/#launch-a-tool-with-friendly-url","text":"User can specify a \"Friendly URL\" for persistent services. This produces endpoint URL in a more friendly/descriptive format: {cloud-pipeline_url}/ friendly_url instead of {cloud-pipeline_url}/ pipeline-XXXX-XXXX . It can be configured at a service launch time in the \" Advanced \" section of the Launch form.","title":"Launch a Tool with \"friendly\" URL"},{"location":"manual/10_Manage_Tools/10.5._Launch_a_Tool/#example-2","text":"In this example we will configure a pretty URL for rstudio Tool. Note : for do that, user account shall be registered within CP users catalog and granted READ & EXECUTE permissions for the rstudio Tool. Navigate to the Tools tab. In the Default registry select the library/rstudio Tool: On opened page hover the \" Run \" button and click on appeared \" Custom settings \" point: Click on \" Advanced \" control ( a ), input desired \" Friendly URL \" ( b ) (name shall be unique) and then click \" Launch \" button ( c ): Open logs page of rstudio Tool, wait until tool successfully started. Click on hyperlink opposite \" Endpoint \" label: In a new tab RStudio will be opened. Check, the URL will be in \"pretty\" format, that you inputted on step 4:","title":"Example 2"},{"location":"manual/10_Manage_Tools/10.5._Launch_a_Tool/#instance-management","text":"Instance management allows to set restrictions on instance types and price types for tool runs. User shall have ROLE_ADMIN or to be an OWNER of the Tool to launch Instance management panel. For more information see 13. Permissions . To open Instance management panel: Click button in the left upper corner of the main tool page. Click \"Instance management\": Such panel will be shown: On this panel you can specify some restrictions on allowed instance types and price types for launching tool. Here you can specify: Field Description Example Allowed tool instance types mask This mask restrict for a tool allowed instance types. If you want for that tool only some of \"large m5...\" instances types will be able, mask would be m5*.large* In that case, before launching tool, dropdown list of available node types will be look like this: Allowed price types In this field you may restrict, what price types will be allowed for a user. If you want \"On-demand\" runs only for that tool will be able, select it in the dropdown list: In that case, before launching tool, dropdown list of price types will be look like this: To apply set restrictions for a tool click button. Setting restrictions on allowed instance types/price types is a convenient way to minimize a number of invalid configurations runs. Such restrictions could be set not only for a tool, but on another levels too. In CP platform next hierarchy is set for applying of inputted allowed instance types (sorted by priority): User level (specified for a user on \"User management\" tab) (see v.0.14 - 12.4. Edit/delete a user ) User group level (specified for a group (role) on \"User management\" tab. If a user is a member of several groups - list of allowed instances will be summarized across all the groups) (see v.0.14 - 12.6. Edit a group/role ) Tool level (specified for a tool on \"Instance management\" panel) (see above ) (global) cluster.allowed.instance.types.docker (specified on \"Cluster\" tab in \"Preferences\" section of system-level settings) (see v.0.14 - 12.10. Manage system-level settings ) (global) cluster.allowed.instance.types (specified on \"Cluster\" tab in \"Preferences\" section of system-level settings) (see v.0.14 - 12.10. Manage system-level settings )","title":"Instance management"},{"location":"manual/10_Manage_Tools/10.6._Tool_security_check/","text":"10.6. Tool security check Force a security scanning procedure Show/hide unscanned Tool versions \"White list\" for Tool versions A registry may potentially contain tools with vulnerable software which may cause damage. To prevent this issue, Cloud Pipeline performs Security scanning feature of the tools that are provided by users and restrict usage of not secure Tools. User shall have ROLE_ADMIN to force a security scanning procedure or to run unscanned Tools/Tools with critical number of vulnerabilities. For more information see 13. Permissions . Force a security scanning procedure In the Cloud Pipeline security scanning is performed on a scheduled basis - every N minutes (configurable parameter). However, a user with an administrator role can manually run a security scanning procedure: Select a Tool and navigate to the Versions tab. Click the SCAN button of a Tool version. Note : If scan results for version are against the security politics (i.e. vulnerabilities number exceeds the threshold) the Run button for version will be deactivated. Only users with ROLE_ADMIN role will be able to run this Tool. Generic users can run this version of Tool only if it is checked with a \"white list\" flag (see below ) or if its \"grace\" period is not elapsed yet (see security.tools.grace.hours setting here v.0.14 - 12.10. Manage system-level settings ). Note : If scan results for the latest version are against the security politics (i.e. vulnerabilities number exceeds the threshold) the Run button for Tool will be deactivated. Only users with ROLE_ADMIN role will be able to run this Tool. Generic users can run this version of Tool only if it is checked with a \"white list\" flag (see below ) or if its \"grace\" period is not elapsed yet (see security.tools.grace.hours setting here v.0.14 - 12.10. Manage system-level settings ). Show/hide unscanned Tool versions Select a Tool and navigate to the Versions tab. Press the View unscanned versions control. Unscanned version(s) will appear in the list of Tool versions. Note : for users with ADMIN role Run button of unscanned version contains \"warning\" sign. Hover over that button and you'll see a suggestion that the version shall be scanned for vulnerabilities. Generic users won't be able to run unscanned Tools (except when they're checked with a \"white list\" flag - see below or if \"grace\" period for this version of Tool is not elapsed yet (see security.tools.grace.hours setting here v.0.14 - 12.10. Manage system-level settings )). If you are user with ADMIN role and press the Run button anyway, you'll see another warning: Hide unscanned Tool versions by clicking Hide unscanned versions control. \"White list\" for Tool versions Generic users can't run tools with vulnerable software or unscanned tools because it may cause damage. But admin may allow users to run certain docker versions even if they are vulnerable/unscanned. For do that admin has to check a such docker version with a special \"white list\" flag. User shall have ROLE_ADMIN to set a \"white list\" flag for an unscanned Tools or Tools with critical number of vulnerabilities. To \"add\" a Tool version into the \"white list\": Open Versions tab on main tool's page. Click Add to white list button in the row of the version you want allow users to run in spite of possible damage: The version with a \"white list\" flag will be shown in green: Now, if user will try to launch this version of docker image, it will be run without any errors during launch time and viewing. If user will try to launch any other \"danger\" version of this tool without a \"white list\" flag, error message will be shown, tool won't be run: To \"delete\" a Tool version from the \"white list\": Open Versions tab on the main tool's page. Click Remove from white list button in the row of the version with a set \"white list\" flag:","title":"10.6 Tool security check"},{"location":"manual/10_Manage_Tools/10.6._Tool_security_check/#106-tool-security-check","text":"Force a security scanning procedure Show/hide unscanned Tool versions \"White list\" for Tool versions A registry may potentially contain tools with vulnerable software which may cause damage. To prevent this issue, Cloud Pipeline performs Security scanning feature of the tools that are provided by users and restrict usage of not secure Tools. User shall have ROLE_ADMIN to force a security scanning procedure or to run unscanned Tools/Tools with critical number of vulnerabilities. For more information see 13. Permissions .","title":"10.6. Tool security check"},{"location":"manual/10_Manage_Tools/10.6._Tool_security_check/#force-a-security-scanning-procedure","text":"In the Cloud Pipeline security scanning is performed on a scheduled basis - every N minutes (configurable parameter). However, a user with an administrator role can manually run a security scanning procedure: Select a Tool and navigate to the Versions tab. Click the SCAN button of a Tool version. Note : If scan results for version are against the security politics (i.e. vulnerabilities number exceeds the threshold) the Run button for version will be deactivated. Only users with ROLE_ADMIN role will be able to run this Tool. Generic users can run this version of Tool only if it is checked with a \"white list\" flag (see below ) or if its \"grace\" period is not elapsed yet (see security.tools.grace.hours setting here v.0.14 - 12.10. Manage system-level settings ). Note : If scan results for the latest version are against the security politics (i.e. vulnerabilities number exceeds the threshold) the Run button for Tool will be deactivated. Only users with ROLE_ADMIN role will be able to run this Tool. Generic users can run this version of Tool only if it is checked with a \"white list\" flag (see below ) or if its \"grace\" period is not elapsed yet (see security.tools.grace.hours setting here v.0.14 - 12.10. Manage system-level settings ).","title":"Force a security scanning procedure"},{"location":"manual/10_Manage_Tools/10.6._Tool_security_check/#showhide-unscanned-tool-versions","text":"Select a Tool and navigate to the Versions tab. Press the View unscanned versions control. Unscanned version(s) will appear in the list of Tool versions. Note : for users with ADMIN role Run button of unscanned version contains \"warning\" sign. Hover over that button and you'll see a suggestion that the version shall be scanned for vulnerabilities. Generic users won't be able to run unscanned Tools (except when they're checked with a \"white list\" flag - see below or if \"grace\" period for this version of Tool is not elapsed yet (see security.tools.grace.hours setting here v.0.14 - 12.10. Manage system-level settings )). If you are user with ADMIN role and press the Run button anyway, you'll see another warning: Hide unscanned Tool versions by clicking Hide unscanned versions control.","title":"Show/hide unscanned Tool versions"},{"location":"manual/10_Manage_Tools/10.6._Tool_security_check/#white-list-for-tool-versions","text":"Generic users can't run tools with vulnerable software or unscanned tools because it may cause damage. But admin may allow users to run certain docker versions even if they are vulnerable/unscanned. For do that admin has to check a such docker version with a special \"white list\" flag. User shall have ROLE_ADMIN to set a \"white list\" flag for an unscanned Tools or Tools with critical number of vulnerabilities. To \"add\" a Tool version into the \"white list\": Open Versions tab on main tool's page. Click Add to white list button in the row of the version you want allow users to run in spite of possible damage: The version with a \"white list\" flag will be shown in green: Now, if user will try to launch this version of docker image, it will be run without any errors during launch time and viewing. If user will try to launch any other \"danger\" version of this tool without a \"white list\" flag, error message will be shown, tool won't be run: To \"delete\" a Tool version from the \"white list\": Open Versions tab on the main tool's page. Click Remove from white list button in the row of the version with a set \"white list\" flag:","title":"\"White list\" for Tool versions"},{"location":"manual/10_Manage_Tools/10.7._Tool_version_menu/","text":"10.7. Tool version menu Vulnerabilities report Version settings Version packages To see the detailed info of Docker image version: Click any Tool version : Tool version menu will be shown: Vulnerabilities report This tab contains the detailed info of the Docker image's scanning results. Here you can see vulnerable components ( a ). Each component has severity estimation ( b ): Expand each component details by clicking the \" Plus \" icon to see more information about it: a ) link to a page of the vulnerability description b ) the component version in which this vulnerability was fixed c ) severity level d ) short description of the vulnerability (it appears when hovering mouse pointer over the link a ) Sort components alphabetically ( 1 ), or by their severity ( 2 ): Version settings On this tab version-level settings are defined. If these settings are specified - they will be applied to each run of the docker image version. If version-level settings are not defined: docker-level settings will be applied for launch. If docker-level settings are not defined: global defaults will be applied. There are 3 groups of parameters that user can specify (they are analogical to the \"Execution environment\" of tool settings, for more details see here ): Execution defaults System parameters Custom parameters For change version-level settings, e.g.: Select an Instance type (e.g. m4.large ). Set the Price type (e.g. Spot ). Input the Disk size (e.g. 30Gb ). Click Save button: Click button to return into the tool menu. Click Run \u2192 Custom settings for the changed tool version. Check that version-level settings are applied: Version packages On this tab user can see the full list software packages installed into a specific Docker image. List of packages is generated from the docker version together with vulnerabilities scanning. This occurs nightly (all dockers are scanned) or if admin explicitly requests scanning by clicking SCAN button for a specific version. Currently the following types of software packages can be scanned: System package manager's database (i.e. yum , apt ) R packages Python packages Software packages are combined into groups named \" Ecosystems \". To view content of any ecosystem, select it in the dropdown list: Information about each package contains: Package name Package description ( if available ) For filter/search packages type some text into a search-query field. Search will be done automatically across all ecosystems :","title":"10.7 Tool version menu"},{"location":"manual/10_Manage_Tools/10.7._Tool_version_menu/#107-tool-version-menu","text":"Vulnerabilities report Version settings Version packages To see the detailed info of Docker image version: Click any Tool version : Tool version menu will be shown:","title":"10.7. Tool version menu"},{"location":"manual/10_Manage_Tools/10.7._Tool_version_menu/#vulnerabilities-report","text":"This tab contains the detailed info of the Docker image's scanning results. Here you can see vulnerable components ( a ). Each component has severity estimation ( b ): Expand each component details by clicking the \" Plus \" icon to see more information about it: a ) link to a page of the vulnerability description b ) the component version in which this vulnerability was fixed c ) severity level d ) short description of the vulnerability (it appears when hovering mouse pointer over the link a ) Sort components alphabetically ( 1 ), or by their severity ( 2 ):","title":"Vulnerabilities report"},{"location":"manual/10_Manage_Tools/10.7._Tool_version_menu/#version-settings","text":"On this tab version-level settings are defined. If these settings are specified - they will be applied to each run of the docker image version. If version-level settings are not defined: docker-level settings will be applied for launch. If docker-level settings are not defined: global defaults will be applied. There are 3 groups of parameters that user can specify (they are analogical to the \"Execution environment\" of tool settings, for more details see here ): Execution defaults System parameters Custom parameters For change version-level settings, e.g.: Select an Instance type (e.g. m4.large ). Set the Price type (e.g. Spot ). Input the Disk size (e.g. 30Gb ). Click Save button: Click button to return into the tool menu. Click Run \u2192 Custom settings for the changed tool version. Check that version-level settings are applied:","title":"Version settings"},{"location":"manual/10_Manage_Tools/10.7._Tool_version_menu/#version-packages","text":"On this tab user can see the full list software packages installed into a specific Docker image. List of packages is generated from the docker version together with vulnerabilities scanning. This occurs nightly (all dockers are scanned) or if admin explicitly requests scanning by clicking SCAN button for a specific version. Currently the following types of software packages can be scanned: System package manager's database (i.e. yum , apt ) R packages Python packages Software packages are combined into groups named \" Ecosystems \". To view content of any ecosystem, select it in the dropdown list: Information about each package contains: Package name Package description ( if available ) For filter/search packages type some text into a search-query field. Search will be done automatically across all ecosystems :","title":"Version packages"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/","text":"10. Manage Tools Overview \"Details\" view pane Registry Tool group Search Show attributes/Hide attributes \"Gear\" icon Personal Docker repository (Tool group) List of Tools Tool information page Description tab Versions tab Settings tab For more information about Docker container lifecycle and EC2 instance lifecycle see Appendix A. EC2 Instance and Docker container lifecycles . Overview The Tools tab represents a list of available docker registries and docker images that contain tools. Using Docker images allows you to configure the same processing environment on each node regardless of the node type. Every Tool object in the Cloud Pipeline is a representation of Docker image. \"Details\" view pane At the top of the page, you'll see the basic objects of the \"Tools\" space. Registry Click on the registry name to see a drop-down list of available Docker registries and choose one. Tool group Press under the arrow to see a list of available Tool groups or to search Tool group by the name. Only the lower-case alphanumeric string is allowed for a Tool group name. Note : when you navigate to the Docker registry, the Tool group shown by default will be chosen based on the following conditions: If a user is included into some group (e.g. \"cancer\") and a Tool group with the same name exists, it will be shown by default. See more about user groups here . If the first condition isn't met, \" library \" group will be shown by default. If \" library \" group doesn't exist, \" default \" group will be shown by default. If the \" default \" group doesn't exist either, \" personal \" group will be shown. If none of the above groups doesn't exist/user doesn't have access to them, the first Tool group from the list will be shown by default. Search field This field helps to find a Tool by name in particular Tool group in a registry. Show attributes/Hide attributes Show attributes/Hide attributes opens the Attributes pane, where you can see and edit a list of key=value attributes of the tool group. See 17. CP objects tagging by additional attributes . \"Gear\" icon The following options are available: Option Description Registry This button allows to create a new registry or Edit/Delete current. Group Allows creating new Tool group or Edit/Delete current. + Enable Tool Enables a Tool in a registry. How to configure Configures the Docker client to push/pull images to/from a registry. Note : Docker client needs to be installed. For installation instructions refer to https://docs.docker.com/install/ . Personal Docker repository (Tool group) All tools within such repository are named as <user_name>/<tool_name> . Note : If a user loads a registry and there is no \"personal\" group in it, it shall be checked whether he has WRITE access to the registry. If No - do not display \"personal\" section (1) in the registry. If Yes - a user will see the message \"Personal tool group was not found in registry\" (2) . On top of that, you'll see a suggestion to explore library Tool group. See how to create personal Tool group here . List of Tools Tools list will be shown after you select group and registry. Tool information page Click on a Tool's name to open Tool information page. In the top right corner you can find the following buttons: Control Description \"Displays\" icon This icon includes: Show attributes/Hide attributes This button is used to show attributes of a Tool. Note : If selected Tool has any defined attribute, attributes pane is shown by default. For more details see 17. CP objects tagging by additional attributes . \"Show issues/Hide issues\" shows/hides the issues of the current Tool to discuss. To learn more see here . \"Gear\" icon Manage Tool permissions or delete a Tool. Run Run an instance with this Tool. Tool information page is divided into 3 tabs. Description tab This tab shows Tool description. For registries with Pipeline authentication option, you'll also see the Docker pull command on this tab if you have READ access to a Tool. Versions tab Choose this tab to see a list of Tool versions. About internal Tool version menu see 10.7. Tool version menu . Each version has the following icons and controls: Control/Label Description Name Name of version Scanning status If the security scanning is forced, you'll see the status \" in progress \" of security scanning. If the security scanning is failed, you'll see the status \" failed \". Last successful scan: The label shows if a version is successfully scanned at any time. The label contains date and time of the last successful attempt. Last scan date The label shows if a version scanning is failed. The label contains date and time of the last scanning attempt. Colored bars Hover over the colored bars to see scan status - a number of vulnerabilities grouped by severity (e.g. Critical, High, Medium, ...). Digest The label shows unique identifier of docker image. Corresponding aliases The label shows aliases of docker image (e.g. if some digest has more than one alias). Image size The label shows the size of docker image. Note : this value is provided for the \"gzipped\" docker image. When pulled to the local workstation or the cloud instance - the size of the image will be greater. Modified date The label shows modified date of docker image. SCAN Control forces the security scanning process. Available only for users with ROLE_ADMIN role. Run Allow to run the particular Tool version with default settings or customize it. Delete Delete the particular Tool version. In addition, the Version tab contains View unscanned version control. The control is visible if unscanned versions exist. More about Security scan feature you could learn here . Settings tab Navigate to this tab to see Tool attributes and execution defaults. Endpoints - specify an endpoint for the service launched in a Tool. Labels - tool labels to briefly describe the Tool. Disk - instance disk size in Gb. Instance type - EC2 instance type. Contains information about an amount of CPU and RAM (in Gb). Default command - default command for Tool execution. Navigate back to the Tools group page from the Tool description with the arrow button on the top-left corner.","title":"10.0 Overview"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#10-manage-tools","text":"Overview \"Details\" view pane Registry Tool group Search Show attributes/Hide attributes \"Gear\" icon Personal Docker repository (Tool group) List of Tools Tool information page Description tab Versions tab Settings tab For more information about Docker container lifecycle and EC2 instance lifecycle see Appendix A. EC2 Instance and Docker container lifecycles .","title":"10. Manage Tools"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#overview","text":"The Tools tab represents a list of available docker registries and docker images that contain tools. Using Docker images allows you to configure the same processing environment on each node regardless of the node type. Every Tool object in the Cloud Pipeline is a representation of Docker image.","title":"Overview"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#details-view-pane","text":"At the top of the page, you'll see the basic objects of the \"Tools\" space.","title":"\"Details\" view pane"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#registry","text":"Click on the registry name to see a drop-down list of available Docker registries and choose one.","title":"Registry"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#tool-group","text":"Press under the arrow to see a list of available Tool groups or to search Tool group by the name. Only the lower-case alphanumeric string is allowed for a Tool group name. Note : when you navigate to the Docker registry, the Tool group shown by default will be chosen based on the following conditions: If a user is included into some group (e.g. \"cancer\") and a Tool group with the same name exists, it will be shown by default. See more about user groups here . If the first condition isn't met, \" library \" group will be shown by default. If \" library \" group doesn't exist, \" default \" group will be shown by default. If the \" default \" group doesn't exist either, \" personal \" group will be shown. If none of the above groups doesn't exist/user doesn't have access to them, the first Tool group from the list will be shown by default.","title":"Tool group"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#search-field","text":"This field helps to find a Tool by name in particular Tool group in a registry.","title":"Search field"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#show-attributeshide-attributes","text":"Show attributes/Hide attributes opens the Attributes pane, where you can see and edit a list of key=value attributes of the tool group. See 17. CP objects tagging by additional attributes .","title":"Show attributes/Hide attributes"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#gear-icon","text":"The following options are available: Option Description Registry This button allows to create a new registry or Edit/Delete current. Group Allows creating new Tool group or Edit/Delete current. + Enable Tool Enables a Tool in a registry. How to configure Configures the Docker client to push/pull images to/from a registry. Note : Docker client needs to be installed. For installation instructions refer to https://docs.docker.com/install/ .","title":"\"Gear\" icon"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#personal-docker-repository-tool-group","text":"All tools within such repository are named as <user_name>/<tool_name> . Note : If a user loads a registry and there is no \"personal\" group in it, it shall be checked whether he has WRITE access to the registry. If No - do not display \"personal\" section (1) in the registry. If Yes - a user will see the message \"Personal tool group was not found in registry\" (2) . On top of that, you'll see a suggestion to explore library Tool group. See how to create personal Tool group here .","title":"Personal Docker repository (Tool group)"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#list-of-tools","text":"Tools list will be shown after you select group and registry.","title":"List of Tools"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#tool-information-page","text":"Click on a Tool's name to open Tool information page. In the top right corner you can find the following buttons: Control Description \"Displays\" icon This icon includes: Show attributes/Hide attributes This button is used to show attributes of a Tool. Note : If selected Tool has any defined attribute, attributes pane is shown by default. For more details see 17. CP objects tagging by additional attributes . \"Show issues/Hide issues\" shows/hides the issues of the current Tool to discuss. To learn more see here . \"Gear\" icon Manage Tool permissions or delete a Tool. Run Run an instance with this Tool. Tool information page is divided into 3 tabs.","title":"Tool information page"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#description-tab","text":"This tab shows Tool description. For registries with Pipeline authentication option, you'll also see the Docker pull command on this tab if you have READ access to a Tool.","title":"Description\u00a0tab"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#versions-tab","text":"Choose this tab to see a list of Tool versions. About internal Tool version menu see 10.7. Tool version menu . Each version has the following icons and controls: Control/Label Description Name Name of version Scanning status If the security scanning is forced, you'll see the status \" in progress \" of security scanning. If the security scanning is failed, you'll see the status \" failed \". Last successful scan: The label shows if a version is successfully scanned at any time. The label contains date and time of the last successful attempt. Last scan date The label shows if a version scanning is failed. The label contains date and time of the last scanning attempt. Colored bars Hover over the colored bars to see scan status - a number of vulnerabilities grouped by severity (e.g. Critical, High, Medium, ...). Digest The label shows unique identifier of docker image. Corresponding aliases The label shows aliases of docker image (e.g. if some digest has more than one alias). Image size The label shows the size of docker image. Note : this value is provided for the \"gzipped\" docker image. When pulled to the local workstation or the cloud instance - the size of the image will be greater. Modified date The label shows modified date of docker image. SCAN Control forces the security scanning process. Available only for users with ROLE_ADMIN role. Run Allow to run the particular Tool version with default settings or customize it. Delete Delete the particular Tool version. In addition, the Version tab contains View unscanned version control. The control is visible if unscanned versions exist. More about Security scan feature you could learn here .","title":"Versions\u00a0tab"},{"location":"manual/10_Manage_Tools/10._Manage_Tools/#settings-tab","text":"Navigate to this tab to see Tool attributes and execution defaults. Endpoints - specify an endpoint for the service launched in a Tool. Labels - tool labels to briefly describe the Tool. Disk - instance disk size in Gb. Instance type - EC2 instance type. Contains information about an amount of CPU and RAM (in Gb). Default command - default command for Tool execution. Navigate back to the Tools group page from the Tool description with the arrow button on the top-left corner.","title":"Settings\u00a0tab"},{"location":"manual/11_Manage_Runs/11.1._Pause_resume_Runs/","text":"11.1. Pause/resume Runs Runs can be paused/resumed by users with ROLE_ADMIN or OWNERS . Cloud Platform currently provides functionality to launch and access services on Cloud hosted calculation nodes. Launching a service takes up to several minutes depending on multiple factors. When work with service is done, instance is terminated and all the local data and environment (installed tools, settings) are completely lost. In order to store the data it should be uploaded to Cloud data storage (AWS S3) before service termination, to save service environment user may user COMMIT option to update a service or create a new one, but for some use cases, e.g. script development in RStudio , these options may be inconvenient. PAUSE and RESUME options allow to reduce time to start a service, have an option to store service state and to reduce expenses for idle services. Stopped instances cost less than running instances. Note : pause/resume options are available only for on-demand instances. Price type can be set during Run configuration in the Advanced tab. Pause/resume run Find a run you want to pause in the Active runs tab and press Pause . Confirm pausing. A Run will have status PAUSING for a short period of time. Then RESUME option will appear. To resume the Run press the Resume button and confirm this action. A Run will have status RESUMING for a short period of time. Then a Run will continue working again.","title":"11.1 Pause/resume Runs"},{"location":"manual/11_Manage_Runs/11.1._Pause_resume_Runs/#111-pauseresume-runs","text":"Runs can be paused/resumed by users with ROLE_ADMIN or OWNERS . Cloud Platform currently provides functionality to launch and access services on Cloud hosted calculation nodes. Launching a service takes up to several minutes depending on multiple factors. When work with service is done, instance is terminated and all the local data and environment (installed tools, settings) are completely lost. In order to store the data it should be uploaded to Cloud data storage (AWS S3) before service termination, to save service environment user may user COMMIT option to update a service or create a new one, but for some use cases, e.g. script development in RStudio , these options may be inconvenient. PAUSE and RESUME options allow to reduce time to start a service, have an option to store service state and to reduce expenses for idle services. Stopped instances cost less than running instances. Note : pause/resume options are available only for on-demand instances. Price type can be set during Run configuration in the Advanced tab.","title":"11.1. Pause/resume Runs"},{"location":"manual/11_Manage_Runs/11.1._Pause_resume_Runs/#pauseresume-run","text":"Find a run you want to pause in the Active runs tab and press Pause . Confirm pausing. A Run will have status PAUSING for a short period of time. Then RESUME option will appear. To resume the Run press the Resume button and confirm this action. A Run will have status RESUMING for a short period of time. Then a Run will continue working again.","title":"Pause/resume run"},{"location":"manual/11_Manage_Runs/11.2._Auto-commit_Docker_image/","text":"11.2. Auto-commit Docker image User shall have ROLE_ADMIN role or be an OWNER of the Run to stop it and auto-commit Docker image. Auto-committing is a useful Cloud Pipeline option that allows to save current Docker image state before stopping a Run. In the Active runs tab select a Run and press STOP Select the checkbox Persist current docker image state , give that Docker image a name and a version (optional), e.g. auto-committed-version ). Press STOP . After that a Run will have a COMMITTING status for a short period of time. And then it will be stopped.","title":"11.2 Auto-commit Docker images"},{"location":"manual/11_Manage_Runs/11.2._Auto-commit_Docker_image/#112-auto-commit-docker-image","text":"User shall have ROLE_ADMIN role or be an OWNER of the Run to stop it and auto-commit Docker image. Auto-committing is a useful Cloud Pipeline option that allows to save current Docker image state before stopping a Run. In the Active runs tab select a Run and press STOP Select the checkbox Persist current docker image state , give that Docker image a name and a version (optional), e.g. auto-committed-version ). Press STOP . After that a Run will have a COMMITTING status for a short period of time. And then it will be stopped.","title":"11.2. Auto-commit Docker image"},{"location":"manual/11_Manage_Runs/11.3._Sharing_with_other_users_or_groups_of_users/","text":"11.3. Sharing with other users or groups of users Overview Sharing a run with user(s) Sharing a run with users group(s) Work with a sharing running instance (for not owners) Overview For certain use cases it is beneficial to be able to share applications with other users/groups. Cloud platform allows ability when runs environments will be accessed for several users, not only for the user, who launched the run ( OWNER ). Please note that sharing of a run - allows to share only the interactive tools endpoints (e.g. rstudio , jupyter , nomachine , etc.). SSH sessions cannot be shared. Sharing a run with user(s) In this example we will share a run with other user(s). Note : for do that, user account shall be registered within CP users catalog and granted READ & EXECUTE for the pipeline/tool. User must be an OWNER of the running instance. Note : in the example we will share a run of the rstudio Tool, that running with \"friendly\" URL (for more information about launching Tools see 10.5. Launch a Tool . Open run logs of the instance: Click on the link opposite the label \" Share with \": In opened pop-up window click button. In appeared window enter user name, for whom you want to share running instance. Confirm selected user by clicking \" Ok \" button: If necessary, add more users. When finished, click \" Ok \" button: In run logs, users names, for which you shared running instance, will be appeared opposite the label \" Share with \": Copy the link opposite the label \" Endpoint \", send it to user(s), for whom (which) you shared the instance: Sharing a run with users group(s) In this example we will share a run with other users group(s). Note : for do that, user account shall be registered within CP users catalog and granted READ & EXECUTE for the pipeline/tool. User must be an OWNER of the running instance. Note : in the example we will share a run of the rstudio Tool, that running with \"friendly\" URL (for more information about launching Tools see 10.5. Launch a Tool . Open run logs of the instance: Click on the link opposite the label \" Share with \": In opened pop-up window click button. In appeared window enter users group name, for whom you want to share running instance. Confirm selected users group by clicking \" OK \" button: If necessary, add more groups. When finished, click \" OK \" button: In run logs, users groups names, for which you shared running instance, will be appeared opposite the label \" Share with \": Copy the link opposite the label \" Endpoint \", send it to users, for which group(s) you shared the instance: Work with a sharing running instance (for not owners) A current user can be accessed to a service, without running own jobs, if that service was shared for a current user. Note : for do that, user account shall be registered within CP users catalog and granted \"sharing\" permission for the instance. Way 1 Log in at CP. Open new tab in browser and input the link of sharing running instance, that you received. The GUI of the Tool, of that running instance was shared, will be displayed. For example, described above, it will be the RStudio GUI: Way 2 On home Dashboard click button. In opened pop-up window enable check-box \" Services \" and click \" OK \" button: On appeared \" Services \" widget at the Home dashboard page \"shared\" service will be displayed: Click on it. The GUI of the Tool, of that running instance was shared, will be displayed.","title":"11.3 Sharing with other users"},{"location":"manual/11_Manage_Runs/11.3._Sharing_with_other_users_or_groups_of_users/#113-sharing-with-other-users-or-groups-of-users","text":"Overview Sharing a run with user(s) Sharing a run with users group(s) Work with a sharing running instance (for not owners)","title":"11.3. Sharing with other users or groups of users"},{"location":"manual/11_Manage_Runs/11.3._Sharing_with_other_users_or_groups_of_users/#overview","text":"For certain use cases it is beneficial to be able to share applications with other users/groups. Cloud platform allows ability when runs environments will be accessed for several users, not only for the user, who launched the run ( OWNER ). Please note that sharing of a run - allows to share only the interactive tools endpoints (e.g. rstudio , jupyter , nomachine , etc.). SSH sessions cannot be shared.","title":"Overview"},{"location":"manual/11_Manage_Runs/11.3._Sharing_with_other_users_or_groups_of_users/#sharing-a-run-with-users","text":"In this example we will share a run with other user(s). Note : for do that, user account shall be registered within CP users catalog and granted READ & EXECUTE for the pipeline/tool. User must be an OWNER of the running instance. Note : in the example we will share a run of the rstudio Tool, that running with \"friendly\" URL (for more information about launching Tools see 10.5. Launch a Tool . Open run logs of the instance: Click on the link opposite the label \" Share with \": In opened pop-up window click button. In appeared window enter user name, for whom you want to share running instance. Confirm selected user by clicking \" Ok \" button: If necessary, add more users. When finished, click \" Ok \" button: In run logs, users names, for which you shared running instance, will be appeared opposite the label \" Share with \": Copy the link opposite the label \" Endpoint \", send it to user(s), for whom (which) you shared the instance:","title":"Sharing a run with user(s)"},{"location":"manual/11_Manage_Runs/11.3._Sharing_with_other_users_or_groups_of_users/#sharing-a-run-with-users-groups","text":"In this example we will share a run with other users group(s). Note : for do that, user account shall be registered within CP users catalog and granted READ & EXECUTE for the pipeline/tool. User must be an OWNER of the running instance. Note : in the example we will share a run of the rstudio Tool, that running with \"friendly\" URL (for more information about launching Tools see 10.5. Launch a Tool . Open run logs of the instance: Click on the link opposite the label \" Share with \": In opened pop-up window click button. In appeared window enter users group name, for whom you want to share running instance. Confirm selected users group by clicking \" OK \" button: If necessary, add more groups. When finished, click \" OK \" button: In run logs, users groups names, for which you shared running instance, will be appeared opposite the label \" Share with \": Copy the link opposite the label \" Endpoint \", send it to users, for which group(s) you shared the instance:","title":"Sharing a run with users group(s)"},{"location":"manual/11_Manage_Runs/11.3._Sharing_with_other_users_or_groups_of_users/#work-with-a-sharing-running-instance-for-not-owners","text":"A current user can be accessed to a service, without running own jobs, if that service was shared for a current user. Note : for do that, user account shall be registered within CP users catalog and granted \"sharing\" permission for the instance.","title":"Work with a sharing running instance (for not owners)"},{"location":"manual/11_Manage_Runs/11.3._Sharing_with_other_users_or_groups_of_users/#way-1","text":"Log in at CP. Open new tab in browser and input the link of sharing running instance, that you received. The GUI of the Tool, of that running instance was shared, will be displayed. For example, described above, it will be the RStudio GUI:","title":"Way 1"},{"location":"manual/11_Manage_Runs/11.3._Sharing_with_other_users_or_groups_of_users/#way-2","text":"On home Dashboard click button. In opened pop-up window enable check-box \" Services \" and click \" OK \" button: On appeared \" Services \" widget at the Home dashboard page \"shared\" service will be displayed: Click on it. The GUI of the Tool, of that running instance was shared, will be displayed.","title":"Way 2"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/","text":"11. Manage Runs Overview ACTIVE RUNS Active run states Active run controls COMPLETED RUNS Completed run states Completed run controls Run information page General information Instance Parameters Tasks Console output Controls Automatically rerun if a spot instance is terminated Overview \" Runs \" provides a list of active and completed pipeline runs. You can get parameters and logs of specific run and stop run here. \" Runs \" space has two tabs: Active runs view Completed runs view. Runs are organized in a table which is the same for both tabs: \"State\" icon - state of the run. Run - run ID. Parent run - parent run ID, if a run was launched by another run. Pipeline - include: pipeline name (upper row) - a name of a pipeline version name (bottom row) - a name of a pipeline version Docker image - a name of docker image. Started - time when a run was started. Completed - time when a run was finished. Elapsed - include: elapsed time (upper row) - a duration of a run estimated price (bottom row) - estimated price of run, which is calculated based on the run duration and selected instance type. This field is updated interactively (i.e. each 5 - 10 seconds). Owner - a user, which launched a run. ACTIVE RUNS This tab displays a list of all pipelines that are currently running. Active run states Rotating - a run is scheduled but is waiting for a calculation node to appear. - now pipeline Docker image is downloaded to the node. - The pipeline is running. The node is appearing and pipeline input data is being downloaded to the node before the \" InitializeEnvironment \" service task appears. Active run controls Control Description PAUSE/RESUME Pauses/resumes a run. Available for on-demand instances only. Learn more about feature here . STOP This control stops a run execution. LOG To open a Run information page, press LOG button. COMPLETED RUNS This tab displays a list of all pipelines runs that are already finished. Completed run states - successful pipeline execution. - unsuccessful pipeline execution. - a pipeline manually stopped. Completed run controls Control Description LINKS This control show input/output links of the pipeline RERUN This control allow rerunning of a completed run. The Launch a pipeline page will be open. LOG To open a Run information page, press LOG button. Run information page Click a row within a run list, \"Run information\" page will appear. It consists of several sections: General information This section displays general information about a run: Field Description State icon state of the run. Run ID unique ID of the run. Owner a name of the user who started pipeline. Scheduled time when a pipeline was launched. Waiting for/Running for time a pipeline has been running. Started time when the node is initialized and a pipeline has started execution. Finished time when a pipeline finished execution. Estimated price price of a run according to a run duration and selected instance type. Instance The \" Instance \" section lists calculation node and execution environment details that were assigned to the run when it was launched. Note : node IP is presented as a hyperlink. Clicking it will navigate to the node details, where technical information and resources utilization is available. Note : Docker image name link leads to a specific Tool's detail page. Parameters The parameters that were assigned to the run when it was launched are contained in this section. Note : parameters with types input/output/common/path are presented as hyperlinks, and will navigate to appropriate location in a Data Storage hierarchy. Note : if a user specifies system environment variables in parameter (e.g. RUN_ID ), GUI will substitute these variables with their values automatically in the \" Run information \" page. Tasks Here you can find a list of tasks of pipeline that are being executed or already finished. Clicking a task and its console output will be loaded in the right panel. Console output Console output shows console output from a whole pipeline or a selected task. It also shows a run failure cause if a run failed. Note : the Follow log control enables auto scrolling of the console output. It is useful for logs monitoring. Follow log is enabled by default, tick the box to turn it off. Controls Note : Completed and active runs have different controls. Example : controls of completed Luigi pipeline. Here's the list of all existing buttons Control Description Stop Allows stopping a run. Show timings Each task will show it's duration if SHOW TIMINGS mode is ON (button in the right upper corner). Commit To docker images running \"sleep infinity\" mode that has been changed via ssh. See 10.4. Edit a Tool . GRAPH VIEW For Luigi and WDL pipelines GRAPH VIEW is available along with a usual plain view of tasks. See 6.1.1 Building WDL pipeline with graphical PipelineBuilder . SSH Allows to shh to the instance running \"sleep infinity\" mode. See 6.1. Create and configure pipeline . Rerun You can rerun via RERUN button. Export logs You can export logs via EXPORT LOGS button. Automatically rerun if a spot instance is terminated In certain cases - AWS may terminate a node, that is used to run a job or an interactive tool. It may be in cases: Spot prices changed AWS experienced a hardware issue These cases aren't a Cloud Platform bug. In these cases: If a job fails due to server-related issue, special message is displayed, describing a reason for the hardware failure: If a batch job fails due to server-related issue and AWS reports one of the following EC2 status codes: Server.SpotInstanceShutdown - AWS stopped a spot instance due to price changes, Server.SpotInstanceTermination - AWS terminated a spot instance due to price changes, Server.InternalError - AWS hardware issue, batch job will be restarted from scratch automatically. Note : this behavior will occur, only if administrator applied and configured it (for more information see 12.10. Manage system-level settings .","title":"11.0 Overview"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#11-manage-runs","text":"Overview ACTIVE RUNS Active run states Active run controls COMPLETED RUNS Completed run states Completed run controls Run information page General information Instance Parameters Tasks Console output Controls Automatically rerun if a spot instance is terminated","title":"11. Manage Runs"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#overview","text":"\" Runs \" provides a list of active and completed pipeline runs. You can get parameters and logs of specific run and stop run here. \" Runs \" space has two tabs: Active runs view Completed runs view. Runs are organized in a table which is the same for both tabs: \"State\" icon - state of the run. Run - run ID. Parent run - parent run ID, if a run was launched by another run. Pipeline - include: pipeline name (upper row) - a name of a pipeline version name (bottom row) - a name of a pipeline version Docker image - a name of docker image. Started - time when a run was started. Completed - time when a run was finished. Elapsed - include: elapsed time (upper row) - a duration of a run estimated price (bottom row) - estimated price of run, which is calculated based on the run duration and selected instance type. This field is updated interactively (i.e. each 5 - 10 seconds). Owner - a user, which launched a run.","title":"Overview"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#active-runs","text":"This tab displays a list of all pipelines that are currently running.","title":"ACTIVE RUNS"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#active-run-states","text":"Rotating - a run is scheduled but is waiting for a calculation node to appear. - now pipeline Docker image is downloaded to the node. - The pipeline is running. The node is appearing and pipeline input data is being downloaded to the node before the \" InitializeEnvironment \" service task appears.","title":"Active run states"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#active-run-controls","text":"Control Description PAUSE/RESUME Pauses/resumes a run. Available for on-demand instances only. Learn more about feature here . STOP This control stops a run execution. LOG To open a Run information page, press LOG button.","title":"Active run controls"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#completed-runs","text":"This tab displays a list of all pipelines runs that are already finished.","title":"COMPLETED RUNS"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#completed-run-states","text":"- successful pipeline execution. - unsuccessful pipeline execution. - a pipeline manually stopped.","title":"Completed run states"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#completed-run-controls","text":"Control Description LINKS This control show input/output links of the pipeline RERUN This control allow rerunning of a completed run. The Launch a pipeline page will be open. LOG To open a Run information page, press LOG button.","title":"Completed run controls"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#run-information-page","text":"Click a row within a run list, \"Run information\" page will appear. It consists of several sections:","title":"Run information page"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#general-information","text":"This section displays general information about a run: Field Description State icon state of the run. Run ID unique ID of the run. Owner a name of the user who started pipeline. Scheduled time when a pipeline was launched. Waiting for/Running for time a pipeline has been running. Started time when the node is initialized and a pipeline has started execution. Finished time when a pipeline finished execution. Estimated price price of a run according to a run duration and selected instance type.","title":"General information"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#instance","text":"The \" Instance \" section lists calculation node and execution environment details that were assigned to the run when it was launched. Note : node IP is presented as a hyperlink. Clicking it will navigate to the node details, where technical information and resources utilization is available. Note : Docker image name link leads to a specific Tool's detail page.","title":"Instance"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#parameters","text":"The parameters that were assigned to the run when it was launched are contained in this section. Note : parameters with types input/output/common/path are presented as hyperlinks, and will navigate to appropriate location in a Data Storage hierarchy. Note : if a user specifies system environment variables in parameter (e.g. RUN_ID ), GUI will substitute these variables with their values automatically in the \" Run information \" page.","title":"Parameters"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#tasks","text":"Here you can find a list of tasks of pipeline that are being executed or already finished. Clicking a task and its console output will be loaded in the right panel.","title":"Tasks"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#console-output","text":"Console output shows console output from a whole pipeline or a selected task. It also shows a run failure cause if a run failed. Note : the Follow log control enables auto scrolling of the console output. It is useful for logs monitoring. Follow log is enabled by default, tick the box to turn it off.","title":"Console output"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#controls","text":"Note : Completed and active runs have different controls. Example : controls of completed Luigi pipeline. Here's the list of all existing buttons Control Description Stop Allows stopping a run. Show timings Each task will show it's duration if SHOW TIMINGS mode is ON (button in the right upper corner). Commit To docker images running \"sleep infinity\" mode that has been changed via ssh. See 10.4. Edit a Tool . GRAPH VIEW For Luigi and WDL pipelines GRAPH VIEW is available along with a usual plain view of tasks. See 6.1.1 Building WDL pipeline with graphical PipelineBuilder . SSH Allows to shh to the instance running \"sleep infinity\" mode. See 6.1. Create and configure pipeline . Rerun You can rerun via RERUN button. Export logs You can export logs via EXPORT LOGS button.","title":"Controls"},{"location":"manual/11_Manage_Runs/11._Manage_Runs/#automatically-rerun-if-a-spot-instance-is-terminated","text":"In certain cases - AWS may terminate a node, that is used to run a job or an interactive tool. It may be in cases: Spot prices changed AWS experienced a hardware issue These cases aren't a Cloud Platform bug. In these cases: If a job fails due to server-related issue, special message is displayed, describing a reason for the hardware failure: If a batch job fails due to server-related issue and AWS reports one of the following EC2 status codes: Server.SpotInstanceShutdown - AWS stopped a spot instance due to price changes, Server.SpotInstanceTermination - AWS terminated a spot instance due to price changes, Server.InternalError - AWS hardware issue, batch job will be restarted from scratch automatically. Note : this behavior will occur, only if administrator applied and configured it (for more information see 12.10. Manage system-level settings .","title":"Automatically rerun if a spot instance is terminated"},{"location":"manual/12_Manage_Settings/12.1._Add_a_new_system_event/","text":"12.1. Add a new system event An administrator can add System event notification only. Navigate to System events tab. Click +ADD . Enter a Title of notification. Enter Body of the notification (optional). Rank notification Severity (\" info \", \" warning \" or \" critical \"). Mark as blocking ( optional ). Blocking event emerges in the middle of the window and requires confirmation from the user to disappear. Mark as active ( optional ). Active notifications will be shown for all users of the Cloud Pipeline until admin sets them inactive. Click Create .","title":"12.1 Add a new system event"},{"location":"manual/12_Manage_Settings/12.1._Add_a_new_system_event/#121-add-a-new-system-event","text":"An administrator can add System event notification only. Navigate to System events tab. Click +ADD . Enter a Title of notification. Enter Body of the notification (optional). Rank notification Severity (\" info \", \" warning \" or \" critical \"). Mark as blocking ( optional ). Blocking event emerges in the middle of the window and requires confirmation from the user to disappear. Mark as active ( optional ). Active notifications will be shown for all users of the Cloud Pipeline until admin sets them inactive. Click Create .","title":"12.1. Add a new system event"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/","text":"12.10. Manage system-level settings User shall have ROLE_ADMIN to read and update system-level settings. Read system-level settings Base URLs Cluster Commit Data Storage Docker security Git Grid engine autoscaling Launch System User Interface Make system-level settings visible to all users Update system-level settings Read system-level settings Hover to the Settings tab. Select the Preferences section. All system-level parameters are categorized into several groups. Base URLs These settings define pipeline URLs: Setting name Description base.api.host REST API endpoint base.pipe.distributions.url URL that is used to download pipeline scripts Cluster Settings in this tab define different cluster options: Setting name Description cluster.keep.alive.minutes If node doesn't have a running pipeline on it for that amount of minutes, it will be shut down cluster.random.scheduling If this property is true, pipeline scheduler will rely on Kubernetes order of pods, otherwise pipelines will be ordered according to their parent (batch) ID cluster.networks.config Config that contains information to start new nodes in AWS cluster.instance.image Default EC2 instance image cluster.batch.retry.count Count of automatically retries to relaunch a job, if one of the EC2 status codes from instance.restart.state.reasons returns, when a batch job fails instance.offer.update.rate How often instance cost is calculated (in milliseconds) cluster.instance.type Default EC2 instance type cluster.max.size Maximal number of nodes to be launched simultaneously cluster.autoscale.rate How often autoscaler checks what tasks are executed on each node (in milliseconds) cluster.min.size Minimal number of nodes to be launched at a time cluster.allowed.instance.types Allowed Amazon EC2 instance types. Can restrict available instance types for launching tools, pipelines, configurations cluster.allowed.instance.types.docker Allowed Amazon EC2 instance types for docker images (tools). Can restrict available instance types for launching tools. Has a higher priority for a tool than cluster.allowed.instance.types cluster.allowed.price.types Allowed price types. Can restrict available price types for launching tools, pipelines, configurations cluster.nodeup.max.threads Maximal number of nodes that can be started simultaneously cluster.spot.bid.price The maximum price per hour that you are willing to pay for a Spot Instance. The default is the On-Demand price cluster.enable.autoscaling Enables/disables Kubernetes autoscaler service instance.restart.state.reasons EC2 status codes, upon receipt of which an instance tries automatically to restart cluster.spot If this is true, spot instances will be launched by default cluster.kill.not.matching.nodes If this property is true, any free node that doesn't match configuration of a pending pod will be scaled down immediately, otherwise it will be left until it will be reused or expired. If most of the time we use nodes with the same configuration set this to true cluster.instance.hdd Default hard drive size for instance (in gigabytes) cluster.ssh.key.name Name of the key that is used to connect to the running node via SSH cluster.spot.alloc.strategy Parameter that sets the strategy of calculating the price limit for instance: on-demand - maximal instance price equals the price of the on-demand instance of the same type; manual - uses value from the cluster.spot.bid.price parameter cluster.nodeup.retry.count Maximal number of tries to start the node cluster.high.non.batch.priority If this property is true, pipelines without parent (batch ID) will have the highest priority, otherwise - the lowest Commit This tab contains various commit settings: Setting name Description commit.username Git username commit.deploy.key Used to SSH for COMMIT. Key is stored in a DB commit.timeout Commit will fail if exceeded (in seconds) Data Storage These settings define storage parameters: Setting name Description storage.temp.credentials.duration Temporary credentials lifetime for AWS operations with S3 (in seconds) storage.max.download.size Chunk size to download (bytes) storage.policy.backup.duration Backup duration time (days) storage.temp.credentials.role This role will be used to allow bucket operations - it will be given temporary credentials storage.system.storage.name Configures a system data storage for storing attachments from e.g. issues storage.cors.policy Set of bucket CORS policies storage.mount.black.list List of directories where Data Storages couldn't be mounted storage.policy.backup.enabled Allows backup by default storage.security.key.id Key that is used for bucket encryption storage.policy Set of data storage policies storage.security.key.arn Amazon Resource Name (ARN) of the AWS Key Management Service (AWS KMS) storage.object.prefix A mandatory prefix for the new creating S3-buckets Docker security This tab contains settings related to Docker security checks: Setting name Description security.tools.scan.all.registries If this is true, all registries will be scanned for Tools vulnerability security.tools.scan.clair.read.timeout Sets timeout for Clair response (in seconds) security.tools.scan.clair.root.url Clair root URL security.tools.policy.deny.not.scanned Allow/deny execution of unscanned Tools security.tools.scan.enabled Enables/disables security scan security.tools.scan.clair.connect.timeout Sets timeout for connection with Clair (in seconds) security.tools.policy.max.medium.vulnerabilities Denies running a Tool if the number of medium vulnerabilities exceeds the threshold. To disable the policy, set to -1 security.tools.policy.max.high.vulnerabilities Denies running a Tool if the number of high vulnerabilities exceeds the threshold. To disable the policy, set to -1 security.tools.policy.max.critical.vulnerabilities Denies running a Tool if the number of critical vulnerabilities exceeds the threshold. To disable the policy, set to -1 security.tools.scan.schedule.cron Security scan schedule security.tools.grace.hours Allows users to run a new docker image (if it is not scanned yet) or an image with a lot of vulnerabilities during a specified period. During this period user will be able to run a tool, but an appropriate message will be displayed. Period lasts from date/time since the docker version became vulnerable or since the docker image's push time (if this version was not scanned yet) Git These settings define git parameters: Setting name Description git.token Token to access Git with pipelines git.user.id User id to access Git with pipelines git.user.name User name to access Git with pipelines git.host IP address where Git service is deployed Grid engine autoscaling These settings define auto-scaled cluster parameters: Setting name Description ge.autoscaling.scale.down.timeout If jobs queue is empty or all jobs are running and there are some idle nodes longer than that timeout - auto-scaled cluster will start to drop idle auto-scaled nodes (\"scale-down\") ge.autoscaling.scale.up.timeout If some jobs are in waiting state longer than that timeout - auto-scaled cluster will start to attach new computation nodes to the cluster (\"scale-up\") Launch Settings in this tab contains default Launch parameters: Setting name Description launch.jwt.token.expiration Lifetime of a pipeline token (in seconds) launch.max.scheduled.number Controls maximum number of scheduled at once runs launch.env.properties Sets of environment variables that will be passed to each running Tool launch.docker.image Default Docker image launch.task.status.update.rate Sets task status update rate, on which application will query kubernetes cluster for running task status, ms. Pod Monitor launch.cmd.template Default cmd template launch.system.parameters System parameters, that are used when launching pipelines System The settings in this tab contain parameters and actions that are performed depending on the system monitoring metrics: Setting name Description system.idle.cpu.threshold Specifies percentage of the CPU utilization, below which action shall be taken system.resource.monitoring.period Specifies period (in seconds) between the users' instances scanning to collect the monitoring metrics system.max.idle.timeout.minutes Specifies a duration in minutes. If CPU utilization is below system.idle.cpu.threshold for this duration - notification will be sent to the user system.idle.action.timeout.minutes Specifies a duration in minutes. If CPU utilization is below system.idle.cpu.threshold for this duration - an action, specified in system.idle.action will be performed system.idle.action Sets which action to perform on the instance, that showed low CPU utilization (that is below system.idle.cpu.threshold ): NOTIFY - only send notification PAUSE - pause an instance if possible (e.g. instance is On-Demand, Spot instances are skipped) PAUSE_OR_STOP - pause an instance if it is On-Demand, stop an instance if it is Spot STOP - Stop an instance, disregarding price-type User Interface Here different user interface settings can be found: Setting name Description ui.pipeline.deployment.name UI deployment name ui.pipe.cli.install.template CLI install templates for different operating systems ui.project.indicator These attributes define a Project folder ui.pipe.cli.configure.template CLI configure templates for different operating systems ui.controls.settings JSON file that contains control settings Make system-level settings visible to all users Hover to the Settings tab. Select the Preferences section. Choose one of the tabs with system level settings (e.g. Cluster ). Press the \" Eye \" button near any setting. Now it will be visible to all users in the Preferences section. Note : press \" Eye \" button again to hide it from all users. Update system-level settings Choose any system-level setting and change its value (e.g. change cluster.keep.alive.minutes value from 10 to 15). Press the Save button. Note : before saving you can press the Revert button to return setting's value to the previous state.","title":"12.10. Manage system-level settings"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#1210-manage-system-level-settings","text":"User shall have ROLE_ADMIN to read and update system-level settings. Read system-level settings Base URLs Cluster Commit Data Storage Docker security Git Grid engine autoscaling Launch System User Interface Make system-level settings visible to all users Update system-level settings","title":"12.10. Manage system-level settings"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#read-system-level-settings","text":"Hover to the Settings tab. Select the Preferences section. All system-level parameters are categorized into several groups.","title":"Read system-level settings"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#base-urls","text":"These settings define pipeline URLs: Setting name Description base.api.host REST API endpoint base.pipe.distributions.url URL that is used to download pipeline scripts","title":"Base URLs"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#cluster","text":"Settings in this tab define different cluster options: Setting name Description cluster.keep.alive.minutes If node doesn't have a running pipeline on it for that amount of minutes, it will be shut down cluster.random.scheduling If this property is true, pipeline scheduler will rely on Kubernetes order of pods, otherwise pipelines will be ordered according to their parent (batch) ID cluster.networks.config Config that contains information to start new nodes in AWS cluster.instance.image Default EC2 instance image cluster.batch.retry.count Count of automatically retries to relaunch a job, if one of the EC2 status codes from instance.restart.state.reasons returns, when a batch job fails instance.offer.update.rate How often instance cost is calculated (in milliseconds) cluster.instance.type Default EC2 instance type cluster.max.size Maximal number of nodes to be launched simultaneously cluster.autoscale.rate How often autoscaler checks what tasks are executed on each node (in milliseconds) cluster.min.size Minimal number of nodes to be launched at a time cluster.allowed.instance.types Allowed Amazon EC2 instance types. Can restrict available instance types for launching tools, pipelines, configurations cluster.allowed.instance.types.docker Allowed Amazon EC2 instance types for docker images (tools). Can restrict available instance types for launching tools. Has a higher priority for a tool than cluster.allowed.instance.types cluster.allowed.price.types Allowed price types. Can restrict available price types for launching tools, pipelines, configurations cluster.nodeup.max.threads Maximal number of nodes that can be started simultaneously cluster.spot.bid.price The maximum price per hour that you are willing to pay for a Spot Instance. The default is the On-Demand price cluster.enable.autoscaling Enables/disables Kubernetes autoscaler service instance.restart.state.reasons EC2 status codes, upon receipt of which an instance tries automatically to restart cluster.spot If this is true, spot instances will be launched by default cluster.kill.not.matching.nodes If this property is true, any free node that doesn't match configuration of a pending pod will be scaled down immediately, otherwise it will be left until it will be reused or expired. If most of the time we use nodes with the same configuration set this to true cluster.instance.hdd Default hard drive size for instance (in gigabytes) cluster.ssh.key.name Name of the key that is used to connect to the running node via SSH cluster.spot.alloc.strategy Parameter that sets the strategy of calculating the price limit for instance: on-demand - maximal instance price equals the price of the on-demand instance of the same type; manual - uses value from the cluster.spot.bid.price parameter cluster.nodeup.retry.count Maximal number of tries to start the node cluster.high.non.batch.priority If this property is true, pipelines without parent (batch ID) will have the highest priority, otherwise - the lowest","title":"Cluster"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#commit","text":"This tab contains various commit settings: Setting name Description commit.username Git username commit.deploy.key Used to SSH for COMMIT. Key is stored in a DB commit.timeout Commit will fail if exceeded (in seconds)","title":"Commit"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#data-storage","text":"These settings define storage parameters: Setting name Description storage.temp.credentials.duration Temporary credentials lifetime for AWS operations with S3 (in seconds) storage.max.download.size Chunk size to download (bytes) storage.policy.backup.duration Backup duration time (days) storage.temp.credentials.role This role will be used to allow bucket operations - it will be given temporary credentials storage.system.storage.name Configures a system data storage for storing attachments from e.g. issues storage.cors.policy Set of bucket CORS policies storage.mount.black.list List of directories where Data Storages couldn't be mounted storage.policy.backup.enabled Allows backup by default storage.security.key.id Key that is used for bucket encryption storage.policy Set of data storage policies storage.security.key.arn Amazon Resource Name (ARN) of the AWS Key Management Service (AWS KMS) storage.object.prefix A mandatory prefix for the new creating S3-buckets","title":"Data Storage"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#docker-security","text":"This tab contains settings related to Docker security checks: Setting name Description security.tools.scan.all.registries If this is true, all registries will be scanned for Tools vulnerability security.tools.scan.clair.read.timeout Sets timeout for Clair response (in seconds) security.tools.scan.clair.root.url Clair root URL security.tools.policy.deny.not.scanned Allow/deny execution of unscanned Tools security.tools.scan.enabled Enables/disables security scan security.tools.scan.clair.connect.timeout Sets timeout for connection with Clair (in seconds) security.tools.policy.max.medium.vulnerabilities Denies running a Tool if the number of medium vulnerabilities exceeds the threshold. To disable the policy, set to -1 security.tools.policy.max.high.vulnerabilities Denies running a Tool if the number of high vulnerabilities exceeds the threshold. To disable the policy, set to -1 security.tools.policy.max.critical.vulnerabilities Denies running a Tool if the number of critical vulnerabilities exceeds the threshold. To disable the policy, set to -1 security.tools.scan.schedule.cron Security scan schedule security.tools.grace.hours Allows users to run a new docker image (if it is not scanned yet) or an image with a lot of vulnerabilities during a specified period. During this period user will be able to run a tool, but an appropriate message will be displayed. Period lasts from date/time since the docker version became vulnerable or since the docker image's push time (if this version was not scanned yet)","title":"Docker security"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#git","text":"These settings define git parameters: Setting name Description git.token Token to access Git with pipelines git.user.id User id to access Git with pipelines git.user.name User name to access Git with pipelines git.host IP address where Git service is deployed","title":"Git"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#grid-engine-autoscaling","text":"These settings define auto-scaled cluster parameters: Setting name Description ge.autoscaling.scale.down.timeout If jobs queue is empty or all jobs are running and there are some idle nodes longer than that timeout - auto-scaled cluster will start to drop idle auto-scaled nodes (\"scale-down\") ge.autoscaling.scale.up.timeout If some jobs are in waiting state longer than that timeout - auto-scaled cluster will start to attach new computation nodes to the cluster (\"scale-up\")","title":"Grid engine autoscaling"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#launch","text":"Settings in this tab contains default Launch parameters: Setting name Description launch.jwt.token.expiration Lifetime of a pipeline token (in seconds) launch.max.scheduled.number Controls maximum number of scheduled at once runs launch.env.properties Sets of environment variables that will be passed to each running Tool launch.docker.image Default Docker image launch.task.status.update.rate Sets task status update rate, on which application will query kubernetes cluster for running task status, ms. Pod Monitor launch.cmd.template Default cmd template launch.system.parameters System parameters, that are used when launching pipelines","title":"Launch"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#system","text":"The settings in this tab contain parameters and actions that are performed depending on the system monitoring metrics: Setting name Description system.idle.cpu.threshold Specifies percentage of the CPU utilization, below which action shall be taken system.resource.monitoring.period Specifies period (in seconds) between the users' instances scanning to collect the monitoring metrics system.max.idle.timeout.minutes Specifies a duration in minutes. If CPU utilization is below system.idle.cpu.threshold for this duration - notification will be sent to the user system.idle.action.timeout.minutes Specifies a duration in minutes. If CPU utilization is below system.idle.cpu.threshold for this duration - an action, specified in system.idle.action will be performed system.idle.action Sets which action to perform on the instance, that showed low CPU utilization (that is below system.idle.cpu.threshold ): NOTIFY - only send notification PAUSE - pause an instance if possible (e.g. instance is On-Demand, Spot instances are skipped) PAUSE_OR_STOP - pause an instance if it is On-Demand, stop an instance if it is Spot STOP - Stop an instance, disregarding price-type","title":"System"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#user-interface","text":"Here different user interface settings can be found: Setting name Description ui.pipeline.deployment.name UI deployment name ui.pipe.cli.install.template CLI install templates for different operating systems ui.project.indicator These attributes define a Project folder ui.pipe.cli.configure.template CLI configure templates for different operating systems ui.controls.settings JSON file that contains control settings","title":"User Interface"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#make-system-level-settings-visible-to-all-users","text":"Hover to the Settings tab. Select the Preferences section. Choose one of the tabs with system level settings (e.g. Cluster ). Press the \" Eye \" button near any setting. Now it will be visible to all users in the Preferences section. Note : press \" Eye \" button again to hide it from all users.","title":"Make system-level settings visible to all users"},{"location":"manual/12_Manage_Settings/12.10._Manage_system-level_settings/#update-system-level-settings","text":"Choose any system-level setting and change its value (e.g. change cluster.keep.alive.minutes value from 10 to 15). Press the Save button. Note : before saving you can press the Revert button to return setting's value to the previous state.","title":"Update system-level settings"},{"location":"manual/12_Manage_Settings/12.2._Edit_a_system_event/","text":"12.2. Edit a system event An administrator can edit System events notifications only. Navigate to System events tab . Click the Edit button. Change any field: Title of the notification. Body of the notification. Notification Severity (\" info \", \" warning \" or \" critical \"). Blocking box. Blocking event emerges in the middle of the window and requires confirmation from the user to disappear. Active box. Active notifications will be shown for all users of the Cloud Pipeline until admin sets them inactive. Click Save .","title":"12.2 Edit a system event"},{"location":"manual/12_Manage_Settings/12.2._Edit_a_system_event/#122-edit-a-system-event","text":"An administrator can edit System events notifications only. Navigate to System events tab . Click the Edit button. Change any field: Title of the notification. Body of the notification. Notification Severity (\" info \", \" warning \" or \" critical \"). Blocking box. Blocking event emerges in the middle of the window and requires confirmation from the user to disappear. Active box. Active notifications will be shown for all users of the Cloud Pipeline until admin sets them inactive. Click Save .","title":"12.2. Edit a system event"},{"location":"manual/12_Manage_Settings/12.3._Create_a_new_user/","text":"12.3. Create a new user User shall have ROLE_ADMIN to create a new user. Navigate to User management tab. Click + Create user control. The Create user form will be opened. This form contains the following sections: Name - a new user's name. Assign group or role - drop-down list suggested the existing roles and groups assign. View of roles and groups that are assigned to a new user. Note : the groups and roles, marked as default, will be shown. Enter the name for a new user. Note : there is no restriction to username format, but it is highly recommended to name a user according to your SSO scheme. Select desired groups and roles to assign the new user. Click Create and the new user will be displayed in the Users tab table.","title":"12.3. Create a new user"},{"location":"manual/12_Manage_Settings/12.3._Create_a_new_user/#123-create-a-new-user","text":"User shall have ROLE_ADMIN to create a new user. Navigate to User management tab. Click + Create user control. The Create user form will be opened. This form contains the following sections: Name - a new user's name. Assign group or role - drop-down list suggested the existing roles and groups assign. View of roles and groups that are assigned to a new user. Note : the groups and roles, marked as default, will be shown. Enter the name for a new user. Note : there is no restriction to username format, but it is highly recommended to name a user according to your SSO scheme. Select desired groups and roles to assign the new user. Click Create and the new user will be displayed in the Users tab table.","title":"12.3. Create a new user"},{"location":"manual/12_Manage_Settings/12.4._Edit_delete_a_user/","text":"12.4. Edit/delete a user Edit a user Default data storage Groups (roles) management Attributes Launch options Delete a user User shall have ROLE_ADMIN to edit/delete users. Edit a user For edit a user: Open Users tab on User management section of system-level settings. Find a user. Click Edit button in the row opposite the user name: Pop-up window will be shown: On this form there are several blocks of the settings for a user. Default data storage Here you can select default data storage for a user: Groups (roles) management In this block you can set groups and roles for the selected user: For more information about changing a set of the roles/groups for the specific user see 12.8. Change a set of roles/groups for a user . Attributes In this block you can set metadata tags (attributes) for a user. These tags represent key/value pairs, same as pipeline/folder tags. For more information see 17. CP objects tagging by additional attributes . Launch options In this block you can specify some restrictions for a user on allowed instance types and price types. Here you can specify: Field Description Example Allowed instance types mask This mask restrict for a specific user allowed instance types for launching tools, pipelines and configurations If you want user will be able to launch runs with only \"m5...\" instances types, mask would be m5* : In that case, before launching tool, pipeline or configuration, dropdown list of available node types will be look like this: Allowed tool instance types mask This mask restrict for a specific user allowed instance types only for tools - launching from tools menu or main dashboard. This mask has higher priority for launching tool than Allowed instance types mask . It's meaning that in case when both masks are set - for the launching tool will be applied Allowed tool instance types mask . If you want user will be able to launch tools with only some of \"large m5...\" instances types, mask would be m5*.large* : In that case, before launching tool, dropdown list of available node types will be look like this: Allowed price types In this field you may restrict, what price types will be allowed for a user. If you want user will be able to launch only \"On-demand\" runs, select it in the dropdown list: In that case, before launching tool, dropdown list of price types will be look like this: To apply set restrictions for a user click Setting restrictions on allowed instance types/price types is a convenient way to minimize a number of invalid configurations runs. Such restrictions could be set not only for a user, but on another levels too. In CP platform next hierarchy is set for applying of inputted allowed instance types (sorted by priority): User level (specified for a user on \"User management\" tab) (see above ) User group level (specified for a group (role) on \"User management\" tab. If a user is a member of several groups - list of allowed instances will be summarized across all the groups) (see v.0.14 - 12.6. Edit a group/role ) Tool level (specified for a tool on \"Instance management\" panel) (see v.0.14 - 10.5. Launch a Tool ) (global) cluster.allowed.instance.types.docker (specified on \"Cluster\" tab in \"Preferences\" section of system-level settings) (see v.0.14 - 12.10. Manage system-level settings ) (global) cluster.allowed.instance.types (specified on \"Cluster\" tab in \"Preferences\" section of system-level settings) (see v.0.14 - 12.10. Manage system-level settings ) After specifying allowed instance types, all GUI forms that allow to select the list of instance types (configurations/launch forms) - will display only valid instance type, according to hierarchy above. For price type specifying - if it is set for the user/group/tool - GUI will allow to select only that price type. Delete a user For delete a user: Open Users tab on User management section of system-level settings. Find a user. Click Edit button in the row opposite the user name. In the opened pop-up window click Delete button in the left bottom corner. Confirm the deletion:","title":"12.4. Edit/delete a user"},{"location":"manual/12_Manage_Settings/12.4._Edit_delete_a_user/#124-editdelete-a-user","text":"Edit a user Default data storage Groups (roles) management Attributes Launch options Delete a user User shall have ROLE_ADMIN to edit/delete users.","title":"12.4. Edit/delete a user"},{"location":"manual/12_Manage_Settings/12.4._Edit_delete_a_user/#edit-a-user","text":"For edit a user: Open Users tab on User management section of system-level settings. Find a user. Click Edit button in the row opposite the user name: Pop-up window will be shown: On this form there are several blocks of the settings for a user.","title":"Edit a user"},{"location":"manual/12_Manage_Settings/12.4._Edit_delete_a_user/#default-data-storage","text":"Here you can select default data storage for a user:","title":"Default data storage"},{"location":"manual/12_Manage_Settings/12.4._Edit_delete_a_user/#groups-roles-management","text":"In this block you can set groups and roles for the selected user: For more information about changing a set of the roles/groups for the specific user see 12.8. Change a set of roles/groups for a user .","title":"Groups (roles) management"},{"location":"manual/12_Manage_Settings/12.4._Edit_delete_a_user/#attributes","text":"In this block you can set metadata tags (attributes) for a user. These tags represent key/value pairs, same as pipeline/folder tags. For more information see 17. CP objects tagging by additional attributes .","title":"Attributes"},{"location":"manual/12_Manage_Settings/12.4._Edit_delete_a_user/#launch-options","text":"In this block you can specify some restrictions for a user on allowed instance types and price types. Here you can specify: Field Description Example Allowed instance types mask This mask restrict for a specific user allowed instance types for launching tools, pipelines and configurations If you want user will be able to launch runs with only \"m5...\" instances types, mask would be m5* : In that case, before launching tool, pipeline or configuration, dropdown list of available node types will be look like this: Allowed tool instance types mask This mask restrict for a specific user allowed instance types only for tools - launching from tools menu or main dashboard. This mask has higher priority for launching tool than Allowed instance types mask . It's meaning that in case when both masks are set - for the launching tool will be applied Allowed tool instance types mask . If you want user will be able to launch tools with only some of \"large m5...\" instances types, mask would be m5*.large* : In that case, before launching tool, dropdown list of available node types will be look like this: Allowed price types In this field you may restrict, what price types will be allowed for a user. If you want user will be able to launch only \"On-demand\" runs, select it in the dropdown list: In that case, before launching tool, dropdown list of price types will be look like this: To apply set restrictions for a user click Setting restrictions on allowed instance types/price types is a convenient way to minimize a number of invalid configurations runs. Such restrictions could be set not only for a user, but on another levels too. In CP platform next hierarchy is set for applying of inputted allowed instance types (sorted by priority): User level (specified for a user on \"User management\" tab) (see above ) User group level (specified for a group (role) on \"User management\" tab. If a user is a member of several groups - list of allowed instances will be summarized across all the groups) (see v.0.14 - 12.6. Edit a group/role ) Tool level (specified for a tool on \"Instance management\" panel) (see v.0.14 - 10.5. Launch a Tool ) (global) cluster.allowed.instance.types.docker (specified on \"Cluster\" tab in \"Preferences\" section of system-level settings) (see v.0.14 - 12.10. Manage system-level settings ) (global) cluster.allowed.instance.types (specified on \"Cluster\" tab in \"Preferences\" section of system-level settings) (see v.0.14 - 12.10. Manage system-level settings ) After specifying allowed instance types, all GUI forms that allow to select the list of instance types (configurations/launch forms) - will display only valid instance type, according to hierarchy above. For price type specifying - if it is set for the user/group/tool - GUI will allow to select only that price type.","title":"Launch options"},{"location":"manual/12_Manage_Settings/12.4._Edit_delete_a_user/#delete-a-user","text":"For delete a user: Open Users tab on User management section of system-level settings. Find a user. Click Edit button in the row opposite the user name. In the opened pop-up window click Delete button in the left bottom corner. Confirm the deletion:","title":"Delete a user"},{"location":"manual/12_Manage_Settings/12.5._Create_a_group/","text":"12.5. Create a group User shall have ROLE_ADMIN to create a new group. Navigate to User management tab. Click Groups tab. Click + Create group button. Enter a name for the new group (e.g. NEW_GROUP ). If you want to grant the group and its permissions to all new users mark the group as Default . Click Create .","title":"12.5. Create a group"},{"location":"manual/12_Manage_Settings/12.5._Create_a_group/#125-create-a-group","text":"User shall have ROLE_ADMIN to create a new group. Navigate to User management tab. Click Groups tab. Click + Create group button. Enter a name for the new group (e.g. NEW_GROUP ). If you want to grant the group and its permissions to all new users mark the group as Default . Click Create .","title":"12.5. Create a group"},{"location":"manual/12_Manage_Settings/12.6._Edit_a_group_role/","text":"12.6. Edit a group (role) Edit a group (role) Default data storage User management Attributes Launch options User shall have ROLE_ADMIN to edit groups/roles. Edit a group (role) For edit a group/role: Open Groups/Roles tab on User management section of the system-level settings. Find a group (role). Click Edit button in the row opposite the user name: Pop-up window will be shown: On this form there are several blocks of the settings for a group/role. Default data storage Here you can select default data storage for a group/role: User management In this block you can change a member list of the selected group/role: For more information see 12.8. Change a set of roles/groups for a user . Attributes In this block you can set metadata tags (attributes) for a group. These tags represent key/value pairs, same as pipeline/folder tags. For more information see 17. CP objects tagging by additional attributes . Launch options In this block you can specify some restrictions for a group of users/role on allowed instance types and price types. Here you can specify: Field Description Example Allowed instance types mask This mask restrict for a specific group/role allowed instance types for launching tools, pipelines and configurations If you want members of a certain group/role will be able to launch runs with only \"m5...\" instances types, mask would be m5* : In that case, before launching tool, pipeline or configuration, dropdown list of available node types will be look like this: Allowed tool instance types mask This mask restrict for a specific group/role allowed instance types only for tools - launching from tools menu or main dashboard. This mask has higher priority for launching tool than Allowed instance types mask . It's meaning that in case when both masks are set - for the launching tool will be applied Allowed tool instance types mask . If you want members of a certain group/role will be able to launch tools with only some of \"large\" \"m5...\" instances types, mask would be m5*.large* : In that case, before launching tool, dropdown list of available node types will be look like this: Allowed price types In this field you may restrict, what price types will be allowed for a group/role. If you want members of a certain group/role will be able to launch \"On-demand\" runs only, select it in the dropdown list: In that case, before launching tool, dropdown list of price types will be look like this: To apply set restrictions for a group/role click button. Setting restrictions on allowed instance types/price types is a convenient way to minimize a number of invalid configurations runs. Such restrictions could be set not only for a group/role, but on another levels too. In CP platform next hierarchy is set for applying of inputted allowed instance types (sorted by priority): User level (specified for a user on \"User management\" tab) (see v.0.14 - 12.4. Edit/delete a user ) User group level (specified for a group (role) on \"User management\" tab. If a user is a member of several groups - list of allowed instances will be summarized across all the groups) (see above ) Tool level (specified for a tool on \"Instance management\" panel) (see v.0.14 - 10.5. Launch a Tool ) (global) cluster.allowed.instance.types.docker (specified on \"Cluster\" tab in \"Preferences\" section of system-level settings) (see v.0.14 - 12.10. Manage system-level settings ) (global) cluster.allowed.instance.types (specified on \"Cluster\" tab in \"Preferences\" section of system-level settings) (see v.0.14 - 12.10. Manage system-level settings ) After specifying allowed instance types, all GUI forms that allow to select the list of instance types (configurations/launch forms) - will display only valid instance type, according to hierarchy above. For price type specifying - if it is set for the user/group/tool - GUI will allow to select only that price type.","title":"12.6. Edit a group/role"},{"location":"manual/12_Manage_Settings/12.6._Edit_a_group_role/#126-edit-a-group-role","text":"Edit a group (role) Default data storage User management Attributes Launch options User shall have ROLE_ADMIN to edit groups/roles.","title":"12.6. Edit a group (role)"},{"location":"manual/12_Manage_Settings/12.6._Edit_a_group_role/#edit-a-group-role","text":"For edit a group/role: Open Groups/Roles tab on User management section of the system-level settings. Find a group (role). Click Edit button in the row opposite the user name: Pop-up window will be shown: On this form there are several blocks of the settings for a group/role.","title":"Edit a group (role)"},{"location":"manual/12_Manage_Settings/12.6._Edit_a_group_role/#default-data-storage","text":"Here you can select default data storage for a group/role:","title":"Default data storage"},{"location":"manual/12_Manage_Settings/12.6._Edit_a_group_role/#user-management","text":"In this block you can change a member list of the selected group/role: For more information see 12.8. Change a set of roles/groups for a user .","title":"User management"},{"location":"manual/12_Manage_Settings/12.6._Edit_a_group_role/#attributes","text":"In this block you can set metadata tags (attributes) for a group. These tags represent key/value pairs, same as pipeline/folder tags. For more information see 17. CP objects tagging by additional attributes .","title":"Attributes"},{"location":"manual/12_Manage_Settings/12.6._Edit_a_group_role/#launch-options","text":"In this block you can specify some restrictions for a group of users/role on allowed instance types and price types. Here you can specify: Field Description Example Allowed instance types mask This mask restrict for a specific group/role allowed instance types for launching tools, pipelines and configurations If you want members of a certain group/role will be able to launch runs with only \"m5...\" instances types, mask would be m5* : In that case, before launching tool, pipeline or configuration, dropdown list of available node types will be look like this: Allowed tool instance types mask This mask restrict for a specific group/role allowed instance types only for tools - launching from tools menu or main dashboard. This mask has higher priority for launching tool than Allowed instance types mask . It's meaning that in case when both masks are set - for the launching tool will be applied Allowed tool instance types mask . If you want members of a certain group/role will be able to launch tools with only some of \"large\" \"m5...\" instances types, mask would be m5*.large* : In that case, before launching tool, dropdown list of available node types will be look like this: Allowed price types In this field you may restrict, what price types will be allowed for a group/role. If you want members of a certain group/role will be able to launch \"On-demand\" runs only, select it in the dropdown list: In that case, before launching tool, dropdown list of price types will be look like this: To apply set restrictions for a group/role click button. Setting restrictions on allowed instance types/price types is a convenient way to minimize a number of invalid configurations runs. Such restrictions could be set not only for a group/role, but on another levels too. In CP platform next hierarchy is set for applying of inputted allowed instance types (sorted by priority): User level (specified for a user on \"User management\" tab) (see v.0.14 - 12.4. Edit/delete a user ) User group level (specified for a group (role) on \"User management\" tab. If a user is a member of several groups - list of allowed instances will be summarized across all the groups) (see above ) Tool level (specified for a tool on \"Instance management\" panel) (see v.0.14 - 10.5. Launch a Tool ) (global) cluster.allowed.instance.types.docker (specified on \"Cluster\" tab in \"Preferences\" section of system-level settings) (see v.0.14 - 12.10. Manage system-level settings ) (global) cluster.allowed.instance.types (specified on \"Cluster\" tab in \"Preferences\" section of system-level settings) (see v.0.14 - 12.10. Manage system-level settings ) After specifying allowed instance types, all GUI forms that allow to select the list of instance types (configurations/launch forms) - will display only valid instance type, according to hierarchy above. For price type specifying - if it is set for the user/group/tool - GUI will allow to select only that price type.","title":"Launch options"},{"location":"manual/12_Manage_Settings/12.7._Delete_a_group/","text":"12.7. Delete a group User shall have ROLE_ADMIN to delete a group. Navigate to User management tab. Move to Groups tab. Click Delete button next to the group's name. Note : system groups are created by the SSO authentication system automatically and can not be found/deleted here.","title":"12.7. Delete a group"},{"location":"manual/12_Manage_Settings/12.7._Delete_a_group/#127-delete-a-group","text":"User shall have ROLE_ADMIN to delete a group. Navigate to User management tab. Move to Groups tab. Click Delete button next to the group's name. Note : system groups are created by the SSO authentication system automatically and can not be found/deleted here.","title":"12.7. Delete a group"},{"location":"manual/12_Manage_Settings/12.8._Change_a_set_of_roles_groups_for_a_user/","text":"12.8. Change a set of roles/groups for a user User shall have ROLE_ADMIN to change groups(roles) for a user. There are two ways to set a role or group to a user: Change a set of roles and groups to a selected user from the Users tab . Change a member list for a selected role or group . Note : the scenarios below shows a process using Roles as an example. Setting groups for a user happens in the same manner. Change a set of roles and groups to a selected user Navigate to User management tab. Make sure that you are in the Users tab area. Find user on the list (you can use Search field - see the picture below, 1 ). Click the Edit button (see the picture below, 2 ). The editing form is open. To assign a role or group to the user, click on \"Add role or group\" field and select the desired item from the drop-down list. When the desired item is selected, the + Add control will be enabled. To delete roles or groups, use the Delete button. Click OK and all changes will be saved and displayed in the Users tab table. Change a member list for a selected role or group Navigate to the User management tab. Move to Roles tab. Click the Edit button next to Role's name. You'll see a list of users assigned to the role. Look for the desired user via Search field. When the user is selected, the +Add user control will be enabled. Click +Add user control to add a new user to the role member list. To delete a user from the role member list, click Delete button next to user's name. Click OK and all changes will be saved and displayed in the Users tab table.","title":"12.8. Change a set of roles/groups for a user"},{"location":"manual/12_Manage_Settings/12.8._Change_a_set_of_roles_groups_for_a_user/#128-change-a-set-of-rolesgroups-for-a-user","text":"User shall have ROLE_ADMIN to change groups(roles) for a user. There are two ways to set a role or group to a user: Change a set of roles and groups to a selected user from the Users tab . Change a member list for a selected role or group . Note : the scenarios below shows a process using Roles as an example. Setting groups for a user happens in the same manner.","title":"12.8. Change a set of roles/groups for a user"},{"location":"manual/12_Manage_Settings/12.8._Change_a_set_of_roles_groups_for_a_user/#change-a-set-of-roles-and-groups-to-a-selected-user","text":"Navigate to User management tab. Make sure that you are in the Users tab area. Find user on the list (you can use Search field - see the picture below, 1 ). Click the Edit button (see the picture below, 2 ). The editing form is open. To assign a role or group to the user, click on \"Add role or group\" field and select the desired item from the drop-down list. When the desired item is selected, the + Add control will be enabled. To delete roles or groups, use the Delete button. Click OK and all changes will be saved and displayed in the Users tab table.","title":"Change a set of roles and groups to a selected user"},{"location":"manual/12_Manage_Settings/12.8._Change_a_set_of_roles_groups_for_a_user/#change-a-member-list-for-a-selected-role-or-group","text":"Navigate to the User management tab. Move to Roles tab. Click the Edit button next to Role's name. You'll see a list of users assigned to the role. Look for the desired user via Search field. When the user is selected, the +Add user control will be enabled. Click +Add user control to add a new user to the role member list. To delete a user from the role member list, click Delete button next to user's name. Click OK and all changes will be saved and displayed in the Users tab table.","title":"Change a member list for a selected role or group"},{"location":"manual/12_Manage_Settings/12.9._Change_email_notification/","text":"12.9. Change email notification User shall have ROLE_ADMIN to manage Email notifications. Navigate to the Settings tab. Select the Email notifications section. Choose any of the email notification types (e.g. LONG_INIT ) on the left: Remove Keep admins informed option. This email notification type will no longer inform admins. Add a new user to the Informed users . While typing system will suggest you users. When you selected all users, click outside this field. Change the Threshold parameter to e.g. 1200. Press the Save button to save all changes to the LONG_INIT email notification template.","title":"12.9. Change email notification"},{"location":"manual/12_Manage_Settings/12.9._Change_email_notification/#129-change-email-notification","text":"User shall have ROLE_ADMIN to manage Email notifications. Navigate to the Settings tab. Select the Email notifications section. Choose any of the email notification types (e.g. LONG_INIT ) on the left: Remove Keep admins informed option. This email notification type will no longer inform admins. Add a new user to the Informed users . While typing system will suggest you users. When you selected all users, click outside this field. Change the Threshold parameter to e.g. 1200. Press the Save button to save all changes to the LONG_INIT email notification template.","title":"12.9. Change email notification"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/","text":"12. Manage Settings Settings consist of CLI, System events, User management tabs, email notifications and Preferences tabs. CLI tab System events Controls User management Users Controls Groups Controls Roles Controls Email notifications Controls Preferences CLI tab \" CLI \" tab generates a CLI installation and configuration commands to set CLI for Cloud Pipeline. See 14.1. Install and setup CLI . Control Description Operation system Choose an operation system from drop-down list and the instruction how to install Cloud Pipeline CLI will appear in the window below. Generate access key Generates access token to be used by CLI. Valid till A date access key expires. System events This tab is visible only for administrator. System events tab represents system events notifications. Here you can create, edit, delete system events notifications. System events notifications are organized into a table. It represents the body of the notification , its severity status (\" info \", \" warning \" or \" critical \") and date of creation , activity status . Note : Variants of activity status: Blocking event emerges in the middle of the window and requires confirmation from the user to disappear. Active notifications will be shown for all users of the Cloud Pipeline until admin sets them inactive. Administrator can edit and delete notifications via corresponding buttons. System events controls Controls are at the top right of the table. Control Description Expand/Collapse This button ( 1 ) shows/hides the body of the event. Refresh To refresh a list of notifications press this control ( 2 ). + ADD This control ( 3 ) allows to create new notification. Edit The control ( 4 ) opens the edit form of the event. Delete To delete an event click the control ( 5 ). User management This tab is visible only for administrator. The User management tab helps to manage user groups and system roles. To grant or refuse permissions to a specific group of users (e.g. project team members), you can just create a user group and grant or refuse permissions to the specific set of objects to the whole group. System roles is one of the principal tool for managing security access to the objects. Even if you have WRITE permission for a folder object, you might be not able to create a pipeline there, if you don't have PIPELINE_MANAGER role. Note : About permissions, you can read more here . User management consists of the 3 following tabs: Users Groups Roles. Users This table view displays a list of users and their additional information: Name - an authenticated domain account (SAML/OAuth/OpenID), e.g. e-mail. Groups - a set of groups assigned to a user. It could be whether CP's user's groups and groups, given to each user automatically by SSO authentication system . Note : automatically created groups based on SSO authentication system are light-grey colored. Roles - a set of system roles assigned to a user. Users tab controls Control Description Search field To search particular user from a list of users, start to enter the user's name (see the picture below, 1 ). + Create user This control (see the picture below, 2 ) opens a \"Create user\" form, which can be used to create a new user. Edit Allows changing a list of roles or groups assigned to a user (see the picture below, 3 ). Groups The \"Groups\" tab shows a set of user groups created in CP. Here you can grant or refuse users in a group membership. Note that this tab displays groups created in CP only, not given by SSO authentication system . Groups tab controls Control Description Search field To search particular group from a list of groups, start to enter the group name (see the picture above, 1 ). + Create group Create a new group (see the picture above, 2 ). Edit This control (see the picture above, 3 ) allows changing a list of users owning this group. Delete Delete a group (see the picture above, 4 ). Roles The \" Roles \" tab shows a set of predefined system roles that couldn't be extended or reduced. Here you can grant or refuse users in a role. There is a list of CP system roles: Role Description ROLE_ADMIN The user gets Read/Write/Execute/Owner permissions to all objects in the system. Note : The owner of the object can manage its Access Control List. OWNER property is assigned to a user has created an object by default. ROLE_USER basic user. PIPELINE_MANAGER allows to create/delete Pipelines (given to each user by default). FOLDER_MANAGER allows to create/delete Folders (given to each user by default). CONFIGURATION_MANAGER allows to create/delete Cluster Configurations (given to each user by default). STORAGE_MANAGER allows to create/delete Data Storages . TOOL_GROUP_MANAGER allows to create/delete Tool groups . ENTITIES_MANAGER allows to create/delete Entities . Set of user's roles combined with permission settings defines allowed actions for the user and therefore the layout of GUI buttons. A user sees GUI options in appliance with his rights. Note : roles 3-8 are checked if a user has WRITE permission for the parent object. Roles tab controls Control Description Search field To search particular group from a list of roles, start to enter the role name (see the picture above, 1 ). Edit Allows changing a list of users assigned the role (see the picture above, 2 ). Email notifications This tab is visible only for administrator. The email notifications helps to keep track of what's happening in the Cloud Pipeline. On the left you can see a list of the email notification templates. Email notifications tab controls Control Descriptions Keep admins informed If set, all emails with such type will be sent to all users with ROLE_ADMIN role. Informed users Select users that will get such email types. Threshold Amount of seconds that is required for the process to generate email. Resend delay Amount of seconds that is required for the process to generate a repeat email notification on that subject. Subject Email notification subject. Body Body of the email notification. Revert Return an email settings to the previous unsaved state. Save Saves current email notification settings. Also you can switch from the Edit to the Preview mode to see how the Subject and the Body of the email notification will actually look: Note : this is the current list of notification templates. It might be extended in the future. Notification type Description LONG_INIT tells that the job is initializing for a long time NEW_ISSUE notifies about new issue NEW_ISSUE_COMMENT tells that an issue was commented LONG_RUNNING tells that the job is running for a long time PIPELINE_RUN_STATUS email about current pipeline status Preferences This tab is visible only for administrator. The Preferences tab contains different global settings for the Cloud Pipeline. These settings determine default behavior of the Cloud Pipeline. On the left you can see a set of sections. Each section contains a list of global settings. See more information here .","title":"12.0 Overview"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#12-manage-settings","text":"Settings consist of CLI, System events, User management tabs, email notifications and Preferences tabs. CLI tab System events Controls User management Users Controls Groups Controls Roles Controls Email notifications Controls Preferences","title":"12. Manage Settings"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#cli-tab","text":"\" CLI \" tab generates a CLI installation and configuration commands to set CLI for Cloud Pipeline. See 14.1. Install and setup CLI . Control Description Operation system Choose an operation system from drop-down list and the instruction how to install Cloud Pipeline CLI will appear in the window below. Generate access key Generates access token to be used by CLI. Valid till A date access key expires.","title":"CLI tab"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#system-events","text":"This tab is visible only for administrator. System events tab represents system events notifications. Here you can create, edit, delete system events notifications. System events notifications are organized into a table. It represents the body of the notification , its severity status (\" info \", \" warning \" or \" critical \") and date of creation , activity status . Note : Variants of activity status: Blocking event emerges in the middle of the window and requires confirmation from the user to disappear. Active notifications will be shown for all users of the Cloud Pipeline until admin sets them inactive. Administrator can edit and delete notifications via corresponding buttons.","title":"System events"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#system-events-controls","text":"Controls are at the top right of the table. Control Description Expand/Collapse This button ( 1 ) shows/hides the body of the event. Refresh To refresh a list of notifications press this control ( 2 ). + ADD This control ( 3 ) allows to create new notification. Edit The control ( 4 ) opens the edit form of the event. Delete To delete an event click the control ( 5 ).","title":"System events controls"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#user-management","text":"This tab is visible only for administrator. The User management tab helps to manage user groups and system roles. To grant or refuse permissions to a specific group of users (e.g. project team members), you can just create a user group and grant or refuse permissions to the specific set of objects to the whole group. System roles is one of the principal tool for managing security access to the objects. Even if you have WRITE permission for a folder object, you might be not able to create a pipeline there, if you don't have PIPELINE_MANAGER role. Note : About permissions, you can read more here . User management consists of the 3 following tabs: Users Groups Roles.","title":"User management"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#users","text":"This table view displays a list of users and their additional information: Name - an authenticated domain account (SAML/OAuth/OpenID), e.g. e-mail. Groups - a set of groups assigned to a user. It could be whether CP's user's groups and groups, given to each user automatically by SSO authentication system . Note : automatically created groups based on SSO authentication system are light-grey colored. Roles - a set of system roles assigned to a user.","title":"Users"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#users-tab-controls","text":"Control Description Search field To search particular user from a list of users, start to enter the user's name (see the picture below, 1 ). + Create user This control (see the picture below, 2 ) opens a \"Create user\" form, which can be used to create a new user. Edit Allows changing a list of roles or groups assigned to a user (see the picture below, 3 ).","title":"Users tab controls"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#groups","text":"The \"Groups\" tab shows a set of user groups created in CP. Here you can grant or refuse users in a group membership. Note that this tab displays groups created in CP only, not given by SSO authentication system .","title":"Groups"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#groups-tab-controls","text":"Control Description Search field To search particular group from a list of groups, start to enter the group name (see the picture above, 1 ). + Create group Create a new group (see the picture above, 2 ). Edit This control (see the picture above, 3 ) allows changing a list of users owning this group. Delete Delete a group (see the picture above, 4 ).","title":"Groups tab controls"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#roles","text":"The \" Roles \" tab shows a set of predefined system roles that couldn't be extended or reduced. Here you can grant or refuse users in a role. There is a list of CP system roles: Role Description ROLE_ADMIN The user gets Read/Write/Execute/Owner permissions to all objects in the system. Note : The owner of the object can manage its Access Control List. OWNER property is assigned to a user has created an object by default. ROLE_USER basic user. PIPELINE_MANAGER allows to create/delete Pipelines (given to each user by default). FOLDER_MANAGER allows to create/delete Folders (given to each user by default). CONFIGURATION_MANAGER allows to create/delete Cluster Configurations (given to each user by default). STORAGE_MANAGER allows to create/delete Data Storages . TOOL_GROUP_MANAGER allows to create/delete Tool groups . ENTITIES_MANAGER allows to create/delete Entities . Set of user's roles combined with permission settings defines allowed actions for the user and therefore the layout of GUI buttons. A user sees GUI options in appliance with his rights. Note : roles 3-8 are checked if a user has WRITE permission for the parent object.","title":"Roles"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#roles-tab-controls","text":"Control Description Search field To search particular group from a list of roles, start to enter the role name (see the picture above, 1 ). Edit Allows changing a list of users assigned the role (see the picture above, 2 ).","title":"Roles tab controls"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#email-notifications","text":"This tab is visible only for administrator. The email notifications helps to keep track of what's happening in the Cloud Pipeline. On the left you can see a list of the email notification templates.","title":"Email notifications"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#email-notifications-tab-controls","text":"Control Descriptions Keep admins informed If set, all emails with such type will be sent to all users with ROLE_ADMIN role. Informed users Select users that will get such email types. Threshold Amount of seconds that is required for the process to generate email. Resend delay Amount of seconds that is required for the process to generate a repeat email notification on that subject. Subject Email notification subject. Body Body of the email notification. Revert Return an email settings to the previous unsaved state. Save Saves current email notification settings. Also you can switch from the Edit to the Preview mode to see how the Subject and the Body of the email notification will actually look: Note : this is the current list of notification templates. It might be extended in the future. Notification type Description LONG_INIT tells that the job is initializing for a long time NEW_ISSUE notifies about new issue NEW_ISSUE_COMMENT tells that an issue was commented LONG_RUNNING tells that the job is running for a long time PIPELINE_RUN_STATUS email about current pipeline status","title":"Email notifications tab controls"},{"location":"manual/12_Manage_Settings/12._Manage_Settings/#preferences","text":"This tab is visible only for administrator. The Preferences tab contains different global settings for the Cloud Pipeline. These settings determine default behavior of the Cloud Pipeline. On the left you can see a set of sections. Each section contains a list of global settings. See more information here .","title":"Preferences"},{"location":"manual/13_Permissions/13._Permissions/","text":"13. Permissions Overview Owner property How to change an owner Admin role Permission settings Overview Security Policies Scheme is organized by 2 principal tools: groups (user groups and system roles) and Access Control List defined for each CP's object. Note : About groups and system roles you can read more here . Object's Access Control List specifies who can work with the object and what he can do with it. It is defined as a pair of attributes: a User or User Group ID Permissions The permission settings are divided into the following options which can be combined for the object: Read Write Execute Below is a mapping of the objects' possible actions to permissions which demonstrates what actions will be allowed or denied to a user or user group. Note : according to Security Policies Scheme WRITE permission is not enough to add/delete any Cloud Pipeline object. A specific *_MANAGER role is required also (about roles see here ). Object User Action Permission Folder View folder Read List folder contents Create object (e.g folder, pipeline, etc.) Write Delete folder Rename folder Change parent Upload metadata Pipeline *Permissions for a pipeline version are inherited from the pipeline View pipeline Read List pipeline attributes Delete a pipeline Write Edit pipeline attributes Change parent Run a pipeline Execute DataStorage *Permissions for files in data storage are inherited from the data storage View datastorage Read List datastorage contents Delete a datastorage Write Edit datastorage attributes/contents Change parent Pipeline run View runs Inherited from a run pipeline View run logs Launch a pipeline Stop a run Rerun Tool run View runs Admin and Owner only View run logs Launch a pipeline Stop a run Rerun Cluster node View a cluster Inherited from a currently assigned run View node details Terminate a node Docker Registry View registry Read Add registry Write Delete registry Edit registry attributes Run a child tool Execute Tool group View tool group Read Create tool group Write Edit tool group Delete tool group Run a child tool Execute Tool View enabled tool Read View disabled tools list Edit tool attributes Write Run tool without a pipeline Execute Instance management Admin and Owner only Run configuration View run configuration Read Delete a run configuration Write Edit run configuration attributes Change parent Run a run configuration Execute Owner property Each object has an additional \"Owner\" property. The owner of the object can manage its Access Control List. Owner property is assigned to a user that created an object. How to change an owner The Owner of an object can be changed easily for: Folders; Pipelines and pipelines versions; Data storages; Run configurations; Docker registries, Tool Groups and Tools. Note : you shall have Owner or Admin role. To change an owner of an object: Select an object. Click \"Gear\" icon in the top-right corner of the screen. Navigate to Permissions tab. Note : To edit permissions: for a Folder - click the \"Gear\" icon \u2192 Edit folder for a Docker registry - click the \"Gear\" icon \u2192 Registry \u2192 Edit . for a Tool group - click the \"Gear\" icon \u2192 Group \u2192 Edit for a Tool - click the \"Gear\" icon \u2192 Permissions . Click owner's name. Now you can edit it: Start to enter a desired username and system will suggest you the existing users. Click \"Apply\" control and the changes will be saved. Admin role Admin property can be given by assigning ROLE_ADMIN to a user (about roles see here ). The user gets Read/Write/Execute/Owner permissions to all objects in the system. Initially, a user with ROLE_ADMIN shall be an authenticated domain account (SAML/OAuth/OpenID) defined during a system deployment (in some properties file or database). Permission settings The permissions could be granted to a user in one of the following ways: the system has a \"default\" system role or user group. This type of system roles or groups assigned by default once a user is created; assigned user groups or system role where every member has the same permissions for specific objects; granted permissions for specific user. The priority of permissions granted for specific object explicitly is higher than the group or role permissions, e.g. if a basic user is included into the group that doesn't have an access to some folder but he has permissions explicitly defined for himself that allow him to work with that folder, he will have an access to it. To assign object's permissions to a user or user group you shall move to the object's page and click the \"Gear\" icon in the top-right corner of the screen and select \"Permissions\" tab. Note : To edit permissions: for a Folder - click the \"Gear\" icon \u2192 Edit folder for a Docker registry - click the \"Gear\" icon \u2192 Registry \u2192 Edit . for a Tool group - click the \"Gear\" icon \u2192 Group \u2192 Edit for a Tool - click the \"Gear\" icon \u2192 Permissions . You can explicitly define permissions for the object for a particular user or group of users (users within the same Group) in the \"Permission\" form by clicking on its name in the \"Groups and users\" list. Note : if you couldn't find the desired user or user group, you can add it via \"Add a user\" and \"Add a user group\" controls (see the picture below, 1 ). The additional section for a particular user or user group suggests you tick the desired grants. Here you can allow or deny specific permission options (see the picture above, 2 ). Note : if you don't tick any possible variant, it will be inherited from the parent object (e.g. a pipeline in a folder inherits permissions from it) (see the picture above, 3 ). Example 1: according to the picture above, a user will get WRITE and EXECUTE permissions for an object, and the READ permission will be inherited from the parent object. Example 2: on the picture below we see Permission form of a run configuration. We grant a user READ and EXECUTE permissions, but deny WRITE permission. So the user is able to see the run configuration and run it, but he can not edit its parameters:","title":"13. Permissions"},{"location":"manual/13_Permissions/13._Permissions/#13-permissions","text":"Overview Owner property How to change an owner Admin role Permission settings","title":"13. Permissions"},{"location":"manual/13_Permissions/13._Permissions/#overview","text":"Security Policies Scheme is organized by 2 principal tools: groups (user groups and system roles) and Access Control List defined for each CP's object. Note : About groups and system roles you can read more here . Object's Access Control List specifies who can work with the object and what he can do with it. It is defined as a pair of attributes: a User or User Group ID Permissions The permission settings are divided into the following options which can be combined for the object: Read Write Execute Below is a mapping of the objects' possible actions to permissions which demonstrates what actions will be allowed or denied to a user or user group. Note : according to Security Policies Scheme WRITE permission is not enough to add/delete any Cloud Pipeline object. A specific *_MANAGER role is required also (about roles see here ). Object User Action Permission Folder View folder Read List folder contents Create object (e.g folder, pipeline, etc.) Write Delete folder Rename folder Change parent Upload metadata Pipeline *Permissions for a pipeline version are inherited from the pipeline View pipeline Read List pipeline attributes Delete a pipeline Write Edit pipeline attributes Change parent Run a pipeline Execute DataStorage *Permissions for files in data storage are inherited from the data storage View datastorage Read List datastorage contents Delete a datastorage Write Edit datastorage attributes/contents Change parent Pipeline run View runs Inherited from a run pipeline View run logs Launch a pipeline Stop a run Rerun Tool run View runs Admin and Owner only View run logs Launch a pipeline Stop a run Rerun Cluster node View a cluster Inherited from a currently assigned run View node details Terminate a node Docker Registry View registry Read Add registry Write Delete registry Edit registry attributes Run a child tool Execute Tool group View tool group Read Create tool group Write Edit tool group Delete tool group Run a child tool Execute Tool View enabled tool Read View disabled tools list Edit tool attributes Write Run tool without a pipeline Execute Instance management Admin and Owner only Run configuration View run configuration Read Delete a run configuration Write Edit run configuration attributes Change parent Run a run configuration Execute","title":"Overview"},{"location":"manual/13_Permissions/13._Permissions/#owner-property","text":"Each object has an additional \"Owner\" property. The owner of the object can manage its Access Control List. Owner property is assigned to a user that created an object.","title":"Owner property"},{"location":"manual/13_Permissions/13._Permissions/#how-to-change-an-owner","text":"The Owner of an object can be changed easily for: Folders; Pipelines and pipelines versions; Data storages; Run configurations; Docker registries, Tool Groups and Tools. Note : you shall have Owner or Admin role. To change an owner of an object: Select an object. Click \"Gear\" icon in the top-right corner of the screen. Navigate to Permissions tab. Note : To edit permissions: for a Folder - click the \"Gear\" icon \u2192 Edit folder for a Docker registry - click the \"Gear\" icon \u2192 Registry \u2192 Edit . for a Tool group - click the \"Gear\" icon \u2192 Group \u2192 Edit for a Tool - click the \"Gear\" icon \u2192 Permissions . Click owner's name. Now you can edit it: Start to enter a desired username and system will suggest you the existing users. Click \"Apply\" control and the changes will be saved.","title":"How to change an owner"},{"location":"manual/13_Permissions/13._Permissions/#admin-role","text":"Admin property can be given by assigning ROLE_ADMIN to a user (about roles see here ). The user gets Read/Write/Execute/Owner permissions to all objects in the system. Initially, a user with ROLE_ADMIN shall be an authenticated domain account (SAML/OAuth/OpenID) defined during a system deployment (in some properties file or database).","title":"Admin role"},{"location":"manual/13_Permissions/13._Permissions/#permission-settings","text":"The permissions could be granted to a user in one of the following ways: the system has a \"default\" system role or user group. This type of system roles or groups assigned by default once a user is created; assigned user groups or system role where every member has the same permissions for specific objects; granted permissions for specific user. The priority of permissions granted for specific object explicitly is higher than the group or role permissions, e.g. if a basic user is included into the group that doesn't have an access to some folder but he has permissions explicitly defined for himself that allow him to work with that folder, he will have an access to it. To assign object's permissions to a user or user group you shall move to the object's page and click the \"Gear\" icon in the top-right corner of the screen and select \"Permissions\" tab. Note : To edit permissions: for a Folder - click the \"Gear\" icon \u2192 Edit folder for a Docker registry - click the \"Gear\" icon \u2192 Registry \u2192 Edit . for a Tool group - click the \"Gear\" icon \u2192 Group \u2192 Edit for a Tool - click the \"Gear\" icon \u2192 Permissions . You can explicitly define permissions for the object for a particular user or group of users (users within the same Group) in the \"Permission\" form by clicking on its name in the \"Groups and users\" list. Note : if you couldn't find the desired user or user group, you can add it via \"Add a user\" and \"Add a user group\" controls (see the picture below, 1 ). The additional section for a particular user or user group suggests you tick the desired grants. Here you can allow or deny specific permission options (see the picture above, 2 ). Note : if you don't tick any possible variant, it will be inherited from the parent object (e.g. a pipeline in a folder inherits permissions from it) (see the picture above, 3 ). Example 1: according to the picture above, a user will get WRITE and EXECUTE permissions for an object, and the READ permission will be inherited from the parent object. Example 2: on the picture below we see Permission form of a run configuration. We grant a user READ and EXECUTE permissions, but deny WRITE permission. So the user is able to see the run configuration and run it, but he can not edit its parameters:","title":"Permission settings"},{"location":"manual/14_CLI/14.1._Install_and_setup_CLI/","text":"14.1. Install and setup CLI How to install and setup CLI : Go to Settings \u2192 CLI tab. Select your Operation System from the list. Follow the installation instructions for your OS (e.g. Linux). Commands below shall be executed in the Terminal. When installation is finished, type pipe in the Terminal to test pipe installation. This command shall produce short description of pipe CLI and pipe CLI commands. Press the Generate access key button. Copy CLI configure command and run it in your Terminal to configure pipe CLI. Now Cloud Pipeline CLI is ready to use. Note : If any exceptions occur during installation, follow the instructions in the Terminal. Notice that Python 2 / Python 3 has to be installed to run CLI. Python can be downloaded here https://www.python.org/downloads/ . Note : pip package manager is required for CLI installation if you selected Operation System \u2192 Other on step 2. Modern Python versions come bundled with pip . On top of that, with this type of installation you'll also need internet connection to install dependencies.","title":"14.1 Install and setup"},{"location":"manual/14_CLI/14.1._Install_and_setup_CLI/#141-install-and-setup-cli","text":"How to install and setup CLI : Go to Settings \u2192 CLI tab. Select your Operation System from the list. Follow the installation instructions for your OS (e.g. Linux). Commands below shall be executed in the Terminal. When installation is finished, type pipe in the Terminal to test pipe installation. This command shall produce short description of pipe CLI and pipe CLI commands. Press the Generate access key button. Copy CLI configure command and run it in your Terminal to configure pipe CLI. Now Cloud Pipeline CLI is ready to use. Note : If any exceptions occur during installation, follow the instructions in the Terminal. Notice that Python 2 / Python 3 has to be installed to run CLI. Python can be downloaded here https://www.python.org/downloads/ . Note : pip package manager is required for CLI installation if you selected Operation System \u2192 Other on step 2. Modern Python versions come bundled with pip . On top of that, with this type of installation you'll also need internet connection to install dependencies.","title":"14.1. Install and setup CLI"},{"location":"manual/14_CLI/14.2._View_and_manage_Attributes_via_CLI/","text":"14.2. View and manage Attributes via CLI View attributes Manage attributes Delete attributes Add and Edit attributes Cloud Pipeline CLI has to be installed. See 14.1. Install and setup CLI . View attributes To view attributes of the object you need READ permission for the object. See 13. Permissions . Command to list all tags for a specific object: pipe tag get <Object class> <Object id/name> Two parameters shall be specified: Object class - defines: Pipeline, Folder, Data Storage, Docker registry, Tool; Object id or name - define a name of an object of a specified class. Note : full path to the object has to be specified. Paths to Docker registry and Tool objects should include registry IP address. The example below lists attributes of the \" new-folder \" directory: pipe tag get folder new-folder Manage attributes A user has to be an administrator ( ROLE_ADMIN ) or an owner ( OWNER ) of the object to edit metadata. See 13. Permissions . A user can add new attributes, edit or delete existing attributes via CLI. Delete attributes To delete attributes the following command is used: pipe tag delete <Object class> <Object id/name> <List of KEYs to delete> Three parameters shall be specified: Object class - define: Pipeline, Folder, Data Storage, Docker registry, Tool Object id/name - define the name of an object of a specified class. Note : full path to the object has to be specified. Paths to Docker registry and Tool objects should include registry IP address. Keys of attributes to delete. The example below deletes attribute \" Attr1 \" from the \" new-folder \" directory: pipe tag delete folder new-folder \"Attr1\" Add and Edit attributes To add new and edit existing attributes the following command is used: pipe tag set <Object class> <Object id/name> <List of KEY=VALUE> Three parameters shall be specified: Object class - defines Pipeline, Folder, Data Storage, Docker registry, Tool. Object id/name - define the name of an object of a specified class. Note : full path to the object has to be specified. Paths to Docker registry and Tool objects should include registry IP address. Tags can be specified as a single KEY=VALUE pair or a list of them. Note : if a specific tag key already exists for an object, it will be overwritten . The example below sets attributes Attr1 = value1 and Attr2 = value2 for the \" new-folder \" directory: pipe tag set folder new-folder \"Attr1\"=\"Value1\" \"Attr2\"=\"Value2\"","title":"14.2 View and manage Attributes"},{"location":"manual/14_CLI/14.2._View_and_manage_Attributes_via_CLI/#142-view-and-manage-attributes-via-cli","text":"View attributes Manage attributes Delete attributes Add and Edit attributes Cloud Pipeline CLI has to be installed. See 14.1. Install and setup CLI .","title":"14.2. View and manage Attributes via CLI"},{"location":"manual/14_CLI/14.2._View_and_manage_Attributes_via_CLI/#view-attributes","text":"To view attributes of the object you need READ permission for the object. See 13. Permissions . Command to list all tags for a specific object: pipe tag get <Object class> <Object id/name> Two parameters shall be specified: Object class - defines: Pipeline, Folder, Data Storage, Docker registry, Tool; Object id or name - define a name of an object of a specified class. Note : full path to the object has to be specified. Paths to Docker registry and Tool objects should include registry IP address. The example below lists attributes of the \" new-folder \" directory: pipe tag get folder new-folder","title":"View attributes"},{"location":"manual/14_CLI/14.2._View_and_manage_Attributes_via_CLI/#manage-attributes","text":"A user has to be an administrator ( ROLE_ADMIN ) or an owner ( OWNER ) of the object to edit metadata. See 13. Permissions . A user can add new attributes, edit or delete existing attributes via CLI.","title":"Manage attributes"},{"location":"manual/14_CLI/14.2._View_and_manage_Attributes_via_CLI/#delete-attributes","text":"To delete attributes the following command is used: pipe tag delete <Object class> <Object id/name> <List of KEYs to delete> Three parameters shall be specified: Object class - define: Pipeline, Folder, Data Storage, Docker registry, Tool Object id/name - define the name of an object of a specified class. Note : full path to the object has to be specified. Paths to Docker registry and Tool objects should include registry IP address. Keys of attributes to delete. The example below deletes attribute \" Attr1 \" from the \" new-folder \" directory: pipe tag delete folder new-folder \"Attr1\"","title":"Delete attributes"},{"location":"manual/14_CLI/14.2._View_and_manage_Attributes_via_CLI/#add-and-edit-attributes","text":"To add new and edit existing attributes the following command is used: pipe tag set <Object class> <Object id/name> <List of KEY=VALUE> Three parameters shall be specified: Object class - defines Pipeline, Folder, Data Storage, Docker registry, Tool. Object id/name - define the name of an object of a specified class. Note : full path to the object has to be specified. Paths to Docker registry and Tool objects should include registry IP address. Tags can be specified as a single KEY=VALUE pair or a list of them. Note : if a specific tag key already exists for an object, it will be overwritten . The example below sets attributes Attr1 = value1 and Attr2 = value2 for the \" new-folder \" directory: pipe tag set folder new-folder \"Attr1\"=\"Value1\" \"Attr2\"=\"Value2\"","title":"Add and Edit attributes"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/","text":"14.3. Manage Storage via CLI Create a data storage Delete a data storage Create a folder in a storage Upload and download data Control File versions Show files versions Restore files Delete an object from a data storage Change backup duration, select STS/LTS duration, enable versioning Cloud Pipeline CLI has to be installed. See 14.1. Install and setup CLI . Create a data storage pipe storage create [OPTIONS] In the example below data storage \" my-personal-storage1 \" for bucket \" my-personal-bucket1 \" is created in the \" ROOT \" folder. -c flag is to create a bucket in the cloud. Note : \"Description\" , \"STS\" , \"LTS\" , \"Backup time\" and \"Type of the cloud for data storage\" fields are left empty. That means that their values will be default. pipe storage create -p my-personal-bucket1 -n my-personal-storage1 -c -f ROOT Delete a data storage pipe storage delete [OPTIONS] In the example below we delete the \" my-personal-bucket1 \" storage. pipe storage delete -n my-personal-bucket1 Create a folder in a storage Create a folder in a data storage with the following: pipe storage mkdir [OPTIONS] FOLDERS... In the example below we will create the folder \" new-folder \" in the data storage \" my-personal-bucket2 \". pipe storage mkdir cp://my-personal-bucket2/new-folder Upload and download data Command to upload/download data: pipe storage cp [OPTIONS] SOURCE DESTINATION Examples below demonstrate how to upload/download data and check the results: # 1. Recursively copy a folder from a local system to S3 bucket (Upload the data). pipe storage cp ~/data cp://input-data/ --recursive # Application will start uploading files and print progress # Output will be similiar to this: # [##############################] 100% 1.fastq 3/3 b # [##############################] 100% 2.fastq 4/4 b # 2. Check that data is uploaded - list cloud storage pipe storage ls cp://input-data --recursive # Listing output will be similiar to this: # Type Labels Modified Size Name # File STANDARD 2017-10-24 17:39:37 3 1.fastq # File STANDARD 2017-10-24 17:39:39 4 2.fastq # 3. Recursively copy a folder from s3 bucket to the local system (Download the data). pipe storage cp cp://input-data/ ~/data --recursive Note : --recursive flag is not needed when you copy a single file. Note : Earlier CLI could not accept remote paths other than cp:// . For backward compatibility s3:// path is also available now and is treated in the same manner as cp:// . Note : Files uploaded via CLI will have the following attributes and values automatically set: CP_OWNER . The value of the attribute will be set as a user ID. CP_SOURCE . The value of the attribute will be set as a local path used to upload. The example below demonstrates automatic file tagging after data uploading to the bucket s3://my-personal-bucket2/ : pipe storage cp tagged-file s3://my-personal-bucket2/ pipe storage get-object-tags s3://my-personal-bucket2/tagged-file Control File versions Show files versions Command to view storage contents: pipe storage ls [OPTIONS] [PATH] In the examples below we check the contents of the data storage. Flag -l is used to show file details: pipe storage ls cp://my-versioning-test -l pipe storage ls cp://my-versioning-test -l -v -v flag is used to view storage contents. It is available for users with ROLE_ADMIN or OWNER properties. Note : file \" 1.txt \" has 2 versions: To view versions of specific file specify its name after the directory name: pipe storage ls cp://my-versioning-test/1.txt -l -v Restore files Command to restore a previous version of a file: pipe storage restore <path to a file> -v <Version> The commands below show how to check file versions, set the previous version of a file as the latest and verify that everything went fine: pipe storage ls cp://my-versioning-test/1.txt -l -v pipe storage restore cp://my-versioning-test/1.txt -v <Version> pipe storage ls cp://my-versioning-test/1.txt -l -v Note : When a specified version of the \" 1.txt \" is restored, a copy of that version is created to become the latest version of the file. You can restore a deleted file without specifying a version. It works only for files with a \" Delete marker \" as the latest version (\" 1.txt \" in the example below): pipe storage restore cp://my-versioning-test/1.txt Note : Before we restored the file \" 1.txt \" its latest version was a \" Delete marker \". After restoration, it disappeared. Delete an object from a data storage Command to set a \" Delete marker \": pipe storage rm <path to a file> Note : the object will remain in a data storage and be available for pipe storage restore command. In the example below we set a \" Delete marker \" to the file \" 2.txt \": pipe storage rm cp://my-versioning-test/2.txt Note : the latest version of the file \" 2.txt \" is marked with \" Delete marker \" now. To completely delete an object from a data storage use -d (--hard-delete) option. Change backup duration, select STS/LTS duration, enable versioning Command to change backup duration, select STS/LTS duration or enable versioning: pipe storage policy [OPTIONS] In the example below backup duration ( -b ) is set to 25 days, STS ( -sts ) and LTS ( -lts ) durations are set to 50 days and 100 days respectively for the data storage \" my-versioning-test \". Also, we enable versioning ( -v ) for that data storage: pipe storage policy -n my-versioning-test -b 25 -sts 50 -lts 100 -v","title":"14.3 Manage Data Storage"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#143-manage-storage-via-cli","text":"Create a data storage Delete a data storage Create a folder in a storage Upload and download data Control File versions Show files versions Restore files Delete an object from a data storage Change backup duration, select STS/LTS duration, enable versioning Cloud Pipeline CLI has to be installed. See 14.1. Install and setup CLI .","title":"14.3. Manage Storage via CLI"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#create-a-data-storage","text":"pipe storage create [OPTIONS] In the example below data storage \" my-personal-storage1 \" for bucket \" my-personal-bucket1 \" is created in the \" ROOT \" folder. -c flag is to create a bucket in the cloud. Note : \"Description\" , \"STS\" , \"LTS\" , \"Backup time\" and \"Type of the cloud for data storage\" fields are left empty. That means that their values will be default. pipe storage create -p my-personal-bucket1 -n my-personal-storage1 -c -f ROOT","title":"Create a data storage"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#delete-a-data-storage","text":"pipe storage delete [OPTIONS] In the example below we delete the \" my-personal-bucket1 \" storage. pipe storage delete -n my-personal-bucket1","title":"Delete a data storage"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#create-a-folder-in-a-storage","text":"Create a folder in a data storage with the following: pipe storage mkdir [OPTIONS] FOLDERS... In the example below we will create the folder \" new-folder \" in the data storage \" my-personal-bucket2 \". pipe storage mkdir cp://my-personal-bucket2/new-folder","title":"Create a folder in a\u00a0storage"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#upload-and-download-data","text":"Command to upload/download data: pipe storage cp [OPTIONS] SOURCE DESTINATION Examples below demonstrate how to upload/download data and check the results: # 1. Recursively copy a folder from a local system to S3 bucket (Upload the data). pipe storage cp ~/data cp://input-data/ --recursive # Application will start uploading files and print progress # Output will be similiar to this: # [##############################] 100% 1.fastq 3/3 b # [##############################] 100% 2.fastq 4/4 b # 2. Check that data is uploaded - list cloud storage pipe storage ls cp://input-data --recursive # Listing output will be similiar to this: # Type Labels Modified Size Name # File STANDARD 2017-10-24 17:39:37 3 1.fastq # File STANDARD 2017-10-24 17:39:39 4 2.fastq # 3. Recursively copy a folder from s3 bucket to the local system (Download the data). pipe storage cp cp://input-data/ ~/data --recursive Note : --recursive flag is not needed when you copy a single file. Note : Earlier CLI could not accept remote paths other than cp:// . For backward compatibility s3:// path is also available now and is treated in the same manner as cp:// . Note : Files uploaded via CLI will have the following attributes and values automatically set: CP_OWNER . The value of the attribute will be set as a user ID. CP_SOURCE . The value of the attribute will be set as a local path used to upload. The example below demonstrates automatic file tagging after data uploading to the bucket s3://my-personal-bucket2/ : pipe storage cp tagged-file s3://my-personal-bucket2/ pipe storage get-object-tags s3://my-personal-bucket2/tagged-file","title":"Upload and download data"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#control-file-versions","text":"","title":"Control File versions"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#show-files-versions","text":"Command to view storage contents: pipe storage ls [OPTIONS] [PATH] In the examples below we check the contents of the data storage. Flag -l is used to show file details: pipe storage ls cp://my-versioning-test -l pipe storage ls cp://my-versioning-test -l -v -v flag is used to view storage contents. It is available for users with ROLE_ADMIN or OWNER properties. Note : file \" 1.txt \" has 2 versions: To view versions of specific file specify its name after the directory name: pipe storage ls cp://my-versioning-test/1.txt -l -v","title":"Show files versions"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#restore-files","text":"Command to restore a previous version of a file: pipe storage restore <path to a file> -v <Version> The commands below show how to check file versions, set the previous version of a file as the latest and verify that everything went fine: pipe storage ls cp://my-versioning-test/1.txt -l -v pipe storage restore cp://my-versioning-test/1.txt -v <Version> pipe storage ls cp://my-versioning-test/1.txt -l -v Note : When a specified version of the \" 1.txt \" is restored, a copy of that version is created to become the latest version of the file. You can restore a deleted file without specifying a version. It works only for files with a \" Delete marker \" as the latest version (\" 1.txt \" in the example below): pipe storage restore cp://my-versioning-test/1.txt Note : Before we restored the file \" 1.txt \" its latest version was a \" Delete marker \". After restoration, it disappeared.","title":"Restore files"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#delete-an-object-from-a-data-storage","text":"Command to set a \" Delete marker \": pipe storage rm <path to a file> Note : the object will remain in a data storage and be available for pipe storage restore command. In the example below we set a \" Delete marker \" to the file \" 2.txt \": pipe storage rm cp://my-versioning-test/2.txt Note : the latest version of the file \" 2.txt \" is marked with \" Delete marker \" now. To completely delete an object from a data storage use -d (--hard-delete) option.","title":"Delete an object from a data storage"},{"location":"manual/14_CLI/14.3._Manage_Storage_via_CLI/#change-backup-duration-select-stslts-duration-enable-versioning","text":"Command to change backup duration, select STS/LTS duration or enable versioning: pipe storage policy [OPTIONS] In the example below backup duration ( -b ) is set to 25 days, STS ( -sts ) and LTS ( -lts ) durations are set to 50 days and 100 days respectively for the data storage \" my-versioning-test \". Also, we enable versioning ( -v ) for that data storage: pipe storage policy -n my-versioning-test -b 25 -sts 50 -lts 100 -v","title":"Change backup duration, select STS/LTS duration, enable versioning"},{"location":"manual/14_CLI/14.4._View_and_manage_Permissions_via_CLI/","text":"14.4. View and manage Permissions via CLI View permissions Manage permissions Example: set permissions for Folder Cloud Pipeline CLI has to be installed. See 14. Command-line interface (CLI) . View permissions To view permissions for the object you need READ permission for the object. See 13. Permissions . Command to list all permissions for a specific object: pipe view-acl -t <Object type> <Object id/name> Two parameters are required: Object type - defines: Pipeline, Folder, Data Storage Object id or name - define a name of an object of a specified class. Note : full path to the object has to be specified. In the example below we check permissions for a folder \" ROOT/manage-permissions-folder \": pipe view-acl -t folder \"ROOT/manage-permissions-folder\" Manage permissions To manage permissions for the object you need to be an OWNER of that object or you need to have the ADMIN role. See 13. Permissions . Command to set permissions for the object: pipe set-acl -t <Object type> -s <User/Group name> -a/-d/-i <w>/<x>/<r> <Object id/name> The following parameters are required: Object type - defines Pipeline, Folder, Data Storage, Docker registry, Tool. User or Group name - the name of the user or the group of users to manage their permissions. Allow( -a ) / Deny( -d ) / Inherit( -i ) WRITE( w ) , READ( r ) and EXECUTE( x ) permissions in any combinations. See examples below. Object id or name - define a name of an object of a specified class to set permissions for. Note : full path to the object has to be specified if the name is not unique (the case for Data Storage , Pipeline ). Example: set permissions for Folder Here we demonstrate how to set permissions for a folder . You can set permissions for other CP objects in the same way. In the example below we grant user TEST_USER READ access and deny WRITE and EXECUTE access to the directory \" new-folder \". pipe set-acl -t folder -s TEST_USER -d wx -a r new-folder","title":"14.4 View and manage Permissions"},{"location":"manual/14_CLI/14.4._View_and_manage_Permissions_via_CLI/#144-view-and-manage-permissions-via-cli","text":"View permissions Manage permissions Example: set permissions for Folder Cloud Pipeline CLI has to be installed. See 14. Command-line interface (CLI) .","title":"14.4. View and manage Permissions via CLI"},{"location":"manual/14_CLI/14.4._View_and_manage_Permissions_via_CLI/#view-permissions","text":"To view permissions for the object you need READ permission for the object. See 13. Permissions . Command to list all permissions for a specific object: pipe view-acl -t <Object type> <Object id/name> Two parameters are required: Object type - defines: Pipeline, Folder, Data Storage Object id or name - define a name of an object of a specified class. Note : full path to the object has to be specified. In the example below we check permissions for a folder \" ROOT/manage-permissions-folder \": pipe view-acl -t folder \"ROOT/manage-permissions-folder\"","title":"View permissions"},{"location":"manual/14_CLI/14.4._View_and_manage_Permissions_via_CLI/#manage-permissions","text":"To manage permissions for the object you need to be an OWNER of that object or you need to have the ADMIN role. See 13. Permissions . Command to set permissions for the object: pipe set-acl -t <Object type> -s <User/Group name> -a/-d/-i <w>/<x>/<r> <Object id/name> The following parameters are required: Object type - defines Pipeline, Folder, Data Storage, Docker registry, Tool. User or Group name - the name of the user or the group of users to manage their permissions. Allow( -a ) / Deny( -d ) / Inherit( -i ) WRITE( w ) , READ( r ) and EXECUTE( x ) permissions in any combinations. See examples below. Object id or name - define a name of an object of a specified class to set permissions for. Note : full path to the object has to be specified if the name is not unique (the case for Data Storage , Pipeline ).","title":"Manage permissions"},{"location":"manual/14_CLI/14.4._View_and_manage_Permissions_via_CLI/#example-set-permissions-for-folder","text":"Here we demonstrate how to set permissions for a folder . You can set permissions for other CP objects in the same way. In the example below we grant user TEST_USER READ access and deny WRITE and EXECUTE access to the directory \" new-folder \". pipe set-acl -t folder -s TEST_USER -d wx -a r new-folder","title":"Example: set permissions for Folder"},{"location":"manual/14_CLI/14._Command-line_interface/","text":"14. Command-line interface (CLI) Introduction Working with CLI CLI options and commands Examples Cloud Pipeline CLI has to be installed. See 14.1. Install and setup CLI . Introduction Working with a Cloud Pipeline from CLI has numerous benefits compared to GUI: It has extra features which are not accessible from GUI such copying or moving files from one storage to another. The uploading file size could exceed 10 Mb. It is more convenient for System administrators. Working with CLI All CLI commands have to be typed into the command line (Terminal) of the computer. pipe [OPTIONS] COMMAND [ARGS]... The pipe is a command-line interface for Cloud Pipeline. It has a number of available commands. Each command has a number of arguments as an input. To get a list of available options and commands type pipe or pipe --help into the command line. CLI options and commands Options --version Show CLI version and exit --help Show help message and exit Commands chown Changes current owner to specified. configure Configures CLI parameters. This command doesn't require any arguments to start. You will be offered to enter token for API authentification, data presentation timezone and URL of pipeline API endpoint. This command can be automatically generated. See here . run Schedules a pipeline execution. set-acl Set object permissions. See 14.4. View and manage Permissions via CLI . stop Stops a running pipeline. storage Storage operations. See 14.3. Manage Storage via CLI . tag Operations with tags. See 14.2. View and manage Attributes via CLI . terminate-node Terminates calculation node. view-acl View object permissions. See 14.4. View and manage Permissions via CLI . view-cluster Lists cluster nodes. view-pipes Lists pipelines definitions. view-runs Lists pipeline runs. Note : To see command's arguments and options type pipe command --help . Examples To see a list of available CLI commands type pipe or pipe --help in the terminal. Note : each command might have its own set of commands that consequently might have their own set of commands... To learn more about a specific command, type the following in the terminal: pipe COMMAND --help . For instance, we can list a number of pipe storage commands with pipe storage --help . Another example - a user can see a list of pipelines runs by pipe view-runs command.","title":"14.0 Overview"},{"location":"manual/14_CLI/14._Command-line_interface/#14-command-line-interface-cli","text":"Introduction Working with CLI CLI options and commands Examples Cloud Pipeline CLI has to be installed. See 14.1. Install and setup CLI .","title":"14. Command-line interface (CLI)"},{"location":"manual/14_CLI/14._Command-line_interface/#introduction","text":"Working with a Cloud Pipeline from CLI has numerous benefits compared to GUI: It has extra features which are not accessible from GUI such copying or moving files from one storage to another. The uploading file size could exceed 10 Mb. It is more convenient for System administrators.","title":"Introduction"},{"location":"manual/14_CLI/14._Command-line_interface/#working-with-cli","text":"All CLI commands have to be typed into the command line (Terminal) of the computer. pipe [OPTIONS] COMMAND [ARGS]... The pipe is a command-line interface for Cloud Pipeline. It has a number of available commands. Each command has a number of arguments as an input. To get a list of available options and commands type pipe or pipe --help into the command line.","title":"Working with CLI"},{"location":"manual/14_CLI/14._Command-line_interface/#cli-options-and-commands","text":"Options --version Show CLI version and exit --help Show help message and exit Commands chown Changes current owner to specified. configure Configures CLI parameters. This command doesn't require any arguments to start. You will be offered to enter token for API authentification, data presentation timezone and URL of pipeline API endpoint. This command can be automatically generated. See here . run Schedules a pipeline execution. set-acl Set object permissions. See 14.4. View and manage Permissions via CLI . stop Stops a running pipeline. storage Storage operations. See 14.3. Manage Storage via CLI . tag Operations with tags. See 14.2. View and manage Attributes via CLI . terminate-node Terminates calculation node. view-acl View object permissions. See 14.4. View and manage Permissions via CLI . view-cluster Lists cluster nodes. view-pipes Lists pipelines definitions. view-runs Lists pipeline runs. Note : To see command's arguments and options type pipe command --help .","title":"CLI options and commands"},{"location":"manual/14_CLI/14._Command-line_interface/#examples","text":"To see a list of available CLI commands type pipe or pipe --help in the terminal. Note : each command might have its own set of commands that consequently might have their own set of commands... To learn more about a specific command, type the following in the terminal: pipe COMMAND --help . For instance, we can list a number of pipe storage commands with pipe storage --help . Another example - a user can see a list of pipelines runs by pipe view-runs command.","title":"Examples"},{"location":"manual/15_Interactive_services/15.1_Starting_an_Interactive_application/","text":"15.1 Starting an Interactive application Starting a service Terminating a service To run a Tool or a Pipeline as an Interactive service you need to have EXECUTE permissions for that Tool/Pipeline. For more information see 13. Permissions . On this page, you'll find an example of launching an Interactive application. Launching steps remain the same for all applications ( Jupiter Notebook , Rstudio , etc.). All launching steps on this page are illustrated with the example of the Rstudio application. Rstudio is a popular IDE for R language that: is designed to make it easy to write scripts; makes it easy to set your working directory and access files on your computer; makes graphics much more accessible to a casual user. Starting a service Both Pipelines and Tools can be run as interactive services. The example below shows launching Tool scenario. Search for a Tool that implements a service (type a service name in the search box - list will be filtered). Navigate to the Tool information page by clicking on the Tool's name and click the Run button then press OK (for more information see 10.5. Launch a Tool ). Service will begin to set up. A service instance will be shown in the same way as batch jobs - within Active Runs menu. Once an instance is created for a service, a link(s) to the web GUI will be shown within \"Run Log\" form. Clicking the link will load the application web interface. In this example, we show the Rstudio web interface. Service configuration includes the following items: For all out of the box services in the Cloud Pipeline, a user will be automatically authenticated within a service. Note : authentication within all new services added by the users shall be configured by themselves. Below is an example of the authentication within the Rstudio service. Besides, for all out of the box services, you'll also find that all STS data storages, that are available to the user, are available within a service. Data from the STS storages will be available as a local file system and you will be able to work with it just as you do on your laptop. Note : currently all Tools that contain s3fs-fuse (file system that allows Linux and Mac OS X to mount an S3 bucket via FUSE) installed will have STS data storages mounted on them. In other cases, users must configure mounting of the data storages by themselves. In case of the Rstudio application, you can find all available data storages in the Home/cloud-data directory in the bottom-right corner of the screen. Only a user that launched a service can access it. Other users (even if a direct link to the service's GUI is known) will have 401 - Unauthorized error . Terminating a service Stopping a service is performed in the same manner as with a batch job. Option 1 . Load a list of \" Active Runs \" and click the STOP button. Option 2 . Load \"Run Logs\" form and click the STOP button.","title":"15.1 Starting an interactive application"},{"location":"manual/15_Interactive_services/15.1_Starting_an_Interactive_application/#151-starting-an-interactive-application","text":"Starting a service Terminating a service To run a Tool or a Pipeline as an Interactive service you need to have EXECUTE permissions for that Tool/Pipeline. For more information see 13. Permissions . On this page, you'll find an example of launching an Interactive application. Launching steps remain the same for all applications ( Jupiter Notebook , Rstudio , etc.). All launching steps on this page are illustrated with the example of the Rstudio application. Rstudio is a popular IDE for R language that: is designed to make it easy to write scripts; makes it easy to set your working directory and access files on your computer; makes graphics much more accessible to a casual user.","title":"15.1 Starting an Interactive application"},{"location":"manual/15_Interactive_services/15.1_Starting_an_Interactive_application/#starting-a-service","text":"Both Pipelines and Tools can be run as interactive services. The example below shows launching Tool scenario. Search for a Tool that implements a service (type a service name in the search box - list will be filtered). Navigate to the Tool information page by clicking on the Tool's name and click the Run button then press OK (for more information see 10.5. Launch a Tool ). Service will begin to set up. A service instance will be shown in the same way as batch jobs - within Active Runs menu. Once an instance is created for a service, a link(s) to the web GUI will be shown within \"Run Log\" form. Clicking the link will load the application web interface. In this example, we show the Rstudio web interface. Service configuration includes the following items: For all out of the box services in the Cloud Pipeline, a user will be automatically authenticated within a service. Note : authentication within all new services added by the users shall be configured by themselves. Below is an example of the authentication within the Rstudio service. Besides, for all out of the box services, you'll also find that all STS data storages, that are available to the user, are available within a service. Data from the STS storages will be available as a local file system and you will be able to work with it just as you do on your laptop. Note : currently all Tools that contain s3fs-fuse (file system that allows Linux and Mac OS X to mount an S3 bucket via FUSE) installed will have STS data storages mounted on them. In other cases, users must configure mounting of the data storages by themselves. In case of the Rstudio application, you can find all available data storages in the Home/cloud-data directory in the bottom-right corner of the screen. Only a user that launched a service can access it. Other users (even if a direct link to the service's GUI is known) will have 401 - Unauthorized error .","title":"Starting a service"},{"location":"manual/15_Interactive_services/15.1_Starting_an_Interactive_application/#terminating-a-service","text":"Stopping a service is performed in the same manner as with a batch job. Option 1 . Load a list of \" Active Runs \" and click the STOP button. Option 2 . Load \"Run Logs\" form and click the STOP button.","title":"Terminating a service"},{"location":"manual/15_Interactive_services/15.2_Using_Terminal_access/","text":"15.2 Using Terminal access Terminal access is available to the OWNER of the running job and users with ADMIN role. With sufficient permissions, Terminal access can be achieved to any running job. For more information see 13. Permissions . All software in the Cloud Pipeline is located in Docker containers , and we can use Terminal access to the Docker container via the Interactive services . This can be useful when: usage of a new bioinformatics tool shall be tested; batch job scripts shall be tested within a real execution environment; docker image shall be extended and saved (install more packages/bioinformatics tools) - see 10.4. Edit a Tool . Using Terminal access Both Pipelines and Tools can be run as interactive services . The example below shows launching tool scenario: Navigate to the list of registered Tools and search for the Tool required (e.g. \"base-generic-centos7\" ). Go to the Tool page and click the arrow near the Run button \u2192 Select \"Custom Settings\" . Launch Tool page form will load (it's the same form that is used to configure a batch run). The following fields shall be filled: Node type Disk size Cloud Region \" Start idle \" box should be chosen. Click the Launch button when all above parameters are set. Once a run is scheduled and configured SSH hyperlink will appear in the \"Run Log\" form in the right upper corner of the form. Note : This link is only visible to the owner of the run and users with ROLE_ADMIN role assigned. Clicking the SSH link will load a new browser tab with an authenticated Terminal . Note : If an unauthorized user will load a direct link, \"Permission denied\" error will be returned.","title":"15.2 Using terminal access"},{"location":"manual/15_Interactive_services/15.2_Using_Terminal_access/#152-using-terminal-access","text":"Terminal access is available to the OWNER of the running job and users with ADMIN role. With sufficient permissions, Terminal access can be achieved to any running job. For more information see 13. Permissions . All software in the Cloud Pipeline is located in Docker containers , and we can use Terminal access to the Docker container via the Interactive services . This can be useful when: usage of a new bioinformatics tool shall be tested; batch job scripts shall be tested within a real execution environment; docker image shall be extended and saved (install more packages/bioinformatics tools) - see 10.4. Edit a Tool .","title":"15.2 Using Terminal access"},{"location":"manual/15_Interactive_services/15.2_Using_Terminal_access/#using-terminal-access","text":"Both Pipelines and Tools can be run as interactive services . The example below shows launching tool scenario: Navigate to the list of registered Tools and search for the Tool required (e.g. \"base-generic-centos7\" ). Go to the Tool page and click the arrow near the Run button \u2192 Select \"Custom Settings\" . Launch Tool page form will load (it's the same form that is used to configure a batch run). The following fields shall be filled: Node type Disk size Cloud Region \" Start idle \" box should be chosen. Click the Launch button when all above parameters are set. Once a run is scheduled and configured SSH hyperlink will appear in the \"Run Log\" form in the right upper corner of the form. Note : This link is only visible to the owner of the run and users with ROLE_ADMIN role assigned. Clicking the SSH link will load a new browser tab with an authenticated Terminal . Note : If an unauthorized user will load a direct link, \"Permission denied\" error will be returned.","title":"Using Terminal access"},{"location":"manual/15_Interactive_services/15._Interactive_services/","text":"15. Interactive services Overview Interactive services - a feature of the Cloud Pipeline that allows to set up an interactive application in a cloud infrastructure and access it via the web interface, leveraging cloud large instances. This is useful when some analysis steps shall be performed in interactive mode. It can also be useful for navigating through the execution environment and for testing purposes. Examples: Debug scripts, that shall be used in batch jobs Perform data post-processing using IDE Install/Delete/Update software. In the Cloud Pipeline, both Tools and Pipelines can be run as interactive services. Supported services Out of the box, Cloud Pipeline provides the following interactive services: RStudio - IDE for R language that helps to make work with R a great deal more convenient. For details see https://www.rstudio.com/ . Jupiter Notebook - a web application that allows creating documents with code pieces, visualizations and narrative text inside. For more information see http://jupyter.org/ . Terminal - a window with a command line (shell). Note : Terminal access is available for all Tools or Pipelines with these Tools. On the other side, you can't get an access to the e.g. Rstudio application if a Tool doesn't contain it. List of services can be extended by users.","title":"15.0 Overview"},{"location":"manual/15_Interactive_services/15._Interactive_services/#15-interactive-services","text":"","title":"15. Interactive services"},{"location":"manual/15_Interactive_services/15._Interactive_services/#overview","text":"Interactive services - a feature of the Cloud Pipeline that allows to set up an interactive application in a cloud infrastructure and access it via the web interface, leveraging cloud large instances. This is useful when some analysis steps shall be performed in interactive mode. It can also be useful for navigating through the execution environment and for testing purposes. Examples: Debug scripts, that shall be used in batch jobs Perform data post-processing using IDE Install/Delete/Update software. In the Cloud Pipeline, both Tools and Pipelines can be run as interactive services.","title":"Overview"},{"location":"manual/15_Interactive_services/15._Interactive_services/#supported-services","text":"Out of the box, Cloud Pipeline provides the following interactive services: RStudio - IDE for R language that helps to make work with R a great deal more convenient. For details see https://www.rstudio.com/ . Jupiter Notebook - a web application that allows creating documents with code pieces, visualizations and narrative text inside. For more information see http://jupyter.org/ . Terminal - a window with a command line (shell). Note : Terminal access is available for all Tools or Pipelines with these Tools. On the other side, you can't get an access to the e.g. Rstudio application if a Tool doesn't contain it. List of services can be extended by users.","title":"Supported services"},{"location":"manual/16_Issues/16._Issues/","text":"16. Issues Open an issue Change an issue title Leave a comment Edit a comment Delete a comment Delete an issue Issues is a great tool to share results with other users or get feedback. It allows keeping the discussion in one place - traceable and linked to specific data. The feature is available for: Folders, including Projects Pipelines Tools. Open an issue To open an issue, a user shall have READ permissions for a discussed object. For more information see 13. Permissions . To open an issue the following steps shall be performed: Navigate to the object you want to discuss and click icon \u2192 Issues . Note : the second way is to navigate to a folder that contains the object you want to discuss and click icon in the desired object's line. The Issues pane will be opened. Click button to create a new issue. Fill up the open form: Title (e.g. new issue ), Description (e.g. error ). Description supports MARKDOWN formatting, thus you can write your text in the Write tab with special symbols and then preview it in the Preview tab. Note : you can address your topic to a specific user if you put @ symbol and start to write username. The system will suggest you choose from the list. After you save your topic, the user will receive e-mail notification. When you finish, press the Create button - the topic will be created. You'll see topic title, the author and how much time past since the topic was created. You can click on it to open and see description. If you want to go back to the list of all discussions, click . Change an issue title To edit an issue title a user shall be OWNER of the discussion. For more information see 13. Permissions . To change an issue title the following steps shall be performed: Navigate to the issue which title you want to change and open it. Click the issue title - the Title field will be open for editing. Enter new title and click out of the field. The new title will be saved. Leave a comment To leave a comment a user shall have READ permissions for a discussioned object. For more information see 13. Permissions . To leave a comment on an issue the following steps shall be performed: Navigate to the object you want to discuss and click icon \u2192 Issues . Note : the second way is to navigate to a folder that contains the object you want to discuss and click icon in the desired object's line. The Issues pane will be opened. Click on an issue you interested in. Fill up the Comment form. Comment supports MARKDOWN formatting, thus you can write your text in the Write tab with special symbols and then preview it in the Preview tab. Note : you can address your comment to a specific user if you put @ symbol and start to write username. The system will suggest you choose from the list. After you save your comment, the user will receive an e-mail notification. Note : you can also drag and drop pictures here, so that everyone can see the issue more clearly: When you finish, press the Send button - the comment will be created. You'll see your comment, the author and how much time past since the comment was created. Edit a comment To edit a comment a user shall be OWNER of the comment. For more information see 13. Permissions . Note : to edit a topic description the same steps should be performed. To edit a comment the following steps shall be performed: Navigate to the comment you want to edit. Press icon - the editing form will be shown. Change your comment and click to save your changes. Note : won't be available until you change something. Note : if you change your mind and want to leave your comment as is, click . The changes will be saved. Delete a comment To delete a comment a user shall be OWNER of the comment. For more information see 13. Permissions . To delete a comment the following steps shall be performed: Navigate to the comment you want to delete. Click icon. Confirm your action in the dialog window. The comment is deleted. Delete an issue To delete an issue a user shall be OWNER of the issue. For more information see 13. Permissions . To delete an issue the following steps shall be performed: Navigate to the issue you want to delete and open it. Click icon. Confirm your action in the dialog window. The issue is deleted.","title":"16. Issues"},{"location":"manual/16_Issues/16._Issues/#16-issues","text":"Open an issue Change an issue title Leave a comment Edit a comment Delete a comment Delete an issue Issues is a great tool to share results with other users or get feedback. It allows keeping the discussion in one place - traceable and linked to specific data. The feature is available for: Folders, including Projects Pipelines Tools.","title":"16. Issues"},{"location":"manual/16_Issues/16._Issues/#open-an-issue","text":"To open an issue, a user shall have READ permissions for a discussed object. For more information see 13. Permissions . To open an issue the following steps shall be performed: Navigate to the object you want to discuss and click icon \u2192 Issues . Note : the second way is to navigate to a folder that contains the object you want to discuss and click icon in the desired object's line. The Issues pane will be opened. Click button to create a new issue. Fill up the open form: Title (e.g. new issue ), Description (e.g. error ). Description supports MARKDOWN formatting, thus you can write your text in the Write tab with special symbols and then preview it in the Preview tab. Note : you can address your topic to a specific user if you put @ symbol and start to write username. The system will suggest you choose from the list. After you save your topic, the user will receive e-mail notification. When you finish, press the Create button - the topic will be created. You'll see topic title, the author and how much time past since the topic was created. You can click on it to open and see description. If you want to go back to the list of all discussions, click .","title":"Open an issue"},{"location":"manual/16_Issues/16._Issues/#change-an-issue-title","text":"To edit an issue title a user shall be OWNER of the discussion. For more information see 13. Permissions . To change an issue title the following steps shall be performed: Navigate to the issue which title you want to change and open it. Click the issue title - the Title field will be open for editing. Enter new title and click out of the field. The new title will be saved.","title":"Change an issue title"},{"location":"manual/16_Issues/16._Issues/#leave-a-comment","text":"To leave a comment a user shall have READ permissions for a discussioned object. For more information see 13. Permissions . To leave a comment on an issue the following steps shall be performed: Navigate to the object you want to discuss and click icon \u2192 Issues . Note : the second way is to navigate to a folder that contains the object you want to discuss and click icon in the desired object's line. The Issues pane will be opened. Click on an issue you interested in. Fill up the Comment form. Comment supports MARKDOWN formatting, thus you can write your text in the Write tab with special symbols and then preview it in the Preview tab. Note : you can address your comment to a specific user if you put @ symbol and start to write username. The system will suggest you choose from the list. After you save your comment, the user will receive an e-mail notification. Note : you can also drag and drop pictures here, so that everyone can see the issue more clearly: When you finish, press the Send button - the comment will be created. You'll see your comment, the author and how much time past since the comment was created.","title":"Leave a comment"},{"location":"manual/16_Issues/16._Issues/#edit-a-comment","text":"To edit a comment a user shall be OWNER of the comment. For more information see 13. Permissions . Note : to edit a topic description the same steps should be performed. To edit a comment the following steps shall be performed: Navigate to the comment you want to edit. Press icon - the editing form will be shown. Change your comment and click to save your changes. Note : won't be available until you change something. Note : if you change your mind and want to leave your comment as is, click . The changes will be saved.","title":"Edit a comment"},{"location":"manual/16_Issues/16._Issues/#delete-a-comment","text":"To delete a comment a user shall be OWNER of the comment. For more information see 13. Permissions . To delete a comment the following steps shall be performed: Navigate to the comment you want to delete. Click icon. Confirm your action in the dialog window. The comment is deleted.","title":"Delete a comment"},{"location":"manual/16_Issues/16._Issues/#delete-an-issue","text":"To delete an issue a user shall be OWNER of the issue. For more information see 13. Permissions . To delete an issue the following steps shall be performed: Navigate to the issue you want to delete and open it. Click icon. Confirm your action in the dialog window. The issue is deleted.","title":"Delete an issue"},{"location":"manual/17_Tagging_by_attributes/17._CP_objects_tagging_by_additional_attributes/","text":"17. CP objects tagging by additional attributes Add attributes Edit attributes Delete attributes Automatic tagging A user can manage custom sets of \" key-values \" attributes for data storage and files. These custom attributes could be used for an additional description of the object and make the search process easier by using attributes as tags. To edit object's attributes, you need to be an OWNER of the object. For more information see 13. Permissions . You can also manage attributes via CLI. See 14.2. View and manage Attributes via CLI . How to navigate to Attributes pane of different objects: Folder Metadata Pipeline Data storage Tool groups and tools User Group of users/role Note : if you were changing the data storage file's attributes, you could return to data storage's attribute by clicking control. Add attributes Navigate to Attributes pane of a selected object. Click + Add button. Enter attribute key and value. Click Add : Edit attributes Navigate to Attributes pane of a selected object. Click the attribute key or value field. Change attribute key or value. Press \" Enter \" keyboard button or just out of the active field. Delete attributes Navigate to Attributes pane of a selected object. Click Trash icon to delete a particular attribute. Note : click Remove all to delete all attributes. Automatic tagging In the Cloud Pipeline files are automatically tagged with the following attributes when uploading them to the data storage via CLI/GUI (see a CLI example 14.3. Manage Storage via CLI ): CP_OWNER . The value of the attribute will be set as a user ID. CP_SOURCE . The value of the attribute will be set as a local path used to upload. Note : this attribute is set only if a file is uploaded through CLI. Note : The exception is that the storage is based on NFS share. Files in such data storage don't have attributes at all. Besides, files are automatically tagged with the following attributes when uploading them to the data storage as a result of a pipeline run: Name Value CP_CALC_CONFIG Instance type CP_DOCKER_IMAGE Tool that was used CP_JOB_CONFIGURATION Pipeline configuration CP_JOB_ID Pipeline ID CP_JOB_NAME Pipeline name CP_JOB_VERSION Pipeline version CP_OWNER User ID CP_RUN_ID Run ID CP_SOURCE Local path used to upload data How to make pipeline use a CP CLI see here .","title":"17. CP objects tagging by additional attributes"},{"location":"manual/17_Tagging_by_attributes/17._CP_objects_tagging_by_additional_attributes/#17-cp-objects-tagging-by-additional-attributes","text":"Add attributes Edit attributes Delete attributes Automatic tagging A user can manage custom sets of \" key-values \" attributes for data storage and files. These custom attributes could be used for an additional description of the object and make the search process easier by using attributes as tags. To edit object's attributes, you need to be an OWNER of the object. For more information see 13. Permissions . You can also manage attributes via CLI. See 14.2. View and manage Attributes via CLI . How to navigate to Attributes pane of different objects: Folder Metadata Pipeline Data storage Tool groups and tools User Group of users/role Note : if you were changing the data storage file's attributes, you could return to data storage's attribute by clicking control.","title":"17. CP objects tagging by additional attributes"},{"location":"manual/17_Tagging_by_attributes/17._CP_objects_tagging_by_additional_attributes/#add-attributes","text":"Navigate to Attributes pane of a selected object. Click + Add button. Enter attribute key and value. Click Add :","title":"Add attributes"},{"location":"manual/17_Tagging_by_attributes/17._CP_objects_tagging_by_additional_attributes/#edit-attributes","text":"Navigate to Attributes pane of a selected object. Click the attribute key or value field. Change attribute key or value. Press \" Enter \" keyboard button or just out of the active field.","title":"Edit attributes"},{"location":"manual/17_Tagging_by_attributes/17._CP_objects_tagging_by_additional_attributes/#delete-attributes","text":"Navigate to Attributes pane of a selected object. Click Trash icon to delete a particular attribute. Note : click Remove all to delete all attributes.","title":"Delete attributes"},{"location":"manual/17_Tagging_by_attributes/17._CP_objects_tagging_by_additional_attributes/#automatic-tagging","text":"In the Cloud Pipeline files are automatically tagged with the following attributes when uploading them to the data storage via CLI/GUI (see a CLI example 14.3. Manage Storage via CLI ): CP_OWNER . The value of the attribute will be set as a user ID. CP_SOURCE . The value of the attribute will be set as a local path used to upload. Note : this attribute is set only if a file is uploaded through CLI. Note : The exception is that the storage is based on NFS share. Files in such data storage don't have attributes at all. Besides, files are automatically tagged with the following attributes when uploading them to the data storage as a result of a pipeline run: Name Value CP_CALC_CONFIG Instance type CP_DOCKER_IMAGE Tool that was used CP_JOB_CONFIGURATION Pipeline configuration CP_JOB_ID Pipeline ID CP_JOB_NAME Pipeline name CP_JOB_VERSION Pipeline version CP_OWNER User ID CP_RUN_ID Run ID CP_SOURCE Local path used to upload data How to make pipeline use a CP CLI see here .","title":"Automatic tagging"},{"location":"manual/18_Home_page/18._Home_page/","text":"18. Home page Home page widgets Activities Data Notifications Tools Pipelines Recently completed runs Active runs Services Adjust Home page view Start a Run from the Home tab Home page widgets Activities This widget lists recent comments/issues/posts that occured for the items that you own (e.g. models, pipelines, projects, folders, etc.). Data List of the data storages (S3 and EFS) that are available to you for READ/WRITE operations. These data storages are available from the Platform GUI. Click an item to navigate to the data storage contents. Note : personal Data storages (i.e. a user is an OWNER of this Storage) will be shown on top, WRITE - second priority, READ - third priority. Note : data storages are tagged with region flag to visually distinguish storage locations. Notifications List of system-wide notifications from administrators posted. These are the same notifications as shown at a login time in the top right corner of the main page. Tools Tools/Compute stacks/Docker images that are added in your PERSONAL repository or available to your group. To run a Tool - please use a RUN button that appears when hovering an item with a mouse. Use a search bar to find tools that are shared by other users and groups. To get a full list of available stacks - please use the Tools menu item in the left toolbar. Note : group-level Tools will be shown on the top of the Tools list. Pipelines Pipelines that are available to you for READ/WRITE operations. This is the same list, as available in the Library hierarchy. Pipeline can be run right from this widget using a RUN button that appears when hovering an item with a mouse. Press the History control to view runs history of a chosen pipeline. Recently completed runs This widget lists your runs that were recently completed. Click a corresponding entry in this widget to navigate to the run details/logs page. Use the Rerun control to rerun selected item. Active runs List of the jobs/tools that are currently in RUNNING or PAUSED state with some information about instance, elapsed time and estimated price, that is calculated based on the run duration and instance type. If this list is empty - start a new run from the Tools or Pipelines widgets. Hover a run item to view a list of available action. Use STOP / PAUSE / RESUME actions to change the state of the run or use OPEN to navigate to the GUI of the interactive job. Select the LINKS button to view/navigate the run input/output parameters. Click a run item to navigate to the details and logs page. Note : only top 50 active runs will be shown, if more than 50 jobs/tools are running - use Explore all active runs link. Services This widget lists direct links to the Interactive services, that are exposed by the launched Tools. Compared to the ACTIVE RUNS widget - this one does NOT show all the active jobs/tools, only links to the web/desktop GUI. If this list is empty - start a new run of an interactive compute stack from the Tools widget. Click an item within this widget to navigate to the corresponding service. Adjust Home page view Dashboard view can be adjusted by clicking the Configure button. In the opened window a user can adjust view of the Home tab by selecting widgets. Selected items will be displayed after you click OK . Restore default layout control is used to restore default tab configuration. Also, user can remove widgets by clicking the \"Delete\" icon on them. Start a Run from the Home tab User shall have EXECUTE rights to run selected Tool/pipeline. Although Runs can be started from the Tools or Library tabs respectively (see 10.5. Launch a Tool and 6.2. Launch a pipeline ), we can start them from the Home tab as well. In the example below we will start a Run from the Tools widget of the Home tab: Navigate to the Home tab. Select a Tool in the Tools widget. Click Run . Run a Tool with custom settings with Run custom control or run them with default settings with Run button. Note : Active runs with endpoints are highlighted in yellow. They expose the Open button that shows endpoints. Clicking them will navigate you to the endpoint URL: Note : all widgets that display Runs show input/output links via LINKS control. Clicking a link will navigate you to the appropriate Data Storage.","title":"18. Home page"},{"location":"manual/18_Home_page/18._Home_page/#18-home-page","text":"Home page widgets Activities Data Notifications Tools Pipelines Recently completed runs Active runs Services Adjust Home page view Start a Run from the Home tab","title":"18. Home page"},{"location":"manual/18_Home_page/18._Home_page/#home-page-widgets","text":"","title":"Home page widgets"},{"location":"manual/18_Home_page/18._Home_page/#activities","text":"This widget lists recent comments/issues/posts that occured for the items that you own (e.g. models, pipelines, projects, folders, etc.).","title":"Activities"},{"location":"manual/18_Home_page/18._Home_page/#data","text":"List of the data storages (S3 and EFS) that are available to you for READ/WRITE operations. These data storages are available from the Platform GUI. Click an item to navigate to the data storage contents. Note : personal Data storages (i.e. a user is an OWNER of this Storage) will be shown on top, WRITE - second priority, READ - third priority. Note : data storages are tagged with region flag to visually distinguish storage locations.","title":"Data"},{"location":"manual/18_Home_page/18._Home_page/#notifications","text":"List of system-wide notifications from administrators posted. These are the same notifications as shown at a login time in the top right corner of the main page.","title":"Notifications"},{"location":"manual/18_Home_page/18._Home_page/#tools","text":"Tools/Compute stacks/Docker images that are added in your PERSONAL repository or available to your group. To run a Tool - please use a RUN button that appears when hovering an item with a mouse. Use a search bar to find tools that are shared by other users and groups. To get a full list of available stacks - please use the Tools menu item in the left toolbar. Note : group-level Tools will be shown on the top of the Tools list.","title":"Tools"},{"location":"manual/18_Home_page/18._Home_page/#pipelines","text":"Pipelines that are available to you for READ/WRITE operations. This is the same list, as available in the Library hierarchy. Pipeline can be run right from this widget using a RUN button that appears when hovering an item with a mouse. Press the History control to view runs history of a chosen pipeline.","title":"Pipelines"},{"location":"manual/18_Home_page/18._Home_page/#recently-completed-runs","text":"This widget lists your runs that were recently completed. Click a corresponding entry in this widget to navigate to the run details/logs page. Use the Rerun control to rerun selected item.","title":"Recently completed runs"},{"location":"manual/18_Home_page/18._Home_page/#active-runs","text":"List of the jobs/tools that are currently in RUNNING or PAUSED state with some information about instance, elapsed time and estimated price, that is calculated based on the run duration and instance type. If this list is empty - start a new run from the Tools or Pipelines widgets. Hover a run item to view a list of available action. Use STOP / PAUSE / RESUME actions to change the state of the run or use OPEN to navigate to the GUI of the interactive job. Select the LINKS button to view/navigate the run input/output parameters. Click a run item to navigate to the details and logs page. Note : only top 50 active runs will be shown, if more than 50 jobs/tools are running - use Explore all active runs link.","title":"Active runs"},{"location":"manual/18_Home_page/18._Home_page/#services","text":"This widget lists direct links to the Interactive services, that are exposed by the launched Tools. Compared to the ACTIVE RUNS widget - this one does NOT show all the active jobs/tools, only links to the web/desktop GUI. If this list is empty - start a new run of an interactive compute stack from the Tools widget. Click an item within this widget to navigate to the corresponding service.","title":"Services"},{"location":"manual/18_Home_page/18._Home_page/#adjust-home-page-view","text":"Dashboard view can be adjusted by clicking the Configure button. In the opened window a user can adjust view of the Home tab by selecting widgets. Selected items will be displayed after you click OK . Restore default layout control is used to restore default tab configuration. Also, user can remove widgets by clicking the \"Delete\" icon on them.","title":"Adjust Home page view"},{"location":"manual/18_Home_page/18._Home_page/#start-a-run-from-the-home-tab","text":"User shall have EXECUTE rights to run selected Tool/pipeline. Although Runs can be started from the Tools or Library tabs respectively (see 10.5. Launch a Tool and 6.2. Launch a pipeline ), we can start them from the Home tab as well. In the example below we will start a Run from the Tools widget of the Home tab: Navigate to the Home tab. Select a Tool in the Tools widget. Click Run . Run a Tool with custom settings with Run custom control or run them with default settings with Run button. Note : Active runs with endpoints are highlighted in yellow. They expose the Open button that shows endpoints. Clicking them will navigate you to the endpoint URL: Note : all widgets that display Runs show input/output links via LINKS control. Clicking a link will navigate you to the appropriate Data Storage.","title":"Start a Run from the Home tab"},{"location":"manual/Appendix_A/Appendix_A._EC2_Instance_and_Docker_container_lifecycles/","text":"Appendix A. EC2 Instance and Docker container lifecycles Overview EC2 Instance lifecycle stages Docker container lifecycle stages Overview In the context of Cloud Pipeline , both life cycles are tied together. A user is charged for instance usage. Note : to run a container you need a launched instance. In the Cloud Pipeline , instances are bought for hourly rates with a minimum of one hour. This is due to the following reasons: It helps to decrease the time for node relaunch. It helps to decrease the time for Docker image and data (\"type\": \"common\") download. Most pipelines will take much longer than 1 hour to complete. Note : same instance configuration must be used in order to reuse currently active nodes. If the node has no running jobs 10 minutes before the new hour of payment begins, it will be terminated. EC2 Instance lifecycle stages The general overview of the EC2 instance lifecycle - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-lifecycle.html . In the Cloud Pipeline there are 3 stages: Pending state - no billing. Running state - you're billed for each second, with a one-minute minimum, that you keep the instance running. Terminated state - no billing. Docker container lifecycle stages A general overview of the Docker container lifecycle - https://medium.com/@nagarwal/lifecycle-of-docker-container-d2da9f85959 . In the Cloud Pipeline there are 2 stages: Running state - possible to execute some commands inside the container. Terminated state - the container is not accessible.","title":"Appendix A. EC2 instance and Docker container lifecycles"},{"location":"manual/Appendix_A/Appendix_A._EC2_Instance_and_Docker_container_lifecycles/#appendix-a-ec2-instance-and-docker-container-lifecycles","text":"Overview EC2 Instance lifecycle stages Docker container lifecycle stages","title":"Appendix A. EC2 Instance and Docker container lifecycles"},{"location":"manual/Appendix_A/Appendix_A._EC2_Instance_and_Docker_container_lifecycles/#overview","text":"In the context of Cloud Pipeline , both life cycles are tied together. A user is charged for instance usage. Note : to run a container you need a launched instance. In the Cloud Pipeline , instances are bought for hourly rates with a minimum of one hour. This is due to the following reasons: It helps to decrease the time for node relaunch. It helps to decrease the time for Docker image and data (\"type\": \"common\") download. Most pipelines will take much longer than 1 hour to complete. Note : same instance configuration must be used in order to reuse currently active nodes. If the node has no running jobs 10 minutes before the new hour of payment begins, it will be terminated.","title":"Overview"},{"location":"manual/Appendix_A/Appendix_A._EC2_Instance_and_Docker_container_lifecycles/#ec2-instance-lifecycle-stages","text":"The general overview of the EC2 instance lifecycle - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-lifecycle.html . In the Cloud Pipeline there are 3 stages: Pending state - no billing. Running state - you're billed for each second, with a one-minute minimum, that you keep the instance running. Terminated state - no billing.","title":"EC2 Instance lifecycle stages"},{"location":"manual/Appendix_A/Appendix_A._EC2_Instance_and_Docker_container_lifecycles/#docker-container-lifecycle-stages","text":"A general overview of the Docker container lifecycle - https://medium.com/@nagarwal/lifecycle-of-docker-container-d2da9f85959 . In the Cloud Pipeline there are 2 stages: Running state - possible to execute some commands inside the container. Terminated state - the container is not accessible.","title":"Docker container lifecycle stages"},{"location":"manual/Appendix_B/Appendix_B._Working_with_a_Project/","text":"Appendix B. Working with a Project Project is a special type of Folder . Projects might be used to organize data and metadata and simplify analysis runs for a large data set. Also, you can set a project attributes as parameters of analysis method configuration. Note : learn more about metadata here . Create a project To create a project you need WRITE permissions for the parent folder and FOLDER_MANAGER role. For more information see 13. Permissions . To create a project in the system, the following steps shall be performed: Navigate to a folder of a future-project destination. Click + Create \u2192 PROJECT . Note PROJECT is an oncology project template. It supports the default structure of an oncology project. The system suggests that you name a new project. Enter a name. Click OK button. You'll be fetched into the new project's parent folder page automatically. The folder has a default attribute: type = project (see picture above, 2 ). Note : It's an essential attribute for a project. Based on this attribute, the system recognizes a folder as a project. If the attribute is removed, the folder is no longer a project. The new project contains (see picture above, 1 ): Method-Configuration folder. It will be a container for all your methods to run. Storage . The storage will be empty. The name of the storage will be set as a default. Here you can see default settings of new storage. History . This is a table that contains all at any time scheduled runs of a project's methods. For now, it's empty. The picture below illustrated how the table looks with an existing run's history. The History GUI repeats the Run space. For more details see 11. Manage Runs . How to add metadata in the project, see here .","title":"Appendix B. Working with a Project"},{"location":"manual/Appendix_B/Appendix_B._Working_with_a_Project/#appendix-b-working-with-a-project","text":"Project is a special type of Folder . Projects might be used to organize data and metadata and simplify analysis runs for a large data set. Also, you can set a project attributes as parameters of analysis method configuration. Note : learn more about metadata here .","title":"Appendix B. Working with a Project"},{"location":"manual/Appendix_B/Appendix_B._Working_with_a_Project/#create-a-project","text":"To create a project you need WRITE permissions for the parent folder and FOLDER_MANAGER role. For more information see 13. Permissions . To create a project in the system, the following steps shall be performed: Navigate to a folder of a future-project destination. Click + Create \u2192 PROJECT . Note PROJECT is an oncology project template. It supports the default structure of an oncology project. The system suggests that you name a new project. Enter a name. Click OK button. You'll be fetched into the new project's parent folder page automatically. The folder has a default attribute: type = project (see picture above, 2 ). Note : It's an essential attribute for a project. Based on this attribute, the system recognizes a folder as a project. If the attribute is removed, the folder is no longer a project. The new project contains (see picture above, 1 ): Method-Configuration folder. It will be a container for all your methods to run. Storage . The storage will be empty. The name of the storage will be set as a default. Here you can see default settings of new storage. History . This is a table that contains all at any time scheduled runs of a project's methods. For now, it's empty. The picture below illustrated how the table looks with an existing run's history. The History GUI repeats the Run space. For more details see 11. Manage Runs . How to add metadata in the project, see here .","title":"Create a project"},{"location":"release_notes/v.0.13/v.0.13_-_Release_notes/","text":"Cloud Pipeline v.0.13 - Release notes Data sharing with external collaborators Batch processing in EU Running instances sharing with other user(s) or group(s) of users Automated instances pause/stop according to the resource usage Tools versions details Data download from external http/ftp resources to the cloud data storage Automatically rerun a batch job if a spot instance is terminated Displaying estimated price of a job run Displaying details of the user profile Breadcrumbs for the Library view Data sharing with external collaborators NGS users often get the raw datasets from the external partners for processing. Typically external collaborator sends such datasets using hard drives. To enable such type of collaboration - S3 buckets within a Cloud Platform can now be \"Shared\". When a bucket is created - owner can set \"Enable sharing\" option. Bucket will be created and can be managed and consumed as any other bucket: But such types of buckets also display a \"Share\" button, which can be used to generate URL, that can be shared with the external collaborator Click \"Share\" Get the URL and send it to the external partner Once external users loads this URL: Authentication is performed using SAML Access is granted according to the user's permissions S3 bucket browser is displayed This collaboration space can be used to exchange large data files (up to 5Tb per one file) Compared to the \" Data download from external http/ftp resources to the cloud data storage \" feature (see below) - this use case considers that external colleague cannot provide a URL for direct download . For more information about data sharing with external collaborators see 8.8. Data sharing . Batch processing in EU Previously all computing nodes and storages were located in the US region. For EU NGS use cases, which operate on huge data volumes, data movement to US took too much time. To workaround this issue - CP Platform was improved to support batch processing and compute nodes management within other Cloud regions. Bucket creation form now allows to set - where to create a data storage: All the buckets, that are shown in the \"Library tree view\", \"Library details form\" and \"Home dashboard\" are now tagged with region flag to visually distinguish storage locations: Library tree Library details view Home dashboard When a pipeline configuration is created within a project or a new run is launched - user can specify to use a specific region for a compute node placement: Project method configuration Launch form configuration Examples of using Cloud regions see in sections 6. Manage Pipeline , 7. Manage Detached configuration , 8. Manage Data Storage . Running instances sharing with other user(s) or group(s) of users For certain use cases it is beneficial to be able to share applications with other users/groups. Thus, providing a \"persistent\" service that can be shared and run in the Cloud platform. Current version introduces a feature that allows to: Specify a \"Friendly URL\" for persistent services. This produces endpoint URL in a more friendly/descriptive format: {cloud-pipeline_url}/ friendly_url instead of {cloud-pipeline_url}/ pipeline-XXXX-XXXX . This can be configured at a service launch time in the \"Advanced\" section of the Launch form (name shall be unique) URL will be generated using the specified name For more information about \"Friendly URL\" for persistent services see here . User can now share a run with others: \"Share with: ...\" parameter, within a run log form, can be used for this Specific users or whole groups can be set for sharing Once this is set - other users will be able to access run's endpoints (not SSH) \"Services\" widget within a Home dashboard page is now capable of listing such \"shared\" services. It is intended to display a \"catalog\" of services, that can be accessed by a current user, without running own jobs. For more information about runs sharing see 11.3. Sharing with other users or groups of users . Automated instances pause/stop according to the resource usage Version v0.12 introduced a PAUSE/RESUME option for the users, which allowed to persist whole state of the environment and resume it. This feature required AWS On-Demand instances to be used, which are more expensive compared to Spots. Current version provides a way to control spendings by automatically pausing on-demand instances if they are not used. Administrators can now control this behaviour using a set of parameters: system.idle.cpu.threshold - specify %% of the average CPU, below which action shall be taken system.resource.monitoring.period - specify period (in seconds) between the users' instances scanning to collect the monitoring metrics system.max.idle.timeout.minutes - specify a duration in minutes. If CPU utilization is below system.idle.cpu.threshold for this duration - notification will be sent to the user. system.idle.action.timeout.minutes - specify a duration in minutes. If CPU utilization is below system.idle.cpu.threshold for this duration - an action, specified in system.idle.action will be performed system.idle.action - which action to perform on the instance, that showed low CPU utilization: NOTIFY - only send notification PAUSE - pause an instance if possible (e.g. instance is On-Demand, Spot instances are skipped) PAUSE_OR_STOP - pause an instance if it is On-Demand, stop an instance if it is Spot STOP - Stop an instance, disregarding price-type Tools versions details Now more information on the docker image version is available: Image size Modified date Unique identifier (Digest) Corresponding aliases (e.g. if some digest has two aliases) For more information see here . Data download from external http/ftp resources to the cloud data storage Users often get the raw datasets from the external partners for processing. Previously, users had to get the data to the local cluster storage and them upload data to the cloud using clommand-line interface. Now users can provide CSV/TSV files with the external links and submit a data transfer job, so that the files will be moved to the cloud storage in the background. Upload CSV/TSV file and view list of samples with the external links: Select the transfer options: Which S3 bucket to use as a destination Which CSV/TSV columns shall be used to get external URLs (if several columns contain URLs - both can be used) ( optionally ) Select whether to rename resulting path to some other value (can be specified as another column cell, e.g. sample name) ( optionally ) Create new folders within destination if several columns are selected for \"Path fields\" option. E.g. if two columns contain URLs and both are selected - then folders will be created for the corresponding column name and used for appropriate files storage. Whether to update external URL within a table to the new location of the files. If set - http/ftp URLs will be changed to s3 path. Such data structure can be then used for a processing by a pipeline (See below - URLs are changed to the S3-clickable hyperlinks): Once transfer job is finished - files will be located in the selected S3 storage: For more information about data downloading from external http/ftp resources to the CP see 5.5. Download data from external resources to the cloud data storage . Automatically rerun a batch job if a spot instance is terminated In certain cases - AWS may terminate a node, that is used to run a job or an interactive tool: Spot prices changed AWS experienced a hardware issue These cases shall not be treated as a Cloud Platform bug. To make it more explicit, the following features are implemented: If a job fails due to server-related issue - a more friendly message is displayed, describing a reason for the hardware failure: If a batch job fails due to server-related issue and AWS reports one of the following EC2 status codes - batch job is restarted from scratch: Server.SpotInstanceShutdown - AWS stopped a spot instance due to price changes Server.SpotInstanceTermination - AWS terminated a spot instance due to price changes Server.InternalError - AWS hardware issue Administrator can configure whether to apply this behavior and how much retries shall be performed: For more information about automatically reruns batch jobs in cases when spot instances are terminated see here and in section 12.10. Manage system-level settings . Displaying estimated price of a job run Now a list of active runs (both \"ACTIVE RUNS\" menu and \"Dashboard\") shows \"Estimated price\", which is calculated based on the run duration and selected instance type. This field is updated interactively (i.e. each 5 - 10 seconds) ACTIVE RUNS menu Dashboard widget For more information and examples of using see in sections 11. Manage Runs , 18. Home page . Displaying details of the user profile Previously only user id was shown within all GUI forms, that displayed object/run OWNER. Now one can get information on the user name/email when hovering user id, which is shown in the GUI. It is shown in the tooltip with all information available from the IdP. Breadcrumbs for the Library view Previously user was able to collapse a \"Library\" tree view using button. This allows to work with the plain objects lists, which is more comfortable for certain users, compared to the hierarchy. But navigation to the upper level of the hierarchy was not convenient in a collapsed mode. Now breadcrumbs are shown in the header of the plain objects list, which allow to view current path and navigate to the upper level by clicking a path item. Expanded mode (default) Collapsed mode","title":"v.0.13"},{"location":"release_notes/v.0.13/v.0.13_-_Release_notes/#cloud-pipeline-v013-release-notes","text":"Data sharing with external collaborators Batch processing in EU Running instances sharing with other user(s) or group(s) of users Automated instances pause/stop according to the resource usage Tools versions details Data download from external http/ftp resources to the cloud data storage Automatically rerun a batch job if a spot instance is terminated Displaying estimated price of a job run Displaying details of the user profile Breadcrumbs for the Library view","title":"Cloud Pipeline v.0.13 - Release notes"},{"location":"release_notes/v.0.13/v.0.13_-_Release_notes/#data-sharing-with-external-collaborators","text":"NGS users often get the raw datasets from the external partners for processing. Typically external collaborator sends such datasets using hard drives. To enable such type of collaboration - S3 buckets within a Cloud Platform can now be \"Shared\". When a bucket is created - owner can set \"Enable sharing\" option. Bucket will be created and can be managed and consumed as any other bucket: But such types of buckets also display a \"Share\" button, which can be used to generate URL, that can be shared with the external collaborator Click \"Share\" Get the URL and send it to the external partner Once external users loads this URL: Authentication is performed using SAML Access is granted according to the user's permissions S3 bucket browser is displayed This collaboration space can be used to exchange large data files (up to 5Tb per one file) Compared to the \" Data download from external http/ftp resources to the cloud data storage \" feature (see below) - this use case considers that external colleague cannot provide a URL for direct download . For more information about data sharing with external collaborators see 8.8. Data sharing .","title":"Data sharing with external collaborators"},{"location":"release_notes/v.0.13/v.0.13_-_Release_notes/#batch-processing-in-eu","text":"Previously all computing nodes and storages were located in the US region. For EU NGS use cases, which operate on huge data volumes, data movement to US took too much time. To workaround this issue - CP Platform was improved to support batch processing and compute nodes management within other Cloud regions. Bucket creation form now allows to set - where to create a data storage: All the buckets, that are shown in the \"Library tree view\", \"Library details form\" and \"Home dashboard\" are now tagged with region flag to visually distinguish storage locations: Library tree Library details view Home dashboard When a pipeline configuration is created within a project or a new run is launched - user can specify to use a specific region for a compute node placement: Project method configuration Launch form configuration Examples of using Cloud regions see in sections 6. Manage Pipeline , 7. Manage Detached configuration , 8. Manage Data Storage .","title":"Batch processing in EU"},{"location":"release_notes/v.0.13/v.0.13_-_Release_notes/#running-instances-sharing-with-other-users-or-groups-of-users","text":"For certain use cases it is beneficial to be able to share applications with other users/groups. Thus, providing a \"persistent\" service that can be shared and run in the Cloud platform. Current version introduces a feature that allows to: Specify a \"Friendly URL\" for persistent services. This produces endpoint URL in a more friendly/descriptive format: {cloud-pipeline_url}/ friendly_url instead of {cloud-pipeline_url}/ pipeline-XXXX-XXXX . This can be configured at a service launch time in the \"Advanced\" section of the Launch form (name shall be unique) URL will be generated using the specified name For more information about \"Friendly URL\" for persistent services see here . User can now share a run with others: \"Share with: ...\" parameter, within a run log form, can be used for this Specific users or whole groups can be set for sharing Once this is set - other users will be able to access run's endpoints (not SSH) \"Services\" widget within a Home dashboard page is now capable of listing such \"shared\" services. It is intended to display a \"catalog\" of services, that can be accessed by a current user, without running own jobs. For more information about runs sharing see 11.3. Sharing with other users or groups of users .","title":"Running instances sharing with other user(s) or group(s) of users"},{"location":"release_notes/v.0.13/v.0.13_-_Release_notes/#automated-instances-pausestop-according-to-the-resource-usage","text":"Version v0.12 introduced a PAUSE/RESUME option for the users, which allowed to persist whole state of the environment and resume it. This feature required AWS On-Demand instances to be used, which are more expensive compared to Spots. Current version provides a way to control spendings by automatically pausing on-demand instances if they are not used. Administrators can now control this behaviour using a set of parameters: system.idle.cpu.threshold - specify %% of the average CPU, below which action shall be taken system.resource.monitoring.period - specify period (in seconds) between the users' instances scanning to collect the monitoring metrics system.max.idle.timeout.minutes - specify a duration in minutes. If CPU utilization is below system.idle.cpu.threshold for this duration - notification will be sent to the user. system.idle.action.timeout.minutes - specify a duration in minutes. If CPU utilization is below system.idle.cpu.threshold for this duration - an action, specified in system.idle.action will be performed system.idle.action - which action to perform on the instance, that showed low CPU utilization: NOTIFY - only send notification PAUSE - pause an instance if possible (e.g. instance is On-Demand, Spot instances are skipped) PAUSE_OR_STOP - pause an instance if it is On-Demand, stop an instance if it is Spot STOP - Stop an instance, disregarding price-type","title":"Automated instances pause/stop according to the resource usage"},{"location":"release_notes/v.0.13/v.0.13_-_Release_notes/#tools-versions-details","text":"Now more information on the docker image version is available: Image size Modified date Unique identifier (Digest) Corresponding aliases (e.g. if some digest has two aliases) For more information see here .","title":"Tools versions details"},{"location":"release_notes/v.0.13/v.0.13_-_Release_notes/#data-download-from-external-httpftp-resources-to-the-cloud-data-storage","text":"Users often get the raw datasets from the external partners for processing. Previously, users had to get the data to the local cluster storage and them upload data to the cloud using clommand-line interface. Now users can provide CSV/TSV files with the external links and submit a data transfer job, so that the files will be moved to the cloud storage in the background. Upload CSV/TSV file and view list of samples with the external links: Select the transfer options: Which S3 bucket to use as a destination Which CSV/TSV columns shall be used to get external URLs (if several columns contain URLs - both can be used) ( optionally ) Select whether to rename resulting path to some other value (can be specified as another column cell, e.g. sample name) ( optionally ) Create new folders within destination if several columns are selected for \"Path fields\" option. E.g. if two columns contain URLs and both are selected - then folders will be created for the corresponding column name and used for appropriate files storage. Whether to update external URL within a table to the new location of the files. If set - http/ftp URLs will be changed to s3 path. Such data structure can be then used for a processing by a pipeline (See below - URLs are changed to the S3-clickable hyperlinks): Once transfer job is finished - files will be located in the selected S3 storage: For more information about data downloading from external http/ftp resources to the CP see 5.5. Download data from external resources to the cloud data storage .","title":"Data download from external http/ftp resources to the cloud data storage"},{"location":"release_notes/v.0.13/v.0.13_-_Release_notes/#automatically-rerun-a-batch-job-if-a-spot-instance-is-terminated","text":"In certain cases - AWS may terminate a node, that is used to run a job or an interactive tool: Spot prices changed AWS experienced a hardware issue These cases shall not be treated as a Cloud Platform bug. To make it more explicit, the following features are implemented: If a job fails due to server-related issue - a more friendly message is displayed, describing a reason for the hardware failure: If a batch job fails due to server-related issue and AWS reports one of the following EC2 status codes - batch job is restarted from scratch: Server.SpotInstanceShutdown - AWS stopped a spot instance due to price changes Server.SpotInstanceTermination - AWS terminated a spot instance due to price changes Server.InternalError - AWS hardware issue Administrator can configure whether to apply this behavior and how much retries shall be performed: For more information about automatically reruns batch jobs in cases when spot instances are terminated see here and in section 12.10. Manage system-level settings .","title":"Automatically rerun a batch job if a spot instance is terminated"},{"location":"release_notes/v.0.13/v.0.13_-_Release_notes/#displaying-estimated-price-of-a-job-run","text":"Now a list of active runs (both \"ACTIVE RUNS\" menu and \"Dashboard\") shows \"Estimated price\", which is calculated based on the run duration and selected instance type. This field is updated interactively (i.e. each 5 - 10 seconds) ACTIVE RUNS menu Dashboard widget For more information and examples of using see in sections 11. Manage Runs , 18. Home page .","title":"Displaying estimated price of a job run"},{"location":"release_notes/v.0.13/v.0.13_-_Release_notes/#displaying-details-of-the-user-profile","text":"Previously only user id was shown within all GUI forms, that displayed object/run OWNER. Now one can get information on the user name/email when hovering user id, which is shown in the GUI. It is shown in the tooltip with all information available from the IdP.","title":"Displaying details of the user profile"},{"location":"release_notes/v.0.13/v.0.13_-_Release_notes/#breadcrumbs-for-the-library-view","text":"Previously user was able to collapse a \"Library\" tree view using button. This allows to work with the plain objects lists, which is more comfortable for certain users, compared to the hierarchy. But navigation to the upper level of the hierarchy was not convenient in a collapsed mode. Now breadcrumbs are shown in the header of the plain objects list, which allow to view current path and navigate to the upper level by clicking a path item. Expanded mode (default) Collapsed mode","title":"Breadcrumbs for the Library view"},{"location":"release_notes/v.0.14/v.0.14_-_Release_notes/","text":"Cloud Pipeline v.0.14 - Release notes Docker image installed packages list Docker image version settings Global Search Default values (restrictions) on instance types for the users (groups) and tools Auto-scaled cluster A mandatory prefix for the new creating S3-buckets \"White list\" for docker images \"Grace\" period for unscanned or vulnerable docker images Docker image installed packages list Often users would like to know the full list software packages installed into a specific Docker images. E.g. to decide which one to run. Now this information is available from the docker version menu. User can click a specific version of the tool \" Packages \" tab will shown in the list of tabs in menu of tool's version. List of packages is generated from the docker version together with vulnerabilities scanning (introduced in v0.12). This occurs nightly (all dockers are scanned) or if admin explicitly requests scanning by clicking SCAN button for a specific version. Currently the following types of software packages can be scanned: System package manager's database (i.e. yum , apt ) R packages Python packages Software packages are combined into groups named \" Ecosystems \". User can select one of the ecosystems in the dropdown list and view its contents: Information about each package contains the package name and its short description ( if available ). User can filter packages by name (search will be done across all ecosystems ) For more details see here . Docker image version settings Previously docker image settings (instance type, disk, default command) were set using the following approaches: Global defaults Docker image settings Specific run launch parameters This introduces a number of limitations, e.g. if a docker image contains two version: one with CPU-only and another with GPU support, user was not able to define which instance type to use for what version. Now settings can be applied to the specific docker version. These settings are defined in the \" Settings \" tab of the tool's version menu: If these (version-level) settings are specified - they will be applied to each run of the docker image: All other settings levels are still remaining in place. If version-specific settings are not defined: docker-level settings will be applied. For more details see here . Global Search While Cloud Pipeline grows in terms of data and pipelines being add/implemented - it is crucial to be able to search for specific datasets or tools. Previously user was able to search only for the jobs runs. In v0.14 of the Cloud Pipeline Platform - a capability to search over all existing objects types is implemented. The following object types are being indexed and can be searched: \" FOLDERS \" - in the folders (\"Library hierarchy\") and metadata entities \" PIPELINES \" - in the pipelines metadata, pipelines files (documents) and configurations \" RUNS \" - in the runs information \" TOOLS \" - in the docker registries, groups and tools \" STORAGES \" - in S3/NFS storages metadata, S3/NFS files (names) \" ISSUES \" - in the issues (discussions) User can open a search form by pressing \"Ctrl+F\" being at any page of the Cloud Pipeline Web GUI ( excluding case when run's logs page is open ) or by clicking on in left menu bar ( global searching form will not be opened if any pop-up window is shown ): To start searching a \"google-like\" query string shall be entered (search can be triggered by pressing \"Enter\" button or automatically if no new input is provided for 2 seconds): Special expressions in query string are available as well (the rules for their assignment are described in pop-up window that appears when hovering over the icon): By default search will occur across all the available object types. If user would to limit search scope - appropriate section can be selected above the query input: To get a brief information on the object found, user can hover an item with a mouse and a \"Preview\" pane will be shown to the right or click an entry to navigate to it's location within the Cloud Pipeline. Tool preview: Pipeline preview: In the \"Preview\" window user can see: name of the found object path to the object in library description ( optionally ) block with indication and highlighting of the object's concrete part, where inputted word was found preview of the found object ( if it's available ) Default values (restrictions) on instance types for the users (groups) and tools Users may make a mistake while selecting instance types and storages when launching a run. This may be: Too large instance type for the job Spot instance for GPU job Not valid data storage path etc. Now admin can restrict certain options for the specific users/groups or tools to minimize a number of invalid configurations runs. Restrictions could be set by different ways on several forms: within \"User management\" tab (for more information see 12.4. Edit/delete a user and 12.6. Edit a group/role ) admin can specify for a user or a group of users (role) allowed price types and allowed instance types for the pipelines, configurations and tool runs: within \"Instance management\" panel in tool settings (for more information see 10.5. Launch a Tool ) admin can specify allowed price types and allowed instance types for Tool runs: within \"Cluster\" tab in \"Preferences\" section of the system-level settings (see here ) admin also can specify allowed price types (with setting cluster.allowed.price.types ) and allowed instance types for the pipelines, configurations and tool runs (with settings cluster.allowed.instance.types and cluster.allowed.instance.types.docker ) as global defaults: Next hierarchy is set for applying of inputted allowed instance types (sorted by priority): User level (specified for a user on \"User management\" tab) User group level (specified for a group (role) on \"User management\" tab. If a user is a member of several groups - list of allowed instances will be summarized across all the groups) Tool level (specified for a tool on \"Instance management\" panel ) (global) cluster.allowed.instance.types.docker (specified on \"Cluster\" tab in \"Preferences\" section of the system-level settings) (global) cluster.allowed.instance.types (specified on \"Cluster\" tab in \"Preferences\" section of the system-level settings) After specifying allowed instance types, all GUI forms that allow to select the list of instance types (configurations/launch forms) - will display only valid instance type, according to hierarchy above. For price type specifying - if it is set for the user/group/tool - GUI will allow to select only that price type. Auto-scaled cluster Previously, when you needed several nodes running at one task, you could configure and launch cluster: master machine and one or several worker machines. Their count was predefined before launching the task and could not changing during the run. In current version, the auto-scaled cluster is implemented. This one differs from \"usual\" cluster in that it can attach or drop additional nodes during the run depending on the queue load. Using such type of cluster will minimize total cost. If during the run there are jobs in waiting state longer than a specific time ( this time threshold is set by admin in \"Preferences\" section of the system-level settings ) - auto-scaled cluster will attach new computation nodes (\"scale-up\"). If during the run the queue is empty or all jobs are running longer than a specific time ( this time threshold is set by admin tab in \"Preferences\" section of the system-level settings ) - auto-scaled cluster will drop existing auto-scaled nodes (\"scale-down\"). For this cluster user may specify total count of child nodes - max count of auto-scaled nodes, and the count of \"persistent\" child nodes - count of nodes that will never be \"scaled-down\". To set auto-scaled cluster click \"Configure cluster\" in \"Exec environment\" panel before launch a run: In pop-up window select \"Auto-scaled cluster\" and specify total count of \"auto-scaled\" nodes: If you want to set a count of \"persistent\" child nodes, click \"Setup default child nodes count\" and specify the value: For more details see here . A mandatory prefix for the new creating S3-buckets Now admin can set storage.object.prefix on \"Data storage\" tab in \"Preferences\" section of the system-level settings: If it is set, all new storages will be created with this prefix (e.g. \" ds \"): For more information see 8.1. Create and edit storage . \"White list\" for docker images Previously user is not able to run a new image if it is not scanned yet or has a lot of vulnerabilities. In current version a \"white list\" option for docker images is implemented. It's meaning that admin may allow users to run certain docker versions even if they are vulnerable/unscanned. For do that admin has to check a docker version in tool-versions list by click on special flag \"Add to white list\" near the \"SCAN\" button: In this case user would be able to run such version of tool and none errors will be displayed during launch time and viewing. Only admin can \"add\"/\"remove\" tool version to the \"white list\". For more details see 10.6. Tool security check . \"Grace\" period for unscanned or vulnerable docker images Previously user is not able to run a new image if it is not scanned yet or has a lot of vulnerabilities. In current version a \"grace\" period option for such docker images is implemented. During this period user will be able to run a tool, but an appropriate message will be displayed when viewing a tool or running it. The duration of \"grace\" period (in hours ) is set on \"Docker security\" tab in \"Preferences\" section of the system-level settings: If this time value is not elapsed from the date/time since the docker version became vulnerable or since the push time (if this version was not scanned yet) - user would be able to run such version of tool, but warning message will be displaying during version launch. Since \"grace\" period is elapsed for this tool's version - behavior will became as for \"usual\" vulnerable/unscanned docker image. For more details see 10.6. Tool security check and 12.10. Manage the system-level settings .","title":"v.0.14"},{"location":"release_notes/v.0.14/v.0.14_-_Release_notes/#cloud-pipeline-v014-release-notes","text":"Docker image installed packages list Docker image version settings Global Search Default values (restrictions) on instance types for the users (groups) and tools Auto-scaled cluster A mandatory prefix for the new creating S3-buckets \"White list\" for docker images \"Grace\" period for unscanned or vulnerable docker images","title":"Cloud Pipeline v.0.14 - Release notes"},{"location":"release_notes/v.0.14/v.0.14_-_Release_notes/#docker-image-installed-packages-list","text":"Often users would like to know the full list software packages installed into a specific Docker images. E.g. to decide which one to run. Now this information is available from the docker version menu. User can click a specific version of the tool \" Packages \" tab will shown in the list of tabs in menu of tool's version. List of packages is generated from the docker version together with vulnerabilities scanning (introduced in v0.12). This occurs nightly (all dockers are scanned) or if admin explicitly requests scanning by clicking SCAN button for a specific version. Currently the following types of software packages can be scanned: System package manager's database (i.e. yum , apt ) R packages Python packages Software packages are combined into groups named \" Ecosystems \". User can select one of the ecosystems in the dropdown list and view its contents: Information about each package contains the package name and its short description ( if available ). User can filter packages by name (search will be done across all ecosystems ) For more details see here .","title":"Docker image installed packages list"},{"location":"release_notes/v.0.14/v.0.14_-_Release_notes/#docker-image-version-settings","text":"Previously docker image settings (instance type, disk, default command) were set using the following approaches: Global defaults Docker image settings Specific run launch parameters This introduces a number of limitations, e.g. if a docker image contains two version: one with CPU-only and another with GPU support, user was not able to define which instance type to use for what version. Now settings can be applied to the specific docker version. These settings are defined in the \" Settings \" tab of the tool's version menu: If these (version-level) settings are specified - they will be applied to each run of the docker image: All other settings levels are still remaining in place. If version-specific settings are not defined: docker-level settings will be applied. For more details see here .","title":"Docker image version settings"},{"location":"release_notes/v.0.14/v.0.14_-_Release_notes/#global-search","text":"While Cloud Pipeline grows in terms of data and pipelines being add/implemented - it is crucial to be able to search for specific datasets or tools. Previously user was able to search only for the jobs runs. In v0.14 of the Cloud Pipeline Platform - a capability to search over all existing objects types is implemented. The following object types are being indexed and can be searched: \" FOLDERS \" - in the folders (\"Library hierarchy\") and metadata entities \" PIPELINES \" - in the pipelines metadata, pipelines files (documents) and configurations \" RUNS \" - in the runs information \" TOOLS \" - in the docker registries, groups and tools \" STORAGES \" - in S3/NFS storages metadata, S3/NFS files (names) \" ISSUES \" - in the issues (discussions) User can open a search form by pressing \"Ctrl+F\" being at any page of the Cloud Pipeline Web GUI ( excluding case when run's logs page is open ) or by clicking on in left menu bar ( global searching form will not be opened if any pop-up window is shown ): To start searching a \"google-like\" query string shall be entered (search can be triggered by pressing \"Enter\" button or automatically if no new input is provided for 2 seconds): Special expressions in query string are available as well (the rules for their assignment are described in pop-up window that appears when hovering over the icon): By default search will occur across all the available object types. If user would to limit search scope - appropriate section can be selected above the query input: To get a brief information on the object found, user can hover an item with a mouse and a \"Preview\" pane will be shown to the right or click an entry to navigate to it's location within the Cloud Pipeline. Tool preview: Pipeline preview: In the \"Preview\" window user can see: name of the found object path to the object in library description ( optionally ) block with indication and highlighting of the object's concrete part, where inputted word was found preview of the found object ( if it's available )","title":"Global Search"},{"location":"release_notes/v.0.14/v.0.14_-_Release_notes/#default-values-restrictions-on-instance-types-for-the-users-groups-and-tools","text":"Users may make a mistake while selecting instance types and storages when launching a run. This may be: Too large instance type for the job Spot instance for GPU job Not valid data storage path etc. Now admin can restrict certain options for the specific users/groups or tools to minimize a number of invalid configurations runs. Restrictions could be set by different ways on several forms: within \"User management\" tab (for more information see 12.4. Edit/delete a user and 12.6. Edit a group/role ) admin can specify for a user or a group of users (role) allowed price types and allowed instance types for the pipelines, configurations and tool runs: within \"Instance management\" panel in tool settings (for more information see 10.5. Launch a Tool ) admin can specify allowed price types and allowed instance types for Tool runs: within \"Cluster\" tab in \"Preferences\" section of the system-level settings (see here ) admin also can specify allowed price types (with setting cluster.allowed.price.types ) and allowed instance types for the pipelines, configurations and tool runs (with settings cluster.allowed.instance.types and cluster.allowed.instance.types.docker ) as global defaults: Next hierarchy is set for applying of inputted allowed instance types (sorted by priority): User level (specified for a user on \"User management\" tab) User group level (specified for a group (role) on \"User management\" tab. If a user is a member of several groups - list of allowed instances will be summarized across all the groups) Tool level (specified for a tool on \"Instance management\" panel ) (global) cluster.allowed.instance.types.docker (specified on \"Cluster\" tab in \"Preferences\" section of the system-level settings) (global) cluster.allowed.instance.types (specified on \"Cluster\" tab in \"Preferences\" section of the system-level settings) After specifying allowed instance types, all GUI forms that allow to select the list of instance types (configurations/launch forms) - will display only valid instance type, according to hierarchy above. For price type specifying - if it is set for the user/group/tool - GUI will allow to select only that price type.","title":"Default values (restrictions) on instance types for the users (groups) and tools"},{"location":"release_notes/v.0.14/v.0.14_-_Release_notes/#auto-scaled-cluster","text":"Previously, when you needed several nodes running at one task, you could configure and launch cluster: master machine and one or several worker machines. Their count was predefined before launching the task and could not changing during the run. In current version, the auto-scaled cluster is implemented. This one differs from \"usual\" cluster in that it can attach or drop additional nodes during the run depending on the queue load. Using such type of cluster will minimize total cost. If during the run there are jobs in waiting state longer than a specific time ( this time threshold is set by admin in \"Preferences\" section of the system-level settings ) - auto-scaled cluster will attach new computation nodes (\"scale-up\"). If during the run the queue is empty or all jobs are running longer than a specific time ( this time threshold is set by admin tab in \"Preferences\" section of the system-level settings ) - auto-scaled cluster will drop existing auto-scaled nodes (\"scale-down\"). For this cluster user may specify total count of child nodes - max count of auto-scaled nodes, and the count of \"persistent\" child nodes - count of nodes that will never be \"scaled-down\". To set auto-scaled cluster click \"Configure cluster\" in \"Exec environment\" panel before launch a run: In pop-up window select \"Auto-scaled cluster\" and specify total count of \"auto-scaled\" nodes: If you want to set a count of \"persistent\" child nodes, click \"Setup default child nodes count\" and specify the value: For more details see here .","title":"Auto-scaled cluster"},{"location":"release_notes/v.0.14/v.0.14_-_Release_notes/#a-mandatory-prefix-for-the-new-creating-s3-buckets","text":"Now admin can set storage.object.prefix on \"Data storage\" tab in \"Preferences\" section of the system-level settings: If it is set, all new storages will be created with this prefix (e.g. \" ds \"): For more information see 8.1. Create and edit storage .","title":"A mandatory prefix for the new creating S3-buckets"},{"location":"release_notes/v.0.14/v.0.14_-_Release_notes/#white-list-for-docker-images","text":"Previously user is not able to run a new image if it is not scanned yet or has a lot of vulnerabilities. In current version a \"white list\" option for docker images is implemented. It's meaning that admin may allow users to run certain docker versions even if they are vulnerable/unscanned. For do that admin has to check a docker version in tool-versions list by click on special flag \"Add to white list\" near the \"SCAN\" button: In this case user would be able to run such version of tool and none errors will be displayed during launch time and viewing. Only admin can \"add\"/\"remove\" tool version to the \"white list\". For more details see 10.6. Tool security check .","title":"\"White list\" for docker images"},{"location":"release_notes/v.0.14/v.0.14_-_Release_notes/#grace-period-for-unscanned-or-vulnerable-docker-images","text":"Previously user is not able to run a new image if it is not scanned yet or has a lot of vulnerabilities. In current version a \"grace\" period option for such docker images is implemented. During this period user will be able to run a tool, but an appropriate message will be displayed when viewing a tool or running it. The duration of \"grace\" period (in hours ) is set on \"Docker security\" tab in \"Preferences\" section of the system-level settings: If this time value is not elapsed from the date/time since the docker version became vulnerable or since the push time (if this version was not scanned yet) - user would be able to run such version of tool, but warning message will be displaying during version launch. Since \"grace\" period is elapsed for this tool's version - behavior will became as for \"usual\" vulnerable/unscanned docker image. For more details see 10.6. Tool security check and 12.10. Manage the system-level settings .","title":"\"Grace\" period for unscanned or vulnerable docker images"}]}